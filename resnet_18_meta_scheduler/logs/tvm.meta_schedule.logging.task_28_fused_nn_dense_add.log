2023-11-10 23:34:23 [INFO] [task_scheduler.cc:160] Initializing Task #28: "fused_nn_dense_add"
2023-11-10 23:34:23 [INFO] [task_scheduler.cc:35] 
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(512)), "float32"), p1: T.Buffer((T.int64(1000), T.int64(512)), "float32"), p2: T.Buffer((T.int64(1), T.int64(1000)), "float32"), T_add: T.Buffer((T.int64(1), T.int64(1000)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        T_matmul_NT = T.alloc_buffer((T.int64(1), T.int64(1000)))
        for i, j, k in T.grid(T.int64(1), T.int64(1000), T.int64(512)):
            with T.block("T_matmul_NT"):
                v_i, v_j, v_k = T.axis.remap("SSR", [i, j, k])
                T.reads(p0[v_i, v_k], p1[v_j, v_k])
                T.writes(T_matmul_NT[v_i, v_j])
                with T.init():
                    T_matmul_NT[v_i, v_j] = T.float32(0)
                T_matmul_NT[v_i, v_j] = T_matmul_NT[v_i, v_j] + p0[v_i, v_k] * p1[v_j, v_k]
        for ax0, ax1 in T.grid(T.int64(1), T.int64(1000)):
            with T.block("T_add"):
                v_ax0, v_ax1 = T.axis.remap("SS", [ax0, ax1])
                T.reads(T_matmul_NT[v_ax0, v_ax1], p2[v_ax0, v_ax1])
                T.writes(T_add[v_ax0, v_ax1])
                T_add[v_ax0, v_ax1] = T_matmul_NT[v_ax0, v_ax1] + p2[v_ax0, v_ax1]
2023-11-10 23:34:23 [INFO] [task_scheduler.cc:164] Total 3 design space(s) generated
2023-11-10 23:34:23 [INFO] [task_scheduler.cc:170] Design space #0:
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(512)), "float32"), p1: T.Buffer((T.int64(1000), T.int64(512)), "float32"), p2: T.Buffer((T.int64(1), T.int64(1000)), "float32"), T_add: T.Buffer((T.int64(1), T.int64(1000)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit": 0})
            T_matmul_NT_local = T.alloc_buffer((T.int64(1), T.int64(1000)), scope="local")
            p0_shared = T.alloc_buffer((T.int64(1), T.int64(512)), scope="shared")
            p1_shared = T.alloc_buffer((T.int64(1000), T.int64(512)), scope="shared")
            for i_0_j_0_fused in T.thread_binding(T.int64(1), thread="blockIdx.x"):
                for i_1_j_1_fused in T.thread_binding(T.int64(10), thread="vthread.x"):
                    for i_2_j_2_fused in T.thread_binding(T.int64(1), thread="threadIdx.x"):
                        for k_0 in range(T.int64(64)):
                            for ax0_ax1_fused in range(T.int64(8)):
                                with T.block("p0_shared"):
                                    v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                    v1 = T.axis.spatial(T.int64(512), k_0 * T.int64(8) + ax0_ax1_fused)
                                    T.reads(p0[v0, v1])
                                    T.writes(p0_shared[v0, v1])
                                    T.block_attr({"meta_schedule.cooperative_fetch": 4})
                                    p0_shared[v0, v1] = p0[v0, v1]
                            for ax0_ax1_fused in range(T.int64(8000)):
                                with T.block("p1_shared"):
                                    v0 = T.axis.spatial(T.int64(1000), ax0_ax1_fused // T.int64(8))
                                    v1 = T.axis.spatial(T.int64(512), k_0 * T.int64(8) + ax0_ax1_fused % T.int64(8))
                                    T.reads(p1[v0, v1])
                                    T.writes(p1_shared[v0, v1])
                                    T.block_attr({"meta_schedule.cooperative_fetch": 4})
                                    p1_shared[v0, v1] = p1[v0, v1]
                            for k_1, i_3, j_3, k_2, i_4, j_4 in T.grid(T.int64(8), T.int64(1), T.int64(100), T.int64(1), T.int64(1), T.int64(1)):
                                with T.block("T_matmul_NT"):
                                    v_i = T.axis.spatial(T.int64(1), i_3 + i_4)
                                    v_j = T.axis.spatial(T.int64(1000), i_1_j_1_fused * T.int64(100) + j_3 + j_4)
                                    v_k = T.axis.reduce(T.int64(512), k_0 * T.int64(8) + k_1 + k_2)
                                    T.reads(p0_shared[v_i, v_k], p1_shared[v_j, v_k])
                                    T.writes(T_matmul_NT_local[v_i, v_j])
                                    T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                    with T.init():
                                        T_matmul_NT_local[v_i, v_j] = T.float32(0)
                                    T_matmul_NT_local[v_i, v_j] = T_matmul_NT_local[v_i, v_j] + p0_shared[v_i, v_k] * p1_shared[v_j, v_k]
                        for ax0, ax1 in T.grid(T.int64(1), T.int64(100)):
                            with T.block("T_matmul_NT_local"):
                                v0 = T.axis.spatial(T.int64(1), ax0)
                                v1 = T.axis.spatial(T.int64(1000), i_1_j_1_fused * T.int64(100) + ax1)
                                T.reads(T_matmul_NT_local[v0, v1], p2[v0, v1])
                                T.writes(T_add[v0, v1])
                                T_add[v0, v1] = T_matmul_NT_local[v0, v1] + p2[v0, v1]
b0 = sch.get_block(name="T_matmul_NT", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l3, l4, l5 = sch.get_loops(block=b0)
v6, v7, v8, v9, v10 = sch.sample_perfect_tile(loop=l3, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l11, l12, l13, l14, l15 = sch.split(loop=l3, factors=[v6, v7, v8, v9, v10], preserve_unit_iters=True)
v16, v17, v18, v19, v20 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[1, 10, 1, 100, 1])
l21, l22, l23, l24, l25 = sch.split(loop=l4, factors=[v16, v17, v18, v19, v20], preserve_unit_iters=True)
v26, v27, v28 = sch.sample_perfect_tile(loop=l5, n=3, max_innermost_factor=64, decision=[64, 8, 1])
l29, l30, l31 = sch.split(loop=l5, factors=[v26, v27, v28], preserve_unit_iters=True)
sch.reorder(l11, l21, l12, l22, l13, l23, l29, l30, l14, l24, l31, l15, l25)
l32 = sch.fuse(l11, l21, preserve_unit_iters=True)
sch.bind(loop=l32, thread_axis="blockIdx.x")
l33 = sch.fuse(l12, l22, preserve_unit_iters=True)
sch.bind(loop=l33, thread_axis="vthread.x")
l34 = sch.fuse(l13, l23, preserve_unit_iters=True)
sch.bind(loop=l34, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b35 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b35, loop=l34, preserve_unit_loops=True, index=-1)
b36 = sch.cache_read(block=b0, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b36, loop=l29, preserve_unit_loops=True, index=-1)
l37, l38, l39, l40, l41, l42 = sch.get_loops(block=b36)
l43 = sch.fuse(l41, l42, preserve_unit_iters=True)
v44 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b36, ann_key="meta_schedule.cooperative_fetch", ann_val=v44)
b45 = sch.cache_read(block=b0, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b45, loop=l29, preserve_unit_loops=True, index=-1)
l46, l47, l48, l49, l50, l51 = sch.get_loops(block=b45)
l52 = sch.fuse(l50, l51, preserve_unit_iters=True)
v53 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b45, ann_key="meta_schedule.cooperative_fetch", ann_val=v53)
sch.reverse_compute_inline(block=b1)
v54 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=0)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v54)
2023-11-10 23:34:23 [INFO] [task_scheduler.cc:170] Design space #1:
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(512)), "float32"), p1: T.Buffer((T.int64(1000), T.int64(512)), "float32"), p2: T.Buffer((T.int64(1), T.int64(1000)), "float32"), T_add: T.Buffer((T.int64(1), T.int64(1000)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit": 64})
            T_matmul_NT_local = T.alloc_buffer((T.int64(1), T.int64(1000)), scope="local")
            p0_shared = T.alloc_buffer((T.int64(1), T.int64(512)), scope="shared")
            p1_shared = T.alloc_buffer((T.int64(1000), T.int64(512)), scope="shared")
            for i_0_j_0_fused in T.thread_binding(T.int64(1), thread="blockIdx.x"):
                for i_1_j_1_fused in T.thread_binding(T.int64(10), thread="vthread.x"):
                    for i_2_j_2_fused in T.thread_binding(T.int64(1), thread="threadIdx.x"):
                        for k_0_fused in T.serial(T.int64(64), annotations={"software_pipeline_async_stages": [0], "software_pipeline_order": [0, 1, 2], "software_pipeline_stage": [0, 0, 2]}):
                            for ax0_ax1_fused in range(T.int64(8)):
                                with T.block("p0_shared"):
                                    v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                    v1 = T.axis.spatial(T.int64(512), k_0_fused * T.int64(8) + ax0_ax1_fused)
                                    T.reads(p0[v0, v1])
                                    T.writes(p0_shared[v0, v1])
                                    T.block_attr({"meta_schedule.cooperative_fetch": 4})
                                    p0_shared[v0, v1] = p0[v0, v1]
                            for ax0_ax1_fused in range(T.int64(8000)):
                                with T.block("p1_shared"):
                                    v0 = T.axis.spatial(T.int64(1000), ax0_ax1_fused // T.int64(8))
                                    v1 = T.axis.spatial(T.int64(512), k_0_fused * T.int64(8) + ax0_ax1_fused % T.int64(8))
                                    T.reads(p1[v0, v1])
                                    T.writes(p1_shared[v0, v1])
                                    T.block_attr({"meta_schedule.cooperative_fetch": 4})
                                    p1_shared[v0, v1] = p1[v0, v1]
                            for k_1, i_3, j_3, k_2, i_4, j_4 in T.grid(T.int64(8), T.int64(1), T.int64(100), T.int64(1), T.int64(1), T.int64(1)):
                                with T.block("T_matmul_NT"):
                                    v_i = T.axis.spatial(T.int64(1), i_3 + i_4)
                                    v_j = T.axis.spatial(T.int64(1000), i_1_j_1_fused * T.int64(100) + j_3 + j_4)
                                    v_k = T.axis.reduce(T.int64(512), k_0_fused * T.int64(8) + k_1 + k_2)
                                    T.reads(p0_shared[v_i, v_k], p1_shared[v_j, v_k])
                                    T.writes(T_matmul_NT_local[v_i, v_j])
                                    T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                    with T.init():
                                        T_matmul_NT_local[v_i, v_j] = T.float32(0)
                                    T_matmul_NT_local[v_i, v_j] = T_matmul_NT_local[v_i, v_j] + p0_shared[v_i, v_k] * p1_shared[v_j, v_k]
                        for ax0, ax1 in T.grid(T.int64(1), T.int64(100)):
                            with T.block("T_matmul_NT_local"):
                                v0 = T.axis.spatial(T.int64(1), ax0)
                                v1 = T.axis.spatial(T.int64(1000), i_1_j_1_fused * T.int64(100) + ax1)
                                T.reads(T_matmul_NT_local[v0, v1], p2[v0, v1])
                                T.writes(T_add[v0, v1])
                                T_add[v0, v1] = T_matmul_NT_local[v0, v1] + p2[v0, v1]
b0 = sch.get_block(name="T_matmul_NT", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l3, l4, l5 = sch.get_loops(block=b0)
v6, v7, v8, v9, v10 = sch.sample_perfect_tile(loop=l3, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l11, l12, l13, l14, l15 = sch.split(loop=l3, factors=[v6, v7, v8, v9, v10], preserve_unit_iters=True)
v16, v17, v18, v19, v20 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[1, 10, 1, 100, 1])
l21, l22, l23, l24, l25 = sch.split(loop=l4, factors=[v16, v17, v18, v19, v20], preserve_unit_iters=True)
v26, v27, v28 = sch.sample_perfect_tile(loop=l5, n=3, max_innermost_factor=64, decision=[64, 8, 1])
l29, l30, l31 = sch.split(loop=l5, factors=[v26, v27, v28], preserve_unit_iters=True)
sch.reorder(l11, l21, l12, l22, l13, l23, l29, l30, l14, l24, l31, l15, l25)
l32 = sch.fuse(l11, l21, preserve_unit_iters=True)
sch.bind(loop=l32, thread_axis="blockIdx.x")
l33 = sch.fuse(l12, l22, preserve_unit_iters=True)
sch.bind(loop=l33, thread_axis="vthread.x")
l34 = sch.fuse(l13, l23, preserve_unit_iters=True)
sch.bind(loop=l34, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b35 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b35, loop=l34, preserve_unit_loops=True, index=-1)
b36 = sch.cache_read(block=b0, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b36, loop=l29, preserve_unit_loops=True, index=-1)
l37, l38, l39, l40, l41, l42 = sch.get_loops(block=b36)
l43 = sch.fuse(l41, l42, preserve_unit_iters=True)
v44 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b36, ann_key="meta_schedule.cooperative_fetch", ann_val=v44)
b45 = sch.cache_read(block=b0, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b45, loop=l29, preserve_unit_loops=True, index=-1)
l46, l47, l48, l49, l50, l51 = sch.get_loops(block=b45)
l52 = sch.fuse(l50, l51, preserve_unit_iters=True)
v53 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b45, ann_key="meta_schedule.cooperative_fetch", ann_val=v53)
l54 = sch.fuse(l29, preserve_unit_iters=True)
sch.annotate(block_or_loop=l54, ann_key="software_pipeline_stage", ann_val=[0, 0, 2])
sch.annotate(block_or_loop=l54, ann_key="software_pipeline_order", ann_val=[0, 1, 2])
sch.annotate(block_or_loop=l54, ann_key="software_pipeline_async_stages", ann_val=[0])
sch.reverse_compute_inline(block=b1)
v55 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=2)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v55)
2023-11-10 23:34:23 [INFO] [task_scheduler.cc:170] Design space #2:
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(512)), "float32"), p1: T.Buffer((T.int64(1000), T.int64(512)), "float32"), p2: T.Buffer((T.int64(1), T.int64(1000)), "float32"), T_add: T.Buffer((T.int64(1), T.int64(1000)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit": 512})
            T_matmul_NT_local = T.alloc_buffer((T.int64(1), T.int64(1000)), scope="local")
            p0_shared = T.alloc_buffer((T.int64(1), T.int64(512)), scope="shared")
            p1_shared = T.alloc_buffer((T.int64(1000), T.int64(512)), scope="shared")
            for i_0_j_0_fused in T.thread_binding(T.int64(1), thread="blockIdx.x"):
                for i_1_j_1_fused in T.thread_binding(T.int64(10), thread="vthread.x"):
                    for i_2_j_2_fused in T.thread_binding(T.int64(1), thread="threadIdx.x"):
                        for k_0_fused in T.serial(T.int64(64), annotations={"software_pipeline_async_stages": [0], "software_pipeline_order": [0, 1, 2], "software_pipeline_stage": [0, 0, 3]}):
                            for ax0_ax1_fused in range(T.int64(8)):
                                with T.block("p0_shared"):
                                    v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                    v1 = T.axis.spatial(T.int64(512), k_0_fused * T.int64(8) + ax0_ax1_fused)
                                    T.reads(p0[v0, v1])
                                    T.writes(p0_shared[v0, v1])
                                    T.block_attr({"meta_schedule.cooperative_fetch": 4})
                                    p0_shared[v0, v1] = p0[v0, v1]
                            for ax0_ax1_fused in range(T.int64(8000)):
                                with T.block("p1_shared"):
                                    v0 = T.axis.spatial(T.int64(1000), ax0_ax1_fused // T.int64(8))
                                    v1 = T.axis.spatial(T.int64(512), k_0_fused * T.int64(8) + ax0_ax1_fused % T.int64(8))
                                    T.reads(p1[v0, v1])
                                    T.writes(p1_shared[v0, v1])
                                    T.block_attr({"meta_schedule.cooperative_fetch": 4})
                                    p1_shared[v0, v1] = p1[v0, v1]
                            for k_1, i_3, j_3, k_2, i_4, j_4 in T.grid(T.int64(8), T.int64(1), T.int64(100), T.int64(1), T.int64(1), T.int64(1)):
                                with T.block("T_matmul_NT"):
                                    v_i = T.axis.spatial(T.int64(1), i_3 + i_4)
                                    v_j = T.axis.spatial(T.int64(1000), i_1_j_1_fused * T.int64(100) + j_3 + j_4)
                                    v_k = T.axis.reduce(T.int64(512), k_0_fused * T.int64(8) + k_1 + k_2)
                                    T.reads(p0_shared[v_i, v_k], p1_shared[v_j, v_k])
                                    T.writes(T_matmul_NT_local[v_i, v_j])
                                    T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                    with T.init():
                                        T_matmul_NT_local[v_i, v_j] = T.float32(0)
                                    T_matmul_NT_local[v_i, v_j] = T_matmul_NT_local[v_i, v_j] + p0_shared[v_i, v_k] * p1_shared[v_j, v_k]
                        for ax0, ax1 in T.grid(T.int64(1), T.int64(100)):
                            with T.block("T_matmul_NT_local"):
                                v0 = T.axis.spatial(T.int64(1), ax0)
                                v1 = T.axis.spatial(T.int64(1000), i_1_j_1_fused * T.int64(100) + ax1)
                                T.reads(T_matmul_NT_local[v0, v1], p2[v0, v1])
                                T.writes(T_add[v0, v1])
                                T_add[v0, v1] = T_matmul_NT_local[v0, v1] + p2[v0, v1]
b0 = sch.get_block(name="T_matmul_NT", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l3, l4, l5 = sch.get_loops(block=b0)
v6, v7, v8, v9, v10 = sch.sample_perfect_tile(loop=l3, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l11, l12, l13, l14, l15 = sch.split(loop=l3, factors=[v6, v7, v8, v9, v10], preserve_unit_iters=True)
v16, v17, v18, v19, v20 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[1, 10, 1, 100, 1])
l21, l22, l23, l24, l25 = sch.split(loop=l4, factors=[v16, v17, v18, v19, v20], preserve_unit_iters=True)
v26, v27, v28 = sch.sample_perfect_tile(loop=l5, n=3, max_innermost_factor=64, decision=[64, 8, 1])
l29, l30, l31 = sch.split(loop=l5, factors=[v26, v27, v28], preserve_unit_iters=True)
sch.reorder(l11, l21, l12, l22, l13, l23, l29, l30, l14, l24, l31, l15, l25)
l32 = sch.fuse(l11, l21, preserve_unit_iters=True)
sch.bind(loop=l32, thread_axis="blockIdx.x")
l33 = sch.fuse(l12, l22, preserve_unit_iters=True)
sch.bind(loop=l33, thread_axis="vthread.x")
l34 = sch.fuse(l13, l23, preserve_unit_iters=True)
sch.bind(loop=l34, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b35 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b35, loop=l34, preserve_unit_loops=True, index=-1)
b36 = sch.cache_read(block=b0, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b36, loop=l29, preserve_unit_loops=True, index=-1)
l37, l38, l39, l40, l41, l42 = sch.get_loops(block=b36)
l43 = sch.fuse(l41, l42, preserve_unit_iters=True)
v44 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b36, ann_key="meta_schedule.cooperative_fetch", ann_val=v44)
b45 = sch.cache_read(block=b0, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b45, loop=l29, preserve_unit_loops=True, index=-1)
l46, l47, l48, l49, l50, l51 = sch.get_loops(block=b45)
l52 = sch.fuse(l50, l51, preserve_unit_iters=True)
v53 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b45, ann_key="meta_schedule.cooperative_fetch", ann_val=v53)
l54 = sch.fuse(l29, preserve_unit_iters=True)
sch.annotate(block_or_loop=l54, ann_key="software_pipeline_stage", ann_val=[0, 0, 3])
sch.annotate(block_or_loop=l54, ann_key="software_pipeline_order", ann_val=[0, 1, 2])
sch.annotate(block_or_loop=l54, ann_key="software_pipeline_async_stages", ann_val=[0])
sch.reverse_compute_inline(block=b1)
v55 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v55)
2023-11-11 00:15:48 [INFO] [evolutionary_search.cc:713] Generating candidates......
2023-11-11 00:15:48 [INFO] [evolutionary_search.cc:715] Picked top 0 candidate(s) from database
2023-11-11 00:15:49 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310a405238)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310e76ec68)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310dc33cd8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310be916f8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56310f803f18)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310eeb7a68)]: 501 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310e771ba8)]: 0 failure(s)
2023-11-11 00:15:50 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310a405238)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310e76ec68)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310dc33cd8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310be916f8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56310f803f18)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310eeb7a68)]: 1005 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310e771ba8)]: 0 failure(s)
2023-11-11 00:15:51 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310a405238)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310e76ec68)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310dc33cd8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310be916f8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56310f803f18)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310eeb7a68)]: 1511 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310e771ba8)]: 0 failure(s)
2023-11-11 00:15:51 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310a405238)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310e76ec68)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310dc33cd8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310be916f8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56310f803f18)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310eeb7a68)]: 2016 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310e771ba8)]: 0 failure(s)
2023-11-11 00:15:52 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310a405238)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310e76ec68)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310dc33cd8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310be916f8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56310f803f18)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310eeb7a68)]: 2517 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310e771ba8)]: 0 failure(s)
2023-11-11 00:15:53 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310a405238)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310e76ec68)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310dc33cd8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310be916f8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56310f803f18)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310eeb7a68)]: 3021 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310e771ba8)]: 0 failure(s)
2023-11-11 00:15:53 [INFO] [evolutionary_search.cc:723] Sampled 51 candidate(s)
2023-11-11 00:15:55 [INFO] [evolutionary_search.cc:621] Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310a405238)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310e76ec68)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310dc33cd8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310be916f8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56310f803f18)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310eeb7a68)]: 112 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310e771ba8)]: 0 failure(s)
2023-11-11 00:15:56 [INFO] [evolutionary_search.cc:621] Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310a405238)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310e76ec68)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310dc33cd8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310be916f8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56310f803f18)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310eeb7a68)]: 116 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310e771ba8)]: 0 failure(s)
2023-11-11 00:15:58 [INFO] [evolutionary_search.cc:621] Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310a405238)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310e76ec68)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310dc33cd8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310be916f8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56310f803f18)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310eeb7a68)]: 101 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310e771ba8)]: 0 failure(s)
2023-11-11 00:16:00 [INFO] [evolutionary_search.cc:621] Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310a405238)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310e76ec68)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310dc33cd8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310be916f8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56310f803f18)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310eeb7a68)]: 90 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310e771ba8)]: 0 failure(s)
2023-11-11 00:16:00 [INFO] [evolutionary_search.cc:649] Scores of the best 64 candidates:
[1 : 16]:	0.9989  0.9972  0.9971  0.9961  0.9952  0.9934  0.9925  0.9919  0.9917  0.9911  0.9905  0.9893  0.9888  0.9887  0.9882  0.9882
[17 : 32]:	0.9859  0.9831  0.9817  0.9812  0.9801  0.9763  0.9749  0.9746  0.9735  0.9731  0.9719  0.9714  0.9694  0.9690  0.9685  0.9681
[33 : 48]:	0.9664  0.9659  0.9646  0.9637  0.9631  0.9608  0.9606  0.9599  0.9597  0.9585  0.9583  0.9574  0.9572  0.9571  0.9555  0.9544
[49 : 64]:	0.9535  0.9535  0.9531  0.9511  0.9508  0.9488  0.9483  0.9472  0.9458  0.9451  0.9451  0.9449  0.9444  0.9441  0.9427  0.9403
2023-11-11 00:16:00 [INFO] [evolutionary_search.cc:727] Got 64 candidate(s) with evolutionary search
2023-11-11 00:16:00 [INFO] [evolutionary_search.cc:730] Sending 63 candidates(s) for measurement
2023-11-11 00:30:46 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #1: GFLOPs: 41.8704. Time: 24.4803 us. Best GFLOPs: 41.8704
2023-11-11 00:30:46 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #2: GFLOPs: 17.2080. Time: 59.5654 us. Best GFLOPs: 41.8704
2023-11-11 00:30:46 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #3: GFLOPs: 1.1367. Time: 901.7012 us. Best GFLOPs: 41.8704
2023-11-11 00:30:46 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #4: GFLOPs: 5.2072. Time: 196.8411 us. Best GFLOPs: 41.8704
2023-11-11 00:30:46 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #5: GFLOPs: 15.9012. Time: 64.4607 us. Best GFLOPs: 41.8704
2023-11-11 00:30:46 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #6: GFLOPs: 25.7156. Time: 39.8590 us. Best GFLOPs: 41.8704
2023-11-11 00:30:46 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #7: GFLOPs: 7.8946. Time: 129.8356 us. Best GFLOPs: 41.8704
2023-11-11 00:30:46 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #8: GFLOPs: 20.2444. Time: 50.6314 us. Best GFLOPs: 41.8704
2023-11-11 00:30:46 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #9: GFLOPs: 17.1601. Time: 59.7315 us. Best GFLOPs: 41.8704
2023-11-11 00:30:46 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #10: GFLOPs: 10.4876. Time: 97.7345 us. Best GFLOPs: 41.8704
2023-11-11 00:30:46 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #11: GFLOPs: 1.7242. Time: 594.4754 us. Best GFLOPs: 41.8704
2023-11-11 00:30:46 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #12: GFLOPs: 10.4612. Time: 97.9815 us. Best GFLOPs: 41.8704
2023-11-11 00:30:46 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #13: GFLOPs: 3.5968. Time: 284.9723 us. Best GFLOPs: 41.8704
2023-11-11 00:30:46 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #14: GFLOPs: 5.8169. Time: 176.2113 us. Best GFLOPs: 41.8704
2023-11-11 00:30:46 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #15: GFLOPs: 3.3106. Time: 309.6094 us. Best GFLOPs: 41.8704
2023-11-11 00:30:46 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #16: GFLOPs: 30.8833. Time: 33.1895 us. Best GFLOPs: 41.8704
2023-11-11 00:30:46 [INFO] [task_scheduler.cc:121] [Task #28: fused_nn_dense_add] Trial #17: Error in building:
LocalBuilder: An exception occurred
Traceback (most recent call last):
  File "/home2/xiachunwei/Software/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/home2/xiachunwei/Software/tvm/python/tvm/meta_schedule/builder/local_builder.py", line 165, in <lambda>
    lambda x: _worker_func(*x),
  File "/home2/xiachunwei/Software/tvm/python/tvm/meta_schedule/builder/local_builder.py", line 231, in _worker_func
    rt_mod: Module = f_build(mod, target, _deserialize_params(params))
  File "/home2/xiachunwei/Software/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/home2/xiachunwei/Software/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
  File "/home2/xiachunwei/Software/tvm/python/tvm/meta_schedule/builder/local_builder.py", line 261, in default_build
    return tvm_build(mod, target=target)
  File "/home2/xiachunwei/Software/tvm/python/tvm/driver/build_module.py", line 281, in build
    rt_mod_host = _driver_ffi.tir_to_runtime(annotated_mods, target_host)
  File "/home2/xiachunwei/Software/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/home2/xiachunwei/Software/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  218: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::TypedPackedFunc<tvm::runtime::Module (tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)>::AssignTypedLambda<tvm::{lambda(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)#6}>(tvm::{lambda(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)#6}, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}> >::Call(tvm::runtime::PackedFuncObj const*, tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)
  217: tvm::TIRToRuntime(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target const&)
  216: tvm::SplitMixedModule(tvm::IRModule, tvm::Target const&, tvm::Target const&)
  215: tvm::ApplyPasses(tvm::IRModule, tvm::transform::Sequential)
  214: tvm::transform::Pass::operator()(tvm::IRModule) const
  213: tvm::transform::Pass::operator()(tvm::IRModule, tvm::transform::PassContext const&) const
  212: tvm::transform::SequentialNode::operator()(tvm::IRModule, tvm::transform::PassContext const&) const
  211: tvm::transform::Pass::operator()(tvm::IRModule, tvm::transform::PassContext const&) const
  210: tvm::tir::transform::PrimFuncPassNode::operator()(tvm::IRModule, tvm::transform::PassContext const&) const
  209: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::TypedPackedFunc<tvm::tir::PrimFunc (tvm::tir::PrimFunc, tvm::IRModule, tvm::transform::PassContext)>::AssignTypedLambda<tvm::tir::transform::LowerTVMBuiltin()::{lambda(tvm::tir::PrimFunc, tvm::IRModule, tvm::transform::PassContext)#1}>(tvm::tir::transform::LowerTVMBuiltin()::{lambda(tvm::tir::PrimFunc, tvm::IRModule, tvm::transform::PassContext)#1})::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}> >::Call(tvm::runtime::PackedFuncObj const*, tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)
  208: tvm::tir::BuiltinLower::VisitBodyAndRealizeAlloca(tvm::tir::Stmt)
  207: tvm::tir::BuiltinLower::GetMaxStack(tvm::tir::Stmt)
  206: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  205: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7r
  204: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  203: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7r
  202: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  201: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7r
  200: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  199: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  198: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7r
  197: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  196: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  195: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7r
  194: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  193: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  192: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7r
  191: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  190: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  189: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7r
  188: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  187: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  186: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7r
  185: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  184: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  183: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7r
  182: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  181: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  180: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7r
  179: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  178: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  177: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7r
  176: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  175: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  174: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7r
  173: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::AttrStmtNode const*)
  172: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  171: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7r
  170: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  169: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  168: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7r
  167: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  166: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7r
  165: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  164: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  163: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7r
  162: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  161: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7r
  160: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  159: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  158: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7r
  157: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  156: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  155: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7r
  154: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::AttrStmtNode const*)
  153: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  152: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7r
  151: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  150: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  149: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7r
  148: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  147: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7r
  146: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  145: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  144: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7r
  143: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  142: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7r
  141: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  140: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  139: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7r
  138: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::AttrStmtNode const*)
  137: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  136: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7r
  135: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  134: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  133: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7r
  132: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  131: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7r
  130: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  129: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  128: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7r
  127: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  126: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7r
  125: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  124: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  123: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7r
  122: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::AttrStmtNode const*)
  121: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  120: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7r
  119: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  118: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  117: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7r
  116: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  115: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7r
  114: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  113: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  112: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7r
  111: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  110: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7r
  109: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  108: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7r
  107: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  106: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7r
  105: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  104: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7r
  103: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  102: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7r
  101: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::AttrStmtNode const*)
  100: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  99: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7r
  98: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::AttrStmtNode const*)
  97: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  96: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7r
  95: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  94: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7r
  93: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  92: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7r
  91: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  90: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7r
  89: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  88: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7r
  87: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  86: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runt
  85: tvm::runtime::ObjectPtr<tvm::runtime::Object> tvm::runtime::Array<tvm::tir::Stmt, void>::MapHelper<tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1}, tvm::tir::Stmt>(tvm::runtime::ObjectPtr<tvm::runtime::Object>, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1})
  84: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  83: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7r
  82: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  81: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7r
  80: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  79: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7r
  78: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  77: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7r
  76: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  75: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7r
  74: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  73: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7r
  72: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  71: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7r
  70: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  69: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runt
  68: tvm::runtime::ObjectPtr<tvm::runtime::Object> tvm::runtime::Array<tvm::tir::Stmt, void>::MapHelper<tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1}, tvm::tir::Stmt>(tvm::runtime::ObjectPtr<tvm::runtime::Object>, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1})
  67: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  66: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7r
  65: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  64: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7r
  63: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  62: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7r
  61: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  60: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7r
  59: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  58: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7r
  57: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  56: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7r
  55: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  54: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7r
  53: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  52: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7r
  51: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  50: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runt
  49: tvm::runtime::ObjectPtr<tvm::runtime::Object> tvm::runtime::Array<tvm::tir::Stmt, void>::MapHelper<tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1}, tvm::tir::Stmt>(tvm::runtime::ObjectPtr<tvm::runtime::Object>, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1})
  48: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  47: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7r
  46: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  45: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7r
  44: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  43: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7r
  42: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  41: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7r
  40: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  39: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7r
  38: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  37: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7r
  36: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  35: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7r
  34: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  33: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7r
  32: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  31: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runt
  30: tvm::runtime::ObjectPtr<tvm::runtime::Object> tvm::runtime::Array<tvm::tir::Stmt, void>::MapHelper<tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1}, tvm::tir::Stmt>(tvm::runtime::ObjectPtr<tvm::runtime::Object>, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1})
  29: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  28: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7r
  27: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  26: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7r
  25: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  24: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7r
  23: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  22: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7r
  21: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  20: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7r
  19: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  18: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7r
  17: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  16: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7r
  15: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  14: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runt
  13: tvm::runtime::ObjectPtr<tvm::runtime::Object> tvm::runtime::Array<tvm::tir::Stmt, void>::MapHelper<tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1}, tvm::tir::Stmt>(tvm::runtime::ObjectPtr<tvm::runtime::Object>, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1})
  12: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  11: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7r
  10: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::AttrStmtNode const*)
  9: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  8: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7r
  7: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  6: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  5: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runt
  4: tvm::tir::StmtExprMutator::VisitExpr(tvm::PrimExpr const&)
  3: _ZZN3tvm3tir11ExprFunctorIFNS_8PrimExprERKS2_EE10InitVTableEvENUlRKNS
  2: tvm::tir::BuiltinLower::VisitExpr_(tvm::tir::CallNode const*)
  1: tvm::tir::BuiltinLower::MakeCallPacked(tvm::tir::CallNode const*, bool)
  0: tvm::tir::APIType(tvm::runtime::DataType)
  File "/home2/xiachunwei/Software/tvm/src/tir/transforms/ir_utils.h", line 157
InternalError: Check failed: t.lanes() == 1 (4 vs. 1) : Cannot pass vector type through packed API.

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(512)), "float32"), p1: T.Buffer((T.int64(1000), T.int64(512)), "float32"), p2: T.Buffer((T.int64(1), T.int64(1000)), "float32"), T_add: T.Buffer((T.int64(1), T.int64(1000)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        T_matmul_NT_local = T.alloc_buffer((T.int64(1), T.int64(1000)), scope="local")
        p0_shared = T.alloc_buffer((T.int64(1), T.int64(512)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(1000), T.int64(512)), scope="shared")
        for i_0_j_0_fused in T.thread_binding(T.int64(2), thread="blockIdx.x"):
            for i_1_j_1_fused in T.thread_binding(T.int64(2), thread="vthread.x"):
                for i_2_j_2_fused in T.thread_binding(T.int64(250), thread="threadIdx.x"):
                    for i_3_init, j_3_init, i_4_init, j_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_init"):
                            v_i = T.axis.spatial(T.int64(1), i_3_init + i_4_init)
                            v_j = T.axis.spatial(T.int64(1000), i_0_j_0_fused * T.int64(500) + i_1_j_1_fused * T.int64(250) + i_2_j_2_fused + j_3_init + j_4_init)
                            T.reads()
                            T.writes(T_matmul_NT_local[v_i, v_j])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            T_matmul_NT_local[v_i, v_j] = T.float32(0)
                    for k_0_fused in T.serial(T.int64(256), annotations={"software_pipeline_async_stages": [0], "software_pipeline_order": [0, 1, 2], "software_pipeline_stage": [0, 0, 3]}):
                        for ax0_ax1_fused_0 in range(T.int64(1)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(250), thread="threadIdx.x"):
                                with T.block("p0_shared"):
                                    v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                    v1 = T.axis.spatial(T.int64(512), k_0_fused * T.int64(2) + (ax0_ax1_fused_0 * T.int64(250) + ax0_ax1_fused_1))
                                    T.where(ax0_ax1_fused_0 * T.int64(250) + ax0_ax1_fused_1 < T.int64(2))
                                    T.reads(p0[v0, v1])
                                    T.writes(p0_shared[v0, v1])
                                    p0_shared[v0, v1] = p0[v0, v1]
                        for ax0_ax1_fused_0 in range(T.int64(1)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(250), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("p1_shared"):
                                        v0 = T.axis.spatial(T.int64(1000), i_0_j_0_fused * T.int64(500) + (ax0_ax1_fused_0 * T.int64(1000) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2) // T.int64(2))
                                        v1 = T.axis.spatial(T.int64(512), k_0_fused * T.int64(2) + (ax0_ax1_fused_0 * T.int64(1000) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2) % T.int64(2))
                                        T.reads(p1[v0, v1])
                                        T.writes(p1_shared[v0, v1])
                                        p1_shared[v0, v1] = p1[v0, v1]
                        for k_1, i_3, j_3, k_2, i_4, j_4 in T.grid(T.int64(2), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                            with T.block("T_matmul_NT_update"):
                                v_i = T.axis.spatial(T.int64(1), i_3 + i_4)
                                v_j = T.axis.spatial(T.int64(1000), i_0_j_0_fused * T.int64(500) + i_1_j_1_fused * T.int64(250) + i_2_j_2_fused + j_3 + j_4)
                                v_k = T.axis.reduce(T.int64(512), k_0_fused * T.int64(2) + k_1 + k_2)
                                T.reads(T_matmul_NT_local[v_i, v_j], p0_shared[v_i, v_k], p1_shared[v_j, v_k])
                                T.writes(T_matmul_NT_local[v_i, v_j])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                T_matmul_NT_local[v_i, v_j] = T_matmul_NT_local[v_i, v_j] + p0_shared[v_i, v_k] * p1_shared[v_j, v_k]
                    for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(1000), i_0_j_0_fused * T.int64(500) + i_1_j_1_fused * T.int64(250) + i_2_j_2_fused + ax1)
                            T.reads(T_matmul_NT_local[v0, v1], p2[v0, v1])
                            T.writes(T_add[v0, v1])
                            T_add[v0, v1] = T_matmul_NT_local[v0, v1] + p2[v0, v1]
b0 = sch.get_block(name="T_matmul_NT", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l3, l4, l5 = sch.get_loops(block=b0)
v6, v7, v8, v9, v10 = sch.sample_perfect_tile(loop=l3, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l11, l12, l13, l14, l15 = sch.split(loop=l3, factors=[v6, v7, v8, v9, v10], preserve_unit_iters=True)
v16, v17, v18, v19, v20 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[2, 2, 250, 1, 1])
l21, l22, l23, l24, l25 = sch.split(loop=l4, factors=[v16, v17, v18, v19, v20], preserve_unit_iters=True)
v26, v27, v28 = sch.sample_perfect_tile(loop=l5, n=3, max_innermost_factor=64, decision=[256, 2, 1])
l29, l30, l31 = sch.split(loop=l5, factors=[v26, v27, v28], preserve_unit_iters=True)
sch.reorder(l11, l21, l12, l22, l13, l23, l29, l30, l14, l24, l31, l15, l25)
l32 = sch.fuse(l11, l21, preserve_unit_iters=True)
sch.bind(loop=l32, thread_axis="blockIdx.x")
l33 = sch.fuse(l12, l22, preserve_unit_iters=True)
sch.bind(loop=l33, thread_axis="vthread.x")
l34 = sch.fuse(l13, l23, preserve_unit_iters=True)
sch.bind(loop=l34, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b35 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b35, loop=l34, preserve_unit_loops=True, index=-1)
b36 = sch.cache_read(block=b0, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b36, loop=l29, preserve_unit_loops=True, index=-1)
l37, l38, l39, l40, l41, l42 = sch.get_loops(block=b36)
l43 = sch.fuse(l41, l42, preserve_unit_iters=True)
v44 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b36, ann_key="meta_schedule.cooperative_fetch", ann_val=v44)
b45 = sch.cache_read(block=b0, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b45, loop=l29, preserve_unit_loops=True, index=-1)
l46, l47, l48, l49, l50, l51 = sch.get_loops(block=b45)
l52 = sch.fuse(l50, l51, preserve_unit_iters=True)
v53 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b45, ann_key="meta_schedule.cooperative_fetch", ann_val=v53)
l54 = sch.fuse(l29, preserve_unit_iters=True)
sch.annotate(block_or_loop=l54, ann_key="software_pipeline_stage", ann_val=[0, 0, 3])
sch.annotate(block_or_loop=l54, ann_key="software_pipeline_order", ann_val=[0, 1, 2])
sch.annotate(block_or_loop=l54, ann_key="software_pipeline_async_stages", ann_val=[0])
sch.reverse_compute_inline(block=b1)
v55 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=0)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v55)
sch.enter_postproc()
sch.unannotate(block_or_loop=b36, ann_key="meta_schedule.cooperative_fetch")
l56, l57, l58, l59, l60 = sch.get_loops(block=b36)
l61, l62 = sch.split(loop=l60, factors=[None, 250], preserve_unit_iters=True)
sch.bind(loop=l62, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b45, ann_key="meta_schedule.cooperative_fetch")
l63, l64, l65, l66, l67 = sch.get_loops(block=b45)
l68, l69, l70 = sch.split(loop=l67, factors=[None, 250, 4], preserve_unit_iters=True)
sch.vectorize(loop=l70)
sch.bind(loop=l69, thread_axis="threadIdx.x")
b71 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b71, ann_key="meta_schedule.unroll_explicit")
b72, b73, b74, b75 = sch.get_child_blocks(b71)
l76, l77, l78, l79, l80, l81 = sch.get_loops(block=b72)
l82, l83, l84, l85, l86, l87, l88 = sch.get_loops(block=b73)
l89, l90, l91, l92, l93, l94, l95, l96, l97, l98 = sch.get_loops(block=b74)
l99, l100, l101, l102, l103 = sch.get_loops(block=b75)
b104 = sch.get_block(name="T_matmul_NT", func_name="main")
l105, l106, l107, l108, l109, l110, l111, l112, l113, l114 = sch.get_loops(block=b104)
b115 = sch.decompose_reduction(block=b104, loop=l108)
2023-11-11 00:30:46 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #18: GFLOPs: 11.6745. Time: 87.7986 us. Best GFLOPs: 41.8704
2023-11-11 00:30:46 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #19: GFLOPs: 11.7393. Time: 87.3136 us. Best GFLOPs: 41.8704
2023-11-11 00:30:46 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #20: GFLOPs: 2.5314. Time: 404.9153 us. Best GFLOPs: 41.8704
2023-11-11 00:30:46 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #21: GFLOPs: 6.6977. Time: 153.0373 us. Best GFLOPs: 41.8704
2023-11-11 00:30:46 [INFO] [task_scheduler.cc:121] [Task #28: fused_nn_dense_add] Trial #22: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/home2/xiachunwei/Software/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/home2/xiachunwei/Software/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 159, in _worker_func
    costs: List[float] = f_run_evaluator(
  File "/home2/xiachunwei/Software/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 387, in default_run_evaluator
    return run_evaluator_common(rt_mod, device, evaluator_config, repeated_args)
  File "/home2/xiachunwei/Software/tvm/python/tvm/meta_schedule/runner/utils.py", line 117, in run_evaluator_common
    profile_result = evaluator(*args)
  File "/home2/xiachunwei/Software/tvm/python/tvm/runtime/module.py", line 397, in evaluator
    blob = feval(*args)
  File "/home2/xiachunwei/Software/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/home2/xiachunwei/Software/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  1: tvm::runtime::profiling::WrapTimeEvaluator(tvm::runtime::PackedFunc, DLDevice, int, int, int, int, int, int, int, tvm::runtime::PackedFunc)::{lambda(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*) [clone .isra.779]
  0: tvm::runtime::CUDADeviceAPI::StreamSync(DLDevice, void*)
  File "/home2/xiachunwei/Software/tvm/src/runtime/cuda/cuda_device_api.cc", line 212
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(512)), "float32"), p1: T.Buffer((T.int64(1000), T.int64(512)), "float32"), p2: T.Buffer((T.int64(1), T.int64(1000)), "float32"), T_add: T.Buffer((T.int64(1), T.int64(1000)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        T_matmul_NT_local = T.alloc_buffer((T.int64(1), T.int64(1000)), scope="local")
        p0_shared = T.alloc_buffer((T.int64(1), T.int64(512)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(1000), T.int64(512)), scope="shared")
        for i_0_j_0_fused in T.thread_binding(T.int64(1), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 64, "pragma_unroll_explicit": 1}):
            for i_1_j_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for i_2_j_2_fused in T.thread_binding(T.int64(125), thread="threadIdx.x"):
                    for i_3_init, j_3_init, i_4_init, j_4_init in T.grid(T.int64(1), T.int64(8), T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_init"):
                            v_i = T.axis.spatial(T.int64(1), i_3_init + i_4_init)
                            v_j = T.axis.spatial(T.int64(1000), i_2_j_2_fused * T.int64(8) + j_3_init + j_4_init)
                            T.reads()
                            T.writes(T_matmul_NT_local[v_i, v_j])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            T_matmul_NT_local[v_i, v_j] = T.float32(0)
                    for k_0 in range(T.int64(256)):
                        for ax0_ax1_fused_0 in range(T.int64(1)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(125), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("p0_shared"):
                                        v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v1 = T.axis.spatial(T.int64(512), k_0 * T.int64(2) + (ax0_ax1_fused_0 * T.int64(250) + ax0_ax1_fused_1 * T.int64(2) + ax0_ax1_fused_2))
                                        T.where((ax0_ax1_fused_0 * T.int64(125) + ax0_ax1_fused_1) * T.int64(2) + ax0_ax1_fused_2 < T.int64(2))
                                        T.reads(p0[v0, v1])
                                        T.writes(p0_shared[v0, v1])
                                        p0_shared[v0, v1] = p0[v0, v1]
                        for ax0_ax1_fused_0 in range(T.int64(4)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(125), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("p1_shared"):
                                        v0 = T.axis.spatial(T.int64(1000), (ax0_ax1_fused_0 * T.int64(500) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2) // T.int64(2))
                                        v1 = T.axis.spatial(T.int64(512), k_0 * T.int64(2) + (ax0_ax1_fused_0 * T.int64(500) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2) % T.int64(2))
                                        T.reads(p1[v0, v1])
                                        T.writes(p1_shared[v0, v1])
                                        p1_shared[v0, v1] = p1[v0, v1]
                        for k_1, i_3, j_3, k_2, i_4, j_4 in T.grid(T.int64(2), T.int64(1), T.int64(8), T.int64(1), T.int64(1), T.int64(1)):
                            with T.block("T_matmul_NT_update"):
                                v_i = T.axis.spatial(T.int64(1), i_3 + i_4)
                                v_j = T.axis.spatial(T.int64(1000), i_2_j_2_fused * T.int64(8) + j_3 + j_4)
                                v_k = T.axis.reduce(T.int64(512), k_0 * T.int64(2) + k_1 + k_2)
                                T.reads(T_matmul_NT_local[v_i, v_j], p0_shared[v_i, v_k], p1_shared[v_j, v_k])
                                T.writes(T_matmul_NT_local[v_i, v_j])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                T_matmul_NT_local[v_i, v_j] = T_matmul_NT_local[v_i, v_j] + p0_shared[v_i, v_k] * p1_shared[v_j, v_k]
                    for ax0, ax1 in T.grid(T.int64(1), T.int64(8)):
                        with T.block("T_matmul_NT_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(1000), i_2_j_2_fused * T.int64(8) + ax1)
                            T.reads(T_matmul_NT_local[v0, v1], p2[v0, v1])
                            T.writes(T_add[v0, v1])
                            T_add[v0, v1] = T_matmul_NT_local[v0, v1] + p2[v0, v1]
b0 = sch.get_block(name="T_matmul_NT", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l3, l4, l5 = sch.get_loops(block=b0)
v6, v7, v8, v9, v10 = sch.sample_perfect_tile(loop=l3, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l11, l12, l13, l14, l15 = sch.split(loop=l3, factors=[v6, v7, v8, v9, v10], preserve_unit_iters=True)
v16, v17, v18, v19, v20 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[1, 1, 125, 8, 1])
l21, l22, l23, l24, l25 = sch.split(loop=l4, factors=[v16, v17, v18, v19, v20], preserve_unit_iters=True)
v26, v27, v28 = sch.sample_perfect_tile(loop=l5, n=3, max_innermost_factor=64, decision=[256, 2, 1])
l29, l30, l31 = sch.split(loop=l5, factors=[v26, v27, v28], preserve_unit_iters=True)
sch.reorder(l11, l21, l12, l22, l13, l23, l29, l30, l14, l24, l31, l15, l25)
l32 = sch.fuse(l11, l21, preserve_unit_iters=True)
sch.bind(loop=l32, thread_axis="blockIdx.x")
l33 = sch.fuse(l12, l22, preserve_unit_iters=True)
sch.bind(loop=l33, thread_axis="vthread.x")
l34 = sch.fuse(l13, l23, preserve_unit_iters=True)
sch.bind(loop=l34, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b35 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b35, loop=l34, preserve_unit_loops=True, index=-1)
b36 = sch.cache_read(block=b0, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b36, loop=l29, preserve_unit_loops=True, index=-1)
l37, l38, l39, l40, l41, l42 = sch.get_loops(block=b36)
l43 = sch.fuse(l41, l42, preserve_unit_iters=True)
v44 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b36, ann_key="meta_schedule.cooperative_fetch", ann_val=v44)
b45 = sch.cache_read(block=b0, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b45, loop=l29, preserve_unit_loops=True, index=-1)
l46, l47, l48, l49, l50, l51 = sch.get_loops(block=b45)
l52 = sch.fuse(l50, l51, preserve_unit_iters=True)
v53 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b45, ann_key="meta_schedule.cooperative_fetch", ann_val=v53)
sch.reverse_compute_inline(block=b1)
v54 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=2)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v54)
sch.enter_postproc()
sch.unannotate(block_or_loop=b36, ann_key="meta_schedule.cooperative_fetch")
l55, l56, l57, l58, l59 = sch.get_loops(block=b36)
l60, l61, l62 = sch.split(loop=l59, factors=[None, 125, 2], preserve_unit_iters=True)
sch.vectorize(loop=l62)
sch.bind(loop=l61, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b45, ann_key="meta_schedule.cooperative_fetch")
l63, l64, l65, l66, l67 = sch.get_loops(block=b45)
l68, l69, l70 = sch.split(loop=l67, factors=[None, 125, 4], preserve_unit_iters=True)
sch.vectorize(loop=l70)
sch.bind(loop=l69, thread_axis="threadIdx.x")
b71 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b71, ann_key="meta_schedule.unroll_explicit")
b72, b73, b74, b75 = sch.get_child_blocks(b71)
l76, l77, l78, l79, l80, l81, l82 = sch.get_loops(block=b72)
l83, l84, l85, l86, l87, l88, l89 = sch.get_loops(block=b73)
l90, l91, l92, l93, l94, l95, l96, l97, l98, l99 = sch.get_loops(block=b74)
sch.annotate(block_or_loop=l90, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l90, ann_key="pragma_unroll_explicit", ann_val=1)
l100, l101, l102, l103, l104 = sch.get_loops(block=b75)
b105 = sch.get_block(name="T_matmul_NT", func_name="main")
l106, l107, l108, l109, l110, l111, l112, l113, l114, l115 = sch.get_loops(block=b105)
b116 = sch.decompose_reduction(block=b105, loop=l109)
2023-11-11 00:30:46 [INFO] [task_scheduler.cc:121] [Task #28: fused_nn_dense_add] Trial #23: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/home2/xiachunwei/Software/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/home2/xiachunwei/Software/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/home2/xiachunwei/Software/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/home2/xiachunwei/Software/tvm/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/home2/xiachunwei/Software/tvm/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/home2/xiachunwei/Software/tvm/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/home2/xiachunwei/Software/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/home2/xiachunwei/Software/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  4: _ZN3tvm7runtime13PackedFun
  3: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  2: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  0: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  File "/home2/xiachunwei/Software/tvm/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(512)), "float32"), p1: T.Buffer((T.int64(1000), T.int64(512)), "float32"), p2: T.Buffer((T.int64(1), T.int64(1000)), "float32"), T_add: T.Buffer((T.int64(1), T.int64(1000)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        T_matmul_NT_local = T.alloc_buffer((T.int64(1), T.int64(1000)), scope="local")
        p0_shared = T.alloc_buffer((T.int64(1), T.int64(512)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(1000), T.int64(512)), scope="shared")
        for i_0_j_0_fused in T.thread_binding(T.int64(1), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 16, "pragma_unroll_explicit": 1}):
            for i_1_j_1_fused in T.thread_binding(T.int64(2), thread="vthread.x"):
                for i_2_j_2_fused in T.thread_binding(T.int64(250), thread="threadIdx.x"):
                    for i_3_init, j_3_init, i_4_init, j_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(2)):
                        with T.block("T_matmul_NT_init"):
                            v_i = T.axis.spatial(T.int64(1), i_3_init + i_4_init)
                            v_j = T.axis.spatial(T.int64(1000), i_1_j_1_fused * T.int64(500) + i_2_j_2_fused * T.int64(2) + j_3_init * T.int64(2) + j_4_init)
                            T.reads()
                            T.writes(T_matmul_NT_local[v_i, v_j])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            T_matmul_NT_local[v_i, v_j] = T.float32(0)
                    for k_0 in range(T.int64(64)):
                        for ax0_ax1_fused_0 in range(T.int64(1)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(250), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("p0_shared"):
                                        v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v1 = T.axis.spatial(T.int64(512), k_0 * T.int64(8) + (ax0_ax1_fused_0 * T.int64(1000) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2))
                                        T.where((ax0_ax1_fused_0 * T.int64(250) + ax0_ax1_fused_1) * T.int64(4) + ax0_ax1_fused_2 < T.int64(8))
                                        T.reads(p0[v0, v1])
                                        T.writes(p0_shared[v0, v1])
                                        p0_shared[v0, v1] = p0[v0, v1]
                        for ax0_ax1_fused_0 in range(T.int64(32)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(250), thread="threadIdx.x"):
                                with T.block("p1_shared"):
                                    v0 = T.axis.spatial(T.int64(1000), (ax0_ax1_fused_0 * T.int64(250) + ax0_ax1_fused_1) // T.int64(8))
                                    v1 = T.axis.spatial(T.int64(512), k_0 * T.int64(8) + (ax0_ax1_fused_0 * T.int64(250) + ax0_ax1_fused_1) % T.int64(8))
                                    T.reads(p1[v0, v1])
                                    T.writes(p1_shared[v0, v1])
                                    p1_shared[v0, v1] = p1[v0, v1]
                        for k_1, i_3, j_3, k_2, i_4, j_4 in T.grid(T.int64(8), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(2)):
                            with T.block("T_matmul_NT_update"):
                                v_i = T.axis.spatial(T.int64(1), i_3 + i_4)
                                v_j = T.axis.spatial(T.int64(1000), i_1_j_1_fused * T.int64(500) + i_2_j_2_fused * T.int64(2) + j_3 * T.int64(2) + j_4)
                                v_k = T.axis.reduce(T.int64(512), k_0 * T.int64(8) + k_1 + k_2)
                                T.reads(T_matmul_NT_local[v_i, v_j], p0_shared[v_i, v_k], p1_shared[v_j, v_k])
                                T.writes(T_matmul_NT_local[v_i, v_j])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                T_matmul_NT_local[v_i, v_j] = T_matmul_NT_local[v_i, v_j] + p0_shared[v_i, v_k] * p1_shared[v_j, v_k]
                    for ax0, ax1 in T.grid(T.int64(1), T.int64(2)):
                        with T.block("T_matmul_NT_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(1000), i_1_j_1_fused * T.int64(500) + i_2_j_2_fused * T.int64(2) + ax1)
                            T.reads(T_matmul_NT_local[v0, v1], p2[v0, v1])
                            T.writes(T_add[v0, v1])
                            T_add[v0, v1] = T_matmul_NT_local[v0, v1] + p2[v0, v1]
b0 = sch.get_block(name="T_matmul_NT", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l3, l4, l5 = sch.get_loops(block=b0)
v6, v7, v8, v9, v10 = sch.sample_perfect_tile(loop=l3, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l11, l12, l13, l14, l15 = sch.split(loop=l3, factors=[v6, v7, v8, v9, v10], preserve_unit_iters=True)
v16, v17, v18, v19, v20 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[1, 2, 250, 1, 2])
l21, l22, l23, l24, l25 = sch.split(loop=l4, factors=[v16, v17, v18, v19, v20], preserve_unit_iters=True)
v26, v27, v28 = sch.sample_perfect_tile(loop=l5, n=3, max_innermost_factor=64, decision=[64, 8, 1])
l29, l30, l31 = sch.split(loop=l5, factors=[v26, v27, v28], preserve_unit_iters=True)
sch.reorder(l11, l21, l12, l22, l13, l23, l29, l30, l14, l24, l31, l15, l25)
l32 = sch.fuse(l11, l21, preserve_unit_iters=True)
sch.bind(loop=l32, thread_axis="blockIdx.x")
l33 = sch.fuse(l12, l22, preserve_unit_iters=True)
sch.bind(loop=l33, thread_axis="vthread.x")
l34 = sch.fuse(l13, l23, preserve_unit_iters=True)
sch.bind(loop=l34, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b35 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b35, loop=l34, preserve_unit_loops=True, index=-1)
b36 = sch.cache_read(block=b0, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b36, loop=l29, preserve_unit_loops=True, index=-1)
l37, l38, l39, l40, l41, l42 = sch.get_loops(block=b36)
l43 = sch.fuse(l41, l42, preserve_unit_iters=True)
v44 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b36, ann_key="meta_schedule.cooperative_fetch", ann_val=v44)
b45 = sch.cache_read(block=b0, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b45, loop=l29, preserve_unit_loops=True, index=-1)
l46, l47, l48, l49, l50, l51 = sch.get_loops(block=b45)
l52 = sch.fuse(l50, l51, preserve_unit_iters=True)
v53 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b45, ann_key="meta_schedule.cooperative_fetch", ann_val=v53)
sch.reverse_compute_inline(block=b1)
v54 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=1)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v54)
sch.enter_postproc()
sch.unannotate(block_or_loop=b36, ann_key="meta_schedule.cooperative_fetch")
l55, l56, l57, l58, l59 = sch.get_loops(block=b36)
l60, l61, l62 = sch.split(loop=l59, factors=[None, 250, 4], preserve_unit_iters=True)
sch.vectorize(loop=l62)
sch.bind(loop=l61, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b45, ann_key="meta_schedule.cooperative_fetch")
l63, l64, l65, l66, l67 = sch.get_loops(block=b45)
l68, l69 = sch.split(loop=l67, factors=[None, 250], preserve_unit_iters=True)
sch.bind(loop=l69, thread_axis="threadIdx.x")
b70 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b70, ann_key="meta_schedule.unroll_explicit")
b71, b72, b73, b74 = sch.get_child_blocks(b70)
l75, l76, l77, l78, l79, l80, l81 = sch.get_loops(block=b71)
l82, l83, l84, l85, l86, l87 = sch.get_loops(block=b72)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b73)
sch.annotate(block_or_loop=l88, ann_key="pragma_auto_unroll_max_step", ann_val=16)
sch.annotate(block_or_loop=l88, ann_key="pragma_unroll_explicit", ann_val=1)
l98, l99, l100, l101, l102 = sch.get_loops(block=b74)
b103 = sch.get_block(name="T_matmul_NT", func_name="main")
l104, l105, l106, l107, l108, l109, l110, l111, l112, l113 = sch.get_loops(block=b103)
b114 = sch.decompose_reduction(block=b103, loop=l107)
2023-11-11 00:30:46 [INFO] [task_scheduler.cc:121] [Task #28: fused_nn_dense_add] Trial #24: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/home2/xiachunwei/Software/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/home2/xiachunwei/Software/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/home2/xiachunwei/Software/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/home2/xiachunwei/Software/tvm/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/home2/xiachunwei/Software/tvm/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/home2/xiachunwei/Software/tvm/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/home2/xiachunwei/Software/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/home2/xiachunwei/Software/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  4: _ZN3tvm7runtime13PackedFun
  3: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  2: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  0: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  File "/home2/xiachunwei/Software/tvm/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(512)), "float32"), p1: T.Buffer((T.int64(1000), T.int64(512)), "float32"), p2: T.Buffer((T.int64(1), T.int64(1000)), "float32"), T_add: T.Buffer((T.int64(1), T.int64(1000)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        T_matmul_NT_local = T.alloc_buffer((T.int64(1), T.int64(1000)), scope="local")
        p0_shared = T.alloc_buffer((T.int64(1), T.int64(512)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(1000), T.int64(512)), scope="shared")
        for i_0_j_0_fused in T.thread_binding(T.int64(5), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 64, "pragma_unroll_explicit": 1}):
            for i_1_j_1_fused in T.thread_binding(T.int64(2), thread="vthread.x"):
                for i_2_j_2_fused in T.thread_binding(T.int64(50), thread="threadIdx.x"):
                    for i_3_init, j_3_init, i_4_init, j_4_init in T.grid(T.int64(1), T.int64(2), T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_init"):
                            v_i = T.axis.spatial(T.int64(1), i_3_init + i_4_init)
                            v_j = T.axis.spatial(T.int64(1000), i_0_j_0_fused * T.int64(200) + i_1_j_1_fused * T.int64(100) + i_2_j_2_fused * T.int64(2) + j_3_init + j_4_init)
                            T.reads()
                            T.writes(T_matmul_NT_local[v_i, v_j])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            T_matmul_NT_local[v_i, v_j] = T.float32(0)
                    for k_0 in range(T.int64(256)):
                        for ax0_ax1_fused_0 in range(T.int64(1)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(50), thread="threadIdx.x"):
                                with T.block("p0_shared"):
                                    v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                    v1 = T.axis.spatial(T.int64(512), k_0 * T.int64(2) + (ax0_ax1_fused_0 * T.int64(50) + ax0_ax1_fused_1))
                                    T.where(ax0_ax1_fused_0 * T.int64(50) + ax0_ax1_fused_1 < T.int64(2))
                                    T.reads(p0[v0, v1])
                                    T.writes(p0_shared[v0, v1])
                                    p0_shared[v0, v1] = p0[v0, v1]
                        for ax0_ax1_fused_0 in range(T.int64(8)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(50), thread="threadIdx.x"):
                                with T.block("p1_shared"):
                                    v0 = T.axis.spatial(T.int64(1000), i_0_j_0_fused * T.int64(200) + (ax0_ax1_fused_0 * T.int64(50) + ax0_ax1_fused_1) // T.int64(2))
                                    v1 = T.axis.spatial(T.int64(512), k_0 * T.int64(2) + (ax0_ax1_fused_0 * T.int64(50) + ax0_ax1_fused_1) % T.int64(2))
                                    T.reads(p1[v0, v1])
                                    T.writes(p1_shared[v0, v1])
                                    p1_shared[v0, v1] = p1[v0, v1]
                        for k_1, i_3, j_3, k_2, i_4, j_4 in T.grid(T.int64(1), T.int64(1), T.int64(2), T.int64(2), T.int64(1), T.int64(1)):
                            with T.block("T_matmul_NT_update"):
                                v_i = T.axis.spatial(T.int64(1), i_3 + i_4)
                                v_j = T.axis.spatial(T.int64(1000), i_0_j_0_fused * T.int64(200) + i_1_j_1_fused * T.int64(100) + i_2_j_2_fused * T.int64(2) + j_3 + j_4)
                                v_k = T.axis.reduce(T.int64(512), k_0 * T.int64(2) + k_1 * T.int64(2) + k_2)
                                T.reads(T_matmul_NT_local[v_i, v_j], p0_shared[v_i, v_k], p1_shared[v_j, v_k])
                                T.writes(T_matmul_NT_local[v_i, v_j])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                T_matmul_NT_local[v_i, v_j] = T_matmul_NT_local[v_i, v_j] + p0_shared[v_i, v_k] * p1_shared[v_j, v_k]
                    for ax0, ax1 in T.grid(T.int64(1), T.int64(2)):
                        with T.block("T_matmul_NT_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(1000), i_0_j_0_fused * T.int64(200) + i_1_j_1_fused * T.int64(100) + i_2_j_2_fused * T.int64(2) + ax1)
                            T.reads(T_matmul_NT_local[v0, v1], p2[v0, v1])
                            T.writes(T_add[v0, v1])
                            T_add[v0, v1] = T_matmul_NT_local[v0, v1] + p2[v0, v1]
b0 = sch.get_block(name="T_matmul_NT", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l3, l4, l5 = sch.get_loops(block=b0)
v6, v7, v8, v9, v10 = sch.sample_perfect_tile(loop=l3, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l11, l12, l13, l14, l15 = sch.split(loop=l3, factors=[v6, v7, v8, v9, v10], preserve_unit_iters=True)
v16, v17, v18, v19, v20 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[5, 2, 50, 2, 1])
l21, l22, l23, l24, l25 = sch.split(loop=l4, factors=[v16, v17, v18, v19, v20], preserve_unit_iters=True)
v26, v27, v28 = sch.sample_perfect_tile(loop=l5, n=3, max_innermost_factor=64, decision=[256, 1, 2])
l29, l30, l31 = sch.split(loop=l5, factors=[v26, v27, v28], preserve_unit_iters=True)
sch.reorder(l11, l21, l12, l22, l13, l23, l29, l30, l14, l24, l31, l15, l25)
l32 = sch.fuse(l11, l21, preserve_unit_iters=True)
sch.bind(loop=l32, thread_axis="blockIdx.x")
l33 = sch.fuse(l12, l22, preserve_unit_iters=True)
sch.bind(loop=l33, thread_axis="vthread.x")
l34 = sch.fuse(l13, l23, preserve_unit_iters=True)
sch.bind(loop=l34, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b35 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b35, loop=l34, preserve_unit_loops=True, index=-1)
b36 = sch.cache_read(block=b0, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b36, loop=l29, preserve_unit_loops=True, index=-1)
l37, l38, l39, l40, l41, l42 = sch.get_loops(block=b36)
l43 = sch.fuse(l41, l42, preserve_unit_iters=True)
v44 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b36, ann_key="meta_schedule.cooperative_fetch", ann_val=v44)
b45 = sch.cache_read(block=b0, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b45, loop=l29, preserve_unit_loops=True, index=-1)
l46, l47, l48, l49, l50, l51 = sch.get_loops(block=b45)
l52 = sch.fuse(l50, l51, preserve_unit_iters=True)
v53 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b45, ann_key="meta_schedule.cooperative_fetch", ann_val=v53)
sch.reverse_compute_inline(block=b1)
v54 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=2)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v54)
sch.enter_postproc()
sch.unannotate(block_or_loop=b36, ann_key="meta_schedule.cooperative_fetch")
l55, l56, l57, l58, l59 = sch.get_loops(block=b36)
l60, l61 = sch.split(loop=l59, factors=[None, 50], preserve_unit_iters=True)
sch.bind(loop=l61, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b45, ann_key="meta_schedule.cooperative_fetch")
l62, l63, l64, l65, l66 = sch.get_loops(block=b45)
l67, l68 = sch.split(loop=l66, factors=[None, 50], preserve_unit_iters=True)
sch.bind(loop=l68, thread_axis="threadIdx.x")
b69 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b69, ann_key="meta_schedule.unroll_explicit")
b70, b71, b72, b73 = sch.get_child_blocks(b69)
l74, l75, l76, l77, l78, l79 = sch.get_loops(block=b70)
l80, l81, l82, l83, l84, l85 = sch.get_loops(block=b71)
l86, l87, l88, l89, l90, l91, l92, l93, l94, l95 = sch.get_loops(block=b72)
sch.annotate(block_or_loop=l86, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l86, ann_key="pragma_unroll_explicit", ann_val=1)
l96, l97, l98, l99, l100 = sch.get_loops(block=b73)
b101 = sch.get_block(name="T_matmul_NT", func_name="main")
l102, l103, l104, l105, l106, l107, l108, l109, l110, l111 = sch.get_loops(block=b101)
b112 = sch.decompose_reduction(block=b101, loop=l105)
2023-11-11 00:30:46 [INFO] [task_scheduler.cc:121] [Task #28: fused_nn_dense_add] Trial #25: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/home2/xiachunwei/Software/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/home2/xiachunwei/Software/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/home2/xiachunwei/Software/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/home2/xiachunwei/Software/tvm/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/home2/xiachunwei/Software/tvm/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/home2/xiachunwei/Software/tvm/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/home2/xiachunwei/Software/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/home2/xiachunwei/Software/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  4: _ZN3tvm7runtime13PackedFun
  3: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  2: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  0: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  File "/home2/xiachunwei/Software/tvm/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(512)), "float32"), p1: T.Buffer((T.int64(1000), T.int64(512)), "float32"), p2: T.Buffer((T.int64(1), T.int64(1000)), "float32"), T_add: T.Buffer((T.int64(1), T.int64(1000)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        T_matmul_NT_local = T.alloc_buffer((T.int64(1), T.int64(1000)), scope="local")
        p0_shared = T.alloc_buffer((T.int64(1), T.int64(512)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(1000), T.int64(512)), scope="shared")
        for i_0_j_0_fused in T.thread_binding(T.int64(4), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 1024, "pragma_unroll_explicit": 1}):
            for i_1_j_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for i_2_j_2_fused in T.thread_binding(T.int64(50), thread="threadIdx.x"):
                    for i_3_init, j_3_init, i_4_init, j_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(5)):
                        with T.block("T_matmul_NT_init"):
                            v_i = T.axis.spatial(T.int64(1), i_3_init + i_4_init)
                            v_j = T.axis.spatial(T.int64(1000), i_0_j_0_fused * T.int64(250) + i_2_j_2_fused * T.int64(5) + j_3_init * T.int64(5) + j_4_init)
                            T.reads()
                            T.writes(T_matmul_NT_local[v_i, v_j])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            T_matmul_NT_local[v_i, v_j] = T.float32(0)
                    for k_0 in range(T.int64(32)):
                        for ax0_ax1_fused_0 in range(T.int64(1)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(50), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("p0_shared"):
                                        v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v1 = T.axis.spatial(T.int64(512), k_0 * T.int64(16) + (ax0_ax1_fused_0 * T.int64(200) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2))
                                        T.where((ax0_ax1_fused_0 * T.int64(50) + ax0_ax1_fused_1) * T.int64(4) + ax0_ax1_fused_2 < T.int64(16))
                                        T.reads(p0[v0, v1])
                                        T.writes(p0_shared[v0, v1])
                                        p0_shared[v0, v1] = p0[v0, v1]
                        for ax0_ax1_fused_0 in range(T.int64(80)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(50), thread="threadIdx.x"):
                                with T.block("p1_shared"):
                                    v0 = T.axis.spatial(T.int64(1000), i_0_j_0_fused * T.int64(250) + (ax0_ax1_fused_0 * T.int64(50) + ax0_ax1_fused_1) // T.int64(16))
                                    v1 = T.axis.spatial(T.int64(512), k_0 * T.int64(16) + (ax0_ax1_fused_0 * T.int64(50) + ax0_ax1_fused_1) % T.int64(16))
                                    T.reads(p1[v0, v1])
                                    T.writes(p1_shared[v0, v1])
                                    p1_shared[v0, v1] = p1[v0, v1]
                        for k_1, i_3, j_3, k_2, i_4, j_4 in T.grid(T.int64(8), T.int64(1), T.int64(1), T.int64(2), T.int64(1), T.int64(5)):
                            with T.block("T_matmul_NT_update"):
                                v_i = T.axis.spatial(T.int64(1), i_3 + i_4)
                                v_j = T.axis.spatial(T.int64(1000), i_0_j_0_fused * T.int64(250) + i_2_j_2_fused * T.int64(5) + j_3 * T.int64(5) + j_4)
                                v_k = T.axis.reduce(T.int64(512), k_0 * T.int64(16) + k_1 * T.int64(2) + k_2)
                                T.reads(T_matmul_NT_local[v_i, v_j], p0_shared[v_i, v_k], p1_shared[v_j, v_k])
                                T.writes(T_matmul_NT_local[v_i, v_j])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                T_matmul_NT_local[v_i, v_j] = T_matmul_NT_local[v_i, v_j] + p0_shared[v_i, v_k] * p1_shared[v_j, v_k]
                    for ax0, ax1 in T.grid(T.int64(1), T.int64(5)):
                        with T.block("T_matmul_NT_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(1000), i_0_j_0_fused * T.int64(250) + i_2_j_2_fused * T.int64(5) + ax1)
                            T.reads(T_matmul_NT_local[v0, v1], p2[v0, v1])
                            T.writes(T_add[v0, v1])
                            T_add[v0, v1] = T_matmul_NT_local[v0, v1] + p2[v0, v1]
b0 = sch.get_block(name="T_matmul_NT", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l3, l4, l5 = sch.get_loops(block=b0)
v6, v7, v8, v9, v10 = sch.sample_perfect_tile(loop=l3, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l11, l12, l13, l14, l15 = sch.split(loop=l3, factors=[v6, v7, v8, v9, v10], preserve_unit_iters=True)
v16, v17, v18, v19, v20 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[4, 1, 50, 1, 5])
l21, l22, l23, l24, l25 = sch.split(loop=l4, factors=[v16, v17, v18, v19, v20], preserve_unit_iters=True)
v26, v27, v28 = sch.sample_perfect_tile(loop=l5, n=3, max_innermost_factor=64, decision=[32, 8, 2])
l29, l30, l31 = sch.split(loop=l5, factors=[v26, v27, v28], preserve_unit_iters=True)
sch.reorder(l11, l21, l12, l22, l13, l23, l29, l30, l14, l24, l31, l15, l25)
l32 = sch.fuse(l11, l21, preserve_unit_iters=True)
sch.bind(loop=l32, thread_axis="blockIdx.x")
l33 = sch.fuse(l12, l22, preserve_unit_iters=True)
sch.bind(loop=l33, thread_axis="vthread.x")
l34 = sch.fuse(l13, l23, preserve_unit_iters=True)
sch.bind(loop=l34, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b35 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b35, loop=l34, preserve_unit_loops=True, index=-1)
b36 = sch.cache_read(block=b0, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b36, loop=l29, preserve_unit_loops=True, index=-1)
l37, l38, l39, l40, l41, l42 = sch.get_loops(block=b36)
l43 = sch.fuse(l41, l42, preserve_unit_iters=True)
v44 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b36, ann_key="meta_schedule.cooperative_fetch", ann_val=v44)
b45 = sch.cache_read(block=b0, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b45, loop=l29, preserve_unit_loops=True, index=-1)
l46, l47, l48, l49, l50, l51 = sch.get_loops(block=b45)
l52 = sch.fuse(l50, l51, preserve_unit_iters=True)
v53 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b45, ann_key="meta_schedule.cooperative_fetch", ann_val=v53)
sch.reverse_compute_inline(block=b1)
v54 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v54)
sch.enter_postproc()
sch.unannotate(block_or_loop=b36, ann_key="meta_schedule.cooperative_fetch")
l55, l56, l57, l58, l59 = sch.get_loops(block=b36)
l60, l61, l62 = sch.split(loop=l59, factors=[None, 50, 4], preserve_unit_iters=True)
sch.vectorize(loop=l62)
sch.bind(loop=l61, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b45, ann_key="meta_schedule.cooperative_fetch")
l63, l64, l65, l66, l67 = sch.get_loops(block=b45)
l68, l69 = sch.split(loop=l67, factors=[None, 50], preserve_unit_iters=True)
sch.bind(loop=l69, thread_axis="threadIdx.x")
b70 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b70, ann_key="meta_schedule.unroll_explicit")
b71, b72, b73, b74 = sch.get_child_blocks(b70)
l75, l76, l77, l78, l79, l80, l81 = sch.get_loops(block=b71)
l82, l83, l84, l85, l86, l87 = sch.get_loops(block=b72)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b73)
sch.annotate(block_or_loop=l88, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l88, ann_key="pragma_unroll_explicit", ann_val=1)
l98, l99, l100, l101, l102 = sch.get_loops(block=b74)
b103 = sch.get_block(name="T_matmul_NT", func_name="main")
l104, l105, l106, l107, l108, l109, l110, l111, l112, l113 = sch.get_loops(block=b103)
b114 = sch.decompose_reduction(block=b103, loop=l107)
2023-11-11 00:30:46 [INFO] [task_scheduler.cc:121] [Task #28: fused_nn_dense_add] Trial #26: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/home2/xiachunwei/Software/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/home2/xiachunwei/Software/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/home2/xiachunwei/Software/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/home2/xiachunwei/Software/tvm/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/home2/xiachunwei/Software/tvm/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/home2/xiachunwei/Software/tvm/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/home2/xiachunwei/Software/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/home2/xiachunwei/Software/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  4: _ZN3tvm7runtime13PackedFun
  3: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  2: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  0: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  File "/home2/xiachunwei/Software/tvm/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(512)), "float32"), p1: T.Buffer((T.int64(1000), T.int64(512)), "float32"), p2: T.Buffer((T.int64(1), T.int64(1000)), "float32"), T_add: T.Buffer((T.int64(1), T.int64(1000)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        T_matmul_NT_local = T.alloc_buffer((T.int64(1), T.int64(1000)), scope="local")
        p0_shared = T.alloc_buffer((T.int64(1), T.int64(512)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(1000), T.int64(512)), scope="shared")
        for i_0_j_0_fused in T.thread_binding(T.int64(2), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 512, "pragma_unroll_explicit": 1}):
            for i_1_j_1_fused in T.thread_binding(T.int64(2), thread="vthread.x"):
                for i_2_j_2_fused in T.thread_binding(T.int64(50), thread="threadIdx.x"):
                    for i_3_init, j_3_init, i_4_init, j_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(5)):
                        with T.block("T_matmul_NT_init"):
                            v_i = T.axis.spatial(T.int64(1), i_3_init + i_4_init)
                            v_j = T.axis.spatial(T.int64(1000), i_0_j_0_fused * T.int64(500) + i_1_j_1_fused * T.int64(250) + i_2_j_2_fused * T.int64(5) + j_3_init * T.int64(5) + j_4_init)
                            T.reads()
                            T.writes(T_matmul_NT_local[v_i, v_j])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            T_matmul_NT_local[v_i, v_j] = T.float32(0)
                    for k_0 in range(T.int64(64)):
                        for ax0_ax1_fused_0 in range(T.int64(1)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(50), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("p0_shared"):
                                        v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v1 = T.axis.spatial(T.int64(512), k_0 * T.int64(8) + (ax0_ax1_fused_0 * T.int64(200) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2))
                                        T.where((ax0_ax1_fused_0 * T.int64(50) + ax0_ax1_fused_1) * T.int64(4) + ax0_ax1_fused_2 < T.int64(8))
                                        T.reads(p0[v0, v1])
                                        T.writes(p0_shared[v0, v1])
                                        p0_shared[v0, v1] = p0[v0, v1]
                        for ax0_ax1_fused_0 in range(T.int64(20)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(50), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("p1_shared"):
                                        v0 = T.axis.spatial(T.int64(1000), i_0_j_0_fused * T.int64(500) + (ax0_ax1_fused_0 * T.int64(200) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2) // T.int64(8))
                                        v1 = T.axis.spatial(T.int64(512), k_0 * T.int64(8) + (ax0_ax1_fused_0 * T.int64(200) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2) % T.int64(8))
                                        T.reads(p1[v0, v1])
                                        T.writes(p1_shared[v0, v1])
                                        p1_shared[v0, v1] = p1[v0, v1]
                        for k_1, i_3, j_3, k_2, i_4, j_4 in T.grid(T.int64(8), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(5)):
                            with T.block("T_matmul_NT_update"):
                                v_i = T.axis.spatial(T.int64(1), i_3 + i_4)
                                v_j = T.axis.spatial(T.int64(1000), i_0_j_0_fused * T.int64(500) + i_1_j_1_fused * T.int64(250) + i_2_j_2_fused * T.int64(5) + j_3 * T.int64(5) + j_4)
                                v_k = T.axis.reduce(T.int64(512), k_0 * T.int64(8) + k_1 + k_2)
                                T.reads(T_matmul_NT_local[v_i, v_j], p0_shared[v_i, v_k], p1_shared[v_j, v_k])
                                T.writes(T_matmul_NT_local[v_i, v_j])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                T_matmul_NT_local[v_i, v_j] = T_matmul_NT_local[v_i, v_j] + p0_shared[v_i, v_k] * p1_shared[v_j, v_k]
                    for ax0, ax1 in T.grid(T.int64(1), T.int64(5)):
                        with T.block("T_matmul_NT_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(1000), i_0_j_0_fused * T.int64(500) + i_1_j_1_fused * T.int64(250) + i_2_j_2_fused * T.int64(5) + ax1)
                            T.reads(T_matmul_NT_local[v0, v1], p2[v0, v1])
                            T.writes(T_add[v0, v1])
                            T_add[v0, v1] = T_matmul_NT_local[v0, v1] + p2[v0, v1]
b0 = sch.get_block(name="T_matmul_NT", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l3, l4, l5 = sch.get_loops(block=b0)
v6, v7, v8, v9, v10 = sch.sample_perfect_tile(loop=l3, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l11, l12, l13, l14, l15 = sch.split(loop=l3, factors=[v6, v7, v8, v9, v10], preserve_unit_iters=True)
v16, v17, v18, v19, v20 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[2, 2, 50, 1, 5])
l21, l22, l23, l24, l25 = sch.split(loop=l4, factors=[v16, v17, v18, v19, v20], preserve_unit_iters=True)
v26, v27, v28 = sch.sample_perfect_tile(loop=l5, n=3, max_innermost_factor=64, decision=[64, 8, 1])
l29, l30, l31 = sch.split(loop=l5, factors=[v26, v27, v28], preserve_unit_iters=True)
sch.reorder(l11, l21, l12, l22, l13, l23, l29, l30, l14, l24, l31, l15, l25)
l32 = sch.fuse(l11, l21, preserve_unit_iters=True)
sch.bind(loop=l32, thread_axis="blockIdx.x")
l33 = sch.fuse(l12, l22, preserve_unit_iters=True)
sch.bind(loop=l33, thread_axis="vthread.x")
l34 = sch.fuse(l13, l23, preserve_unit_iters=True)
sch.bind(loop=l34, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b35 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b35, loop=l34, preserve_unit_loops=True, index=-1)
b36 = sch.cache_read(block=b0, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b36, loop=l29, preserve_unit_loops=True, index=-1)
l37, l38, l39, l40, l41, l42 = sch.get_loops(block=b36)
l43 = sch.fuse(l41, l42, preserve_unit_iters=True)
v44 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b36, ann_key="meta_schedule.cooperative_fetch", ann_val=v44)
b45 = sch.cache_read(block=b0, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b45, loop=l29, preserve_unit_loops=True, index=-1)
l46, l47, l48, l49, l50, l51 = sch.get_loops(block=b45)
l52 = sch.fuse(l50, l51, preserve_unit_iters=True)
v53 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b45, ann_key="meta_schedule.cooperative_fetch", ann_val=v53)
sch.reverse_compute_inline(block=b1)
v54 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v54)
sch.enter_postproc()
sch.unannotate(block_or_loop=b36, ann_key="meta_schedule.cooperative_fetch")
l55, l56, l57, l58, l59 = sch.get_loops(block=b36)
l60, l61, l62 = sch.split(loop=l59, factors=[None, 50, 4], preserve_unit_iters=True)
sch.vectorize(loop=l62)
sch.bind(loop=l61, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b45, ann_key="meta_schedule.cooperative_fetch")
l63, l64, l65, l66, l67 = sch.get_loops(block=b45)
l68, l69, l70 = sch.split(loop=l67, factors=[None, 50, 4], preserve_unit_iters=True)
sch.vectorize(loop=l70)
sch.bind(loop=l69, thread_axis="threadIdx.x")
b71 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b71, ann_key="meta_schedule.unroll_explicit")
b72, b73, b74, b75 = sch.get_child_blocks(b71)
l76, l77, l78, l79, l80, l81, l82 = sch.get_loops(block=b72)
l83, l84, l85, l86, l87, l88, l89 = sch.get_loops(block=b73)
l90, l91, l92, l93, l94, l95, l96, l97, l98, l99 = sch.get_loops(block=b74)
sch.annotate(block_or_loop=l90, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l90, ann_key="pragma_unroll_explicit", ann_val=1)
l100, l101, l102, l103, l104 = sch.get_loops(block=b75)
b105 = sch.get_block(name="T_matmul_NT", func_name="main")
l106, l107, l108, l109, l110, l111, l112, l113, l114, l115 = sch.get_loops(block=b105)
b116 = sch.decompose_reduction(block=b105, loop=l109)
2023-11-11 00:30:46 [INFO] [task_scheduler.cc:121] [Task #28: fused_nn_dense_add] Trial #27: Error in running:
LocalRunner: An exception occurred
Subprocess terminated
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(512)), "float32"), p1: T.Buffer((T.int64(1000), T.int64(512)), "float32"), p2: T.Buffer((T.int64(1), T.int64(1000)), "float32"), T_add: T.Buffer((T.int64(1), T.int64(1000)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        T_matmul_NT_local = T.alloc_buffer((T.int64(1), T.int64(1000)), scope="local")
        p0_shared = T.alloc_buffer((T.int64(1), T.int64(512)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(1000), T.int64(512)), scope="shared")
        for i_0_j_0_fused in T.thread_binding(T.int64(1), thread="blockIdx.x"):
            for i_1_j_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for i_2_j_2_fused in T.thread_binding(T.int64(100), thread="threadIdx.x"):
                    for i_3_init, j_3_init, i_4_init, j_4_init in T.grid(T.int64(1), T.int64(10), T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_init"):
                            v_i = T.axis.spatial(T.int64(1), i_3_init + i_4_init)
                            v_j = T.axis.spatial(T.int64(1000), i_2_j_2_fused * T.int64(10) + j_3_init + j_4_init)
                            T.reads()
                            T.writes(T_matmul_NT_local[v_i, v_j])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            T_matmul_NT_local[v_i, v_j] = T.float32(0)
                    for k_0_fused in T.serial(T.int64(128), annotations={"software_pipeline_async_stages": [0], "software_pipeline_order": [0, 1, 2], "software_pipeline_stage": [0, 0, 2]}):
                        for ax0_ax1_fused_0 in range(T.int64(1)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(100), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("p0_shared"):
                                        v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v1 = T.axis.spatial(T.int64(512), k_0_fused * T.int64(4) + (ax0_ax1_fused_0 * T.int64(400) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2))
                                        T.where((ax0_ax1_fused_0 * T.int64(100) + ax0_ax1_fused_1) * T.int64(4) + ax0_ax1_fused_2 < T.int64(4))
                                        T.reads(p0[v0, v1])
                                        T.writes(p0_shared[v0, v1])
                                        p0_shared[v0, v1] = p0[v0, v1]
                        for ax0_ax1_fused_0 in range(T.int64(20)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(100), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("p1_shared"):
                                        v0 = T.axis.spatial(T.int64(1000), (ax0_ax1_fused_0 * T.int64(200) + ax0_ax1_fused_1 * T.int64(2) + ax0_ax1_fused_2) // T.int64(4))
                                        v1 = T.axis.spatial(T.int64(512), k_0_fused * T.int64(4) + (ax0_ax1_fused_0 * T.int64(200) + ax0_ax1_fused_1 * T.int64(2) + ax0_ax1_fused_2) % T.int64(4))
                                        T.reads(p1[v0, v1])
                                        T.writes(p1_shared[v0, v1])
                                        p1_shared[v0, v1] = p1[v0, v1]
                        for k_1, i_3, j_3, k_2, i_4, j_4 in T.grid(T.int64(4), T.int64(1), T.int64(10), T.int64(1), T.int64(1), T.int64(1)):
                            with T.block("T_matmul_NT_update"):
                                v_i = T.axis.spatial(T.int64(1), i_3 + i_4)
                                v_j = T.axis.spatial(T.int64(1000), i_2_j_2_fused * T.int64(10) + j_3 + j_4)
                                v_k = T.axis.reduce(T.int64(512), k_0_fused * T.int64(4) + k_1 + k_2)
                                T.reads(T_matmul_NT_local[v_i, v_j], p0_shared[v_i, v_k], p1_shared[v_j, v_k])
                                T.writes(T_matmul_NT_local[v_i, v_j])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                T_matmul_NT_local[v_i, v_j] = T_matmul_NT_local[v_i, v_j] + p0_shared[v_i, v_k] * p1_shared[v_j, v_k]
                    for ax0, ax1 in T.grid(T.int64(1), T.int64(10)):
                        with T.block("T_matmul_NT_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(1000), i_2_j_2_fused * T.int64(10) + ax1)
                            T.reads(T_matmul_NT_local[v0, v1], p2[v0, v1])
                            T.writes(T_add[v0, v1])
                            T_add[v0, v1] = T_matmul_NT_local[v0, v1] + p2[v0, v1]
b0 = sch.get_block(name="T_matmul_NT", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l3, l4, l5 = sch.get_loops(block=b0)
v6, v7, v8, v9, v10 = sch.sample_perfect_tile(loop=l3, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l11, l12, l13, l14, l15 = sch.split(loop=l3, factors=[v6, v7, v8, v9, v10], preserve_unit_iters=True)
v16, v17, v18, v19, v20 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[1, 1, 100, 10, 1])
l21, l22, l23, l24, l25 = sch.split(loop=l4, factors=[v16, v17, v18, v19, v20], preserve_unit_iters=True)
v26, v27, v28 = sch.sample_perfect_tile(loop=l5, n=3, max_innermost_factor=64, decision=[128, 4, 1])
l29, l30, l31 = sch.split(loop=l5, factors=[v26, v27, v28], preserve_unit_iters=True)
sch.reorder(l11, l21, l12, l22, l13, l23, l29, l30, l14, l24, l31, l15, l25)
l32 = sch.fuse(l11, l21, preserve_unit_iters=True)
sch.bind(loop=l32, thread_axis="blockIdx.x")
l33 = sch.fuse(l12, l22, preserve_unit_iters=True)
sch.bind(loop=l33, thread_axis="vthread.x")
l34 = sch.fuse(l13, l23, preserve_unit_iters=True)
sch.bind(loop=l34, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b35 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b35, loop=l34, preserve_unit_loops=True, index=-1)
b36 = sch.cache_read(block=b0, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b36, loop=l29, preserve_unit_loops=True, index=-1)
l37, l38, l39, l40, l41, l42 = sch.get_loops(block=b36)
l43 = sch.fuse(l41, l42, preserve_unit_iters=True)
v44 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b36, ann_key="meta_schedule.cooperative_fetch", ann_val=v44)
b45 = sch.cache_read(block=b0, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b45, loop=l29, preserve_unit_loops=True, index=-1)
l46, l47, l48, l49, l50, l51 = sch.get_loops(block=b45)
l52 = sch.fuse(l50, l51, preserve_unit_iters=True)
v53 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b45, ann_key="meta_schedule.cooperative_fetch", ann_val=v53)
l54 = sch.fuse(l29, preserve_unit_iters=True)
sch.annotate(block_or_loop=l54, ann_key="software_pipeline_stage", ann_val=[0, 0, 2])
sch.annotate(block_or_loop=l54, ann_key="software_pipeline_order", ann_val=[0, 1, 2])
sch.annotate(block_or_loop=l54, ann_key="software_pipeline_async_stages", ann_val=[0])
sch.reverse_compute_inline(block=b1)
v55 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=0)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v55)
sch.enter_postproc()
sch.unannotate(block_or_loop=b36, ann_key="meta_schedule.cooperative_fetch")
l56, l57, l58, l59, l60 = sch.get_loops(block=b36)
l61, l62, l63 = sch.split(loop=l60, factors=[None, 100, 4], preserve_unit_iters=True)
sch.vectorize(loop=l63)
sch.bind(loop=l62, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b45, ann_key="meta_schedule.cooperative_fetch")
l64, l65, l66, l67, l68 = sch.get_loops(block=b45)
l69, l70, l71 = sch.split(loop=l68, factors=[None, 100, 2], preserve_unit_iters=True)
sch.vectorize(loop=l71)
sch.bind(loop=l70, thread_axis="threadIdx.x")
b72 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b72, ann_key="meta_schedule.unroll_explicit")
b73, b74, b75, b76 = sch.get_child_blocks(b72)
l77, l78, l79, l80, l81, l82, l83 = sch.get_loops(block=b73)
l84, l85, l86, l87, l88, l89, l90 = sch.get_loops(block=b74)
l91, l92, l93, l94, l95, l96, l97, l98, l99, l100 = sch.get_loops(block=b75)
l101, l102, l103, l104, l105 = sch.get_loops(block=b76)
b106 = sch.get_block(name="T_matmul_NT", func_name="main")
l107, l108, l109, l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b106)
b117 = sch.decompose_reduction(block=b106, loop=l110)
2023-11-11 00:30:46 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #28: GFLOPs: 38.5032. Time: 26.6212 us. Best GFLOPs: 41.8704
2023-11-11 00:30:46 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #29: GFLOPs: 13.1100. Time: 78.1849 us. Best GFLOPs: 41.8704
2023-11-11 00:30:46 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #30: GFLOPs: 6.0551. Time: 169.2782 us. Best GFLOPs: 41.8704
2023-11-11 00:30:46 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #31: GFLOPs: 14.0892. Time: 72.7506 us. Best GFLOPs: 41.8704
2023-11-11 00:30:46 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #32: GFLOPs: 4.8002. Time: 213.5332 us. Best GFLOPs: 41.8704
2023-11-11 00:30:46 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #33: GFLOPs: 21.1587. Time: 48.4434 us. Best GFLOPs: 41.8704
2023-11-11 00:30:46 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #34: GFLOPs: 28.3619. Time: 36.1401 us. Best GFLOPs: 41.8704
2023-11-11 00:30:46 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #35: GFLOPs: 5.8808. Time: 174.2970 us. Best GFLOPs: 41.8704
2023-11-11 00:30:46 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #36: GFLOPs: 12.8957. Time: 79.4841 us. Best GFLOPs: 41.8704
2023-11-11 00:30:46 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #37: GFLOPs: 16.6834. Time: 61.4384 us. Best GFLOPs: 41.8704
2023-11-11 00:30:46 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #38: GFLOPs: 14.0842. Time: 72.7766 us. Best GFLOPs: 41.8704
2023-11-11 00:30:46 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #39: GFLOPs: 3.1396. Time: 326.4711 us. Best GFLOPs: 41.8704
2023-11-11 00:30:46 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #40: GFLOPs: 41.9944. Time: 24.4080 us. Best GFLOPs: 41.9944
2023-11-11 00:30:46 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #41: GFLOPs: 6.0603. Time: 169.1334 us. Best GFLOPs: 41.9944
2023-11-11 00:30:46 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #42: GFLOPs: 80.9361. Time: 12.6643 us. Best GFLOPs: 80.9361
2023-11-11 00:30:46 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #43: GFLOPs: 24.7497. Time: 41.4147 us. Best GFLOPs: 80.9361
2023-11-11 00:30:46 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #44: GFLOPs: 44.2565. Time: 23.1604 us. Best GFLOPs: 80.9361
2023-11-11 00:30:46 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #45: GFLOPs: 27.5913. Time: 37.1494 us. Best GFLOPs: 80.9361
2023-11-11 00:30:46 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #46: GFLOPs: 15.1957. Time: 67.4534 us. Best GFLOPs: 80.9361
2023-11-11 00:30:46 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #47: GFLOPs: 3.0935. Time: 331.3449 us. Best GFLOPs: 80.9361
2023-11-11 00:30:46 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #48: GFLOPs: 9.6625. Time: 106.0799 us. Best GFLOPs: 80.9361
2023-11-11 00:30:46 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #49: GFLOPs: 24.5868. Time: 41.6890 us. Best GFLOPs: 80.9361
2023-11-11 00:30:46 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #50: GFLOPs: 13.1906. Time: 77.7067 us. Best GFLOPs: 80.9361
2023-11-11 00:30:46 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #51: GFLOPs: 14.4014. Time: 71.1735 us. Best GFLOPs: 80.9361
2023-11-11 00:30:46 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #52: GFLOPs: 10.8775. Time: 94.2316 us. Best GFLOPs: 80.9361
2023-11-11 00:30:46 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #53: GFLOPs: 14.1447. Time: 72.4655 us. Best GFLOPs: 80.9361
2023-11-11 00:30:46 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #54: GFLOPs: 13.2071. Time: 77.6095 us. Best GFLOPs: 80.9361
2023-11-11 00:30:46 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #55: GFLOPs: 12.8699. Time: 79.6431 us. Best GFLOPs: 80.9361
2023-11-11 00:30:46 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #56: GFLOPs: 22.1891. Time: 46.1938 us. Best GFLOPs: 80.9361
2023-11-11 00:30:46 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #57: GFLOPs: 17.5314. Time: 58.4665 us. Best GFLOPs: 80.9361
2023-11-11 00:30:46 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #58: GFLOPs: 29.1736. Time: 35.1345 us. Best GFLOPs: 80.9361
2023-11-11 00:30:46 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #59: GFLOPs: 10.3988. Time: 98.5692 us. Best GFLOPs: 80.9361
2023-11-11 00:30:46 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #60: GFLOPs: 24.5717. Time: 41.7146 us. Best GFLOPs: 80.9361
2023-11-11 00:30:46 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #61: GFLOPs: 3.1458. Time: 325.8306 us. Best GFLOPs: 80.9361
2023-11-11 00:30:46 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #62: GFLOPs: 32.2691. Time: 31.7641 us. Best GFLOPs: 80.9361
2023-11-11 00:30:46 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #63: GFLOPs: 5.7540. Time: 178.1362 us. Best GFLOPs: 80.9361
2023-11-11 02:09:14 [INFO] [evolutionary_search.cc:713] Generating candidates......
2023-11-11 02:09:14 [INFO] [evolutionary_search.cc:715] Picked top 56 candidate(s) from database
2023-11-11 02:09:15 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310a405238)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310e76ec68)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310dc33cd8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310be916f8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56310f803f18)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310eeb7a68)]: 451 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310e771ba8)]: 0 failure(s)
2023-11-11 02:09:16 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310a405238)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310e76ec68)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310dc33cd8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310be916f8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56310f803f18)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310eeb7a68)]: 898 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310e771ba8)]: 0 failure(s)
2023-11-11 02:09:16 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310a405238)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310e76ec68)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310dc33cd8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310be916f8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56310f803f18)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310eeb7a68)]: 1342 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310e771ba8)]: 0 failure(s)
2023-11-11 02:09:17 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310a405238)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310e76ec68)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310dc33cd8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310be916f8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56310f803f18)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310eeb7a68)]: 1793 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310e771ba8)]: 0 failure(s)
2023-11-11 02:09:18 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310a405238)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310e76ec68)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310dc33cd8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310be916f8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56310f803f18)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310eeb7a68)]: 2244 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310e771ba8)]: 0 failure(s)
2023-11-11 02:09:18 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310a405238)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310e76ec68)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310dc33cd8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310be916f8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56310f803f18)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310eeb7a68)]: 2691 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310e771ba8)]: 0 failure(s)
2023-11-11 02:09:19 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310a405238)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310e76ec68)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310dc33cd8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310be916f8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56310f803f18)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310eeb7a68)]: 3137 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310e771ba8)]: 0 failure(s)
2023-11-11 02:09:19 [INFO] [evolutionary_search.cc:723] Sampled 55 candidate(s)
2023-11-11 02:09:21 [INFO] [evolutionary_search.cc:621] Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310a405238)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310e76ec68)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310dc33cd8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310be916f8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56310f803f18)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310eeb7a68)]: 118 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310e771ba8)]: 0 failure(s)
2023-11-11 02:09:23 [INFO] [evolutionary_search.cc:621] Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310a405238)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310e76ec68)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310dc33cd8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310be916f8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56310f803f18)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310eeb7a68)]: 108 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310e771ba8)]: 0 failure(s)
2023-11-11 02:09:26 [INFO] [evolutionary_search.cc:621] Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310a405238)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310e76ec68)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310dc33cd8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310be916f8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56310f803f18)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310eeb7a68)]: 129 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310e771ba8)]: 0 failure(s)
2023-11-11 02:09:28 [INFO] [evolutionary_search.cc:621] Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310a405238)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310e76ec68)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310dc33cd8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310be916f8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56310f803f18)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310eeb7a68)]: 117 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310e771ba8)]: 0 failure(s)
2023-11-11 02:09:29 [INFO] [evolutionary_search.cc:649] Scores of the best 64 candidates:
[1 : 16]:	2.0177  1.9594  1.9594  1.9581  1.9397  1.9233  1.9233  1.8801  1.8210  1.8210  1.8167  1.8167  1.8094  1.7656  1.7656  1.7656
[17 : 32]:	1.7305  1.7285  1.7285  1.7196  1.7196  1.7104  1.7104  1.7104  1.6929  1.6859  1.6630  1.6630  1.6630  1.6588  1.6038  1.5945
[33 : 48]:	1.5945  1.5923  1.5923  1.5923  1.5797  1.5797  1.5797  1.5754  1.5754  1.5720  1.5720  1.5706  1.5663  1.5663  1.5601  1.5371
[49 : 64]:	1.5371  1.5308  1.5168  1.4961  1.4487  1.4442  1.4361  1.4081  1.3920  1.3834  1.3733  1.3711  1.3644  1.3527  1.3276  1.2988
2023-11-11 02:09:29 [INFO] [evolutionary_search.cc:727] Got 64 candidate(s) with evolutionary search
2023-11-11 02:09:29 [INFO] [evolutionary_search.cc:730] Sending 64 candidates(s) for measurement
2023-11-11 02:09:57 [INFO] [task_scheduler.cc:121] [Task #28: fused_nn_dense_add] Trial #64: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/home2/xiachunwei/Software/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/home2/xiachunwei/Software/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 159, in _worker_func
    costs: List[float] = f_run_evaluator(
  File "/home2/xiachunwei/Software/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 387, in default_run_evaluator
    return run_evaluator_common(rt_mod, device, evaluator_config, repeated_args)
  File "/home2/xiachunwei/Software/tvm/python/tvm/meta_schedule/runner/utils.py", line 117, in run_evaluator_common
    profile_result = evaluator(*args)
  File "/home2/xiachunwei/Software/tvm/python/tvm/runtime/module.py", line 397, in evaluator
    blob = feval(*args)
  File "/home2/xiachunwei/Software/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/home2/xiachunwei/Software/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  1: tvm::runtime::profiling::WrapTimeEvaluator(tvm::runtime::PackedFunc, DLDevice, int, int, int, int, int, int, int, tvm::runtime::PackedFunc)::{lambda(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*) [clone .isra.779]
  0: tvm::runtime::CUDADeviceAPI::StreamSync(DLDevice, void*)
  File "/home2/xiachunwei/Software/tvm/src/runtime/cuda/cuda_device_api.cc", line 212
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(512)), "float32"), p1: T.Buffer((T.int64(1000), T.int64(512)), "float32"), p2: T.Buffer((T.int64(1), T.int64(1000)), "float32"), T_add: T.Buffer((T.int64(1), T.int64(1000)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        T_matmul_NT_local = T.alloc_buffer((T.int64(1), T.int64(1000)), scope="local")
        p0_shared = T.alloc_buffer((T.int64(1), T.int64(512)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(1000), T.int64(512)), scope="shared")
        for i_0_j_0_fused in T.thread_binding(T.int64(20), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 512, "pragma_unroll_explicit": 1}):
            for i_1_j_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for i_2_j_2_fused in T.thread_binding(T.int64(50), thread="threadIdx.x"):
                    for i_3_init, j_3_init, i_4_init, j_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_init"):
                            v_i = T.axis.spatial(T.int64(1), i_3_init + i_4_init)
                            v_j = T.axis.spatial(T.int64(1000), i_0_j_0_fused * T.int64(50) + i_2_j_2_fused + j_3_init + j_4_init)
                            T.reads()
                            T.writes(T_matmul_NT_local[v_i, v_j])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            T_matmul_NT_local[v_i, v_j] = T.float32(0)
                    for k_0_fused in T.serial(T.int64(256), annotations={"software_pipeline_async_stages": [0], "software_pipeline_order": [0, 1, 2], "software_pipeline_stage": [0, 0, 2]}):
                        for ax0_ax1_fused_0 in range(T.int64(1)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(50), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("p0_shared"):
                                        v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v1 = T.axis.spatial(T.int64(512), k_0_fused * T.int64(2) + (ax0_ax1_fused_0 * T.int64(100) + ax0_ax1_fused_1 * T.int64(2) + ax0_ax1_fused_2))
                                        T.where((ax0_ax1_fused_0 * T.int64(50) + ax0_ax1_fused_1) * T.int64(2) + ax0_ax1_fused_2 < T.int64(2))
                                        T.reads(p0[v0, v1])
                                        T.writes(p0_shared[v0, v1])
                                        p0_shared[v0, v1] = p0[v0, v1]
                        for ax0_ax1_fused_0 in range(T.int64(1)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(50), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("p1_shared"):
                                        v0 = T.axis.spatial(T.int64(1000), i_0_j_0_fused * T.int64(50) + (ax0_ax1_fused_0 * T.int64(200) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2) // T.int64(2))
                                        v1 = T.axis.spatial(T.int64(512), k_0_fused * T.int64(2) + (ax0_ax1_fused_0 * T.int64(200) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2) % T.int64(2))
                                        T.where((ax0_ax1_fused_0 * T.int64(50) + ax0_ax1_fused_1) * T.int64(4) + ax0_ax1_fused_2 < T.int64(100))
                                        T.reads(p1[v0, v1])
                                        T.writes(p1_shared[v0, v1])
                                        p1_shared[v0, v1] = p1[v0, v1]
                        for k_1, i_3, j_3, k_2, i_4, j_4 in T.grid(T.int64(2), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                            with T.block("T_matmul_NT_update"):
                                v_i = T.axis.spatial(T.int64(1), i_3 + i_4)
                                v_j = T.axis.spatial(T.int64(1000), i_0_j_0_fused * T.int64(50) + i_2_j_2_fused + j_3 + j_4)
                                v_k = T.axis.reduce(T.int64(512), k_0_fused * T.int64(2) + k_1 + k_2)
                                T.reads(T_matmul_NT_local[v_i, v_j], p0_shared[v_i, v_k], p1_shared[v_j, v_k])
                                T.writes(T_matmul_NT_local[v_i, v_j])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                T_matmul_NT_local[v_i, v_j] = T_matmul_NT_local[v_i, v_j] + p0_shared[v_i, v_k] * p1_shared[v_j, v_k]
                    for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(1000), i_0_j_0_fused * T.int64(50) + i_2_j_2_fused + ax1)
                            T.reads(T_matmul_NT_local[v0, v1], p2[v0, v1])
                            T.writes(T_add[v0, v1])
                            T_add[v0, v1] = T_matmul_NT_local[v0, v1] + p2[v0, v1]
b0 = sch.get_block(name="T_matmul_NT", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l3, l4, l5 = sch.get_loops(block=b0)
v6, v7, v8, v9, v10 = sch.sample_perfect_tile(loop=l3, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l11, l12, l13, l14, l15 = sch.split(loop=l3, factors=[v6, v7, v8, v9, v10], preserve_unit_iters=True)
v16, v17, v18, v19, v20 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[20, 1, 50, 1, 1])
l21, l22, l23, l24, l25 = sch.split(loop=l4, factors=[v16, v17, v18, v19, v20], preserve_unit_iters=True)
v26, v27, v28 = sch.sample_perfect_tile(loop=l5, n=3, max_innermost_factor=64, decision=[256, 2, 1])
l29, l30, l31 = sch.split(loop=l5, factors=[v26, v27, v28], preserve_unit_iters=True)
sch.reorder(l11, l21, l12, l22, l13, l23, l29, l30, l14, l24, l31, l15, l25)
l32 = sch.fuse(l11, l21, preserve_unit_iters=True)
sch.bind(loop=l32, thread_axis="blockIdx.x")
l33 = sch.fuse(l12, l22, preserve_unit_iters=True)
sch.bind(loop=l33, thread_axis="vthread.x")
l34 = sch.fuse(l13, l23, preserve_unit_iters=True)
sch.bind(loop=l34, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b35 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b35, loop=l34, preserve_unit_loops=True, index=-1)
b36 = sch.cache_read(block=b0, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b36, loop=l29, preserve_unit_loops=True, index=-1)
l37, l38, l39, l40, l41, l42 = sch.get_loops(block=b36)
l43 = sch.fuse(l41, l42, preserve_unit_iters=True)
v44 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b36, ann_key="meta_schedule.cooperative_fetch", ann_val=v44)
b45 = sch.cache_read(block=b0, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b45, loop=l29, preserve_unit_loops=True, index=-1)
l46, l47, l48, l49, l50, l51 = sch.get_loops(block=b45)
l52 = sch.fuse(l50, l51, preserve_unit_iters=True)
v53 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b45, ann_key="meta_schedule.cooperative_fetch", ann_val=v53)
l54 = sch.fuse(l29, preserve_unit_iters=True)
sch.annotate(block_or_loop=l54, ann_key="software_pipeline_stage", ann_val=[0, 0, 2])
sch.annotate(block_or_loop=l54, ann_key="software_pipeline_order", ann_val=[0, 1, 2])
sch.annotate(block_or_loop=l54, ann_key="software_pipeline_async_stages", ann_val=[0])
sch.reverse_compute_inline(block=b1)
v55 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v55)
sch.enter_postproc()
sch.unannotate(block_or_loop=b36, ann_key="meta_schedule.cooperative_fetch")
l56, l57, l58, l59, l60 = sch.get_loops(block=b36)
l61, l62, l63 = sch.split(loop=l60, factors=[None, 50, 2], preserve_unit_iters=True)
sch.vectorize(loop=l63)
sch.bind(loop=l62, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b45, ann_key="meta_schedule.cooperative_fetch")
l64, l65, l66, l67, l68 = sch.get_loops(block=b45)
l69, l70, l71 = sch.split(loop=l68, factors=[None, 50, 4], preserve_unit_iters=True)
sch.vectorize(loop=l71)
sch.bind(loop=l70, thread_axis="threadIdx.x")
b72 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b72, ann_key="meta_schedule.unroll_explicit")
b73, b74, b75, b76 = sch.get_child_blocks(b72)
l77, l78, l79, l80, l81, l82, l83 = sch.get_loops(block=b73)
l84, l85, l86, l87, l88, l89, l90 = sch.get_loops(block=b74)
l91, l92, l93, l94, l95, l96, l97, l98, l99, l100 = sch.get_loops(block=b75)
sch.annotate(block_or_loop=l91, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l91, ann_key="pragma_unroll_explicit", ann_val=1)
l101, l102, l103, l104, l105 = sch.get_loops(block=b76)
b106 = sch.get_block(name="T_matmul_NT", func_name="main")
l107, l108, l109, l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b106)
b117 = sch.decompose_reduction(block=b106, loop=l110)
2023-11-11 02:09:57 [INFO] [task_scheduler.cc:121] [Task #28: fused_nn_dense_add] Trial #65: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/home2/xiachunwei/Software/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/home2/xiachunwei/Software/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/home2/xiachunwei/Software/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/home2/xiachunwei/Software/tvm/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/home2/xiachunwei/Software/tvm/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/home2/xiachunwei/Software/tvm/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/home2/xiachunwei/Software/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/home2/xiachunwei/Software/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  4: _ZN3tvm7runtime13PackedFun
  3: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  2: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  0: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  File "/home2/xiachunwei/Software/tvm/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(512)), "float32"), p1: T.Buffer((T.int64(1000), T.int64(512)), "float32"), p2: T.Buffer((T.int64(1), T.int64(1000)), "float32"), T_add: T.Buffer((T.int64(1), T.int64(1000)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        T_matmul_NT_local = T.alloc_buffer((T.int64(1), T.int64(1000)), scope="local")
        p0_shared = T.alloc_buffer((T.int64(1), T.int64(512)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(1000), T.int64(512)), scope="shared")
        for i_0_j_0_fused in T.thread_binding(T.int64(25), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 512, "pragma_unroll_explicit": 1}):
            for i_1_j_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for i_2_j_2_fused in T.thread_binding(T.int64(40), thread="threadIdx.x"):
                    for i_3_init, j_3_init, i_4_init, j_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_init"):
                            v_i = T.axis.spatial(T.int64(1), i_3_init + i_4_init)
                            v_j = T.axis.spatial(T.int64(1000), i_0_j_0_fused * T.int64(40) + i_2_j_2_fused + j_3_init + j_4_init)
                            T.reads()
                            T.writes(T_matmul_NT_local[v_i, v_j])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            T_matmul_NT_local[v_i, v_j] = T.float32(0)
                    for k_0_fused in T.serial(T.int64(256), annotations={"software_pipeline_async_stages": [0], "software_pipeline_order": [0, 1, 2], "software_pipeline_stage": [0, 0, 3]}):
                        for ax0_ax1_fused_0 in range(T.int64(1)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(40), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("p0_shared"):
                                        v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v1 = T.axis.spatial(T.int64(512), k_0_fused * T.int64(2) + (ax0_ax1_fused_0 * T.int64(80) + ax0_ax1_fused_1 * T.int64(2) + ax0_ax1_fused_2))
                                        T.where((ax0_ax1_fused_0 * T.int64(40) + ax0_ax1_fused_1) * T.int64(2) + ax0_ax1_fused_2 < T.int64(2))
                                        T.reads(p0[v0, v1])
                                        T.writes(p0_shared[v0, v1])
                                        p0_shared[v0, v1] = p0[v0, v1]
                        for ax0_ax1_fused_0 in range(T.int64(1)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(40), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("p1_shared"):
                                        v0 = T.axis.spatial(T.int64(1000), i_0_j_0_fused * T.int64(40) + (ax0_ax1_fused_0 * T.int64(160) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2) // T.int64(2))
                                        v1 = T.axis.spatial(T.int64(512), k_0_fused * T.int64(2) + (ax0_ax1_fused_0 * T.int64(160) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2) % T.int64(2))
                                        T.where((ax0_ax1_fused_0 * T.int64(40) + ax0_ax1_fused_1) * T.int64(4) + ax0_ax1_fused_2 < T.int64(80))
                                        T.reads(p1[v0, v1])
                                        T.writes(p1_shared[v0, v1])
                                        p1_shared[v0, v1] = p1[v0, v1]
                        for k_1, i_3, j_3, k_2, i_4, j_4 in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(1), T.int64(1)):
                            with T.block("T_matmul_NT_update"):
                                v_i = T.axis.spatial(T.int64(1), i_3 + i_4)
                                v_j = T.axis.spatial(T.int64(1000), i_0_j_0_fused * T.int64(40) + i_2_j_2_fused + j_3 + j_4)
                                v_k = T.axis.reduce(T.int64(512), k_0_fused * T.int64(2) + k_1 * T.int64(2) + k_2)
                                T.reads(T_matmul_NT_local[v_i, v_j], p0_shared[v_i, v_k], p1_shared[v_j, v_k])
                                T.writes(T_matmul_NT_local[v_i, v_j])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                T_matmul_NT_local[v_i, v_j] = T_matmul_NT_local[v_i, v_j] + p0_shared[v_i, v_k] * p1_shared[v_j, v_k]
                    for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(1000), i_0_j_0_fused * T.int64(40) + i_2_j_2_fused + ax1)
                            T.reads(T_matmul_NT_local[v0, v1], p2[v0, v1])
                            T.writes(T_add[v0, v1])
                            T_add[v0, v1] = T_matmul_NT_local[v0, v1] + p2[v0, v1]
b0 = sch.get_block(name="T_matmul_NT", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l3, l4, l5 = sch.get_loops(block=b0)
v6, v7, v8, v9, v10 = sch.sample_perfect_tile(loop=l3, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l11, l12, l13, l14, l15 = sch.split(loop=l3, factors=[v6, v7, v8, v9, v10], preserve_unit_iters=True)
v16, v17, v18, v19, v20 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[25, 1, 40, 1, 1])
l21, l22, l23, l24, l25 = sch.split(loop=l4, factors=[v16, v17, v18, v19, v20], preserve_unit_iters=True)
v26, v27, v28 = sch.sample_perfect_tile(loop=l5, n=3, max_innermost_factor=64, decision=[256, 1, 2])
l29, l30, l31 = sch.split(loop=l5, factors=[v26, v27, v28], preserve_unit_iters=True)
sch.reorder(l11, l21, l12, l22, l13, l23, l29, l30, l14, l24, l31, l15, l25)
l32 = sch.fuse(l11, l21, preserve_unit_iters=True)
sch.bind(loop=l32, thread_axis="blockIdx.x")
l33 = sch.fuse(l12, l22, preserve_unit_iters=True)
sch.bind(loop=l33, thread_axis="vthread.x")
l34 = sch.fuse(l13, l23, preserve_unit_iters=True)
sch.bind(loop=l34, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b35 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b35, loop=l34, preserve_unit_loops=True, index=-1)
b36 = sch.cache_read(block=b0, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b36, loop=l29, preserve_unit_loops=True, index=-1)
l37, l38, l39, l40, l41, l42 = sch.get_loops(block=b36)
l43 = sch.fuse(l41, l42, preserve_unit_iters=True)
v44 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b36, ann_key="meta_schedule.cooperative_fetch", ann_val=v44)
b45 = sch.cache_read(block=b0, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b45, loop=l29, preserve_unit_loops=True, index=-1)
l46, l47, l48, l49, l50, l51 = sch.get_loops(block=b45)
l52 = sch.fuse(l50, l51, preserve_unit_iters=True)
v53 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b45, ann_key="meta_schedule.cooperative_fetch", ann_val=v53)
l54 = sch.fuse(l29, preserve_unit_iters=True)
sch.annotate(block_or_loop=l54, ann_key="software_pipeline_stage", ann_val=[0, 0, 3])
sch.annotate(block_or_loop=l54, ann_key="software_pipeline_order", ann_val=[0, 1, 2])
sch.annotate(block_or_loop=l54, ann_key="software_pipeline_async_stages", ann_val=[0])
sch.reverse_compute_inline(block=b1)
v55 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v55)
sch.enter_postproc()
sch.unannotate(block_or_loop=b36, ann_key="meta_schedule.cooperative_fetch")
l56, l57, l58, l59, l60 = sch.get_loops(block=b36)
l61, l62, l63 = sch.split(loop=l60, factors=[None, 40, 2], preserve_unit_iters=True)
sch.vectorize(loop=l63)
sch.bind(loop=l62, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b45, ann_key="meta_schedule.cooperative_fetch")
l64, l65, l66, l67, l68 = sch.get_loops(block=b45)
l69, l70, l71 = sch.split(loop=l68, factors=[None, 40, 4], preserve_unit_iters=True)
sch.vectorize(loop=l71)
sch.bind(loop=l70, thread_axis="threadIdx.x")
b72 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b72, ann_key="meta_schedule.unroll_explicit")
b73, b74, b75, b76 = sch.get_child_blocks(b72)
l77, l78, l79, l80, l81, l82, l83 = sch.get_loops(block=b73)
l84, l85, l86, l87, l88, l89, l90 = sch.get_loops(block=b74)
l91, l92, l93, l94, l95, l96, l97, l98, l99, l100 = sch.get_loops(block=b75)
sch.annotate(block_or_loop=l91, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l91, ann_key="pragma_unroll_explicit", ann_val=1)
l101, l102, l103, l104, l105 = sch.get_loops(block=b76)
b106 = sch.get_block(name="T_matmul_NT", func_name="main")
l107, l108, l109, l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b106)
b117 = sch.decompose_reduction(block=b106, loop=l110)
2023-11-11 02:09:57 [INFO] [task_scheduler.cc:121] [Task #28: fused_nn_dense_add] Trial #66: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/home2/xiachunwei/Software/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/home2/xiachunwei/Software/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/home2/xiachunwei/Software/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/home2/xiachunwei/Software/tvm/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/home2/xiachunwei/Software/tvm/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/home2/xiachunwei/Software/tvm/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/home2/xiachunwei/Software/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/home2/xiachunwei/Software/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  4: _ZN3tvm7runtime13PackedFun
  3: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  2: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  0: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  File "/home2/xiachunwei/Software/tvm/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(512)), "float32"), p1: T.Buffer((T.int64(1000), T.int64(512)), "float32"), p2: T.Buffer((T.int64(1), T.int64(1000)), "float32"), T_add: T.Buffer((T.int64(1), T.int64(1000)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        T_matmul_NT_local = T.alloc_buffer((T.int64(1), T.int64(1000)), scope="local")
        p0_shared = T.alloc_buffer((T.int64(1), T.int64(512)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(1000), T.int64(512)), scope="shared")
        for i_0_j_0_fused in T.thread_binding(T.int64(25), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 512, "pragma_unroll_explicit": 1}):
            for i_1_j_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for i_2_j_2_fused in T.thread_binding(T.int64(40), thread="threadIdx.x"):
                    for i_3_init, j_3_init, i_4_init, j_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_init"):
                            v_i = T.axis.spatial(T.int64(1), i_3_init + i_4_init)
                            v_j = T.axis.spatial(T.int64(1000), i_0_j_0_fused * T.int64(40) + i_2_j_2_fused + j_3_init + j_4_init)
                            T.reads()
                            T.writes(T_matmul_NT_local[v_i, v_j])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            T_matmul_NT_local[v_i, v_j] = T.float32(0)
                    for k_0_fused in T.serial(T.int64(256), annotations={"software_pipeline_async_stages": [0], "software_pipeline_order": [0, 1, 2], "software_pipeline_stage": [0, 0, 2]}):
                        for ax0_ax1_fused_0 in range(T.int64(1)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(40), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("p0_shared"):
                                        v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v1 = T.axis.spatial(T.int64(512), k_0_fused * T.int64(2) + (ax0_ax1_fused_0 * T.int64(80) + ax0_ax1_fused_1 * T.int64(2) + ax0_ax1_fused_2))
                                        T.where((ax0_ax1_fused_0 * T.int64(40) + ax0_ax1_fused_1) * T.int64(2) + ax0_ax1_fused_2 < T.int64(2))
                                        T.reads(p0[v0, v1])
                                        T.writes(p0_shared[v0, v1])
                                        p0_shared[v0, v1] = p0[v0, v1]
                        for ax0_ax1_fused_0 in range(T.int64(1)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(40), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("p1_shared"):
                                        v0 = T.axis.spatial(T.int64(1000), i_0_j_0_fused * T.int64(40) + (ax0_ax1_fused_0 * T.int64(160) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2) // T.int64(2))
                                        v1 = T.axis.spatial(T.int64(512), k_0_fused * T.int64(2) + (ax0_ax1_fused_0 * T.int64(160) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2) % T.int64(2))
                                        T.where((ax0_ax1_fused_0 * T.int64(40) + ax0_ax1_fused_1) * T.int64(4) + ax0_ax1_fused_2 < T.int64(80))
                                        T.reads(p1[v0, v1])
                                        T.writes(p1_shared[v0, v1])
                                        p1_shared[v0, v1] = p1[v0, v1]
                        for k_1, i_3, j_3, k_2, i_4, j_4 in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(1), T.int64(1)):
                            with T.block("T_matmul_NT_update"):
                                v_i = T.axis.spatial(T.int64(1), i_3 + i_4)
                                v_j = T.axis.spatial(T.int64(1000), i_0_j_0_fused * T.int64(40) + i_2_j_2_fused + j_3 + j_4)
                                v_k = T.axis.reduce(T.int64(512), k_0_fused * T.int64(2) + k_1 * T.int64(2) + k_2)
                                T.reads(T_matmul_NT_local[v_i, v_j], p0_shared[v_i, v_k], p1_shared[v_j, v_k])
                                T.writes(T_matmul_NT_local[v_i, v_j])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                T_matmul_NT_local[v_i, v_j] = T_matmul_NT_local[v_i, v_j] + p0_shared[v_i, v_k] * p1_shared[v_j, v_k]
                    for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(1000), i_0_j_0_fused * T.int64(40) + i_2_j_2_fused + ax1)
                            T.reads(T_matmul_NT_local[v0, v1], p2[v0, v1])
                            T.writes(T_add[v0, v1])
                            T_add[v0, v1] = T_matmul_NT_local[v0, v1] + p2[v0, v1]
b0 = sch.get_block(name="T_matmul_NT", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l3, l4, l5 = sch.get_loops(block=b0)
v6, v7, v8, v9, v10 = sch.sample_perfect_tile(loop=l3, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l11, l12, l13, l14, l15 = sch.split(loop=l3, factors=[v6, v7, v8, v9, v10], preserve_unit_iters=True)
v16, v17, v18, v19, v20 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[25, 1, 40, 1, 1])
l21, l22, l23, l24, l25 = sch.split(loop=l4, factors=[v16, v17, v18, v19, v20], preserve_unit_iters=True)
v26, v27, v28 = sch.sample_perfect_tile(loop=l5, n=3, max_innermost_factor=64, decision=[256, 1, 2])
l29, l30, l31 = sch.split(loop=l5, factors=[v26, v27, v28], preserve_unit_iters=True)
sch.reorder(l11, l21, l12, l22, l13, l23, l29, l30, l14, l24, l31, l15, l25)
l32 = sch.fuse(l11, l21, preserve_unit_iters=True)
sch.bind(loop=l32, thread_axis="blockIdx.x")
l33 = sch.fuse(l12, l22, preserve_unit_iters=True)
sch.bind(loop=l33, thread_axis="vthread.x")
l34 = sch.fuse(l13, l23, preserve_unit_iters=True)
sch.bind(loop=l34, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b35 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b35, loop=l34, preserve_unit_loops=True, index=-1)
b36 = sch.cache_read(block=b0, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b36, loop=l29, preserve_unit_loops=True, index=-1)
l37, l38, l39, l40, l41, l42 = sch.get_loops(block=b36)
l43 = sch.fuse(l41, l42, preserve_unit_iters=True)
v44 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b36, ann_key="meta_schedule.cooperative_fetch", ann_val=v44)
b45 = sch.cache_read(block=b0, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b45, loop=l29, preserve_unit_loops=True, index=-1)
l46, l47, l48, l49, l50, l51 = sch.get_loops(block=b45)
l52 = sch.fuse(l50, l51, preserve_unit_iters=True)
v53 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b45, ann_key="meta_schedule.cooperative_fetch", ann_val=v53)
l54 = sch.fuse(l29, preserve_unit_iters=True)
sch.annotate(block_or_loop=l54, ann_key="software_pipeline_stage", ann_val=[0, 0, 2])
sch.annotate(block_or_loop=l54, ann_key="software_pipeline_order", ann_val=[0, 1, 2])
sch.annotate(block_or_loop=l54, ann_key="software_pipeline_async_stages", ann_val=[0])
sch.reverse_compute_inline(block=b1)
v55 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v55)
sch.enter_postproc()
sch.unannotate(block_or_loop=b36, ann_key="meta_schedule.cooperative_fetch")
l56, l57, l58, l59, l60 = sch.get_loops(block=b36)
l61, l62, l63 = sch.split(loop=l60, factors=[None, 40, 2], preserve_unit_iters=True)
sch.vectorize(loop=l63)
sch.bind(loop=l62, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b45, ann_key="meta_schedule.cooperative_fetch")
l64, l65, l66, l67, l68 = sch.get_loops(block=b45)
l69, l70, l71 = sch.split(loop=l68, factors=[None, 40, 4], preserve_unit_iters=True)
sch.vectorize(loop=l71)
sch.bind(loop=l70, thread_axis="threadIdx.x")
b72 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b72, ann_key="meta_schedule.unroll_explicit")
b73, b74, b75, b76 = sch.get_child_blocks(b72)
l77, l78, l79, l80, l81, l82, l83 = sch.get_loops(block=b73)
l84, l85, l86, l87, l88, l89, l90 = sch.get_loops(block=b74)
l91, l92, l93, l94, l95, l96, l97, l98, l99, l100 = sch.get_loops(block=b75)
sch.annotate(block_or_loop=l91, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l91, ann_key="pragma_unroll_explicit", ann_val=1)
l101, l102, l103, l104, l105 = sch.get_loops(block=b76)
b106 = sch.get_block(name="T_matmul_NT", func_name="main")
l107, l108, l109, l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b106)
b117 = sch.decompose_reduction(block=b106, loop=l110)
2023-11-11 02:09:57 [INFO] [task_scheduler.cc:121] [Task #28: fused_nn_dense_add] Trial #67: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/home2/xiachunwei/Software/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/home2/xiachunwei/Software/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/home2/xiachunwei/Software/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/home2/xiachunwei/Software/tvm/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/home2/xiachunwei/Software/tvm/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/home2/xiachunwei/Software/tvm/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/home2/xiachunwei/Software/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/home2/xiachunwei/Software/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  4: _ZN3tvm7runtime13PackedFun
  3: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  2: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  0: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  File "/home2/xiachunwei/Software/tvm/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(512)), "float32"), p1: T.Buffer((T.int64(1000), T.int64(512)), "float32"), p2: T.Buffer((T.int64(1), T.int64(1000)), "float32"), T_add: T.Buffer((T.int64(1), T.int64(1000)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        T_matmul_NT_local = T.alloc_buffer((T.int64(1), T.int64(1000)), scope="local")
        p0_shared = T.alloc_buffer((T.int64(1), T.int64(512)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(1000), T.int64(512)), scope="shared")
        for i_0_j_0_fused in T.thread_binding(T.int64(20), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 512, "pragma_unroll_explicit": 1}):
            for i_1_j_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for i_2_j_2_fused in T.thread_binding(T.int64(50), thread="threadIdx.x"):
                    for i_3_init, j_3_init, i_4_init, j_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_init"):
                            v_i = T.axis.spatial(T.int64(1), i_3_init + i_4_init)
                            v_j = T.axis.spatial(T.int64(1000), i_0_j_0_fused * T.int64(50) + i_2_j_2_fused + j_3_init + j_4_init)
                            T.reads()
                            T.writes(T_matmul_NT_local[v_i, v_j])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            T_matmul_NT_local[v_i, v_j] = T.float32(0)
                    for k_0_fused in T.serial(T.int64(256), annotations={"software_pipeline_async_stages": [0], "software_pipeline_order": [0, 1, 2], "software_pipeline_stage": [0, 0, 2]}):
                        for ax0_ax1_fused_0 in range(T.int64(1)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(50), thread="threadIdx.x"):
                                with T.block("p0_shared"):
                                    v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                    v1 = T.axis.spatial(T.int64(512), k_0_fused * T.int64(2) + (ax0_ax1_fused_0 * T.int64(50) + ax0_ax1_fused_1))
                                    T.where(ax0_ax1_fused_0 * T.int64(50) + ax0_ax1_fused_1 < T.int64(2))
                                    T.reads(p0[v0, v1])
                                    T.writes(p0_shared[v0, v1])
                                    p0_shared[v0, v1] = p0[v0, v1]
                        for ax0_ax1_fused_0 in range(T.int64(1)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(50), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("p1_shared"):
                                        v0 = T.axis.spatial(T.int64(1000), i_0_j_0_fused * T.int64(50) + (ax0_ax1_fused_0 * T.int64(200) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2) // T.int64(2))
                                        v1 = T.axis.spatial(T.int64(512), k_0_fused * T.int64(2) + (ax0_ax1_fused_0 * T.int64(200) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2) % T.int64(2))
                                        T.where((ax0_ax1_fused_0 * T.int64(50) + ax0_ax1_fused_1) * T.int64(4) + ax0_ax1_fused_2 < T.int64(100))
                                        T.reads(p1[v0, v1])
                                        T.writes(p1_shared[v0, v1])
                                        p1_shared[v0, v1] = p1[v0, v1]
                        for k_1, i_3, j_3, k_2, i_4, j_4 in T.grid(T.int64(2), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                            with T.block("T_matmul_NT_update"):
                                v_i = T.axis.spatial(T.int64(1), i_3 + i_4)
                                v_j = T.axis.spatial(T.int64(1000), i_0_j_0_fused * T.int64(50) + i_2_j_2_fused + j_3 + j_4)
                                v_k = T.axis.reduce(T.int64(512), k_0_fused * T.int64(2) + k_1 + k_2)
                                T.reads(T_matmul_NT_local[v_i, v_j], p0_shared[v_i, v_k], p1_shared[v_j, v_k])
                                T.writes(T_matmul_NT_local[v_i, v_j])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                T_matmul_NT_local[v_i, v_j] = T_matmul_NT_local[v_i, v_j] + p0_shared[v_i, v_k] * p1_shared[v_j, v_k]
                    for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(1000), i_0_j_0_fused * T.int64(50) + i_2_j_2_fused + ax1)
                            T.reads(T_matmul_NT_local[v0, v1], p2[v0, v1])
                            T.writes(T_add[v0, v1])
                            T_add[v0, v1] = T_matmul_NT_local[v0, v1] + p2[v0, v1]
b0 = sch.get_block(name="T_matmul_NT", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l3, l4, l5 = sch.get_loops(block=b0)
v6, v7, v8, v9, v10 = sch.sample_perfect_tile(loop=l3, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l11, l12, l13, l14, l15 = sch.split(loop=l3, factors=[v6, v7, v8, v9, v10], preserve_unit_iters=True)
v16, v17, v18, v19, v20 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[20, 1, 50, 1, 1])
l21, l22, l23, l24, l25 = sch.split(loop=l4, factors=[v16, v17, v18, v19, v20], preserve_unit_iters=True)
v26, v27, v28 = sch.sample_perfect_tile(loop=l5, n=3, max_innermost_factor=64, decision=[256, 2, 1])
l29, l30, l31 = sch.split(loop=l5, factors=[v26, v27, v28], preserve_unit_iters=True)
sch.reorder(l11, l21, l12, l22, l13, l23, l29, l30, l14, l24, l31, l15, l25)
l32 = sch.fuse(l11, l21, preserve_unit_iters=True)
sch.bind(loop=l32, thread_axis="blockIdx.x")
l33 = sch.fuse(l12, l22, preserve_unit_iters=True)
sch.bind(loop=l33, thread_axis="vthread.x")
l34 = sch.fuse(l13, l23, preserve_unit_iters=True)
sch.bind(loop=l34, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b35 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b35, loop=l34, preserve_unit_loops=True, index=-1)
b36 = sch.cache_read(block=b0, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b36, loop=l29, preserve_unit_loops=True, index=-1)
l37, l38, l39, l40, l41, l42 = sch.get_loops(block=b36)
l43 = sch.fuse(l41, l42, preserve_unit_iters=True)
v44 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b36, ann_key="meta_schedule.cooperative_fetch", ann_val=v44)
b45 = sch.cache_read(block=b0, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b45, loop=l29, preserve_unit_loops=True, index=-1)
l46, l47, l48, l49, l50, l51 = sch.get_loops(block=b45)
l52 = sch.fuse(l50, l51, preserve_unit_iters=True)
v53 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b45, ann_key="meta_schedule.cooperative_fetch", ann_val=v53)
l54 = sch.fuse(l29, preserve_unit_iters=True)
sch.annotate(block_or_loop=l54, ann_key="software_pipeline_stage", ann_val=[0, 0, 2])
sch.annotate(block_or_loop=l54, ann_key="software_pipeline_order", ann_val=[0, 1, 2])
sch.annotate(block_or_loop=l54, ann_key="software_pipeline_async_stages", ann_val=[0])
sch.reverse_compute_inline(block=b1)
v55 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v55)
sch.enter_postproc()
sch.unannotate(block_or_loop=b36, ann_key="meta_schedule.cooperative_fetch")
l56, l57, l58, l59, l60 = sch.get_loops(block=b36)
l61, l62 = sch.split(loop=l60, factors=[None, 50], preserve_unit_iters=True)
sch.bind(loop=l62, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b45, ann_key="meta_schedule.cooperative_fetch")
l63, l64, l65, l66, l67 = sch.get_loops(block=b45)
l68, l69, l70 = sch.split(loop=l67, factors=[None, 50, 4], preserve_unit_iters=True)
sch.vectorize(loop=l70)
sch.bind(loop=l69, thread_axis="threadIdx.x")
b71 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b71, ann_key="meta_schedule.unroll_explicit")
b72, b73, b74, b75 = sch.get_child_blocks(b71)
l76, l77, l78, l79, l80, l81 = sch.get_loops(block=b72)
l82, l83, l84, l85, l86, l87, l88 = sch.get_loops(block=b73)
l89, l90, l91, l92, l93, l94, l95, l96, l97, l98 = sch.get_loops(block=b74)
sch.annotate(block_or_loop=l89, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l89, ann_key="pragma_unroll_explicit", ann_val=1)
l99, l100, l101, l102, l103 = sch.get_loops(block=b75)
b104 = sch.get_block(name="T_matmul_NT", func_name="main")
l105, l106, l107, l108, l109, l110, l111, l112, l113, l114 = sch.get_loops(block=b104)
b115 = sch.decompose_reduction(block=b104, loop=l108)
2023-11-11 02:09:57 [INFO] [task_scheduler.cc:121] [Task #28: fused_nn_dense_add] Trial #68: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/home2/xiachunwei/Software/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/home2/xiachunwei/Software/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/home2/xiachunwei/Software/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/home2/xiachunwei/Software/tvm/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/home2/xiachunwei/Software/tvm/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/home2/xiachunwei/Software/tvm/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/home2/xiachunwei/Software/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/home2/xiachunwei/Software/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  4: _ZN3tvm7runtime13PackedFun
  3: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  2: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  0: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  File "/home2/xiachunwei/Software/tvm/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(512)), "float32"), p1: T.Buffer((T.int64(1000), T.int64(512)), "float32"), p2: T.Buffer((T.int64(1), T.int64(1000)), "float32"), T_add: T.Buffer((T.int64(1), T.int64(1000)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        T_matmul_NT_local = T.alloc_buffer((T.int64(1), T.int64(1000)), scope="local")
        p0_shared = T.alloc_buffer((T.int64(1), T.int64(512)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(1000), T.int64(512)), scope="shared")
        for i_0_j_0_fused in T.thread_binding(T.int64(20), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 512, "pragma_unroll_explicit": 1}):
            for i_1_j_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for i_2_j_2_fused in T.thread_binding(T.int64(50), thread="threadIdx.x"):
                    for i_3_init, j_3_init, i_4_init, j_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_init"):
                            v_i = T.axis.spatial(T.int64(1), i_3_init + i_4_init)
                            v_j = T.axis.spatial(T.int64(1000), i_0_j_0_fused * T.int64(50) + i_2_j_2_fused + j_3_init + j_4_init)
                            T.reads()
                            T.writes(T_matmul_NT_local[v_i, v_j])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            T_matmul_NT_local[v_i, v_j] = T.float32(0)
                    for k_0_fused in T.serial(T.int64(256), annotations={"software_pipeline_async_stages": [0], "software_pipeline_order": [0, 1, 2], "software_pipeline_stage": [0, 0, 2]}):
                        for ax0_ax1_fused_0 in range(T.int64(1)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(50), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("p0_shared"):
                                        v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v1 = T.axis.spatial(T.int64(512), k_0_fused * T.int64(2) + (ax0_ax1_fused_0 * T.int64(100) + ax0_ax1_fused_1 * T.int64(2) + ax0_ax1_fused_2))
                                        T.where((ax0_ax1_fused_0 * T.int64(50) + ax0_ax1_fused_1) * T.int64(2) + ax0_ax1_fused_2 < T.int64(2))
                                        T.reads(p0[v0, v1])
                                        T.writes(p0_shared[v0, v1])
                                        p0_shared[v0, v1] = p0[v0, v1]
                        for ax0_ax1_fused_0 in range(T.int64(2)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(50), thread="threadIdx.x"):
                                with T.block("p1_shared"):
                                    v0 = T.axis.spatial(T.int64(1000), i_0_j_0_fused * T.int64(50) + (ax0_ax1_fused_0 * T.int64(50) + ax0_ax1_fused_1) // T.int64(2))
                                    v1 = T.axis.spatial(T.int64(512), k_0_fused * T.int64(2) + (ax0_ax1_fused_0 * T.int64(50) + ax0_ax1_fused_1) % T.int64(2))
                                    T.reads(p1[v0, v1])
                                    T.writes(p1_shared[v0, v1])
                                    p1_shared[v0, v1] = p1[v0, v1]
                        for k_1, i_3, j_3, k_2, i_4, j_4 in T.grid(T.int64(2), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                            with T.block("T_matmul_NT_update"):
                                v_i = T.axis.spatial(T.int64(1), i_3 + i_4)
                                v_j = T.axis.spatial(T.int64(1000), i_0_j_0_fused * T.int64(50) + i_2_j_2_fused + j_3 + j_4)
                                v_k = T.axis.reduce(T.int64(512), k_0_fused * T.int64(2) + k_1 + k_2)
                                T.reads(T_matmul_NT_local[v_i, v_j], p0_shared[v_i, v_k], p1_shared[v_j, v_k])
                                T.writes(T_matmul_NT_local[v_i, v_j])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                T_matmul_NT_local[v_i, v_j] = T_matmul_NT_local[v_i, v_j] + p0_shared[v_i, v_k] * p1_shared[v_j, v_k]
                    for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(1000), i_0_j_0_fused * T.int64(50) + i_2_j_2_fused + ax1)
                            T.reads(T_matmul_NT_local[v0, v1], p2[v0, v1])
                            T.writes(T_add[v0, v1])
                            T_add[v0, v1] = T_matmul_NT_local[v0, v1] + p2[v0, v1]
b0 = sch.get_block(name="T_matmul_NT", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l3, l4, l5 = sch.get_loops(block=b0)
v6, v7, v8, v9, v10 = sch.sample_perfect_tile(loop=l3, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l11, l12, l13, l14, l15 = sch.split(loop=l3, factors=[v6, v7, v8, v9, v10], preserve_unit_iters=True)
v16, v17, v18, v19, v20 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[20, 1, 50, 1, 1])
l21, l22, l23, l24, l25 = sch.split(loop=l4, factors=[v16, v17, v18, v19, v20], preserve_unit_iters=True)
v26, v27, v28 = sch.sample_perfect_tile(loop=l5, n=3, max_innermost_factor=64, decision=[256, 2, 1])
l29, l30, l31 = sch.split(loop=l5, factors=[v26, v27, v28], preserve_unit_iters=True)
sch.reorder(l11, l21, l12, l22, l13, l23, l29, l30, l14, l24, l31, l15, l25)
l32 = sch.fuse(l11, l21, preserve_unit_iters=True)
sch.bind(loop=l32, thread_axis="blockIdx.x")
l33 = sch.fuse(l12, l22, preserve_unit_iters=True)
sch.bind(loop=l33, thread_axis="vthread.x")
l34 = sch.fuse(l13, l23, preserve_unit_iters=True)
sch.bind(loop=l34, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b35 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b35, loop=l34, preserve_unit_loops=True, index=-1)
b36 = sch.cache_read(block=b0, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b36, loop=l29, preserve_unit_loops=True, index=-1)
l37, l38, l39, l40, l41, l42 = sch.get_loops(block=b36)
l43 = sch.fuse(l41, l42, preserve_unit_iters=True)
v44 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b36, ann_key="meta_schedule.cooperative_fetch", ann_val=v44)
b45 = sch.cache_read(block=b0, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b45, loop=l29, preserve_unit_loops=True, index=-1)
l46, l47, l48, l49, l50, l51 = sch.get_loops(block=b45)
l52 = sch.fuse(l50, l51, preserve_unit_iters=True)
v53 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b45, ann_key="meta_schedule.cooperative_fetch", ann_val=v53)
l54 = sch.fuse(l29, preserve_unit_iters=True)
sch.annotate(block_or_loop=l54, ann_key="software_pipeline_stage", ann_val=[0, 0, 2])
sch.annotate(block_or_loop=l54, ann_key="software_pipeline_order", ann_val=[0, 1, 2])
sch.annotate(block_or_loop=l54, ann_key="software_pipeline_async_stages", ann_val=[0])
sch.reverse_compute_inline(block=b1)
v55 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v55)
sch.enter_postproc()
sch.unannotate(block_or_loop=b36, ann_key="meta_schedule.cooperative_fetch")
l56, l57, l58, l59, l60 = sch.get_loops(block=b36)
l61, l62, l63 = sch.split(loop=l60, factors=[None, 50, 2], preserve_unit_iters=True)
sch.vectorize(loop=l63)
sch.bind(loop=l62, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b45, ann_key="meta_schedule.cooperative_fetch")
l64, l65, l66, l67, l68 = sch.get_loops(block=b45)
l69, l70 = sch.split(loop=l68, factors=[None, 50], preserve_unit_iters=True)
sch.bind(loop=l70, thread_axis="threadIdx.x")
b71 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b71, ann_key="meta_schedule.unroll_explicit")
b72, b73, b74, b75 = sch.get_child_blocks(b71)
l76, l77, l78, l79, l80, l81, l82 = sch.get_loops(block=b72)
l83, l84, l85, l86, l87, l88 = sch.get_loops(block=b73)
l89, l90, l91, l92, l93, l94, l95, l96, l97, l98 = sch.get_loops(block=b74)
sch.annotate(block_or_loop=l89, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l89, ann_key="pragma_unroll_explicit", ann_val=1)
l99, l100, l101, l102, l103 = sch.get_loops(block=b75)
b104 = sch.get_block(name="T_matmul_NT", func_name="main")
l105, l106, l107, l108, l109, l110, l111, l112, l113, l114 = sch.get_loops(block=b104)
b115 = sch.decompose_reduction(block=b104, loop=l108)
2023-11-11 02:09:57 [INFO] [task_scheduler.cc:121] [Task #28: fused_nn_dense_add] Trial #69: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/home2/xiachunwei/Software/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/home2/xiachunwei/Software/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/home2/xiachunwei/Software/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/home2/xiachunwei/Software/tvm/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/home2/xiachunwei/Software/tvm/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/home2/xiachunwei/Software/tvm/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/home2/xiachunwei/Software/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/home2/xiachunwei/Software/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  4: _ZN3tvm7runtime13PackedFun
  3: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  2: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  0: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  File "/home2/xiachunwei/Software/tvm/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(512)), "float32"), p1: T.Buffer((T.int64(1000), T.int64(512)), "float32"), p2: T.Buffer((T.int64(1), T.int64(1000)), "float32"), T_add: T.Buffer((T.int64(1), T.int64(1000)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        T_matmul_NT_local = T.alloc_buffer((T.int64(1), T.int64(1000)), scope="local")
        p0_shared = T.alloc_buffer((T.int64(1), T.int64(512)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(1000), T.int64(512)), scope="shared")
        for i_0_j_0_fused in T.thread_binding(T.int64(20), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 512, "pragma_unroll_explicit": 1}):
            for i_1_j_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for i_2_j_2_fused in T.thread_binding(T.int64(50), thread="threadIdx.x"):
                    for i_3_init, j_3_init, i_4_init, j_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_init"):
                            v_i = T.axis.spatial(T.int64(1), i_3_init + i_4_init)
                            v_j = T.axis.spatial(T.int64(1000), i_0_j_0_fused * T.int64(50) + i_2_j_2_fused + j_3_init + j_4_init)
                            T.reads()
                            T.writes(T_matmul_NT_local[v_i, v_j])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            T_matmul_NT_local[v_i, v_j] = T.float32(0)
                    for k_0_fused in T.serial(T.int64(256), annotations={"software_pipeline_async_stages": [0], "software_pipeline_order": [0, 1, 2], "software_pipeline_stage": [0, 0, 2]}):
                        for ax0_ax1_fused_0 in range(T.int64(1)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(50), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("p0_shared"):
                                        v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v1 = T.axis.spatial(T.int64(512), k_0_fused * T.int64(2) + (ax0_ax1_fused_0 * T.int64(100) + ax0_ax1_fused_1 * T.int64(2) + ax0_ax1_fused_2))
                                        T.where((ax0_ax1_fused_0 * T.int64(50) + ax0_ax1_fused_1) * T.int64(2) + ax0_ax1_fused_2 < T.int64(2))
                                        T.reads(p0[v0, v1])
                                        T.writes(p0_shared[v0, v1])
                                        p0_shared[v0, v1] = p0[v0, v1]
                        for ax0_ax1_fused_0 in range(T.int64(1)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(50), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("p1_shared"):
                                        v0 = T.axis.spatial(T.int64(1000), i_0_j_0_fused * T.int64(50) + (ax0_ax1_fused_0 * T.int64(100) + ax0_ax1_fused_1 * T.int64(2) + ax0_ax1_fused_2) // T.int64(2))
                                        v1 = T.axis.spatial(T.int64(512), k_0_fused * T.int64(2) + (ax0_ax1_fused_0 * T.int64(100) + ax0_ax1_fused_1 * T.int64(2) + ax0_ax1_fused_2) % T.int64(2))
                                        T.reads(p1[v0, v1])
                                        T.writes(p1_shared[v0, v1])
                                        p1_shared[v0, v1] = p1[v0, v1]
                        for k_1, i_3, j_3, k_2, i_4, j_4 in T.grid(T.int64(2), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                            with T.block("T_matmul_NT_update"):
                                v_i = T.axis.spatial(T.int64(1), i_3 + i_4)
                                v_j = T.axis.spatial(T.int64(1000), i_0_j_0_fused * T.int64(50) + i_2_j_2_fused + j_3 + j_4)
                                v_k = T.axis.reduce(T.int64(512), k_0_fused * T.int64(2) + k_1 + k_2)
                                T.reads(T_matmul_NT_local[v_i, v_j], p0_shared[v_i, v_k], p1_shared[v_j, v_k])
                                T.writes(T_matmul_NT_local[v_i, v_j])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                T_matmul_NT_local[v_i, v_j] = T_matmul_NT_local[v_i, v_j] + p0_shared[v_i, v_k] * p1_shared[v_j, v_k]
                    for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(1000), i_0_j_0_fused * T.int64(50) + i_2_j_2_fused + ax1)
                            T.reads(T_matmul_NT_local[v0, v1], p2[v0, v1])
                            T.writes(T_add[v0, v1])
                            T_add[v0, v1] = T_matmul_NT_local[v0, v1] + p2[v0, v1]
b0 = sch.get_block(name="T_matmul_NT", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l3, l4, l5 = sch.get_loops(block=b0)
v6, v7, v8, v9, v10 = sch.sample_perfect_tile(loop=l3, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l11, l12, l13, l14, l15 = sch.split(loop=l3, factors=[v6, v7, v8, v9, v10], preserve_unit_iters=True)
v16, v17, v18, v19, v20 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[20, 1, 50, 1, 1])
l21, l22, l23, l24, l25 = sch.split(loop=l4, factors=[v16, v17, v18, v19, v20], preserve_unit_iters=True)
v26, v27, v28 = sch.sample_perfect_tile(loop=l5, n=3, max_innermost_factor=64, decision=[256, 2, 1])
l29, l30, l31 = sch.split(loop=l5, factors=[v26, v27, v28], preserve_unit_iters=True)
sch.reorder(l11, l21, l12, l22, l13, l23, l29, l30, l14, l24, l31, l15, l25)
l32 = sch.fuse(l11, l21, preserve_unit_iters=True)
sch.bind(loop=l32, thread_axis="blockIdx.x")
l33 = sch.fuse(l12, l22, preserve_unit_iters=True)
sch.bind(loop=l33, thread_axis="vthread.x")
l34 = sch.fuse(l13, l23, preserve_unit_iters=True)
sch.bind(loop=l34, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b35 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b35, loop=l34, preserve_unit_loops=True, index=-1)
b36 = sch.cache_read(block=b0, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b36, loop=l29, preserve_unit_loops=True, index=-1)
l37, l38, l39, l40, l41, l42 = sch.get_loops(block=b36)
l43 = sch.fuse(l41, l42, preserve_unit_iters=True)
v44 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b36, ann_key="meta_schedule.cooperative_fetch", ann_val=v44)
b45 = sch.cache_read(block=b0, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b45, loop=l29, preserve_unit_loops=True, index=-1)
l46, l47, l48, l49, l50, l51 = sch.get_loops(block=b45)
l52 = sch.fuse(l50, l51, preserve_unit_iters=True)
v53 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b45, ann_key="meta_schedule.cooperative_fetch", ann_val=v53)
l54 = sch.fuse(l29, preserve_unit_iters=True)
sch.annotate(block_or_loop=l54, ann_key="software_pipeline_stage", ann_val=[0, 0, 2])
sch.annotate(block_or_loop=l54, ann_key="software_pipeline_order", ann_val=[0, 1, 2])
sch.annotate(block_or_loop=l54, ann_key="software_pipeline_async_stages", ann_val=[0])
sch.reverse_compute_inline(block=b1)
v55 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v55)
sch.enter_postproc()
sch.unannotate(block_or_loop=b36, ann_key="meta_schedule.cooperative_fetch")
l56, l57, l58, l59, l60 = sch.get_loops(block=b36)
l61, l62, l63 = sch.split(loop=l60, factors=[None, 50, 2], preserve_unit_iters=True)
sch.vectorize(loop=l63)
sch.bind(loop=l62, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b45, ann_key="meta_schedule.cooperative_fetch")
l64, l65, l66, l67, l68 = sch.get_loops(block=b45)
l69, l70, l71 = sch.split(loop=l68, factors=[None, 50, 2], preserve_unit_iters=True)
sch.vectorize(loop=l71)
sch.bind(loop=l70, thread_axis="threadIdx.x")
b72 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b72, ann_key="meta_schedule.unroll_explicit")
b73, b74, b75, b76 = sch.get_child_blocks(b72)
l77, l78, l79, l80, l81, l82, l83 = sch.get_loops(block=b73)
l84, l85, l86, l87, l88, l89, l90 = sch.get_loops(block=b74)
l91, l92, l93, l94, l95, l96, l97, l98, l99, l100 = sch.get_loops(block=b75)
sch.annotate(block_or_loop=l91, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l91, ann_key="pragma_unroll_explicit", ann_val=1)
l101, l102, l103, l104, l105 = sch.get_loops(block=b76)
b106 = sch.get_block(name="T_matmul_NT", func_name="main")
l107, l108, l109, l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b106)
b117 = sch.decompose_reduction(block=b106, loop=l110)
2023-11-11 02:09:57 [INFO] [task_scheduler.cc:121] [Task #28: fused_nn_dense_add] Trial #70: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/home2/xiachunwei/Software/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/home2/xiachunwei/Software/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/home2/xiachunwei/Software/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/home2/xiachunwei/Software/tvm/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/home2/xiachunwei/Software/tvm/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/home2/xiachunwei/Software/tvm/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/home2/xiachunwei/Software/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/home2/xiachunwei/Software/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  4: _ZN3tvm7runtime13PackedFun
  3: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  2: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  0: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  File "/home2/xiachunwei/Software/tvm/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(512)), "float32"), p1: T.Buffer((T.int64(1000), T.int64(512)), "float32"), p2: T.Buffer((T.int64(1), T.int64(1000)), "float32"), T_add: T.Buffer((T.int64(1), T.int64(1000)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        T_matmul_NT_local = T.alloc_buffer((T.int64(1), T.int64(1000)), scope="local")
        p0_shared = T.alloc_buffer((T.int64(1), T.int64(512)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(1000), T.int64(512)), scope="shared")
        for i_0_j_0_fused in T.thread_binding(T.int64(20), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 512, "pragma_unroll_explicit": 1}):
            for i_1_j_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for i_2_j_2_fused in T.thread_binding(T.int64(50), thread="threadIdx.x"):
                    for i_3_init, j_3_init, i_4_init, j_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_init"):
                            v_i = T.axis.spatial(T.int64(1), i_3_init + i_4_init)
                            v_j = T.axis.spatial(T.int64(1000), i_0_j_0_fused * T.int64(50) + i_2_j_2_fused + j_3_init + j_4_init)
                            T.reads()
                            T.writes(T_matmul_NT_local[v_i, v_j])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            T_matmul_NT_local[v_i, v_j] = T.float32(0)
                    for k_0_fused in T.serial(T.int64(256), annotations={"software_pipeline_async_stages": [0], "software_pipeline_order": [0, 1, 2], "software_pipeline_stage": [0, 0, 2]}):
                        for ax0_ax1_fused_0 in range(T.int64(1)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(50), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("p0_shared"):
                                        v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v1 = T.axis.spatial(T.int64(512), k_0_fused * T.int64(2) + (ax0_ax1_fused_0 * T.int64(100) + ax0_ax1_fused_1 * T.int64(2) + ax0_ax1_fused_2))
                                        T.where((ax0_ax1_fused_0 * T.int64(50) + ax0_ax1_fused_1) * T.int64(2) + ax0_ax1_fused_2 < T.int64(2))
                                        T.reads(p0[v0, v1])
                                        T.writes(p0_shared[v0, v1])
                                        p0_shared[v0, v1] = p0[v0, v1]
                        for ax0_ax1_fused_0 in range(T.int64(1)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(50), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("p1_shared"):
                                        v0 = T.axis.spatial(T.int64(1000), i_0_j_0_fused * T.int64(50) + (ax0_ax1_fused_0 * T.int64(100) + ax0_ax1_fused_1 * T.int64(2) + ax0_ax1_fused_2) // T.int64(2))
                                        v1 = T.axis.spatial(T.int64(512), k_0_fused * T.int64(2) + (ax0_ax1_fused_0 * T.int64(100) + ax0_ax1_fused_1 * T.int64(2) + ax0_ax1_fused_2) % T.int64(2))
                                        T.reads(p1[v0, v1])
                                        T.writes(p1_shared[v0, v1])
                                        p1_shared[v0, v1] = p1[v0, v1]
                        for k_1, i_3, j_3, k_2, i_4, j_4 in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(1), T.int64(1)):
                            with T.block("T_matmul_NT_update"):
                                v_i = T.axis.spatial(T.int64(1), i_3 + i_4)
                                v_j = T.axis.spatial(T.int64(1000), i_0_j_0_fused * T.int64(50) + i_2_j_2_fused + j_3 + j_4)
                                v_k = T.axis.reduce(T.int64(512), k_0_fused * T.int64(2) + k_1 * T.int64(2) + k_2)
                                T.reads(T_matmul_NT_local[v_i, v_j], p0_shared[v_i, v_k], p1_shared[v_j, v_k])
                                T.writes(T_matmul_NT_local[v_i, v_j])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                T_matmul_NT_local[v_i, v_j] = T_matmul_NT_local[v_i, v_j] + p0_shared[v_i, v_k] * p1_shared[v_j, v_k]
                    for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(1000), i_0_j_0_fused * T.int64(50) + i_2_j_2_fused + ax1)
                            T.reads(T_matmul_NT_local[v0, v1], p2[v0, v1])
                            T.writes(T_add[v0, v1])
                            T_add[v0, v1] = T_matmul_NT_local[v0, v1] + p2[v0, v1]
b0 = sch.get_block(name="T_matmul_NT", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l3, l4, l5 = sch.get_loops(block=b0)
v6, v7, v8, v9, v10 = sch.sample_perfect_tile(loop=l3, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l11, l12, l13, l14, l15 = sch.split(loop=l3, factors=[v6, v7, v8, v9, v10], preserve_unit_iters=True)
v16, v17, v18, v19, v20 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[20, 1, 50, 1, 1])
l21, l22, l23, l24, l25 = sch.split(loop=l4, factors=[v16, v17, v18, v19, v20], preserve_unit_iters=True)
v26, v27, v28 = sch.sample_perfect_tile(loop=l5, n=3, max_innermost_factor=64, decision=[256, 1, 2])
l29, l30, l31 = sch.split(loop=l5, factors=[v26, v27, v28], preserve_unit_iters=True)
sch.reorder(l11, l21, l12, l22, l13, l23, l29, l30, l14, l24, l31, l15, l25)
l32 = sch.fuse(l11, l21, preserve_unit_iters=True)
sch.bind(loop=l32, thread_axis="blockIdx.x")
l33 = sch.fuse(l12, l22, preserve_unit_iters=True)
sch.bind(loop=l33, thread_axis="vthread.x")
l34 = sch.fuse(l13, l23, preserve_unit_iters=True)
sch.bind(loop=l34, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b35 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b35, loop=l34, preserve_unit_loops=True, index=-1)
b36 = sch.cache_read(block=b0, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b36, loop=l29, preserve_unit_loops=True, index=-1)
l37, l38, l39, l40, l41, l42 = sch.get_loops(block=b36)
l43 = sch.fuse(l41, l42, preserve_unit_iters=True)
v44 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b36, ann_key="meta_schedule.cooperative_fetch", ann_val=v44)
b45 = sch.cache_read(block=b0, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b45, loop=l29, preserve_unit_loops=True, index=-1)
l46, l47, l48, l49, l50, l51 = sch.get_loops(block=b45)
l52 = sch.fuse(l50, l51, preserve_unit_iters=True)
v53 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b45, ann_key="meta_schedule.cooperative_fetch", ann_val=v53)
l54 = sch.fuse(l29, preserve_unit_iters=True)
sch.annotate(block_or_loop=l54, ann_key="software_pipeline_stage", ann_val=[0, 0, 2])
sch.annotate(block_or_loop=l54, ann_key="software_pipeline_order", ann_val=[0, 1, 2])
sch.annotate(block_or_loop=l54, ann_key="software_pipeline_async_stages", ann_val=[0])
sch.reverse_compute_inline(block=b1)
v55 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v55)
sch.enter_postproc()
sch.unannotate(block_or_loop=b36, ann_key="meta_schedule.cooperative_fetch")
l56, l57, l58, l59, l60 = sch.get_loops(block=b36)
l61, l62, l63 = sch.split(loop=l60, factors=[None, 50, 2], preserve_unit_iters=True)
sch.vectorize(loop=l63)
sch.bind(loop=l62, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b45, ann_key="meta_schedule.cooperative_fetch")
l64, l65, l66, l67, l68 = sch.get_loops(block=b45)
l69, l70, l71 = sch.split(loop=l68, factors=[None, 50, 2], preserve_unit_iters=True)
sch.vectorize(loop=l71)
sch.bind(loop=l70, thread_axis="threadIdx.x")
b72 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b72, ann_key="meta_schedule.unroll_explicit")
b73, b74, b75, b76 = sch.get_child_blocks(b72)
l77, l78, l79, l80, l81, l82, l83 = sch.get_loops(block=b73)
l84, l85, l86, l87, l88, l89, l90 = sch.get_loops(block=b74)
l91, l92, l93, l94, l95, l96, l97, l98, l99, l100 = sch.get_loops(block=b75)
sch.annotate(block_or_loop=l91, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l91, ann_key="pragma_unroll_explicit", ann_val=1)
l101, l102, l103, l104, l105 = sch.get_loops(block=b76)
b106 = sch.get_block(name="T_matmul_NT", func_name="main")
l107, l108, l109, l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b106)
b117 = sch.decompose_reduction(block=b106, loop=l110)
2023-11-11 02:09:57 [INFO] [task_scheduler.cc:121] [Task #28: fused_nn_dense_add] Trial #71: Error in running:
LocalRunner: An exception occurred
Subprocess terminated
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(512)), "float32"), p1: T.Buffer((T.int64(1000), T.int64(512)), "float32"), p2: T.Buffer((T.int64(1), T.int64(1000)), "float32"), T_add: T.Buffer((T.int64(1), T.int64(1000)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        T_matmul_NT_local = T.alloc_buffer((T.int64(1), T.int64(1000)), scope="local")
        p0_shared = T.alloc_buffer((T.int64(1), T.int64(512)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(1000), T.int64(512)), scope="shared")
        for i_0_j_0_fused in T.thread_binding(T.int64(20), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 512, "pragma_unroll_explicit": 1}):
            for i_1_j_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for i_2_j_2_fused in T.thread_binding(T.int64(50), thread="threadIdx.x"):
                    for i_3_init, j_3_init, i_4_init, j_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_init"):
                            v_i = T.axis.spatial(T.int64(1), i_3_init + i_4_init)
                            v_j = T.axis.spatial(T.int64(1000), i_0_j_0_fused * T.int64(50) + i_2_j_2_fused + j_3_init + j_4_init)
                            T.reads()
                            T.writes(T_matmul_NT_local[v_i, v_j])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            T_matmul_NT_local[v_i, v_j] = T.float32(0)
                    for k_0_fused in T.serial(T.int64(256), annotations={"software_pipeline_async_stages": [0], "software_pipeline_order": [0, 1, 2], "software_pipeline_stage": [0, 0, 2]}):
                        for ax0_ax1_fused_0 in range(T.int64(1)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(50), thread="threadIdx.x"):
                                with T.block("p0_shared"):
                                    v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                    v1 = T.axis.spatial(T.int64(512), k_0_fused * T.int64(2) + (ax0_ax1_fused_0 * T.int64(50) + ax0_ax1_fused_1))
                                    T.where(ax0_ax1_fused_0 * T.int64(50) + ax0_ax1_fused_1 < T.int64(2))
                                    T.reads(p0[v0, v1])
                                    T.writes(p0_shared[v0, v1])
                                    p0_shared[v0, v1] = p0[v0, v1]
                        for ax0_ax1_fused_0 in range(T.int64(2)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(50), thread="threadIdx.x"):
                                with T.block("p1_shared"):
                                    v0 = T.axis.spatial(T.int64(1000), i_0_j_0_fused * T.int64(50) + (ax0_ax1_fused_0 * T.int64(50) + ax0_ax1_fused_1) // T.int64(2))
                                    v1 = T.axis.spatial(T.int64(512), k_0_fused * T.int64(2) + (ax0_ax1_fused_0 * T.int64(50) + ax0_ax1_fused_1) % T.int64(2))
                                    T.reads(p1[v0, v1])
                                    T.writes(p1_shared[v0, v1])
                                    p1_shared[v0, v1] = p1[v0, v1]
                        for k_1, i_3, j_3, k_2, i_4, j_4 in T.grid(T.int64(2), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                            with T.block("T_matmul_NT_update"):
                                v_i = T.axis.spatial(T.int64(1), i_3 + i_4)
                                v_j = T.axis.spatial(T.int64(1000), i_0_j_0_fused * T.int64(50) + i_2_j_2_fused + j_3 + j_4)
                                v_k = T.axis.reduce(T.int64(512), k_0_fused * T.int64(2) + k_1 + k_2)
                                T.reads(T_matmul_NT_local[v_i, v_j], p0_shared[v_i, v_k], p1_shared[v_j, v_k])
                                T.writes(T_matmul_NT_local[v_i, v_j])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                T_matmul_NT_local[v_i, v_j] = T_matmul_NT_local[v_i, v_j] + p0_shared[v_i, v_k] * p1_shared[v_j, v_k]
                    for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(1000), i_0_j_0_fused * T.int64(50) + i_2_j_2_fused + ax1)
                            T.reads(T_matmul_NT_local[v0, v1], p2[v0, v1])
                            T.writes(T_add[v0, v1])
                            T_add[v0, v1] = T_matmul_NT_local[v0, v1] + p2[v0, v1]
b0 = sch.get_block(name="T_matmul_NT", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l3, l4, l5 = sch.get_loops(block=b0)
v6, v7, v8, v9, v10 = sch.sample_perfect_tile(loop=l3, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l11, l12, l13, l14, l15 = sch.split(loop=l3, factors=[v6, v7, v8, v9, v10], preserve_unit_iters=True)
v16, v17, v18, v19, v20 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[20, 1, 50, 1, 1])
l21, l22, l23, l24, l25 = sch.split(loop=l4, factors=[v16, v17, v18, v19, v20], preserve_unit_iters=True)
v26, v27, v28 = sch.sample_perfect_tile(loop=l5, n=3, max_innermost_factor=64, decision=[256, 2, 1])
l29, l30, l31 = sch.split(loop=l5, factors=[v26, v27, v28], preserve_unit_iters=True)
sch.reorder(l11, l21, l12, l22, l13, l23, l29, l30, l14, l24, l31, l15, l25)
l32 = sch.fuse(l11, l21, preserve_unit_iters=True)
sch.bind(loop=l32, thread_axis="blockIdx.x")
l33 = sch.fuse(l12, l22, preserve_unit_iters=True)
sch.bind(loop=l33, thread_axis="vthread.x")
l34 = sch.fuse(l13, l23, preserve_unit_iters=True)
sch.bind(loop=l34, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b35 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b35, loop=l34, preserve_unit_loops=True, index=-1)
b36 = sch.cache_read(block=b0, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b36, loop=l29, preserve_unit_loops=True, index=-1)
l37, l38, l39, l40, l41, l42 = sch.get_loops(block=b36)
l43 = sch.fuse(l41, l42, preserve_unit_iters=True)
v44 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b36, ann_key="meta_schedule.cooperative_fetch", ann_val=v44)
b45 = sch.cache_read(block=b0, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b45, loop=l29, preserve_unit_loops=True, index=-1)
l46, l47, l48, l49, l50, l51 = sch.get_loops(block=b45)
l52 = sch.fuse(l50, l51, preserve_unit_iters=True)
v53 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b45, ann_key="meta_schedule.cooperative_fetch", ann_val=v53)
l54 = sch.fuse(l29, preserve_unit_iters=True)
sch.annotate(block_or_loop=l54, ann_key="software_pipeline_stage", ann_val=[0, 0, 2])
sch.annotate(block_or_loop=l54, ann_key="software_pipeline_order", ann_val=[0, 1, 2])
sch.annotate(block_or_loop=l54, ann_key="software_pipeline_async_stages", ann_val=[0])
sch.reverse_compute_inline(block=b1)
v55 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v55)
sch.enter_postproc()
sch.unannotate(block_or_loop=b36, ann_key="meta_schedule.cooperative_fetch")
l56, l57, l58, l59, l60 = sch.get_loops(block=b36)
l61, l62 = sch.split(loop=l60, factors=[None, 50], preserve_unit_iters=True)
sch.bind(loop=l62, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b45, ann_key="meta_schedule.cooperative_fetch")
l63, l64, l65, l66, l67 = sch.get_loops(block=b45)
l68, l69 = sch.split(loop=l67, factors=[None, 50], preserve_unit_iters=True)
sch.bind(loop=l69, thread_axis="threadIdx.x")
b70 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b70, ann_key="meta_schedule.unroll_explicit")
b71, b72, b73, b74 = sch.get_child_blocks(b70)
l75, l76, l77, l78, l79, l80 = sch.get_loops(block=b71)
l81, l82, l83, l84, l85, l86 = sch.get_loops(block=b72)
l87, l88, l89, l90, l91, l92, l93, l94, l95, l96 = sch.get_loops(block=b73)
sch.annotate(block_or_loop=l87, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l87, ann_key="pragma_unroll_explicit", ann_val=1)
l97, l98, l99, l100, l101 = sch.get_loops(block=b74)
b102 = sch.get_block(name="T_matmul_NT", func_name="main")
l103, l104, l105, l106, l107, l108, l109, l110, l111, l112 = sch.get_loops(block=b102)
b113 = sch.decompose_reduction(block=b102, loop=l106)
2023-11-11 02:09:57 [INFO] [task_scheduler.cc:121] [Task #28: fused_nn_dense_add] Trial #72: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/home2/xiachunwei/Software/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/home2/xiachunwei/Software/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 159, in _worker_func
    costs: List[float] = f_run_evaluator(
  File "/home2/xiachunwei/Software/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 387, in default_run_evaluator
    return run_evaluator_common(rt_mod, device, evaluator_config, repeated_args)
  File "/home2/xiachunwei/Software/tvm/python/tvm/meta_schedule/runner/utils.py", line 117, in run_evaluator_common
    profile_result = evaluator(*args)
  File "/home2/xiachunwei/Software/tvm/python/tvm/runtime/module.py", line 397, in evaluator
    blob = feval(*args)
  File "/home2/xiachunwei/Software/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/home2/xiachunwei/Software/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  1: tvm::runtime::profiling::WrapTimeEvaluator(tvm::runtime::PackedFunc, DLDevice, int, int, int, int, int, int, int, tvm::runtime::PackedFunc)::{lambda(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*) [clone .isra.779]
  0: tvm::runtime::CUDADeviceAPI::StreamSync(DLDevice, void*)
  File "/home2/xiachunwei/Software/tvm/src/runtime/cuda/cuda_device_api.cc", line 212
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(512)), "float32"), p1: T.Buffer((T.int64(1000), T.int64(512)), "float32"), p2: T.Buffer((T.int64(1), T.int64(1000)), "float32"), T_add: T.Buffer((T.int64(1), T.int64(1000)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        T_matmul_NT_local = T.alloc_buffer((T.int64(1), T.int64(1000)), scope="local")
        p0_shared = T.alloc_buffer((T.int64(1), T.int64(512)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(1000), T.int64(512)), scope="shared")
        for i_0_j_0_fused in T.thread_binding(T.int64(25), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 1024, "pragma_unroll_explicit": 1}):
            for i_1_j_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for i_2_j_2_fused in T.thread_binding(T.int64(40), thread="threadIdx.x"):
                    for i_3_init, j_3_init, i_4_init, j_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_init"):
                            v_i = T.axis.spatial(T.int64(1), i_3_init + i_4_init)
                            v_j = T.axis.spatial(T.int64(1000), i_0_j_0_fused * T.int64(40) + i_2_j_2_fused + j_3_init + j_4_init)
                            T.reads()
                            T.writes(T_matmul_NT_local[v_i, v_j])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            T_matmul_NT_local[v_i, v_j] = T.float32(0)
                    for k_0_fused in T.serial(T.int64(256), annotations={"software_pipeline_async_stages": [0], "software_pipeline_order": [0, 1, 2], "software_pipeline_stage": [0, 0, 2]}):
                        for ax0_ax1_fused_0 in range(T.int64(1)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(40), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("p0_shared"):
                                        v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v1 = T.axis.spatial(T.int64(512), k_0_fused * T.int64(2) + (ax0_ax1_fused_0 * T.int64(80) + ax0_ax1_fused_1 * T.int64(2) + ax0_ax1_fused_2))
                                        T.where((ax0_ax1_fused_0 * T.int64(40) + ax0_ax1_fused_1) * T.int64(2) + ax0_ax1_fused_2 < T.int64(2))
                                        T.reads(p0[v0, v1])
                                        T.writes(p0_shared[v0, v1])
                                        p0_shared[v0, v1] = p0[v0, v1]
                        for ax0_ax1_fused_0 in range(T.int64(1)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(40), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("p1_shared"):
                                        v0 = T.axis.spatial(T.int64(1000), i_0_j_0_fused * T.int64(40) + (ax0_ax1_fused_0 * T.int64(160) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2) // T.int64(2))
                                        v1 = T.axis.spatial(T.int64(512), k_0_fused * T.int64(2) + (ax0_ax1_fused_0 * T.int64(160) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2) % T.int64(2))
                                        T.where((ax0_ax1_fused_0 * T.int64(40) + ax0_ax1_fused_1) * T.int64(4) + ax0_ax1_fused_2 < T.int64(80))
                                        T.reads(p1[v0, v1])
                                        T.writes(p1_shared[v0, v1])
                                        p1_shared[v0, v1] = p1[v0, v1]
                        for k_1, i_3, j_3, k_2, i_4, j_4 in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(1), T.int64(1)):
                            with T.block("T_matmul_NT_update"):
                                v_i = T.axis.spatial(T.int64(1), i_3 + i_4)
                                v_j = T.axis.spatial(T.int64(1000), i_0_j_0_fused * T.int64(40) + i_2_j_2_fused + j_3 + j_4)
                                v_k = T.axis.reduce(T.int64(512), k_0_fused * T.int64(2) + k_1 * T.int64(2) + k_2)
                                T.reads(T_matmul_NT_local[v_i, v_j], p0_shared[v_i, v_k], p1_shared[v_j, v_k])
                                T.writes(T_matmul_NT_local[v_i, v_j])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                T_matmul_NT_local[v_i, v_j] = T_matmul_NT_local[v_i, v_j] + p0_shared[v_i, v_k] * p1_shared[v_j, v_k]
                    for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(1000), i_0_j_0_fused * T.int64(40) + i_2_j_2_fused + ax1)
                            T.reads(T_matmul_NT_local[v0, v1], p2[v0, v1])
                            T.writes(T_add[v0, v1])
                            T_add[v0, v1] = T_matmul_NT_local[v0, v1] + p2[v0, v1]
b0 = sch.get_block(name="T_matmul_NT", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l3, l4, l5 = sch.get_loops(block=b0)
v6, v7, v8, v9, v10 = sch.sample_perfect_tile(loop=l3, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l11, l12, l13, l14, l15 = sch.split(loop=l3, factors=[v6, v7, v8, v9, v10], preserve_unit_iters=True)
v16, v17, v18, v19, v20 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[25, 1, 40, 1, 1])
l21, l22, l23, l24, l25 = sch.split(loop=l4, factors=[v16, v17, v18, v19, v20], preserve_unit_iters=True)
v26, v27, v28 = sch.sample_perfect_tile(loop=l5, n=3, max_innermost_factor=64, decision=[256, 1, 2])
l29, l30, l31 = sch.split(loop=l5, factors=[v26, v27, v28], preserve_unit_iters=True)
sch.reorder(l11, l21, l12, l22, l13, l23, l29, l30, l14, l24, l31, l15, l25)
l32 = sch.fuse(l11, l21, preserve_unit_iters=True)
sch.bind(loop=l32, thread_axis="blockIdx.x")
l33 = sch.fuse(l12, l22, preserve_unit_iters=True)
sch.bind(loop=l33, thread_axis="vthread.x")
l34 = sch.fuse(l13, l23, preserve_unit_iters=True)
sch.bind(loop=l34, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b35 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b35, loop=l34, preserve_unit_loops=True, index=-1)
b36 = sch.cache_read(block=b0, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b36, loop=l29, preserve_unit_loops=True, index=-1)
l37, l38, l39, l40, l41, l42 = sch.get_loops(block=b36)
l43 = sch.fuse(l41, l42, preserve_unit_iters=True)
v44 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b36, ann_key="meta_schedule.cooperative_fetch", ann_val=v44)
b45 = sch.cache_read(block=b0, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b45, loop=l29, preserve_unit_loops=True, index=-1)
l46, l47, l48, l49, l50, l51 = sch.get_loops(block=b45)
l52 = sch.fuse(l50, l51, preserve_unit_iters=True)
v53 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b45, ann_key="meta_schedule.cooperative_fetch", ann_val=v53)
l54 = sch.fuse(l29, preserve_unit_iters=True)
sch.annotate(block_or_loop=l54, ann_key="software_pipeline_stage", ann_val=[0, 0, 2])
sch.annotate(block_or_loop=l54, ann_key="software_pipeline_order", ann_val=[0, 1, 2])
sch.annotate(block_or_loop=l54, ann_key="software_pipeline_async_stages", ann_val=[0])
sch.reverse_compute_inline(block=b1)
v55 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v55)
sch.enter_postproc()
sch.unannotate(block_or_loop=b36, ann_key="meta_schedule.cooperative_fetch")
l56, l57, l58, l59, l60 = sch.get_loops(block=b36)
l61, l62, l63 = sch.split(loop=l60, factors=[None, 40, 2], preserve_unit_iters=True)
sch.vectorize(loop=l63)
sch.bind(loop=l62, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b45, ann_key="meta_schedule.cooperative_fetch")
l64, l65, l66, l67, l68 = sch.get_loops(block=b45)
l69, l70, l71 = sch.split(loop=l68, factors=[None, 40, 4], preserve_unit_iters=True)
sch.vectorize(loop=l71)
sch.bind(loop=l70, thread_axis="threadIdx.x")
b72 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b72, ann_key="meta_schedule.unroll_explicit")
b73, b74, b75, b76 = sch.get_child_blocks(b72)
l77, l78, l79, l80, l81, l82, l83 = sch.get_loops(block=b73)
l84, l85, l86, l87, l88, l89, l90 = sch.get_loops(block=b74)
l91, l92, l93, l94, l95, l96, l97, l98, l99, l100 = sch.get_loops(block=b75)
sch.annotate(block_or_loop=l91, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l91, ann_key="pragma_unroll_explicit", ann_val=1)
l101, l102, l103, l104, l105 = sch.get_loops(block=b76)
b106 = sch.get_block(name="T_matmul_NT", func_name="main")
l107, l108, l109, l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b106)
b117 = sch.decompose_reduction(block=b106, loop=l110)
2023-11-11 02:09:57 [INFO] [task_scheduler.cc:121] [Task #28: fused_nn_dense_add] Trial #73: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/home2/xiachunwei/Software/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/home2/xiachunwei/Software/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/home2/xiachunwei/Software/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/home2/xiachunwei/Software/tvm/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/home2/xiachunwei/Software/tvm/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/home2/xiachunwei/Software/tvm/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/home2/xiachunwei/Software/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/home2/xiachunwei/Software/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  4: _ZN3tvm7runtime13PackedFun
  3: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  2: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  0: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  File "/home2/xiachunwei/Software/tvm/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(512)), "float32"), p1: T.Buffer((T.int64(1000), T.int64(512)), "float32"), p2: T.Buffer((T.int64(1), T.int64(1000)), "float32"), T_add: T.Buffer((T.int64(1), T.int64(1000)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        T_matmul_NT_local = T.alloc_buffer((T.int64(1), T.int64(1000)), scope="local")
        p0_shared = T.alloc_buffer((T.int64(1), T.int64(512)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(1000), T.int64(512)), scope="shared")
        for i_0_j_0_fused in T.thread_binding(T.int64(25), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 1024, "pragma_unroll_explicit": 1}):
            for i_1_j_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for i_2_j_2_fused in T.thread_binding(T.int64(40), thread="threadIdx.x"):
                    for i_3_init, j_3_init, i_4_init, j_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_init"):
                            v_i = T.axis.spatial(T.int64(1), i_3_init + i_4_init)
                            v_j = T.axis.spatial(T.int64(1000), i_0_j_0_fused * T.int64(40) + i_2_j_2_fused + j_3_init + j_4_init)
                            T.reads()
                            T.writes(T_matmul_NT_local[v_i, v_j])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            T_matmul_NT_local[v_i, v_j] = T.float32(0)
                    for k_0_fused in T.serial(T.int64(256), annotations={"software_pipeline_async_stages": [0], "software_pipeline_order": [0, 1, 2], "software_pipeline_stage": [0, 0, 2]}):
                        for ax0_ax1_fused_0 in range(T.int64(1)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(40), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("p0_shared"):
                                        v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v1 = T.axis.spatial(T.int64(512), k_0_fused * T.int64(2) + (ax0_ax1_fused_0 * T.int64(80) + ax0_ax1_fused_1 * T.int64(2) + ax0_ax1_fused_2))
                                        T.where((ax0_ax1_fused_0 * T.int64(40) + ax0_ax1_fused_1) * T.int64(2) + ax0_ax1_fused_2 < T.int64(2))
                                        T.reads(p0[v0, v1])
                                        T.writes(p0_shared[v0, v1])
                                        p0_shared[v0, v1] = p0[v0, v1]
                        for ax0_ax1_fused_0 in range(T.int64(1)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(40), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("p1_shared"):
                                        v0 = T.axis.spatial(T.int64(1000), i_0_j_0_fused * T.int64(40) + (ax0_ax1_fused_0 * T.int64(160) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2) // T.int64(2))
                                        v1 = T.axis.spatial(T.int64(512), k_0_fused * T.int64(2) + (ax0_ax1_fused_0 * T.int64(160) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2) % T.int64(2))
                                        T.where((ax0_ax1_fused_0 * T.int64(40) + ax0_ax1_fused_1) * T.int64(4) + ax0_ax1_fused_2 < T.int64(80))
                                        T.reads(p1[v0, v1])
                                        T.writes(p1_shared[v0, v1])
                                        p1_shared[v0, v1] = p1[v0, v1]
                        for k_1, i_3, j_3, k_2, i_4, j_4 in T.grid(T.int64(2), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                            with T.block("T_matmul_NT_update"):
                                v_i = T.axis.spatial(T.int64(1), i_3 + i_4)
                                v_j = T.axis.spatial(T.int64(1000), i_0_j_0_fused * T.int64(40) + i_2_j_2_fused + j_3 + j_4)
                                v_k = T.axis.reduce(T.int64(512), k_0_fused * T.int64(2) + k_1 + k_2)
                                T.reads(T_matmul_NT_local[v_i, v_j], p0_shared[v_i, v_k], p1_shared[v_j, v_k])
                                T.writes(T_matmul_NT_local[v_i, v_j])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                T_matmul_NT_local[v_i, v_j] = T_matmul_NT_local[v_i, v_j] + p0_shared[v_i, v_k] * p1_shared[v_j, v_k]
                    for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(1000), i_0_j_0_fused * T.int64(40) + i_2_j_2_fused + ax1)
                            T.reads(T_matmul_NT_local[v0, v1], p2[v0, v1])
                            T.writes(T_add[v0, v1])
                            T_add[v0, v1] = T_matmul_NT_local[v0, v1] + p2[v0, v1]
b0 = sch.get_block(name="T_matmul_NT", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l3, l4, l5 = sch.get_loops(block=b0)
v6, v7, v8, v9, v10 = sch.sample_perfect_tile(loop=l3, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l11, l12, l13, l14, l15 = sch.split(loop=l3, factors=[v6, v7, v8, v9, v10], preserve_unit_iters=True)
v16, v17, v18, v19, v20 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[25, 1, 40, 1, 1])
l21, l22, l23, l24, l25 = sch.split(loop=l4, factors=[v16, v17, v18, v19, v20], preserve_unit_iters=True)
v26, v27, v28 = sch.sample_perfect_tile(loop=l5, n=3, max_innermost_factor=64, decision=[256, 2, 1])
l29, l30, l31 = sch.split(loop=l5, factors=[v26, v27, v28], preserve_unit_iters=True)
sch.reorder(l11, l21, l12, l22, l13, l23, l29, l30, l14, l24, l31, l15, l25)
l32 = sch.fuse(l11, l21, preserve_unit_iters=True)
sch.bind(loop=l32, thread_axis="blockIdx.x")
l33 = sch.fuse(l12, l22, preserve_unit_iters=True)
sch.bind(loop=l33, thread_axis="vthread.x")
l34 = sch.fuse(l13, l23, preserve_unit_iters=True)
sch.bind(loop=l34, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b35 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b35, loop=l34, preserve_unit_loops=True, index=-1)
b36 = sch.cache_read(block=b0, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b36, loop=l29, preserve_unit_loops=True, index=-1)
l37, l38, l39, l40, l41, l42 = sch.get_loops(block=b36)
l43 = sch.fuse(l41, l42, preserve_unit_iters=True)
v44 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b36, ann_key="meta_schedule.cooperative_fetch", ann_val=v44)
b45 = sch.cache_read(block=b0, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b45, loop=l29, preserve_unit_loops=True, index=-1)
l46, l47, l48, l49, l50, l51 = sch.get_loops(block=b45)
l52 = sch.fuse(l50, l51, preserve_unit_iters=True)
v53 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b45, ann_key="meta_schedule.cooperative_fetch", ann_val=v53)
l54 = sch.fuse(l29, preserve_unit_iters=True)
sch.annotate(block_or_loop=l54, ann_key="software_pipeline_stage", ann_val=[0, 0, 2])
sch.annotate(block_or_loop=l54, ann_key="software_pipeline_order", ann_val=[0, 1, 2])
sch.annotate(block_or_loop=l54, ann_key="software_pipeline_async_stages", ann_val=[0])
sch.reverse_compute_inline(block=b1)
v55 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v55)
sch.enter_postproc()
sch.unannotate(block_or_loop=b36, ann_key="meta_schedule.cooperative_fetch")
l56, l57, l58, l59, l60 = sch.get_loops(block=b36)
l61, l62, l63 = sch.split(loop=l60, factors=[None, 40, 2], preserve_unit_iters=True)
sch.vectorize(loop=l63)
sch.bind(loop=l62, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b45, ann_key="meta_schedule.cooperative_fetch")
l64, l65, l66, l67, l68 = sch.get_loops(block=b45)
l69, l70, l71 = sch.split(loop=l68, factors=[None, 40, 4], preserve_unit_iters=True)
sch.vectorize(loop=l71)
sch.bind(loop=l70, thread_axis="threadIdx.x")
b72 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b72, ann_key="meta_schedule.unroll_explicit")
b73, b74, b75, b76 = sch.get_child_blocks(b72)
l77, l78, l79, l80, l81, l82, l83 = sch.get_loops(block=b73)
l84, l85, l86, l87, l88, l89, l90 = sch.get_loops(block=b74)
l91, l92, l93, l94, l95, l96, l97, l98, l99, l100 = sch.get_loops(block=b75)
sch.annotate(block_or_loop=l91, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l91, ann_key="pragma_unroll_explicit", ann_val=1)
l101, l102, l103, l104, l105 = sch.get_loops(block=b76)
b106 = sch.get_block(name="T_matmul_NT", func_name="main")
l107, l108, l109, l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b106)
b117 = sch.decompose_reduction(block=b106, loop=l110)
2023-11-11 02:09:57 [INFO] [task_scheduler.cc:121] [Task #28: fused_nn_dense_add] Trial #74: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/home2/xiachunwei/Software/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/home2/xiachunwei/Software/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/home2/xiachunwei/Software/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/home2/xiachunwei/Software/tvm/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/home2/xiachunwei/Software/tvm/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/home2/xiachunwei/Software/tvm/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/home2/xiachunwei/Software/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/home2/xiachunwei/Software/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  4: _ZN3tvm7runtime13PackedFun
  3: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  2: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  0: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  File "/home2/xiachunwei/Software/tvm/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(512)), "float32"), p1: T.Buffer((T.int64(1000), T.int64(512)), "float32"), p2: T.Buffer((T.int64(1), T.int64(1000)), "float32"), T_add: T.Buffer((T.int64(1), T.int64(1000)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        T_matmul_NT_local = T.alloc_buffer((T.int64(1), T.int64(1000)), scope="local")
        p0_shared = T.alloc_buffer((T.int64(1), T.int64(512)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(1000), T.int64(512)), scope="shared")
        for i_0_j_0_fused in T.thread_binding(T.int64(25), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 1024, "pragma_unroll_explicit": 1}):
            for i_1_j_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for i_2_j_2_fused in T.thread_binding(T.int64(40), thread="threadIdx.x"):
                    for i_3_init, j_3_init, i_4_init, j_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_init"):
                            v_i = T.axis.spatial(T.int64(1), i_3_init + i_4_init)
                            v_j = T.axis.spatial(T.int64(1000), i_0_j_0_fused * T.int64(40) + i_2_j_2_fused + j_3_init + j_4_init)
                            T.reads()
                            T.writes(T_matmul_NT_local[v_i, v_j])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            T_matmul_NT_local[v_i, v_j] = T.float32(0)
                    for k_0_fused in T.serial(T.int64(256), annotations={"software_pipeline_async_stages": [0], "software_pipeline_order": [0, 1, 2], "software_pipeline_stage": [0, 0, 3]}):
                        for ax0_ax1_fused_0 in range(T.int64(1)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(40), thread="threadIdx.x"):
                                with T.block("p0_shared"):
                                    v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                    v1 = T.axis.spatial(T.int64(512), k_0_fused * T.int64(2) + (ax0_ax1_fused_0 * T.int64(40) + ax0_ax1_fused_1))
                                    T.where(ax0_ax1_fused_0 * T.int64(40) + ax0_ax1_fused_1 < T.int64(2))
                                    T.reads(p0[v0, v1])
                                    T.writes(p0_shared[v0, v1])
                                    p0_shared[v0, v1] = p0[v0, v1]
                        for ax0_ax1_fused_0 in range(T.int64(1)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(40), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("p1_shared"):
                                        v0 = T.axis.spatial(T.int64(1000), i_0_j_0_fused * T.int64(40) + (ax0_ax1_fused_0 * T.int64(160) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2) // T.int64(2))
                                        v1 = T.axis.spatial(T.int64(512), k_0_fused * T.int64(2) + (ax0_ax1_fused_0 * T.int64(160) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2) % T.int64(2))
                                        T.where((ax0_ax1_fused_0 * T.int64(40) + ax0_ax1_fused_1) * T.int64(4) + ax0_ax1_fused_2 < T.int64(80))
                                        T.reads(p1[v0, v1])
                                        T.writes(p1_shared[v0, v1])
                                        p1_shared[v0, v1] = p1[v0, v1]
                        for k_1, i_3, j_3, k_2, i_4, j_4 in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(1), T.int64(1)):
                            with T.block("T_matmul_NT_update"):
                                v_i = T.axis.spatial(T.int64(1), i_3 + i_4)
                                v_j = T.axis.spatial(T.int64(1000), i_0_j_0_fused * T.int64(40) + i_2_j_2_fused + j_3 + j_4)
                                v_k = T.axis.reduce(T.int64(512), k_0_fused * T.int64(2) + k_1 * T.int64(2) + k_2)
                                T.reads(T_matmul_NT_local[v_i, v_j], p0_shared[v_i, v_k], p1_shared[v_j, v_k])
                                T.writes(T_matmul_NT_local[v_i, v_j])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                T_matmul_NT_local[v_i, v_j] = T_matmul_NT_local[v_i, v_j] + p0_shared[v_i, v_k] * p1_shared[v_j, v_k]
                    for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(1000), i_0_j_0_fused * T.int64(40) + i_2_j_2_fused + ax1)
                            T.reads(T_matmul_NT_local[v0, v1], p2[v0, v1])
                            T.writes(T_add[v0, v1])
                            T_add[v0, v1] = T_matmul_NT_local[v0, v1] + p2[v0, v1]
b0 = sch.get_block(name="T_matmul_NT", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l3, l4, l5 = sch.get_loops(block=b0)
v6, v7, v8, v9, v10 = sch.sample_perfect_tile(loop=l3, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l11, l12, l13, l14, l15 = sch.split(loop=l3, factors=[v6, v7, v8, v9, v10], preserve_unit_iters=True)
v16, v17, v18, v19, v20 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[25, 1, 40, 1, 1])
l21, l22, l23, l24, l25 = sch.split(loop=l4, factors=[v16, v17, v18, v19, v20], preserve_unit_iters=True)
v26, v27, v28 = sch.sample_perfect_tile(loop=l5, n=3, max_innermost_factor=64, decision=[256, 1, 2])
l29, l30, l31 = sch.split(loop=l5, factors=[v26, v27, v28], preserve_unit_iters=True)
sch.reorder(l11, l21, l12, l22, l13, l23, l29, l30, l14, l24, l31, l15, l25)
l32 = sch.fuse(l11, l21, preserve_unit_iters=True)
sch.bind(loop=l32, thread_axis="blockIdx.x")
l33 = sch.fuse(l12, l22, preserve_unit_iters=True)
sch.bind(loop=l33, thread_axis="vthread.x")
l34 = sch.fuse(l13, l23, preserve_unit_iters=True)
sch.bind(loop=l34, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b35 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b35, loop=l34, preserve_unit_loops=True, index=-1)
b36 = sch.cache_read(block=b0, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b36, loop=l29, preserve_unit_loops=True, index=-1)
l37, l38, l39, l40, l41, l42 = sch.get_loops(block=b36)
l43 = sch.fuse(l41, l42, preserve_unit_iters=True)
v44 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b36, ann_key="meta_schedule.cooperative_fetch", ann_val=v44)
b45 = sch.cache_read(block=b0, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b45, loop=l29, preserve_unit_loops=True, index=-1)
l46, l47, l48, l49, l50, l51 = sch.get_loops(block=b45)
l52 = sch.fuse(l50, l51, preserve_unit_iters=True)
v53 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b45, ann_key="meta_schedule.cooperative_fetch", ann_val=v53)
l54 = sch.fuse(l29, preserve_unit_iters=True)
sch.annotate(block_or_loop=l54, ann_key="software_pipeline_stage", ann_val=[0, 0, 3])
sch.annotate(block_or_loop=l54, ann_key="software_pipeline_order", ann_val=[0, 1, 2])
sch.annotate(block_or_loop=l54, ann_key="software_pipeline_async_stages", ann_val=[0])
sch.reverse_compute_inline(block=b1)
v55 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v55)
sch.enter_postproc()
sch.unannotate(block_or_loop=b36, ann_key="meta_schedule.cooperative_fetch")
l56, l57, l58, l59, l60 = sch.get_loops(block=b36)
l61, l62 = sch.split(loop=l60, factors=[None, 40], preserve_unit_iters=True)
sch.bind(loop=l62, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b45, ann_key="meta_schedule.cooperative_fetch")
l63, l64, l65, l66, l67 = sch.get_loops(block=b45)
l68, l69, l70 = sch.split(loop=l67, factors=[None, 40, 4], preserve_unit_iters=True)
sch.vectorize(loop=l70)
sch.bind(loop=l69, thread_axis="threadIdx.x")
b71 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b71, ann_key="meta_schedule.unroll_explicit")
b72, b73, b74, b75 = sch.get_child_blocks(b71)
l76, l77, l78, l79, l80, l81 = sch.get_loops(block=b72)
l82, l83, l84, l85, l86, l87, l88 = sch.get_loops(block=b73)
l89, l90, l91, l92, l93, l94, l95, l96, l97, l98 = sch.get_loops(block=b74)
sch.annotate(block_or_loop=l89, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l89, ann_key="pragma_unroll_explicit", ann_val=1)
l99, l100, l101, l102, l103 = sch.get_loops(block=b75)
b104 = sch.get_block(name="T_matmul_NT", func_name="main")
l105, l106, l107, l108, l109, l110, l111, l112, l113, l114 = sch.get_loops(block=b104)
b115 = sch.decompose_reduction(block=b104, loop=l108)
2023-11-11 02:09:57 [INFO] [task_scheduler.cc:121] [Task #28: fused_nn_dense_add] Trial #75: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/home2/xiachunwei/Software/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/home2/xiachunwei/Software/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/home2/xiachunwei/Software/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/home2/xiachunwei/Software/tvm/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/home2/xiachunwei/Software/tvm/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/home2/xiachunwei/Software/tvm/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/home2/xiachunwei/Software/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/home2/xiachunwei/Software/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  4: _ZN3tvm7runtime13PackedFun
  3: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  2: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  0: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  File "/home2/xiachunwei/Software/tvm/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(512)), "float32"), p1: T.Buffer((T.int64(1000), T.int64(512)), "float32"), p2: T.Buffer((T.int64(1), T.int64(1000)), "float32"), T_add: T.Buffer((T.int64(1), T.int64(1000)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        T_matmul_NT_local = T.alloc_buffer((T.int64(1), T.int64(1000)), scope="local")
        p0_shared = T.alloc_buffer((T.int64(1), T.int64(512)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(1000), T.int64(512)), scope="shared")
        for i_0_j_0_fused in T.thread_binding(T.int64(25), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 1024, "pragma_unroll_explicit": 1}):
            for i_1_j_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for i_2_j_2_fused in T.thread_binding(T.int64(40), thread="threadIdx.x"):
                    for i_3_init, j_3_init, i_4_init, j_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_init"):
                            v_i = T.axis.spatial(T.int64(1), i_3_init + i_4_init)
                            v_j = T.axis.spatial(T.int64(1000), i_0_j_0_fused * T.int64(40) + i_2_j_2_fused + j_3_init + j_4_init)
                            T.reads()
                            T.writes(T_matmul_NT_local[v_i, v_j])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            T_matmul_NT_local[v_i, v_j] = T.float32(0)
                    for k_0_fused in T.serial(T.int64(256), annotations={"software_pipeline_async_stages": [0], "software_pipeline_order": [0, 1, 2], "software_pipeline_stage": [0, 0, 2]}):
                        for ax0_ax1_fused_0 in range(T.int64(1)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(40), thread="threadIdx.x"):
                                with T.block("p0_shared"):
                                    v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                    v1 = T.axis.spatial(T.int64(512), k_0_fused * T.int64(2) + (ax0_ax1_fused_0 * T.int64(40) + ax0_ax1_fused_1))
                                    T.where(ax0_ax1_fused_0 * T.int64(40) + ax0_ax1_fused_1 < T.int64(2))
                                    T.reads(p0[v0, v1])
                                    T.writes(p0_shared[v0, v1])
                                    p0_shared[v0, v1] = p0[v0, v1]
                        for ax0_ax1_fused_0 in range(T.int64(1)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(40), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("p1_shared"):
                                        v0 = T.axis.spatial(T.int64(1000), i_0_j_0_fused * T.int64(40) + (ax0_ax1_fused_0 * T.int64(160) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2) // T.int64(2))
                                        v1 = T.axis.spatial(T.int64(512), k_0_fused * T.int64(2) + (ax0_ax1_fused_0 * T.int64(160) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2) % T.int64(2))
                                        T.where((ax0_ax1_fused_0 * T.int64(40) + ax0_ax1_fused_1) * T.int64(4) + ax0_ax1_fused_2 < T.int64(80))
                                        T.reads(p1[v0, v1])
                                        T.writes(p1_shared[v0, v1])
                                        p1_shared[v0, v1] = p1[v0, v1]
                        for k_1, i_3, j_3, k_2, i_4, j_4 in T.grid(T.int64(2), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                            with T.block("T_matmul_NT_update"):
                                v_i = T.axis.spatial(T.int64(1), i_3 + i_4)
                                v_j = T.axis.spatial(T.int64(1000), i_0_j_0_fused * T.int64(40) + i_2_j_2_fused + j_3 + j_4)
                                v_k = T.axis.reduce(T.int64(512), k_0_fused * T.int64(2) + k_1 + k_2)
                                T.reads(T_matmul_NT_local[v_i, v_j], p0_shared[v_i, v_k], p1_shared[v_j, v_k])
                                T.writes(T_matmul_NT_local[v_i, v_j])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                T_matmul_NT_local[v_i, v_j] = T_matmul_NT_local[v_i, v_j] + p0_shared[v_i, v_k] * p1_shared[v_j, v_k]
                    for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(1000), i_0_j_0_fused * T.int64(40) + i_2_j_2_fused + ax1)
                            T.reads(T_matmul_NT_local[v0, v1], p2[v0, v1])
                            T.writes(T_add[v0, v1])
                            T_add[v0, v1] = T_matmul_NT_local[v0, v1] + p2[v0, v1]
b0 = sch.get_block(name="T_matmul_NT", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l3, l4, l5 = sch.get_loops(block=b0)
v6, v7, v8, v9, v10 = sch.sample_perfect_tile(loop=l3, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l11, l12, l13, l14, l15 = sch.split(loop=l3, factors=[v6, v7, v8, v9, v10], preserve_unit_iters=True)
v16, v17, v18, v19, v20 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[25, 1, 40, 1, 1])
l21, l22, l23, l24, l25 = sch.split(loop=l4, factors=[v16, v17, v18, v19, v20], preserve_unit_iters=True)
v26, v27, v28 = sch.sample_perfect_tile(loop=l5, n=3, max_innermost_factor=64, decision=[256, 2, 1])
l29, l30, l31 = sch.split(loop=l5, factors=[v26, v27, v28], preserve_unit_iters=True)
sch.reorder(l11, l21, l12, l22, l13, l23, l29, l30, l14, l24, l31, l15, l25)
l32 = sch.fuse(l11, l21, preserve_unit_iters=True)
sch.bind(loop=l32, thread_axis="blockIdx.x")
l33 = sch.fuse(l12, l22, preserve_unit_iters=True)
sch.bind(loop=l33, thread_axis="vthread.x")
l34 = sch.fuse(l13, l23, preserve_unit_iters=True)
sch.bind(loop=l34, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b35 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b35, loop=l34, preserve_unit_loops=True, index=-1)
b36 = sch.cache_read(block=b0, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b36, loop=l29, preserve_unit_loops=True, index=-1)
l37, l38, l39, l40, l41, l42 = sch.get_loops(block=b36)
l43 = sch.fuse(l41, l42, preserve_unit_iters=True)
v44 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b36, ann_key="meta_schedule.cooperative_fetch", ann_val=v44)
b45 = sch.cache_read(block=b0, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b45, loop=l29, preserve_unit_loops=True, index=-1)
l46, l47, l48, l49, l50, l51 = sch.get_loops(block=b45)
l52 = sch.fuse(l50, l51, preserve_unit_iters=True)
v53 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b45, ann_key="meta_schedule.cooperative_fetch", ann_val=v53)
l54 = sch.fuse(l29, preserve_unit_iters=True)
sch.annotate(block_or_loop=l54, ann_key="software_pipeline_stage", ann_val=[0, 0, 2])
sch.annotate(block_or_loop=l54, ann_key="software_pipeline_order", ann_val=[0, 1, 2])
sch.annotate(block_or_loop=l54, ann_key="software_pipeline_async_stages", ann_val=[0])
sch.reverse_compute_inline(block=b1)
v55 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v55)
sch.enter_postproc()
sch.unannotate(block_or_loop=b36, ann_key="meta_schedule.cooperative_fetch")
l56, l57, l58, l59, l60 = sch.get_loops(block=b36)
l61, l62 = sch.split(loop=l60, factors=[None, 40], preserve_unit_iters=True)
sch.bind(loop=l62, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b45, ann_key="meta_schedule.cooperative_fetch")
l63, l64, l65, l66, l67 = sch.get_loops(block=b45)
l68, l69, l70 = sch.split(loop=l67, factors=[None, 40, 4], preserve_unit_iters=True)
sch.vectorize(loop=l70)
sch.bind(loop=l69, thread_axis="threadIdx.x")
b71 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b71, ann_key="meta_schedule.unroll_explicit")
b72, b73, b74, b75 = sch.get_child_blocks(b71)
l76, l77, l78, l79, l80, l81 = sch.get_loops(block=b72)
l82, l83, l84, l85, l86, l87, l88 = sch.get_loops(block=b73)
l89, l90, l91, l92, l93, l94, l95, l96, l97, l98 = sch.get_loops(block=b74)
sch.annotate(block_or_loop=l89, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l89, ann_key="pragma_unroll_explicit", ann_val=1)
l99, l100, l101, l102, l103 = sch.get_loops(block=b75)
b104 = sch.get_block(name="T_matmul_NT", func_name="main")
l105, l106, l107, l108, l109, l110, l111, l112, l113, l114 = sch.get_loops(block=b104)
b115 = sch.decompose_reduction(block=b104, loop=l108)
2023-11-11 02:09:57 [INFO] [task_scheduler.cc:121] [Task #28: fused_nn_dense_add] Trial #76: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/home2/xiachunwei/Software/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/home2/xiachunwei/Software/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/home2/xiachunwei/Software/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/home2/xiachunwei/Software/tvm/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/home2/xiachunwei/Software/tvm/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/home2/xiachunwei/Software/tvm/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/home2/xiachunwei/Software/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/home2/xiachunwei/Software/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  4: _ZN3tvm7runtime13PackedFun
  3: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  2: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  0: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  File "/home2/xiachunwei/Software/tvm/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(512)), "float32"), p1: T.Buffer((T.int64(1000), T.int64(512)), "float32"), p2: T.Buffer((T.int64(1), T.int64(1000)), "float32"), T_add: T.Buffer((T.int64(1), T.int64(1000)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        T_matmul_NT_local = T.alloc_buffer((T.int64(1), T.int64(1000)), scope="local")
        p0_shared = T.alloc_buffer((T.int64(1), T.int64(512)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(1000), T.int64(512)), scope="shared")
        for i_0_j_0_fused in T.thread_binding(T.int64(20), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 1024, "pragma_unroll_explicit": 1}):
            for i_1_j_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for i_2_j_2_fused in T.thread_binding(T.int64(50), thread="threadIdx.x"):
                    for i_3_init, j_3_init, i_4_init, j_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_init"):
                            v_i = T.axis.spatial(T.int64(1), i_3_init + i_4_init)
                            v_j = T.axis.spatial(T.int64(1000), i_0_j_0_fused * T.int64(50) + i_2_j_2_fused + j_3_init + j_4_init)
                            T.reads()
                            T.writes(T_matmul_NT_local[v_i, v_j])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            T_matmul_NT_local[v_i, v_j] = T.float32(0)
                    for k_0_fused in T.serial(T.int64(256), annotations={"software_pipeline_async_stages": [0], "software_pipeline_order": [0, 1, 2], "software_pipeline_stage": [0, 0, 2]}):
                        for ax0_ax1_fused_0 in range(T.int64(1)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(50), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("p0_shared"):
                                        v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v1 = T.axis.spatial(T.int64(512), k_0_fused * T.int64(2) + (ax0_ax1_fused_0 * T.int64(100) + ax0_ax1_fused_1 * T.int64(2) + ax0_ax1_fused_2))
                                        T.where((ax0_ax1_fused_0 * T.int64(50) + ax0_ax1_fused_1) * T.int64(2) + ax0_ax1_fused_2 < T.int64(2))
                                        T.reads(p0[v0, v1])
                                        T.writes(p0_shared[v0, v1])
                                        p0_shared[v0, v1] = p0[v0, v1]
                        for ax0_ax1_fused_0 in range(T.int64(2)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(50), thread="threadIdx.x"):
                                with T.block("p1_shared"):
                                    v0 = T.axis.spatial(T.int64(1000), i_0_j_0_fused * T.int64(50) + (ax0_ax1_fused_0 * T.int64(50) + ax0_ax1_fused_1) // T.int64(2))
                                    v1 = T.axis.spatial(T.int64(512), k_0_fused * T.int64(2) + (ax0_ax1_fused_0 * T.int64(50) + ax0_ax1_fused_1) % T.int64(2))
                                    T.reads(p1[v0, v1])
                                    T.writes(p1_shared[v0, v1])
                                    p1_shared[v0, v1] = p1[v0, v1]
                        for k_1, i_3, j_3, k_2, i_4, j_4 in T.grid(T.int64(2), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                            with T.block("T_matmul_NT_update"):
                                v_i = T.axis.spatial(T.int64(1), i_3 + i_4)
                                v_j = T.axis.spatial(T.int64(1000), i_0_j_0_fused * T.int64(50) + i_2_j_2_fused + j_3 + j_4)
                                v_k = T.axis.reduce(T.int64(512), k_0_fused * T.int64(2) + k_1 + k_2)
                                T.reads(T_matmul_NT_local[v_i, v_j], p0_shared[v_i, v_k], p1_shared[v_j, v_k])
                                T.writes(T_matmul_NT_local[v_i, v_j])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                T_matmul_NT_local[v_i, v_j] = T_matmul_NT_local[v_i, v_j] + p0_shared[v_i, v_k] * p1_shared[v_j, v_k]
                    for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(1000), i_0_j_0_fused * T.int64(50) + i_2_j_2_fused + ax1)
                            T.reads(T_matmul_NT_local[v0, v1], p2[v0, v1])
                            T.writes(T_add[v0, v1])
                            T_add[v0, v1] = T_matmul_NT_local[v0, v1] + p2[v0, v1]
b0 = sch.get_block(name="T_matmul_NT", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l3, l4, l5 = sch.get_loops(block=b0)
v6, v7, v8, v9, v10 = sch.sample_perfect_tile(loop=l3, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l11, l12, l13, l14, l15 = sch.split(loop=l3, factors=[v6, v7, v8, v9, v10], preserve_unit_iters=True)
v16, v17, v18, v19, v20 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[20, 1, 50, 1, 1])
l21, l22, l23, l24, l25 = sch.split(loop=l4, factors=[v16, v17, v18, v19, v20], preserve_unit_iters=True)
v26, v27, v28 = sch.sample_perfect_tile(loop=l5, n=3, max_innermost_factor=64, decision=[256, 2, 1])
l29, l30, l31 = sch.split(loop=l5, factors=[v26, v27, v28], preserve_unit_iters=True)
sch.reorder(l11, l21, l12, l22, l13, l23, l29, l30, l14, l24, l31, l15, l25)
l32 = sch.fuse(l11, l21, preserve_unit_iters=True)
sch.bind(loop=l32, thread_axis="blockIdx.x")
l33 = sch.fuse(l12, l22, preserve_unit_iters=True)
sch.bind(loop=l33, thread_axis="vthread.x")
l34 = sch.fuse(l13, l23, preserve_unit_iters=True)
sch.bind(loop=l34, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b35 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b35, loop=l34, preserve_unit_loops=True, index=-1)
b36 = sch.cache_read(block=b0, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b36, loop=l29, preserve_unit_loops=True, index=-1)
l37, l38, l39, l40, l41, l42 = sch.get_loops(block=b36)
l43 = sch.fuse(l41, l42, preserve_unit_iters=True)
v44 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b36, ann_key="meta_schedule.cooperative_fetch", ann_val=v44)
b45 = sch.cache_read(block=b0, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b45, loop=l29, preserve_unit_loops=True, index=-1)
l46, l47, l48, l49, l50, l51 = sch.get_loops(block=b45)
l52 = sch.fuse(l50, l51, preserve_unit_iters=True)
v53 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b45, ann_key="meta_schedule.cooperative_fetch", ann_val=v53)
l54 = sch.fuse(l29, preserve_unit_iters=True)
sch.annotate(block_or_loop=l54, ann_key="software_pipeline_stage", ann_val=[0, 0, 2])
sch.annotate(block_or_loop=l54, ann_key="software_pipeline_order", ann_val=[0, 1, 2])
sch.annotate(block_or_loop=l54, ann_key="software_pipeline_async_stages", ann_val=[0])
sch.reverse_compute_inline(block=b1)
v55 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v55)
sch.enter_postproc()
sch.unannotate(block_or_loop=b36, ann_key="meta_schedule.cooperative_fetch")
l56, l57, l58, l59, l60 = sch.get_loops(block=b36)
l61, l62, l63 = sch.split(loop=l60, factors=[None, 50, 2], preserve_unit_iters=True)
sch.vectorize(loop=l63)
sch.bind(loop=l62, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b45, ann_key="meta_schedule.cooperative_fetch")
l64, l65, l66, l67, l68 = sch.get_loops(block=b45)
l69, l70 = sch.split(loop=l68, factors=[None, 50], preserve_unit_iters=True)
sch.bind(loop=l70, thread_axis="threadIdx.x")
b71 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b71, ann_key="meta_schedule.unroll_explicit")
b72, b73, b74, b75 = sch.get_child_blocks(b71)
l76, l77, l78, l79, l80, l81, l82 = sch.get_loops(block=b72)
l83, l84, l85, l86, l87, l88 = sch.get_loops(block=b73)
l89, l90, l91, l92, l93, l94, l95, l96, l97, l98 = sch.get_loops(block=b74)
sch.annotate(block_or_loop=l89, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l89, ann_key="pragma_unroll_explicit", ann_val=1)
l99, l100, l101, l102, l103 = sch.get_loops(block=b75)
b104 = sch.get_block(name="T_matmul_NT", func_name="main")
l105, l106, l107, l108, l109, l110, l111, l112, l113, l114 = sch.get_loops(block=b104)
b115 = sch.decompose_reduction(block=b104, loop=l108)
2023-11-11 02:09:57 [INFO] [task_scheduler.cc:121] [Task #28: fused_nn_dense_add] Trial #77: Error in running:
LocalRunner: An exception occurred
Subprocess terminated
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(512)), "float32"), p1: T.Buffer((T.int64(1000), T.int64(512)), "float32"), p2: T.Buffer((T.int64(1), T.int64(1000)), "float32"), T_add: T.Buffer((T.int64(1), T.int64(1000)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        T_matmul_NT_local = T.alloc_buffer((T.int64(1), T.int64(1000)), scope="local")
        p0_shared = T.alloc_buffer((T.int64(1), T.int64(512)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(1000), T.int64(512)), scope="shared")
        for i_0_j_0_fused in T.thread_binding(T.int64(25), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 64, "pragma_unroll_explicit": 1}):
            for i_1_j_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for i_2_j_2_fused in T.thread_binding(T.int64(40), thread="threadIdx.x"):
                    for i_3_init, j_3_init, i_4_init, j_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_init"):
                            v_i = T.axis.spatial(T.int64(1), i_3_init + i_4_init)
                            v_j = T.axis.spatial(T.int64(1000), i_0_j_0_fused * T.int64(40) + i_2_j_2_fused + j_3_init + j_4_init)
                            T.reads()
                            T.writes(T_matmul_NT_local[v_i, v_j])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            T_matmul_NT_local[v_i, v_j] = T.float32(0)
                    for k_0_fused in T.serial(T.int64(256), annotations={"software_pipeline_async_stages": [0], "software_pipeline_order": [0, 1, 2], "software_pipeline_stage": [0, 0, 3]}):
                        for ax0_ax1_fused_0 in range(T.int64(1)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(40), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("p0_shared"):
                                        v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v1 = T.axis.spatial(T.int64(512), k_0_fused * T.int64(2) + (ax0_ax1_fused_0 * T.int64(80) + ax0_ax1_fused_1 * T.int64(2) + ax0_ax1_fused_2))
                                        T.where((ax0_ax1_fused_0 * T.int64(40) + ax0_ax1_fused_1) * T.int64(2) + ax0_ax1_fused_2 < T.int64(2))
                                        T.reads(p0[v0, v1])
                                        T.writes(p0_shared[v0, v1])
                                        p0_shared[v0, v1] = p0[v0, v1]
                        for ax0_ax1_fused_0 in range(T.int64(1)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(40), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("p1_shared"):
                                        v0 = T.axis.spatial(T.int64(1000), i_0_j_0_fused * T.int64(40) + (ax0_ax1_fused_0 * T.int64(160) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2) // T.int64(2))
                                        v1 = T.axis.spatial(T.int64(512), k_0_fused * T.int64(2) + (ax0_ax1_fused_0 * T.int64(160) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2) % T.int64(2))
                                        T.where((ax0_ax1_fused_0 * T.int64(40) + ax0_ax1_fused_1) * T.int64(4) + ax0_ax1_fused_2 < T.int64(80))
                                        T.reads(p1[v0, v1])
                                        T.writes(p1_shared[v0, v1])
                                        p1_shared[v0, v1] = p1[v0, v1]
                        for k_1, i_3, j_3, k_2, i_4, j_4 in T.grid(T.int64(2), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                            with T.block("T_matmul_NT_update"):
                                v_i = T.axis.spatial(T.int64(1), i_3 + i_4)
                                v_j = T.axis.spatial(T.int64(1000), i_0_j_0_fused * T.int64(40) + i_2_j_2_fused + j_3 + j_4)
                                v_k = T.axis.reduce(T.int64(512), k_0_fused * T.int64(2) + k_1 + k_2)
                                T.reads(T_matmul_NT_local[v_i, v_j], p0_shared[v_i, v_k], p1_shared[v_j, v_k])
                                T.writes(T_matmul_NT_local[v_i, v_j])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                T_matmul_NT_local[v_i, v_j] = T_matmul_NT_local[v_i, v_j] + p0_shared[v_i, v_k] * p1_shared[v_j, v_k]
                    for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(1000), i_0_j_0_fused * T.int64(40) + i_2_j_2_fused + ax1)
                            T.reads(T_matmul_NT_local[v0, v1], p2[v0, v1])
                            T.writes(T_add[v0, v1])
                            T_add[v0, v1] = T_matmul_NT_local[v0, v1] + p2[v0, v1]
b0 = sch.get_block(name="T_matmul_NT", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l3, l4, l5 = sch.get_loops(block=b0)
v6, v7, v8, v9, v10 = sch.sample_perfect_tile(loop=l3, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l11, l12, l13, l14, l15 = sch.split(loop=l3, factors=[v6, v7, v8, v9, v10], preserve_unit_iters=True)
v16, v17, v18, v19, v20 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[25, 1, 40, 1, 1])
l21, l22, l23, l24, l25 = sch.split(loop=l4, factors=[v16, v17, v18, v19, v20], preserve_unit_iters=True)
v26, v27, v28 = sch.sample_perfect_tile(loop=l5, n=3, max_innermost_factor=64, decision=[256, 2, 1])
l29, l30, l31 = sch.split(loop=l5, factors=[v26, v27, v28], preserve_unit_iters=True)
sch.reorder(l11, l21, l12, l22, l13, l23, l29, l30, l14, l24, l31, l15, l25)
l32 = sch.fuse(l11, l21, preserve_unit_iters=True)
sch.bind(loop=l32, thread_axis="blockIdx.x")
l33 = sch.fuse(l12, l22, preserve_unit_iters=True)
sch.bind(loop=l33, thread_axis="vthread.x")
l34 = sch.fuse(l13, l23, preserve_unit_iters=True)
sch.bind(loop=l34, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b35 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b35, loop=l34, preserve_unit_loops=True, index=-1)
b36 = sch.cache_read(block=b0, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b36, loop=l29, preserve_unit_loops=True, index=-1)
l37, l38, l39, l40, l41, l42 = sch.get_loops(block=b36)
l43 = sch.fuse(l41, l42, preserve_unit_iters=True)
v44 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b36, ann_key="meta_schedule.cooperative_fetch", ann_val=v44)
b45 = sch.cache_read(block=b0, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b45, loop=l29, preserve_unit_loops=True, index=-1)
l46, l47, l48, l49, l50, l51 = sch.get_loops(block=b45)
l52 = sch.fuse(l50, l51, preserve_unit_iters=True)
v53 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b45, ann_key="meta_schedule.cooperative_fetch", ann_val=v53)
l54 = sch.fuse(l29, preserve_unit_iters=True)
sch.annotate(block_or_loop=l54, ann_key="software_pipeline_stage", ann_val=[0, 0, 3])
sch.annotate(block_or_loop=l54, ann_key="software_pipeline_order", ann_val=[0, 1, 2])
sch.annotate(block_or_loop=l54, ann_key="software_pipeline_async_stages", ann_val=[0])
sch.reverse_compute_inline(block=b1)
v55 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=2)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v55)
sch.enter_postproc()
sch.unannotate(block_or_loop=b36, ann_key="meta_schedule.cooperative_fetch")
l56, l57, l58, l59, l60 = sch.get_loops(block=b36)
l61, l62, l63 = sch.split(loop=l60, factors=[None, 40, 2], preserve_unit_iters=True)
sch.vectorize(loop=l63)
sch.bind(loop=l62, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b45, ann_key="meta_schedule.cooperative_fetch")
l64, l65, l66, l67, l68 = sch.get_loops(block=b45)
l69, l70, l71 = sch.split(loop=l68, factors=[None, 40, 4], preserve_unit_iters=True)
sch.vectorize(loop=l71)
sch.bind(loop=l70, thread_axis="threadIdx.x")
b72 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b72, ann_key="meta_schedule.unroll_explicit")
b73, b74, b75, b76 = sch.get_child_blocks(b72)
l77, l78, l79, l80, l81, l82, l83 = sch.get_loops(block=b73)
l84, l85, l86, l87, l88, l89, l90 = sch.get_loops(block=b74)
l91, l92, l93, l94, l95, l96, l97, l98, l99, l100 = sch.get_loops(block=b75)
sch.annotate(block_or_loop=l91, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l91, ann_key="pragma_unroll_explicit", ann_val=1)
l101, l102, l103, l104, l105 = sch.get_loops(block=b76)
b106 = sch.get_block(name="T_matmul_NT", func_name="main")
l107, l108, l109, l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b106)
b117 = sch.decompose_reduction(block=b106, loop=l110)
2023-11-11 02:09:57 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #78: GFLOPs: 15.5277. Time: 66.0112 us. Best GFLOPs: 80.9361
2023-11-11 02:09:57 [INFO] [task_scheduler.cc:121] [Task #28: fused_nn_dense_add] Trial #79: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/home2/xiachunwei/Software/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/home2/xiachunwei/Software/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 159, in _worker_func
    costs: List[float] = f_run_evaluator(
  File "/home2/xiachunwei/Software/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 387, in default_run_evaluator
    return run_evaluator_common(rt_mod, device, evaluator_config, repeated_args)
  File "/home2/xiachunwei/Software/tvm/python/tvm/meta_schedule/runner/utils.py", line 117, in run_evaluator_common
    profile_result = evaluator(*args)
  File "/home2/xiachunwei/Software/tvm/python/tvm/runtime/module.py", line 397, in evaluator
    blob = feval(*args)
  File "/home2/xiachunwei/Software/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/home2/xiachunwei/Software/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  1: tvm::runtime::profiling::WrapTimeEvaluator(tvm::runtime::PackedFunc, DLDevice, int, int, int, int, int, int, int, tvm::runtime::PackedFunc)::{lambda(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*) [clone .isra.779]
  0: tvm::runtime::CUDADeviceAPI::StreamSync(DLDevice, void*)
  File "/home2/xiachunwei/Software/tvm/src/runtime/cuda/cuda_device_api.cc", line 212
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(512)), "float32"), p1: T.Buffer((T.int64(1000), T.int64(512)), "float32"), p2: T.Buffer((T.int64(1), T.int64(1000)), "float32"), T_add: T.Buffer((T.int64(1), T.int64(1000)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        T_matmul_NT_local = T.alloc_buffer((T.int64(1), T.int64(1000)), scope="local")
        p0_shared = T.alloc_buffer((T.int64(1), T.int64(512)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(1000), T.int64(512)), scope="shared")
        for i_0_j_0_fused in T.thread_binding(T.int64(25), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 64, "pragma_unroll_explicit": 1}):
            for i_1_j_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for i_2_j_2_fused in T.thread_binding(T.int64(40), thread="threadIdx.x"):
                    for i_3_init, j_3_init, i_4_init, j_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_init"):
                            v_i = T.axis.spatial(T.int64(1), i_3_init + i_4_init)
                            v_j = T.axis.spatial(T.int64(1000), i_0_j_0_fused * T.int64(40) + i_2_j_2_fused + j_3_init + j_4_init)
                            T.reads()
                            T.writes(T_matmul_NT_local[v_i, v_j])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            T_matmul_NT_local[v_i, v_j] = T.float32(0)
                    for k_0_fused in T.serial(T.int64(256), annotations={"software_pipeline_async_stages": [0], "software_pipeline_order": [0, 1, 2], "software_pipeline_stage": [0, 0, 2]}):
                        for ax0_ax1_fused_0 in range(T.int64(1)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(40), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("p0_shared"):
                                        v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v1 = T.axis.spatial(T.int64(512), k_0_fused * T.int64(2) + (ax0_ax1_fused_0 * T.int64(80) + ax0_ax1_fused_1 * T.int64(2) + ax0_ax1_fused_2))
                                        T.where((ax0_ax1_fused_0 * T.int64(40) + ax0_ax1_fused_1) * T.int64(2) + ax0_ax1_fused_2 < T.int64(2))
                                        T.reads(p0[v0, v1])
                                        T.writes(p0_shared[v0, v1])
                                        p0_shared[v0, v1] = p0[v0, v1]
                        for ax0_ax1_fused_0 in range(T.int64(1)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(40), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("p1_shared"):
                                        v0 = T.axis.spatial(T.int64(1000), i_0_j_0_fused * T.int64(40) + (ax0_ax1_fused_0 * T.int64(160) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2) // T.int64(2))
                                        v1 = T.axis.spatial(T.int64(512), k_0_fused * T.int64(2) + (ax0_ax1_fused_0 * T.int64(160) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2) % T.int64(2))
                                        T.where((ax0_ax1_fused_0 * T.int64(40) + ax0_ax1_fused_1) * T.int64(4) + ax0_ax1_fused_2 < T.int64(80))
                                        T.reads(p1[v0, v1])
                                        T.writes(p1_shared[v0, v1])
                                        p1_shared[v0, v1] = p1[v0, v1]
                        for k_1, i_3, j_3, k_2, i_4, j_4 in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(1), T.int64(1)):
                            with T.block("T_matmul_NT_update"):
                                v_i = T.axis.spatial(T.int64(1), i_3 + i_4)
                                v_j = T.axis.spatial(T.int64(1000), i_0_j_0_fused * T.int64(40) + i_2_j_2_fused + j_3 + j_4)
                                v_k = T.axis.reduce(T.int64(512), k_0_fused * T.int64(2) + k_1 * T.int64(2) + k_2)
                                T.reads(T_matmul_NT_local[v_i, v_j], p0_shared[v_i, v_k], p1_shared[v_j, v_k])
                                T.writes(T_matmul_NT_local[v_i, v_j])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                T_matmul_NT_local[v_i, v_j] = T_matmul_NT_local[v_i, v_j] + p0_shared[v_i, v_k] * p1_shared[v_j, v_k]
                    for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(1000), i_0_j_0_fused * T.int64(40) + i_2_j_2_fused + ax1)
                            T.reads(T_matmul_NT_local[v0, v1], p2[v0, v1])
                            T.writes(T_add[v0, v1])
                            T_add[v0, v1] = T_matmul_NT_local[v0, v1] + p2[v0, v1]
b0 = sch.get_block(name="T_matmul_NT", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l3, l4, l5 = sch.get_loops(block=b0)
v6, v7, v8, v9, v10 = sch.sample_perfect_tile(loop=l3, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l11, l12, l13, l14, l15 = sch.split(loop=l3, factors=[v6, v7, v8, v9, v10], preserve_unit_iters=True)
v16, v17, v18, v19, v20 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[25, 1, 40, 1, 1])
l21, l22, l23, l24, l25 = sch.split(loop=l4, factors=[v16, v17, v18, v19, v20], preserve_unit_iters=True)
v26, v27, v28 = sch.sample_perfect_tile(loop=l5, n=3, max_innermost_factor=64, decision=[256, 1, 2])
l29, l30, l31 = sch.split(loop=l5, factors=[v26, v27, v28], preserve_unit_iters=True)
sch.reorder(l11, l21, l12, l22, l13, l23, l29, l30, l14, l24, l31, l15, l25)
l32 = sch.fuse(l11, l21, preserve_unit_iters=True)
sch.bind(loop=l32, thread_axis="blockIdx.x")
l33 = sch.fuse(l12, l22, preserve_unit_iters=True)
sch.bind(loop=l33, thread_axis="vthread.x")
l34 = sch.fuse(l13, l23, preserve_unit_iters=True)
sch.bind(loop=l34, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b35 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b35, loop=l34, preserve_unit_loops=True, index=-1)
b36 = sch.cache_read(block=b0, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b36, loop=l29, preserve_unit_loops=True, index=-1)
l37, l38, l39, l40, l41, l42 = sch.get_loops(block=b36)
l43 = sch.fuse(l41, l42, preserve_unit_iters=True)
v44 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b36, ann_key="meta_schedule.cooperative_fetch", ann_val=v44)
b45 = sch.cache_read(block=b0, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b45, loop=l29, preserve_unit_loops=True, index=-1)
l46, l47, l48, l49, l50, l51 = sch.get_loops(block=b45)
l52 = sch.fuse(l50, l51, preserve_unit_iters=True)
v53 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b45, ann_key="meta_schedule.cooperative_fetch", ann_val=v53)
l54 = sch.fuse(l29, preserve_unit_iters=True)
sch.annotate(block_or_loop=l54, ann_key="software_pipeline_stage", ann_val=[0, 0, 2])
sch.annotate(block_or_loop=l54, ann_key="software_pipeline_order", ann_val=[0, 1, 2])
sch.annotate(block_or_loop=l54, ann_key="software_pipeline_async_stages", ann_val=[0])
sch.reverse_compute_inline(block=b1)
v55 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=2)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v55)
sch.enter_postproc()
sch.unannotate(block_or_loop=b36, ann_key="meta_schedule.cooperative_fetch")
l56, l57, l58, l59, l60 = sch.get_loops(block=b36)
l61, l62, l63 = sch.split(loop=l60, factors=[None, 40, 2], preserve_unit_iters=True)
sch.vectorize(loop=l63)
sch.bind(loop=l62, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b45, ann_key="meta_schedule.cooperative_fetch")
l64, l65, l66, l67, l68 = sch.get_loops(block=b45)
l69, l70, l71 = sch.split(loop=l68, factors=[None, 40, 4], preserve_unit_iters=True)
sch.vectorize(loop=l71)
sch.bind(loop=l70, thread_axis="threadIdx.x")
b72 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b72, ann_key="meta_schedule.unroll_explicit")
b73, b74, b75, b76 = sch.get_child_blocks(b72)
l77, l78, l79, l80, l81, l82, l83 = sch.get_loops(block=b73)
l84, l85, l86, l87, l88, l89, l90 = sch.get_loops(block=b74)
l91, l92, l93, l94, l95, l96, l97, l98, l99, l100 = sch.get_loops(block=b75)
sch.annotate(block_or_loop=l91, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l91, ann_key="pragma_unroll_explicit", ann_val=1)
l101, l102, l103, l104, l105 = sch.get_loops(block=b76)
b106 = sch.get_block(name="T_matmul_NT", func_name="main")
l107, l108, l109, l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b106)
b117 = sch.decompose_reduction(block=b106, loop=l110)
2023-11-11 02:09:57 [INFO] [task_scheduler.cc:121] [Task #28: fused_nn_dense_add] Trial #80: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/home2/xiachunwei/Software/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/home2/xiachunwei/Software/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/home2/xiachunwei/Software/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/home2/xiachunwei/Software/tvm/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/home2/xiachunwei/Software/tvm/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/home2/xiachunwei/Software/tvm/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/home2/xiachunwei/Software/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/home2/xiachunwei/Software/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  4: _ZN3tvm7runtime13PackedFun
  3: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  2: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  0: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  File "/home2/xiachunwei/Software/tvm/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(512)), "float32"), p1: T.Buffer((T.int64(1000), T.int64(512)), "float32"), p2: T.Buffer((T.int64(1), T.int64(1000)), "float32"), T_add: T.Buffer((T.int64(1), T.int64(1000)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        T_matmul_NT_local = T.alloc_buffer((T.int64(1), T.int64(1000)), scope="local")
        p0_shared = T.alloc_buffer((T.int64(1), T.int64(512)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(1000), T.int64(512)), scope="shared")
        for i_0_j_0_fused in T.thread_binding(T.int64(25), thread="blockIdx.x"):
            for i_1_j_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for i_2_j_2_fused in T.thread_binding(T.int64(40), thread="threadIdx.x"):
                    for i_3_init, j_3_init, i_4_init, j_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_init"):
                            v_i = T.axis.spatial(T.int64(1), i_3_init + i_4_init)
                            v_j = T.axis.spatial(T.int64(1000), i_0_j_0_fused * T.int64(40) + i_2_j_2_fused + j_3_init + j_4_init)
                            T.reads()
                            T.writes(T_matmul_NT_local[v_i, v_j])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            T_matmul_NT_local[v_i, v_j] = T.float32(0)
                    for k_0_fused in T.serial(T.int64(256), annotations={"software_pipeline_async_stages": [0], "software_pipeline_order": [0, 1, 2], "software_pipeline_stage": [0, 0, 3]}):
                        for ax0_ax1_fused_0 in range(T.int64(1)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(40), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("p0_shared"):
                                        v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v1 = T.axis.spatial(T.int64(512), k_0_fused * T.int64(2) + (ax0_ax1_fused_0 * T.int64(80) + ax0_ax1_fused_1 * T.int64(2) + ax0_ax1_fused_2))
                                        T.where((ax0_ax1_fused_0 * T.int64(40) + ax0_ax1_fused_1) * T.int64(2) + ax0_ax1_fused_2 < T.int64(2))
                                        T.reads(p0[v0, v1])
                                        T.writes(p0_shared[v0, v1])
                                        p0_shared[v0, v1] = p0[v0, v1]
                        for ax0_ax1_fused_0 in range(T.int64(1)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(40), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("p1_shared"):
                                        v0 = T.axis.spatial(T.int64(1000), i_0_j_0_fused * T.int64(40) + (ax0_ax1_fused_0 * T.int64(160) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2) // T.int64(2))
                                        v1 = T.axis.spatial(T.int64(512), k_0_fused * T.int64(2) + (ax0_ax1_fused_0 * T.int64(160) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2) % T.int64(2))
                                        T.where((ax0_ax1_fused_0 * T.int64(40) + ax0_ax1_fused_1) * T.int64(4) + ax0_ax1_fused_2 < T.int64(80))
                                        T.reads(p1[v0, v1])
                                        T.writes(p1_shared[v0, v1])
                                        p1_shared[v0, v1] = p1[v0, v1]
                        for k_1, i_3, j_3, k_2, i_4, j_4 in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(1), T.int64(1)):
                            with T.block("T_matmul_NT_update"):
                                v_i = T.axis.spatial(T.int64(1), i_3 + i_4)
                                v_j = T.axis.spatial(T.int64(1000), i_0_j_0_fused * T.int64(40) + i_2_j_2_fused + j_3 + j_4)
                                v_k = T.axis.reduce(T.int64(512), k_0_fused * T.int64(2) + k_1 * T.int64(2) + k_2)
                                T.reads(T_matmul_NT_local[v_i, v_j], p0_shared[v_i, v_k], p1_shared[v_j, v_k])
                                T.writes(T_matmul_NT_local[v_i, v_j])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                T_matmul_NT_local[v_i, v_j] = T_matmul_NT_local[v_i, v_j] + p0_shared[v_i, v_k] * p1_shared[v_j, v_k]
                    for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(1000), i_0_j_0_fused * T.int64(40) + i_2_j_2_fused + ax1)
                            T.reads(T_matmul_NT_local[v0, v1], p2[v0, v1])
                            T.writes(T_add[v0, v1])
                            T_add[v0, v1] = T_matmul_NT_local[v0, v1] + p2[v0, v1]
b0 = sch.get_block(name="T_matmul_NT", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l3, l4, l5 = sch.get_loops(block=b0)
v6, v7, v8, v9, v10 = sch.sample_perfect_tile(loop=l3, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l11, l12, l13, l14, l15 = sch.split(loop=l3, factors=[v6, v7, v8, v9, v10], preserve_unit_iters=True)
v16, v17, v18, v19, v20 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[25, 1, 40, 1, 1])
l21, l22, l23, l24, l25 = sch.split(loop=l4, factors=[v16, v17, v18, v19, v20], preserve_unit_iters=True)
v26, v27, v28 = sch.sample_perfect_tile(loop=l5, n=3, max_innermost_factor=64, decision=[256, 1, 2])
l29, l30, l31 = sch.split(loop=l5, factors=[v26, v27, v28], preserve_unit_iters=True)
sch.reorder(l11, l21, l12, l22, l13, l23, l29, l30, l14, l24, l31, l15, l25)
l32 = sch.fuse(l11, l21, preserve_unit_iters=True)
sch.bind(loop=l32, thread_axis="blockIdx.x")
l33 = sch.fuse(l12, l22, preserve_unit_iters=True)
sch.bind(loop=l33, thread_axis="vthread.x")
l34 = sch.fuse(l13, l23, preserve_unit_iters=True)
sch.bind(loop=l34, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b35 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b35, loop=l34, preserve_unit_loops=True, index=-1)
b36 = sch.cache_read(block=b0, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b36, loop=l29, preserve_unit_loops=True, index=-1)
l37, l38, l39, l40, l41, l42 = sch.get_loops(block=b36)
l43 = sch.fuse(l41, l42, preserve_unit_iters=True)
v44 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b36, ann_key="meta_schedule.cooperative_fetch", ann_val=v44)
b45 = sch.cache_read(block=b0, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b45, loop=l29, preserve_unit_loops=True, index=-1)
l46, l47, l48, l49, l50, l51 = sch.get_loops(block=b45)
l52 = sch.fuse(l50, l51, preserve_unit_iters=True)
v53 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b45, ann_key="meta_schedule.cooperative_fetch", ann_val=v53)
l54 = sch.fuse(l29, preserve_unit_iters=True)
sch.annotate(block_or_loop=l54, ann_key="software_pipeline_stage", ann_val=[0, 0, 3])
sch.annotate(block_or_loop=l54, ann_key="software_pipeline_order", ann_val=[0, 1, 2])
sch.annotate(block_or_loop=l54, ann_key="software_pipeline_async_stages", ann_val=[0])
sch.reverse_compute_inline(block=b1)
v55 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=0)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v55)
sch.enter_postproc()
sch.unannotate(block_or_loop=b36, ann_key="meta_schedule.cooperative_fetch")
l56, l57, l58, l59, l60 = sch.get_loops(block=b36)
l61, l62, l63 = sch.split(loop=l60, factors=[None, 40, 2], preserve_unit_iters=True)
sch.vectorize(loop=l63)
sch.bind(loop=l62, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b45, ann_key="meta_schedule.cooperative_fetch")
l64, l65, l66, l67, l68 = sch.get_loops(block=b45)
l69, l70, l71 = sch.split(loop=l68, factors=[None, 40, 4], preserve_unit_iters=True)
sch.vectorize(loop=l71)
sch.bind(loop=l70, thread_axis="threadIdx.x")
b72 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b72, ann_key="meta_schedule.unroll_explicit")
b73, b74, b75, b76 = sch.get_child_blocks(b72)
l77, l78, l79, l80, l81, l82, l83 = sch.get_loops(block=b73)
l84, l85, l86, l87, l88, l89, l90 = sch.get_loops(block=b74)
l91, l92, l93, l94, l95, l96, l97, l98, l99, l100 = sch.get_loops(block=b75)
l101, l102, l103, l104, l105 = sch.get_loops(block=b76)
b106 = sch.get_block(name="T_matmul_NT", func_name="main")
l107, l108, l109, l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b106)
b117 = sch.decompose_reduction(block=b106, loop=l110)
2023-11-11 02:09:57 [INFO] [task_scheduler.cc:121] [Task #28: fused_nn_dense_add] Trial #81: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/home2/xiachunwei/Software/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/home2/xiachunwei/Software/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/home2/xiachunwei/Software/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/home2/xiachunwei/Software/tvm/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/home2/xiachunwei/Software/tvm/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/home2/xiachunwei/Software/tvm/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/home2/xiachunwei/Software/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/home2/xiachunwei/Software/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  4: _ZN3tvm7runtime13PackedFun
  3: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  2: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  0: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  File "/home2/xiachunwei/Software/tvm/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(512)), "float32"), p1: T.Buffer((T.int64(1000), T.int64(512)), "float32"), p2: T.Buffer((T.int64(1), T.int64(1000)), "float32"), T_add: T.Buffer((T.int64(1), T.int64(1000)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        T_matmul_NT_local = T.alloc_buffer((T.int64(1), T.int64(1000)), scope="local")
        p0_shared = T.alloc_buffer((T.int64(1), T.int64(512)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(1000), T.int64(512)), scope="shared")
        for i_0_j_0_fused in T.thread_binding(T.int64(25), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 512, "pragma_unroll_explicit": 1}):
            for i_1_j_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for i_2_j_2_fused in T.thread_binding(T.int64(40), thread="threadIdx.x"):
                    for i_3_init, j_3_init, i_4_init, j_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_init"):
                            v_i = T.axis.spatial(T.int64(1), i_3_init + i_4_init)
                            v_j = T.axis.spatial(T.int64(1000), i_0_j_0_fused * T.int64(40) + i_2_j_2_fused + j_3_init + j_4_init)
                            T.reads()
                            T.writes(T_matmul_NT_local[v_i, v_j])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            T_matmul_NT_local[v_i, v_j] = T.float32(0)
                    for k_0_fused in T.serial(T.int64(256), annotations={"software_pipeline_async_stages": [0], "software_pipeline_order": [0, 1, 2], "software_pipeline_stage": [0, 0, 2]}):
                        for ax0_ax1_fused_0 in range(T.int64(1)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(40), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("p0_shared"):
                                        v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v1 = T.axis.spatial(T.int64(512), k_0_fused * T.int64(2) + (ax0_ax1_fused_0 * T.int64(80) + ax0_ax1_fused_1 * T.int64(2) + ax0_ax1_fused_2))
                                        T.where((ax0_ax1_fused_0 * T.int64(40) + ax0_ax1_fused_1) * T.int64(2) + ax0_ax1_fused_2 < T.int64(2))
                                        T.reads(p0[v0, v1])
                                        T.writes(p0_shared[v0, v1])
                                        p0_shared[v0, v1] = p0[v0, v1]
                        for ax0_ax1_fused_0 in range(T.int64(2)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(40), thread="threadIdx.x"):
                                with T.block("p1_shared"):
                                    v0 = T.axis.spatial(T.int64(1000), i_0_j_0_fused * T.int64(40) + (ax0_ax1_fused_0 * T.int64(40) + ax0_ax1_fused_1) // T.int64(2))
                                    v1 = T.axis.spatial(T.int64(512), k_0_fused * T.int64(2) + (ax0_ax1_fused_0 * T.int64(40) + ax0_ax1_fused_1) % T.int64(2))
                                    T.reads(p1[v0, v1])
                                    T.writes(p1_shared[v0, v1])
                                    p1_shared[v0, v1] = p1[v0, v1]
                        for k_1, i_3, j_3, k_2, i_4, j_4 in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(1), T.int64(1)):
                            with T.block("T_matmul_NT_update"):
                                v_i = T.axis.spatial(T.int64(1), i_3 + i_4)
                                v_j = T.axis.spatial(T.int64(1000), i_0_j_0_fused * T.int64(40) + i_2_j_2_fused + j_3 + j_4)
                                v_k = T.axis.reduce(T.int64(512), k_0_fused * T.int64(2) + k_1 * T.int64(2) + k_2)
                                T.reads(T_matmul_NT_local[v_i, v_j], p0_shared[v_i, v_k], p1_shared[v_j, v_k])
                                T.writes(T_matmul_NT_local[v_i, v_j])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                T_matmul_NT_local[v_i, v_j] = T_matmul_NT_local[v_i, v_j] + p0_shared[v_i, v_k] * p1_shared[v_j, v_k]
                    for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(1000), i_0_j_0_fused * T.int64(40) + i_2_j_2_fused + ax1)
                            T.reads(T_matmul_NT_local[v0, v1], p2[v0, v1])
                            T.writes(T_add[v0, v1])
                            T_add[v0, v1] = T_matmul_NT_local[v0, v1] + p2[v0, v1]
b0 = sch.get_block(name="T_matmul_NT", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l3, l4, l5 = sch.get_loops(block=b0)
v6, v7, v8, v9, v10 = sch.sample_perfect_tile(loop=l3, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l11, l12, l13, l14, l15 = sch.split(loop=l3, factors=[v6, v7, v8, v9, v10], preserve_unit_iters=True)
v16, v17, v18, v19, v20 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[25, 1, 40, 1, 1])
l21, l22, l23, l24, l25 = sch.split(loop=l4, factors=[v16, v17, v18, v19, v20], preserve_unit_iters=True)
v26, v27, v28 = sch.sample_perfect_tile(loop=l5, n=3, max_innermost_factor=64, decision=[256, 1, 2])
l29, l30, l31 = sch.split(loop=l5, factors=[v26, v27, v28], preserve_unit_iters=True)
sch.reorder(l11, l21, l12, l22, l13, l23, l29, l30, l14, l24, l31, l15, l25)
l32 = sch.fuse(l11, l21, preserve_unit_iters=True)
sch.bind(loop=l32, thread_axis="blockIdx.x")
l33 = sch.fuse(l12, l22, preserve_unit_iters=True)
sch.bind(loop=l33, thread_axis="vthread.x")
l34 = sch.fuse(l13, l23, preserve_unit_iters=True)
sch.bind(loop=l34, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b35 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b35, loop=l34, preserve_unit_loops=True, index=-1)
b36 = sch.cache_read(block=b0, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b36, loop=l29, preserve_unit_loops=True, index=-1)
l37, l38, l39, l40, l41, l42 = sch.get_loops(block=b36)
l43 = sch.fuse(l41, l42, preserve_unit_iters=True)
v44 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b36, ann_key="meta_schedule.cooperative_fetch", ann_val=v44)
b45 = sch.cache_read(block=b0, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b45, loop=l29, preserve_unit_loops=True, index=-1)
l46, l47, l48, l49, l50, l51 = sch.get_loops(block=b45)
l52 = sch.fuse(l50, l51, preserve_unit_iters=True)
v53 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b45, ann_key="meta_schedule.cooperative_fetch", ann_val=v53)
l54 = sch.fuse(l29, preserve_unit_iters=True)
sch.annotate(block_or_loop=l54, ann_key="software_pipeline_stage", ann_val=[0, 0, 2])
sch.annotate(block_or_loop=l54, ann_key="software_pipeline_order", ann_val=[0, 1, 2])
sch.annotate(block_or_loop=l54, ann_key="software_pipeline_async_stages", ann_val=[0])
sch.reverse_compute_inline(block=b1)
v55 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v55)
sch.enter_postproc()
sch.unannotate(block_or_loop=b36, ann_key="meta_schedule.cooperative_fetch")
l56, l57, l58, l59, l60 = sch.get_loops(block=b36)
l61, l62, l63 = sch.split(loop=l60, factors=[None, 40, 2], preserve_unit_iters=True)
sch.vectorize(loop=l63)
sch.bind(loop=l62, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b45, ann_key="meta_schedule.cooperative_fetch")
l64, l65, l66, l67, l68 = sch.get_loops(block=b45)
l69, l70 = sch.split(loop=l68, factors=[None, 40], preserve_unit_iters=True)
sch.bind(loop=l70, thread_axis="threadIdx.x")
b71 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b71, ann_key="meta_schedule.unroll_explicit")
b72, b73, b74, b75 = sch.get_child_blocks(b71)
l76, l77, l78, l79, l80, l81, l82 = sch.get_loops(block=b72)
l83, l84, l85, l86, l87, l88 = sch.get_loops(block=b73)
l89, l90, l91, l92, l93, l94, l95, l96, l97, l98 = sch.get_loops(block=b74)
sch.annotate(block_or_loop=l89, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l89, ann_key="pragma_unroll_explicit", ann_val=1)
l99, l100, l101, l102, l103 = sch.get_loops(block=b75)
b104 = sch.get_block(name="T_matmul_NT", func_name="main")
l105, l106, l107, l108, l109, l110, l111, l112, l113, l114 = sch.get_loops(block=b104)
b115 = sch.decompose_reduction(block=b104, loop=l108)
2023-11-11 02:09:57 [INFO] [task_scheduler.cc:121] [Task #28: fused_nn_dense_add] Trial #82: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/home2/xiachunwei/Software/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/home2/xiachunwei/Software/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/home2/xiachunwei/Software/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/home2/xiachunwei/Software/tvm/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/home2/xiachunwei/Software/tvm/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/home2/xiachunwei/Software/tvm/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/home2/xiachunwei/Software/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/home2/xiachunwei/Software/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  4: _ZN3tvm7runtime13PackedFun
  3: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  2: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  0: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  File "/home2/xiachunwei/Software/tvm/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(512)), "float32"), p1: T.Buffer((T.int64(1000), T.int64(512)), "float32"), p2: T.Buffer((T.int64(1), T.int64(1000)), "float32"), T_add: T.Buffer((T.int64(1), T.int64(1000)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        T_matmul_NT_local = T.alloc_buffer((T.int64(1), T.int64(1000)), scope="local")
        p0_shared = T.alloc_buffer((T.int64(1), T.int64(512)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(1000), T.int64(512)), scope="shared")
        for i_0_j_0_fused in T.thread_binding(T.int64(25), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 512, "pragma_unroll_explicit": 1}):
            for i_1_j_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for i_2_j_2_fused in T.thread_binding(T.int64(40), thread="threadIdx.x"):
                    for i_3_init, j_3_init, i_4_init, j_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_init"):
                            v_i = T.axis.spatial(T.int64(1), i_3_init + i_4_init)
                            v_j = T.axis.spatial(T.int64(1000), i_0_j_0_fused * T.int64(40) + i_2_j_2_fused + j_3_init + j_4_init)
                            T.reads()
                            T.writes(T_matmul_NT_local[v_i, v_j])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            T_matmul_NT_local[v_i, v_j] = T.float32(0)
                    for k_0_fused in T.serial(T.int64(256), annotations={"software_pipeline_async_stages": [0], "software_pipeline_order": [0, 1, 2], "software_pipeline_stage": [0, 0, 2]}):
                        for ax0_ax1_fused_0 in range(T.int64(1)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(40), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("p0_shared"):
                                        v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v1 = T.axis.spatial(T.int64(512), k_0_fused * T.int64(2) + (ax0_ax1_fused_0 * T.int64(80) + ax0_ax1_fused_1 * T.int64(2) + ax0_ax1_fused_2))
                                        T.where((ax0_ax1_fused_0 * T.int64(40) + ax0_ax1_fused_1) * T.int64(2) + ax0_ax1_fused_2 < T.int64(2))
                                        T.reads(p0[v0, v1])
                                        T.writes(p0_shared[v0, v1])
                                        p0_shared[v0, v1] = p0[v0, v1]
                        for ax0_ax1_fused_0 in range(T.int64(2)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(40), thread="threadIdx.x"):
                                with T.block("p1_shared"):
                                    v0 = T.axis.spatial(T.int64(1000), i_0_j_0_fused * T.int64(40) + (ax0_ax1_fused_0 * T.int64(40) + ax0_ax1_fused_1) // T.int64(2))
                                    v1 = T.axis.spatial(T.int64(512), k_0_fused * T.int64(2) + (ax0_ax1_fused_0 * T.int64(40) + ax0_ax1_fused_1) % T.int64(2))
                                    T.reads(p1[v0, v1])
                                    T.writes(p1_shared[v0, v1])
                                    p1_shared[v0, v1] = p1[v0, v1]
                        for k_1, i_3, j_3, k_2, i_4, j_4 in T.grid(T.int64(2), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                            with T.block("T_matmul_NT_update"):
                                v_i = T.axis.spatial(T.int64(1), i_3 + i_4)
                                v_j = T.axis.spatial(T.int64(1000), i_0_j_0_fused * T.int64(40) + i_2_j_2_fused + j_3 + j_4)
                                v_k = T.axis.reduce(T.int64(512), k_0_fused * T.int64(2) + k_1 + k_2)
                                T.reads(T_matmul_NT_local[v_i, v_j], p0_shared[v_i, v_k], p1_shared[v_j, v_k])
                                T.writes(T_matmul_NT_local[v_i, v_j])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                T_matmul_NT_local[v_i, v_j] = T_matmul_NT_local[v_i, v_j] + p0_shared[v_i, v_k] * p1_shared[v_j, v_k]
                    for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(1000), i_0_j_0_fused * T.int64(40) + i_2_j_2_fused + ax1)
                            T.reads(T_matmul_NT_local[v0, v1], p2[v0, v1])
                            T.writes(T_add[v0, v1])
                            T_add[v0, v1] = T_matmul_NT_local[v0, v1] + p2[v0, v1]
b0 = sch.get_block(name="T_matmul_NT", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l3, l4, l5 = sch.get_loops(block=b0)
v6, v7, v8, v9, v10 = sch.sample_perfect_tile(loop=l3, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l11, l12, l13, l14, l15 = sch.split(loop=l3, factors=[v6, v7, v8, v9, v10], preserve_unit_iters=True)
v16, v17, v18, v19, v20 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[25, 1, 40, 1, 1])
l21, l22, l23, l24, l25 = sch.split(loop=l4, factors=[v16, v17, v18, v19, v20], preserve_unit_iters=True)
v26, v27, v28 = sch.sample_perfect_tile(loop=l5, n=3, max_innermost_factor=64, decision=[256, 2, 1])
l29, l30, l31 = sch.split(loop=l5, factors=[v26, v27, v28], preserve_unit_iters=True)
sch.reorder(l11, l21, l12, l22, l13, l23, l29, l30, l14, l24, l31, l15, l25)
l32 = sch.fuse(l11, l21, preserve_unit_iters=True)
sch.bind(loop=l32, thread_axis="blockIdx.x")
l33 = sch.fuse(l12, l22, preserve_unit_iters=True)
sch.bind(loop=l33, thread_axis="vthread.x")
l34 = sch.fuse(l13, l23, preserve_unit_iters=True)
sch.bind(loop=l34, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b35 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b35, loop=l34, preserve_unit_loops=True, index=-1)
b36 = sch.cache_read(block=b0, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b36, loop=l29, preserve_unit_loops=True, index=-1)
l37, l38, l39, l40, l41, l42 = sch.get_loops(block=b36)
l43 = sch.fuse(l41, l42, preserve_unit_iters=True)
v44 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b36, ann_key="meta_schedule.cooperative_fetch", ann_val=v44)
b45 = sch.cache_read(block=b0, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b45, loop=l29, preserve_unit_loops=True, index=-1)
l46, l47, l48, l49, l50, l51 = sch.get_loops(block=b45)
l52 = sch.fuse(l50, l51, preserve_unit_iters=True)
v53 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b45, ann_key="meta_schedule.cooperative_fetch", ann_val=v53)
l54 = sch.fuse(l29, preserve_unit_iters=True)
sch.annotate(block_or_loop=l54, ann_key="software_pipeline_stage", ann_val=[0, 0, 2])
sch.annotate(block_or_loop=l54, ann_key="software_pipeline_order", ann_val=[0, 1, 2])
sch.annotate(block_or_loop=l54, ann_key="software_pipeline_async_stages", ann_val=[0])
sch.reverse_compute_inline(block=b1)
v55 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v55)
sch.enter_postproc()
sch.unannotate(block_or_loop=b36, ann_key="meta_schedule.cooperative_fetch")
l56, l57, l58, l59, l60 = sch.get_loops(block=b36)
l61, l62, l63 = sch.split(loop=l60, factors=[None, 40, 2], preserve_unit_iters=True)
sch.vectorize(loop=l63)
sch.bind(loop=l62, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b45, ann_key="meta_schedule.cooperative_fetch")
l64, l65, l66, l67, l68 = sch.get_loops(block=b45)
l69, l70 = sch.split(loop=l68, factors=[None, 40], preserve_unit_iters=True)
sch.bind(loop=l70, thread_axis="threadIdx.x")
b71 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b71, ann_key="meta_schedule.unroll_explicit")
b72, b73, b74, b75 = sch.get_child_blocks(b71)
l76, l77, l78, l79, l80, l81, l82 = sch.get_loops(block=b72)
l83, l84, l85, l86, l87, l88 = sch.get_loops(block=b73)
l89, l90, l91, l92, l93, l94, l95, l96, l97, l98 = sch.get_loops(block=b74)
sch.annotate(block_or_loop=l89, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l89, ann_key="pragma_unroll_explicit", ann_val=1)
l99, l100, l101, l102, l103 = sch.get_loops(block=b75)
b104 = sch.get_block(name="T_matmul_NT", func_name="main")
l105, l106, l107, l108, l109, l110, l111, l112, l113, l114 = sch.get_loops(block=b104)
b115 = sch.decompose_reduction(block=b104, loop=l108)
2023-11-11 02:09:57 [INFO] [task_scheduler.cc:121] [Task #28: fused_nn_dense_add] Trial #83: Error in running:
LocalRunner: An exception occurred
Subprocess terminated
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(512)), "float32"), p1: T.Buffer((T.int64(1000), T.int64(512)), "float32"), p2: T.Buffer((T.int64(1), T.int64(1000)), "float32"), T_add: T.Buffer((T.int64(1), T.int64(1000)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        T_matmul_NT_local = T.alloc_buffer((T.int64(1), T.int64(1000)), scope="local")
        p0_shared = T.alloc_buffer((T.int64(1), T.int64(512)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(1000), T.int64(512)), scope="shared")
        for i_0_j_0_fused in T.thread_binding(T.int64(25), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 16, "pragma_unroll_explicit": 1}):
            for i_1_j_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for i_2_j_2_fused in T.thread_binding(T.int64(40), thread="threadIdx.x"):
                    for i_3_init, j_3_init, i_4_init, j_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_init"):
                            v_i = T.axis.spatial(T.int64(1), i_3_init + i_4_init)
                            v_j = T.axis.spatial(T.int64(1000), i_0_j_0_fused * T.int64(40) + i_2_j_2_fused + j_3_init + j_4_init)
                            T.reads()
                            T.writes(T_matmul_NT_local[v_i, v_j])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            T_matmul_NT_local[v_i, v_j] = T.float32(0)
                    for k_0 in range(T.int64(256)):
                        for ax0_ax1_fused_0 in range(T.int64(1)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(40), thread="threadIdx.x"):
                                with T.block("p0_shared"):
                                    v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                    v1 = T.axis.spatial(T.int64(512), k_0 * T.int64(2) + (ax0_ax1_fused_0 * T.int64(40) + ax0_ax1_fused_1))
                                    T.where(ax0_ax1_fused_0 * T.int64(40) + ax0_ax1_fused_1 < T.int64(2))
                                    T.reads(p0[v0, v1])
                                    T.writes(p0_shared[v0, v1])
                                    p0_shared[v0, v1] = p0[v0, v1]
                        for ax0_ax1_fused_0 in range(T.int64(1)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(40), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("p1_shared"):
                                        v0 = T.axis.spatial(T.int64(1000), i_0_j_0_fused * T.int64(40) + (ax0_ax1_fused_0 * T.int64(160) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2) // T.int64(2))
                                        v1 = T.axis.spatial(T.int64(512), k_0 * T.int64(2) + (ax0_ax1_fused_0 * T.int64(160) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2) % T.int64(2))
                                        T.where((ax0_ax1_fused_0 * T.int64(40) + ax0_ax1_fused_1) * T.int64(4) + ax0_ax1_fused_2 < T.int64(80))
                                        T.reads(p1[v0, v1])
                                        T.writes(p1_shared[v0, v1])
                                        p1_shared[v0, v1] = p1[v0, v1]
                        for k_1, i_3, j_3, k_2, i_4, j_4 in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(1), T.int64(1)):
                            with T.block("T_matmul_NT_update"):
                                v_i = T.axis.spatial(T.int64(1), i_3 + i_4)
                                v_j = T.axis.spatial(T.int64(1000), i_0_j_0_fused * T.int64(40) + i_2_j_2_fused + j_3 + j_4)
                                v_k = T.axis.reduce(T.int64(512), k_0 * T.int64(2) + k_1 * T.int64(2) + k_2)
                                T.reads(T_matmul_NT_local[v_i, v_j], p0_shared[v_i, v_k], p1_shared[v_j, v_k])
                                T.writes(T_matmul_NT_local[v_i, v_j])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                T_matmul_NT_local[v_i, v_j] = T_matmul_NT_local[v_i, v_j] + p0_shared[v_i, v_k] * p1_shared[v_j, v_k]
                    for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(1000), i_0_j_0_fused * T.int64(40) + i_2_j_2_fused + ax1)
                            T.reads(T_matmul_NT_local[v0, v1], p2[v0, v1])
                            T.writes(T_add[v0, v1])
                            T_add[v0, v1] = T_matmul_NT_local[v0, v1] + p2[v0, v1]
b0 = sch.get_block(name="T_matmul_NT", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l3, l4, l5 = sch.get_loops(block=b0)
v6, v7, v8, v9, v10 = sch.sample_perfect_tile(loop=l3, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l11, l12, l13, l14, l15 = sch.split(loop=l3, factors=[v6, v7, v8, v9, v10], preserve_unit_iters=True)
v16, v17, v18, v19, v20 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[25, 1, 40, 1, 1])
l21, l22, l23, l24, l25 = sch.split(loop=l4, factors=[v16, v17, v18, v19, v20], preserve_unit_iters=True)
v26, v27, v28 = sch.sample_perfect_tile(loop=l5, n=3, max_innermost_factor=64, decision=[256, 1, 2])
l29, l30, l31 = sch.split(loop=l5, factors=[v26, v27, v28], preserve_unit_iters=True)
sch.reorder(l11, l21, l12, l22, l13, l23, l29, l30, l14, l24, l31, l15, l25)
l32 = sch.fuse(l11, l21, preserve_unit_iters=True)
sch.bind(loop=l32, thread_axis="blockIdx.x")
l33 = sch.fuse(l12, l22, preserve_unit_iters=True)
sch.bind(loop=l33, thread_axis="vthread.x")
l34 = sch.fuse(l13, l23, preserve_unit_iters=True)
sch.bind(loop=l34, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b35 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b35, loop=l34, preserve_unit_loops=True, index=-1)
b36 = sch.cache_read(block=b0, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b36, loop=l29, preserve_unit_loops=True, index=-1)
l37, l38, l39, l40, l41, l42 = sch.get_loops(block=b36)
l43 = sch.fuse(l41, l42, preserve_unit_iters=True)
v44 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b36, ann_key="meta_schedule.cooperative_fetch", ann_val=v44)
b45 = sch.cache_read(block=b0, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b45, loop=l29, preserve_unit_loops=True, index=-1)
l46, l47, l48, l49, l50, l51 = sch.get_loops(block=b45)
l52 = sch.fuse(l50, l51, preserve_unit_iters=True)
v53 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b45, ann_key="meta_schedule.cooperative_fetch", ann_val=v53)
sch.reverse_compute_inline(block=b1)
v54 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=1)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v54)
sch.enter_postproc()
sch.unannotate(block_or_loop=b36, ann_key="meta_schedule.cooperative_fetch")
l55, l56, l57, l58, l59 = sch.get_loops(block=b36)
l60, l61 = sch.split(loop=l59, factors=[None, 40], preserve_unit_iters=True)
sch.bind(loop=l61, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b45, ann_key="meta_schedule.cooperative_fetch")
l62, l63, l64, l65, l66 = sch.get_loops(block=b45)
l67, l68, l69 = sch.split(loop=l66, factors=[None, 40, 4], preserve_unit_iters=True)
sch.vectorize(loop=l69)
sch.bind(loop=l68, thread_axis="threadIdx.x")
b70 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b70, ann_key="meta_schedule.unroll_explicit")
b71, b72, b73, b74 = sch.get_child_blocks(b70)
l75, l76, l77, l78, l79, l80 = sch.get_loops(block=b71)
l81, l82, l83, l84, l85, l86, l87 = sch.get_loops(block=b72)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b73)
sch.annotate(block_or_loop=l88, ann_key="pragma_auto_unroll_max_step", ann_val=16)
sch.annotate(block_or_loop=l88, ann_key="pragma_unroll_explicit", ann_val=1)
l98, l99, l100, l101, l102 = sch.get_loops(block=b74)
b103 = sch.get_block(name="T_matmul_NT", func_name="main")
l104, l105, l106, l107, l108, l109, l110, l111, l112, l113 = sch.get_loops(block=b103)
b114 = sch.decompose_reduction(block=b103, loop=l107)
2023-11-11 02:09:57 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #84: GFLOPs: 15.9986. Time: 64.0682 us. Best GFLOPs: 80.9361
2023-11-11 02:09:57 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #85: GFLOPs: 15.9984. Time: 64.0691 us. Best GFLOPs: 80.9361
2023-11-11 02:09:57 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #86: GFLOPs: 15.9968. Time: 64.0753 us. Best GFLOPs: 80.9361
2023-11-11 02:09:57 [INFO] [task_scheduler.cc:121] [Task #28: fused_nn_dense_add] Trial #87: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/home2/xiachunwei/Software/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/home2/xiachunwei/Software/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 159, in _worker_func
    costs: List[float] = f_run_evaluator(
  File "/home2/xiachunwei/Software/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 387, in default_run_evaluator
    return run_evaluator_common(rt_mod, device, evaluator_config, repeated_args)
  File "/home2/xiachunwei/Software/tvm/python/tvm/meta_schedule/runner/utils.py", line 117, in run_evaluator_common
    profile_result = evaluator(*args)
  File "/home2/xiachunwei/Software/tvm/python/tvm/runtime/module.py", line 397, in evaluator
    blob = feval(*args)
  File "/home2/xiachunwei/Software/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/home2/xiachunwei/Software/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  1: tvm::runtime::profiling::WrapTimeEvaluator(tvm::runtime::PackedFunc, DLDevice, int, int, int, int, int, int, int, tvm::runtime::PackedFunc)::{lambda(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*) [clone .isra.779]
  0: tvm::runtime::CUDADeviceAPI::StreamSync(DLDevice, void*)
  File "/home2/xiachunwei/Software/tvm/src/runtime/cuda/cuda_device_api.cc", line 212
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(512)), "float32"), p1: T.Buffer((T.int64(1000), T.int64(512)), "float32"), p2: T.Buffer((T.int64(1), T.int64(1000)), "float32"), T_add: T.Buffer((T.int64(1), T.int64(1000)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        T_matmul_NT_local = T.alloc_buffer((T.int64(1), T.int64(1000)), scope="local")
        p0_shared = T.alloc_buffer((T.int64(1), T.int64(512)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(1000), T.int64(512)), scope="shared")
        for i_0_j_0_fused in T.thread_binding(T.int64(25), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 64, "pragma_unroll_explicit": 1}):
            for i_1_j_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for i_2_j_2_fused in T.thread_binding(T.int64(40), thread="threadIdx.x"):
                    for i_3_init, j_3_init, i_4_init, j_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_init"):
                            v_i = T.axis.spatial(T.int64(1), i_3_init + i_4_init)
                            v_j = T.axis.spatial(T.int64(1000), i_0_j_0_fused * T.int64(40) + i_2_j_2_fused + j_3_init + j_4_init)
                            T.reads()
                            T.writes(T_matmul_NT_local[v_i, v_j])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            T_matmul_NT_local[v_i, v_j] = T.float32(0)
                    for k_0 in range(T.int64(256)):
                        for ax0_ax1_fused_0 in range(T.int64(1)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(40), thread="threadIdx.x"):
                                with T.block("p0_shared"):
                                    v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                    v1 = T.axis.spatial(T.int64(512), k_0 * T.int64(2) + (ax0_ax1_fused_0 * T.int64(40) + ax0_ax1_fused_1))
                                    T.where(ax0_ax1_fused_0 * T.int64(40) + ax0_ax1_fused_1 < T.int64(2))
                                    T.reads(p0[v0, v1])
                                    T.writes(p0_shared[v0, v1])
                                    p0_shared[v0, v1] = p0[v0, v1]
                        for ax0_ax1_fused_0 in range(T.int64(1)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(40), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("p1_shared"):
                                        v0 = T.axis.spatial(T.int64(1000), i_0_j_0_fused * T.int64(40) + (ax0_ax1_fused_0 * T.int64(160) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2) // T.int64(2))
                                        v1 = T.axis.spatial(T.int64(512), k_0 * T.int64(2) + (ax0_ax1_fused_0 * T.int64(160) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2) % T.int64(2))
                                        T.where((ax0_ax1_fused_0 * T.int64(40) + ax0_ax1_fused_1) * T.int64(4) + ax0_ax1_fused_2 < T.int64(80))
                                        T.reads(p1[v0, v1])
                                        T.writes(p1_shared[v0, v1])
                                        p1_shared[v0, v1] = p1[v0, v1]
                        for k_1, i_3, j_3, k_2, i_4, j_4 in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(1), T.int64(1)):
                            with T.block("T_matmul_NT_update"):
                                v_i = T.axis.spatial(T.int64(1), i_3 + i_4)
                                v_j = T.axis.spatial(T.int64(1000), i_0_j_0_fused * T.int64(40) + i_2_j_2_fused + j_3 + j_4)
                                v_k = T.axis.reduce(T.int64(512), k_0 * T.int64(2) + k_1 * T.int64(2) + k_2)
                                T.reads(T_matmul_NT_local[v_i, v_j], p0_shared[v_i, v_k], p1_shared[v_j, v_k])
                                T.writes(T_matmul_NT_local[v_i, v_j])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                T_matmul_NT_local[v_i, v_j] = T_matmul_NT_local[v_i, v_j] + p0_shared[v_i, v_k] * p1_shared[v_j, v_k]
                    for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(1000), i_0_j_0_fused * T.int64(40) + i_2_j_2_fused + ax1)
                            T.reads(T_matmul_NT_local[v0, v1], p2[v0, v1])
                            T.writes(T_add[v0, v1])
                            T_add[v0, v1] = T_matmul_NT_local[v0, v1] + p2[v0, v1]
b0 = sch.get_block(name="T_matmul_NT", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l3, l4, l5 = sch.get_loops(block=b0)
v6, v7, v8, v9, v10 = sch.sample_perfect_tile(loop=l3, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l11, l12, l13, l14, l15 = sch.split(loop=l3, factors=[v6, v7, v8, v9, v10], preserve_unit_iters=True)
v16, v17, v18, v19, v20 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[25, 1, 40, 1, 1])
l21, l22, l23, l24, l25 = sch.split(loop=l4, factors=[v16, v17, v18, v19, v20], preserve_unit_iters=True)
v26, v27, v28 = sch.sample_perfect_tile(loop=l5, n=3, max_innermost_factor=64, decision=[256, 1, 2])
l29, l30, l31 = sch.split(loop=l5, factors=[v26, v27, v28], preserve_unit_iters=True)
sch.reorder(l11, l21, l12, l22, l13, l23, l29, l30, l14, l24, l31, l15, l25)
l32 = sch.fuse(l11, l21, preserve_unit_iters=True)
sch.bind(loop=l32, thread_axis="blockIdx.x")
l33 = sch.fuse(l12, l22, preserve_unit_iters=True)
sch.bind(loop=l33, thread_axis="vthread.x")
l34 = sch.fuse(l13, l23, preserve_unit_iters=True)
sch.bind(loop=l34, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b35 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b35, loop=l34, preserve_unit_loops=True, index=-1)
b36 = sch.cache_read(block=b0, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b36, loop=l29, preserve_unit_loops=True, index=-1)
l37, l38, l39, l40, l41, l42 = sch.get_loops(block=b36)
l43 = sch.fuse(l41, l42, preserve_unit_iters=True)
v44 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b36, ann_key="meta_schedule.cooperative_fetch", ann_val=v44)
b45 = sch.cache_read(block=b0, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b45, loop=l29, preserve_unit_loops=True, index=-1)
l46, l47, l48, l49, l50, l51 = sch.get_loops(block=b45)
l52 = sch.fuse(l50, l51, preserve_unit_iters=True)
v53 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b45, ann_key="meta_schedule.cooperative_fetch", ann_val=v53)
sch.reverse_compute_inline(block=b1)
v54 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=2)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v54)
sch.enter_postproc()
sch.unannotate(block_or_loop=b36, ann_key="meta_schedule.cooperative_fetch")
l55, l56, l57, l58, l59 = sch.get_loops(block=b36)
l60, l61 = sch.split(loop=l59, factors=[None, 40], preserve_unit_iters=True)
sch.bind(loop=l61, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b45, ann_key="meta_schedule.cooperative_fetch")
l62, l63, l64, l65, l66 = sch.get_loops(block=b45)
l67, l68, l69 = sch.split(loop=l66, factors=[None, 40, 4], preserve_unit_iters=True)
sch.vectorize(loop=l69)
sch.bind(loop=l68, thread_axis="threadIdx.x")
b70 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b70, ann_key="meta_schedule.unroll_explicit")
b71, b72, b73, b74 = sch.get_child_blocks(b70)
l75, l76, l77, l78, l79, l80 = sch.get_loops(block=b71)
l81, l82, l83, l84, l85, l86, l87 = sch.get_loops(block=b72)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b73)
sch.annotate(block_or_loop=l88, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l88, ann_key="pragma_unroll_explicit", ann_val=1)
l98, l99, l100, l101, l102 = sch.get_loops(block=b74)
b103 = sch.get_block(name="T_matmul_NT", func_name="main")
l104, l105, l106, l107, l108, l109, l110, l111, l112, l113 = sch.get_loops(block=b103)
b114 = sch.decompose_reduction(block=b103, loop=l107)
2023-11-11 02:09:57 [INFO] [task_scheduler.cc:121] [Task #28: fused_nn_dense_add] Trial #88: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/home2/xiachunwei/Software/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/home2/xiachunwei/Software/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/home2/xiachunwei/Software/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/home2/xiachunwei/Software/tvm/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/home2/xiachunwei/Software/tvm/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/home2/xiachunwei/Software/tvm/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/home2/xiachunwei/Software/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/home2/xiachunwei/Software/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  4: _ZN3tvm7runtime13PackedFun
  3: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  2: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  0: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  File "/home2/xiachunwei/Software/tvm/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(512)), "float32"), p1: T.Buffer((T.int64(1000), T.int64(512)), "float32"), p2: T.Buffer((T.int64(1), T.int64(1000)), "float32"), T_add: T.Buffer((T.int64(1), T.int64(1000)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        T_matmul_NT_local = T.alloc_buffer((T.int64(1), T.int64(1000)), scope="local")
        p0_shared = T.alloc_buffer((T.int64(1), T.int64(512)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(1000), T.int64(512)), scope="shared")
        for i_0_j_0_fused in T.thread_binding(T.int64(20), thread="blockIdx.x"):
            for i_1_j_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for i_2_j_2_fused in T.thread_binding(T.int64(50), thread="threadIdx.x"):
                    for i_3_init, j_3_init, i_4_init, j_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_init"):
                            v_i = T.axis.spatial(T.int64(1), i_3_init + i_4_init)
                            v_j = T.axis.spatial(T.int64(1000), i_0_j_0_fused * T.int64(50) + i_2_j_2_fused + j_3_init + j_4_init)
                            T.reads()
                            T.writes(T_matmul_NT_local[v_i, v_j])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            T_matmul_NT_local[v_i, v_j] = T.float32(0)
                    for k_0_fused in T.serial(T.int64(256), annotations={"software_pipeline_async_stages": [0], "software_pipeline_order": [0, 1, 2], "software_pipeline_stage": [0, 0, 2]}):
                        for ax0_ax1_fused_0 in range(T.int64(1)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(50), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("p0_shared"):
                                        v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v1 = T.axis.spatial(T.int64(512), k_0_fused * T.int64(2) + (ax0_ax1_fused_0 * T.int64(100) + ax0_ax1_fused_1 * T.int64(2) + ax0_ax1_fused_2))
                                        T.where((ax0_ax1_fused_0 * T.int64(50) + ax0_ax1_fused_1) * T.int64(2) + ax0_ax1_fused_2 < T.int64(2))
                                        T.reads(p0[v0, v1])
                                        T.writes(p0_shared[v0, v1])
                                        p0_shared[v0, v1] = p0[v0, v1]
                        for ax0_ax1_fused_0 in range(T.int64(1)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(50), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("p1_shared"):
                                        v0 = T.axis.spatial(T.int64(1000), i_0_j_0_fused * T.int64(50) + (ax0_ax1_fused_0 * T.int64(200) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2) // T.int64(2))
                                        v1 = T.axis.spatial(T.int64(512), k_0_fused * T.int64(2) + (ax0_ax1_fused_0 * T.int64(200) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2) % T.int64(2))
                                        T.where((ax0_ax1_fused_0 * T.int64(50) + ax0_ax1_fused_1) * T.int64(4) + ax0_ax1_fused_2 < T.int64(100))
                                        T.reads(p1[v0, v1])
                                        T.writes(p1_shared[v0, v1])
                                        p1_shared[v0, v1] = p1[v0, v1]
                        for k_1, i_3, j_3, k_2, i_4, j_4 in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(1), T.int64(1)):
                            with T.block("T_matmul_NT_update"):
                                v_i = T.axis.spatial(T.int64(1), i_3 + i_4)
                                v_j = T.axis.spatial(T.int64(1000), i_0_j_0_fused * T.int64(50) + i_2_j_2_fused + j_3 + j_4)
                                v_k = T.axis.reduce(T.int64(512), k_0_fused * T.int64(2) + k_1 * T.int64(2) + k_2)
                                T.reads(T_matmul_NT_local[v_i, v_j], p0_shared[v_i, v_k], p1_shared[v_j, v_k])
                                T.writes(T_matmul_NT_local[v_i, v_j])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                T_matmul_NT_local[v_i, v_j] = T_matmul_NT_local[v_i, v_j] + p0_shared[v_i, v_k] * p1_shared[v_j, v_k]
                    for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(1000), i_0_j_0_fused * T.int64(50) + i_2_j_2_fused + ax1)
                            T.reads(T_matmul_NT_local[v0, v1], p2[v0, v1])
                            T.writes(T_add[v0, v1])
                            T_add[v0, v1] = T_matmul_NT_local[v0, v1] + p2[v0, v1]
b0 = sch.get_block(name="T_matmul_NT", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l3, l4, l5 = sch.get_loops(block=b0)
v6, v7, v8, v9, v10 = sch.sample_perfect_tile(loop=l3, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l11, l12, l13, l14, l15 = sch.split(loop=l3, factors=[v6, v7, v8, v9, v10], preserve_unit_iters=True)
v16, v17, v18, v19, v20 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[20, 1, 50, 1, 1])
l21, l22, l23, l24, l25 = sch.split(loop=l4, factors=[v16, v17, v18, v19, v20], preserve_unit_iters=True)
v26, v27, v28 = sch.sample_perfect_tile(loop=l5, n=3, max_innermost_factor=64, decision=[256, 1, 2])
l29, l30, l31 = sch.split(loop=l5, factors=[v26, v27, v28], preserve_unit_iters=True)
sch.reorder(l11, l21, l12, l22, l13, l23, l29, l30, l14, l24, l31, l15, l25)
l32 = sch.fuse(l11, l21, preserve_unit_iters=True)
sch.bind(loop=l32, thread_axis="blockIdx.x")
l33 = sch.fuse(l12, l22, preserve_unit_iters=True)
sch.bind(loop=l33, thread_axis="vthread.x")
l34 = sch.fuse(l13, l23, preserve_unit_iters=True)
sch.bind(loop=l34, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b35 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b35, loop=l34, preserve_unit_loops=True, index=-1)
b36 = sch.cache_read(block=b0, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b36, loop=l29, preserve_unit_loops=True, index=-1)
l37, l38, l39, l40, l41, l42 = sch.get_loops(block=b36)
l43 = sch.fuse(l41, l42, preserve_unit_iters=True)
v44 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b36, ann_key="meta_schedule.cooperative_fetch", ann_val=v44)
b45 = sch.cache_read(block=b0, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b45, loop=l29, preserve_unit_loops=True, index=-1)
l46, l47, l48, l49, l50, l51 = sch.get_loops(block=b45)
l52 = sch.fuse(l50, l51, preserve_unit_iters=True)
v53 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b45, ann_key="meta_schedule.cooperative_fetch", ann_val=v53)
l54 = sch.fuse(l29, preserve_unit_iters=True)
sch.annotate(block_or_loop=l54, ann_key="software_pipeline_stage", ann_val=[0, 0, 2])
sch.annotate(block_or_loop=l54, ann_key="software_pipeline_order", ann_val=[0, 1, 2])
sch.annotate(block_or_loop=l54, ann_key="software_pipeline_async_stages", ann_val=[0])
sch.reverse_compute_inline(block=b1)
v55 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=0)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v55)
sch.enter_postproc()
sch.unannotate(block_or_loop=b36, ann_key="meta_schedule.cooperative_fetch")
l56, l57, l58, l59, l60 = sch.get_loops(block=b36)
l61, l62, l63 = sch.split(loop=l60, factors=[None, 50, 2], preserve_unit_iters=True)
sch.vectorize(loop=l63)
sch.bind(loop=l62, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b45, ann_key="meta_schedule.cooperative_fetch")
l64, l65, l66, l67, l68 = sch.get_loops(block=b45)
l69, l70, l71 = sch.split(loop=l68, factors=[None, 50, 4], preserve_unit_iters=True)
sch.vectorize(loop=l71)
sch.bind(loop=l70, thread_axis="threadIdx.x")
b72 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b72, ann_key="meta_schedule.unroll_explicit")
b73, b74, b75, b76 = sch.get_child_blocks(b72)
l77, l78, l79, l80, l81, l82, l83 = sch.get_loops(block=b73)
l84, l85, l86, l87, l88, l89, l90 = sch.get_loops(block=b74)
l91, l92, l93, l94, l95, l96, l97, l98, l99, l100 = sch.get_loops(block=b75)
l101, l102, l103, l104, l105 = sch.get_loops(block=b76)
b106 = sch.get_block(name="T_matmul_NT", func_name="main")
l107, l108, l109, l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b106)
b117 = sch.decompose_reduction(block=b106, loop=l110)
2023-11-11 02:09:57 [INFO] [task_scheduler.cc:121] [Task #28: fused_nn_dense_add] Trial #89: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/home2/xiachunwei/Software/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/home2/xiachunwei/Software/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/home2/xiachunwei/Software/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/home2/xiachunwei/Software/tvm/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/home2/xiachunwei/Software/tvm/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/home2/xiachunwei/Software/tvm/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/home2/xiachunwei/Software/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/home2/xiachunwei/Software/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  4: _ZN3tvm7runtime13PackedFun
  3: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  2: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  0: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  File "/home2/xiachunwei/Software/tvm/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(512)), "float32"), p1: T.Buffer((T.int64(1000), T.int64(512)), "float32"), p2: T.Buffer((T.int64(1), T.int64(1000)), "float32"), T_add: T.Buffer((T.int64(1), T.int64(1000)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        T_matmul_NT_local = T.alloc_buffer((T.int64(1), T.int64(1000)), scope="local")
        p0_shared = T.alloc_buffer((T.int64(1), T.int64(512)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(1000), T.int64(512)), scope="shared")
        for i_0_j_0_fused in T.thread_binding(T.int64(8), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 512, "pragma_unroll_explicit": 1}):
            for i_1_j_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for i_2_j_2_fused in T.thread_binding(T.int64(125), thread="threadIdx.x"):
                    for i_3_init, j_3_init, i_4_init, j_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_init"):
                            v_i = T.axis.spatial(T.int64(1), i_3_init + i_4_init)
                            v_j = T.axis.spatial(T.int64(1000), i_0_j_0_fused * T.int64(125) + i_2_j_2_fused + j_3_init + j_4_init)
                            T.reads()
                            T.writes(T_matmul_NT_local[v_i, v_j])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            T_matmul_NT_local[v_i, v_j] = T.float32(0)
                    for k_0 in range(T.int64(256)):
                        for ax0_ax1_fused_0 in range(T.int64(1)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(125), thread="threadIdx.x"):
                                with T.block("p0_shared"):
                                    v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                    v1 = T.axis.spatial(T.int64(512), k_0 * T.int64(2) + (ax0_ax1_fused_0 * T.int64(125) + ax0_ax1_fused_1))
                                    T.where(ax0_ax1_fused_0 * T.int64(125) + ax0_ax1_fused_1 < T.int64(2))
                                    T.reads(p0[v0, v1])
                                    T.writes(p0_shared[v0, v1])
                                    p0_shared[v0, v1] = p0[v0, v1]
                        for ax0_ax1_fused_0 in range(T.int64(1)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(125), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("p1_shared"):
                                        v0 = T.axis.spatial(T.int64(1000), i_0_j_0_fused * T.int64(125) + (ax0_ax1_fused_0 * T.int64(250) + ax0_ax1_fused_1 * T.int64(2) + ax0_ax1_fused_2) // T.int64(2))
                                        v1 = T.axis.spatial(T.int64(512), k_0 * T.int64(2) + (ax0_ax1_fused_0 * T.int64(250) + ax0_ax1_fused_1 * T.int64(2) + ax0_ax1_fused_2) % T.int64(2))
                                        T.reads(p1[v0, v1])
                                        T.writes(p1_shared[v0, v1])
                                        p1_shared[v0, v1] = p1[v0, v1]
                        for k_1, i_3, j_3, k_2, i_4, j_4 in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(1), T.int64(1)):
                            with T.block("T_matmul_NT_update"):
                                v_i = T.axis.spatial(T.int64(1), i_3 + i_4)
                                v_j = T.axis.spatial(T.int64(1000), i_0_j_0_fused * T.int64(125) + i_2_j_2_fused + j_3 + j_4)
                                v_k = T.axis.reduce(T.int64(512), k_0 * T.int64(2) + k_1 * T.int64(2) + k_2)
                                T.reads(T_matmul_NT_local[v_i, v_j], p0_shared[v_i, v_k], p1_shared[v_j, v_k])
                                T.writes(T_matmul_NT_local[v_i, v_j])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                T_matmul_NT_local[v_i, v_j] = T_matmul_NT_local[v_i, v_j] + p0_shared[v_i, v_k] * p1_shared[v_j, v_k]
                    for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(1000), i_0_j_0_fused * T.int64(125) + i_2_j_2_fused + ax1)
                            T.reads(T_matmul_NT_local[v0, v1], p2[v0, v1])
                            T.writes(T_add[v0, v1])
                            T_add[v0, v1] = T_matmul_NT_local[v0, v1] + p2[v0, v1]
b0 = sch.get_block(name="T_matmul_NT", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l3, l4, l5 = sch.get_loops(block=b0)
v6, v7, v8, v9, v10 = sch.sample_perfect_tile(loop=l3, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l11, l12, l13, l14, l15 = sch.split(loop=l3, factors=[v6, v7, v8, v9, v10], preserve_unit_iters=True)
v16, v17, v18, v19, v20 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[8, 1, 125, 1, 1])
l21, l22, l23, l24, l25 = sch.split(loop=l4, factors=[v16, v17, v18, v19, v20], preserve_unit_iters=True)
v26, v27, v28 = sch.sample_perfect_tile(loop=l5, n=3, max_innermost_factor=64, decision=[256, 1, 2])
l29, l30, l31 = sch.split(loop=l5, factors=[v26, v27, v28], preserve_unit_iters=True)
sch.reorder(l11, l21, l12, l22, l13, l23, l29, l30, l14, l24, l31, l15, l25)
l32 = sch.fuse(l11, l21, preserve_unit_iters=True)
sch.bind(loop=l32, thread_axis="blockIdx.x")
l33 = sch.fuse(l12, l22, preserve_unit_iters=True)
sch.bind(loop=l33, thread_axis="vthread.x")
l34 = sch.fuse(l13, l23, preserve_unit_iters=True)
sch.bind(loop=l34, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b35 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b35, loop=l34, preserve_unit_loops=True, index=-1)
b36 = sch.cache_read(block=b0, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b36, loop=l29, preserve_unit_loops=True, index=-1)
l37, l38, l39, l40, l41, l42 = sch.get_loops(block=b36)
l43 = sch.fuse(l41, l42, preserve_unit_iters=True)
v44 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b36, ann_key="meta_schedule.cooperative_fetch", ann_val=v44)
b45 = sch.cache_read(block=b0, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b45, loop=l29, preserve_unit_loops=True, index=-1)
l46, l47, l48, l49, l50, l51 = sch.get_loops(block=b45)
l52 = sch.fuse(l50, l51, preserve_unit_iters=True)
v53 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b45, ann_key="meta_schedule.cooperative_fetch", ann_val=v53)
sch.reverse_compute_inline(block=b1)
v54 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v54)
sch.enter_postproc()
sch.unannotate(block_or_loop=b36, ann_key="meta_schedule.cooperative_fetch")
l55, l56, l57, l58, l59 = sch.get_loops(block=b36)
l60, l61 = sch.split(loop=l59, factors=[None, 125], preserve_unit_iters=True)
sch.bind(loop=l61, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b45, ann_key="meta_schedule.cooperative_fetch")
l62, l63, l64, l65, l66 = sch.get_loops(block=b45)
l67, l68, l69 = sch.split(loop=l66, factors=[None, 125, 2], preserve_unit_iters=True)
sch.vectorize(loop=l69)
sch.bind(loop=l68, thread_axis="threadIdx.x")
b70 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b70, ann_key="meta_schedule.unroll_explicit")
b71, b72, b73, b74 = sch.get_child_blocks(b70)
l75, l76, l77, l78, l79, l80 = sch.get_loops(block=b71)
l81, l82, l83, l84, l85, l86, l87 = sch.get_loops(block=b72)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b73)
sch.annotate(block_or_loop=l88, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l88, ann_key="pragma_unroll_explicit", ann_val=1)
l98, l99, l100, l101, l102 = sch.get_loops(block=b74)
b103 = sch.get_block(name="T_matmul_NT", func_name="main")
l104, l105, l106, l107, l108, l109, l110, l111, l112, l113 = sch.get_loops(block=b103)
b114 = sch.decompose_reduction(block=b103, loop=l107)
2023-11-11 02:09:57 [INFO] [task_scheduler.cc:121] [Task #28: fused_nn_dense_add] Trial #90: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/home2/xiachunwei/Software/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/home2/xiachunwei/Software/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/home2/xiachunwei/Software/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/home2/xiachunwei/Software/tvm/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/home2/xiachunwei/Software/tvm/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/home2/xiachunwei/Software/tvm/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/home2/xiachunwei/Software/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/home2/xiachunwei/Software/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  4: _ZN3tvm7runtime13PackedFun
  3: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  2: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  0: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  File "/home2/xiachunwei/Software/tvm/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(512)), "float32"), p1: T.Buffer((T.int64(1000), T.int64(512)), "float32"), p2: T.Buffer((T.int64(1), T.int64(1000)), "float32"), T_add: T.Buffer((T.int64(1), T.int64(1000)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        T_matmul_NT_local = T.alloc_buffer((T.int64(1), T.int64(1000)), scope="local")
        p0_shared = T.alloc_buffer((T.int64(1), T.int64(512)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(1000), T.int64(512)), scope="shared")
        for i_0_j_0_fused in T.thread_binding(T.int64(25), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 16, "pragma_unroll_explicit": 1}):
            for i_1_j_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for i_2_j_2_fused in T.thread_binding(T.int64(40), thread="threadIdx.x"):
                    for i_3_init, j_3_init, i_4_init, j_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_init"):
                            v_i = T.axis.spatial(T.int64(1), i_3_init + i_4_init)
                            v_j = T.axis.spatial(T.int64(1000), i_0_j_0_fused * T.int64(40) + i_2_j_2_fused + j_3_init + j_4_init)
                            T.reads()
                            T.writes(T_matmul_NT_local[v_i, v_j])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            T_matmul_NT_local[v_i, v_j] = T.float32(0)
                    for k_0_fused in T.serial(T.int64(256), annotations={"software_pipeline_async_stages": [0], "software_pipeline_order": [0, 1, 2], "software_pipeline_stage": [0, 0, 2]}):
                        for ax0_ax1_fused_0 in range(T.int64(1)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(40), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("p0_shared"):
                                        v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v1 = T.axis.spatial(T.int64(512), k_0_fused * T.int64(2) + (ax0_ax1_fused_0 * T.int64(80) + ax0_ax1_fused_1 * T.int64(2) + ax0_ax1_fused_2))
                                        T.where((ax0_ax1_fused_0 * T.int64(40) + ax0_ax1_fused_1) * T.int64(2) + ax0_ax1_fused_2 < T.int64(2))
                                        T.reads(p0[v0, v1])
                                        T.writes(p0_shared[v0, v1])
                                        p0_shared[v0, v1] = p0[v0, v1]
                        for ax0_ax1_fused_0 in range(T.int64(2)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(40), thread="threadIdx.x"):
                                with T.block("p1_shared"):
                                    v0 = T.axis.spatial(T.int64(1000), i_0_j_0_fused * T.int64(40) + (ax0_ax1_fused_0 * T.int64(40) + ax0_ax1_fused_1) // T.int64(2))
                                    v1 = T.axis.spatial(T.int64(512), k_0_fused * T.int64(2) + (ax0_ax1_fused_0 * T.int64(40) + ax0_ax1_fused_1) % T.int64(2))
                                    T.reads(p1[v0, v1])
                                    T.writes(p1_shared[v0, v1])
                                    p1_shared[v0, v1] = p1[v0, v1]
                        for k_1, i_3, j_3, k_2, i_4, j_4 in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(1), T.int64(1)):
                            with T.block("T_matmul_NT_update"):
                                v_i = T.axis.spatial(T.int64(1), i_3 + i_4)
                                v_j = T.axis.spatial(T.int64(1000), i_0_j_0_fused * T.int64(40) + i_2_j_2_fused + j_3 + j_4)
                                v_k = T.axis.reduce(T.int64(512), k_0_fused * T.int64(2) + k_1 * T.int64(2) + k_2)
                                T.reads(T_matmul_NT_local[v_i, v_j], p0_shared[v_i, v_k], p1_shared[v_j, v_k])
                                T.writes(T_matmul_NT_local[v_i, v_j])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                T_matmul_NT_local[v_i, v_j] = T_matmul_NT_local[v_i, v_j] + p0_shared[v_i, v_k] * p1_shared[v_j, v_k]
                    for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(1000), i_0_j_0_fused * T.int64(40) + i_2_j_2_fused + ax1)
                            T.reads(T_matmul_NT_local[v0, v1], p2[v0, v1])
                            T.writes(T_add[v0, v1])
                            T_add[v0, v1] = T_matmul_NT_local[v0, v1] + p2[v0, v1]
b0 = sch.get_block(name="T_matmul_NT", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l3, l4, l5 = sch.get_loops(block=b0)
v6, v7, v8, v9, v10 = sch.sample_perfect_tile(loop=l3, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l11, l12, l13, l14, l15 = sch.split(loop=l3, factors=[v6, v7, v8, v9, v10], preserve_unit_iters=True)
v16, v17, v18, v19, v20 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[25, 1, 40, 1, 1])
l21, l22, l23, l24, l25 = sch.split(loop=l4, factors=[v16, v17, v18, v19, v20], preserve_unit_iters=True)
v26, v27, v28 = sch.sample_perfect_tile(loop=l5, n=3, max_innermost_factor=64, decision=[256, 1, 2])
l29, l30, l31 = sch.split(loop=l5, factors=[v26, v27, v28], preserve_unit_iters=True)
sch.reorder(l11, l21, l12, l22, l13, l23, l29, l30, l14, l24, l31, l15, l25)
l32 = sch.fuse(l11, l21, preserve_unit_iters=True)
sch.bind(loop=l32, thread_axis="blockIdx.x")
l33 = sch.fuse(l12, l22, preserve_unit_iters=True)
sch.bind(loop=l33, thread_axis="vthread.x")
l34 = sch.fuse(l13, l23, preserve_unit_iters=True)
sch.bind(loop=l34, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b35 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b35, loop=l34, preserve_unit_loops=True, index=-1)
b36 = sch.cache_read(block=b0, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b36, loop=l29, preserve_unit_loops=True, index=-1)
l37, l38, l39, l40, l41, l42 = sch.get_loops(block=b36)
l43 = sch.fuse(l41, l42, preserve_unit_iters=True)
v44 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b36, ann_key="meta_schedule.cooperative_fetch", ann_val=v44)
b45 = sch.cache_read(block=b0, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b45, loop=l29, preserve_unit_loops=True, index=-1)
l46, l47, l48, l49, l50, l51 = sch.get_loops(block=b45)
l52 = sch.fuse(l50, l51, preserve_unit_iters=True)
v53 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b45, ann_key="meta_schedule.cooperative_fetch", ann_val=v53)
l54 = sch.fuse(l29, preserve_unit_iters=True)
sch.annotate(block_or_loop=l54, ann_key="software_pipeline_stage", ann_val=[0, 0, 2])
sch.annotate(block_or_loop=l54, ann_key="software_pipeline_order", ann_val=[0, 1, 2])
sch.annotate(block_or_loop=l54, ann_key="software_pipeline_async_stages", ann_val=[0])
sch.reverse_compute_inline(block=b1)
v55 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=1)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v55)
sch.enter_postproc()
sch.unannotate(block_or_loop=b36, ann_key="meta_schedule.cooperative_fetch")
l56, l57, l58, l59, l60 = sch.get_loops(block=b36)
l61, l62, l63 = sch.split(loop=l60, factors=[None, 40, 2], preserve_unit_iters=True)
sch.vectorize(loop=l63)
sch.bind(loop=l62, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b45, ann_key="meta_schedule.cooperative_fetch")
l64, l65, l66, l67, l68 = sch.get_loops(block=b45)
l69, l70 = sch.split(loop=l68, factors=[None, 40], preserve_unit_iters=True)
sch.bind(loop=l70, thread_axis="threadIdx.x")
b71 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b71, ann_key="meta_schedule.unroll_explicit")
b72, b73, b74, b75 = sch.get_child_blocks(b71)
l76, l77, l78, l79, l80, l81, l82 = sch.get_loops(block=b72)
l83, l84, l85, l86, l87, l88 = sch.get_loops(block=b73)
l89, l90, l91, l92, l93, l94, l95, l96, l97, l98 = sch.get_loops(block=b74)
sch.annotate(block_or_loop=l89, ann_key="pragma_auto_unroll_max_step", ann_val=16)
sch.annotate(block_or_loop=l89, ann_key="pragma_unroll_explicit", ann_val=1)
l99, l100, l101, l102, l103 = sch.get_loops(block=b75)
b104 = sch.get_block(name="T_matmul_NT", func_name="main")
l105, l106, l107, l108, l109, l110, l111, l112, l113, l114 = sch.get_loops(block=b104)
b115 = sch.decompose_reduction(block=b104, loop=l108)
2023-11-11 02:09:57 [INFO] [task_scheduler.cc:121] [Task #28: fused_nn_dense_add] Trial #91: Error in running:
LocalRunner: An exception occurred
Subprocess terminated
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(512)), "float32"), p1: T.Buffer((T.int64(1000), T.int64(512)), "float32"), p2: T.Buffer((T.int64(1), T.int64(1000)), "float32"), T_add: T.Buffer((T.int64(1), T.int64(1000)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        T_matmul_NT_local = T.alloc_buffer((T.int64(1), T.int64(1000)), scope="local")
        p0_shared = T.alloc_buffer((T.int64(1), T.int64(512)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(1000), T.int64(512)), scope="shared")
        for i_0_j_0_fused in T.thread_binding(T.int64(25), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 16, "pragma_unroll_explicit": 1}):
            for i_1_j_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for i_2_j_2_fused in T.thread_binding(T.int64(40), thread="threadIdx.x"):
                    for i_3_init, j_3_init, i_4_init, j_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_init"):
                            v_i = T.axis.spatial(T.int64(1), i_3_init + i_4_init)
                            v_j = T.axis.spatial(T.int64(1000), i_0_j_0_fused * T.int64(40) + i_2_j_2_fused + j_3_init + j_4_init)
                            T.reads()
                            T.writes(T_matmul_NT_local[v_i, v_j])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            T_matmul_NT_local[v_i, v_j] = T.float32(0)
                    for k_0 in range(T.int64(256)):
                        for ax0_ax1_fused_0 in range(T.int64(1)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(40), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("p0_shared"):
                                        v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v1 = T.axis.spatial(T.int64(512), k_0 * T.int64(2) + (ax0_ax1_fused_0 * T.int64(80) + ax0_ax1_fused_1 * T.int64(2) + ax0_ax1_fused_2))
                                        T.where((ax0_ax1_fused_0 * T.int64(40) + ax0_ax1_fused_1) * T.int64(2) + ax0_ax1_fused_2 < T.int64(2))
                                        T.reads(p0[v0, v1])
                                        T.writes(p0_shared[v0, v1])
                                        p0_shared[v0, v1] = p0[v0, v1]
                        for ax0_ax1_fused_0 in range(T.int64(2)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(40), thread="threadIdx.x"):
                                with T.block("p1_shared"):
                                    v0 = T.axis.spatial(T.int64(1000), i_0_j_0_fused * T.int64(40) + (ax0_ax1_fused_0 * T.int64(40) + ax0_ax1_fused_1) // T.int64(2))
                                    v1 = T.axis.spatial(T.int64(512), k_0 * T.int64(2) + (ax0_ax1_fused_0 * T.int64(40) + ax0_ax1_fused_1) % T.int64(2))
                                    T.reads(p1[v0, v1])
                                    T.writes(p1_shared[v0, v1])
                                    p1_shared[v0, v1] = p1[v0, v1]
                        for k_1, i_3, j_3, k_2, i_4, j_4 in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(1), T.int64(1)):
                            with T.block("T_matmul_NT_update"):
                                v_i = T.axis.spatial(T.int64(1), i_3 + i_4)
                                v_j = T.axis.spatial(T.int64(1000), i_0_j_0_fused * T.int64(40) + i_2_j_2_fused + j_3 + j_4)
                                v_k = T.axis.reduce(T.int64(512), k_0 * T.int64(2) + k_1 * T.int64(2) + k_2)
                                T.reads(T_matmul_NT_local[v_i, v_j], p0_shared[v_i, v_k], p1_shared[v_j, v_k])
                                T.writes(T_matmul_NT_local[v_i, v_j])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                T_matmul_NT_local[v_i, v_j] = T_matmul_NT_local[v_i, v_j] + p0_shared[v_i, v_k] * p1_shared[v_j, v_k]
                    for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(1000), i_0_j_0_fused * T.int64(40) + i_2_j_2_fused + ax1)
                            T.reads(T_matmul_NT_local[v0, v1], p2[v0, v1])
                            T.writes(T_add[v0, v1])
                            T_add[v0, v1] = T_matmul_NT_local[v0, v1] + p2[v0, v1]
b0 = sch.get_block(name="T_matmul_NT", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l3, l4, l5 = sch.get_loops(block=b0)
v6, v7, v8, v9, v10 = sch.sample_perfect_tile(loop=l3, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l11, l12, l13, l14, l15 = sch.split(loop=l3, factors=[v6, v7, v8, v9, v10], preserve_unit_iters=True)
v16, v17, v18, v19, v20 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[25, 1, 40, 1, 1])
l21, l22, l23, l24, l25 = sch.split(loop=l4, factors=[v16, v17, v18, v19, v20], preserve_unit_iters=True)
v26, v27, v28 = sch.sample_perfect_tile(loop=l5, n=3, max_innermost_factor=64, decision=[256, 1, 2])
l29, l30, l31 = sch.split(loop=l5, factors=[v26, v27, v28], preserve_unit_iters=True)
sch.reorder(l11, l21, l12, l22, l13, l23, l29, l30, l14, l24, l31, l15, l25)
l32 = sch.fuse(l11, l21, preserve_unit_iters=True)
sch.bind(loop=l32, thread_axis="blockIdx.x")
l33 = sch.fuse(l12, l22, preserve_unit_iters=True)
sch.bind(loop=l33, thread_axis="vthread.x")
l34 = sch.fuse(l13, l23, preserve_unit_iters=True)
sch.bind(loop=l34, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b35 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b35, loop=l34, preserve_unit_loops=True, index=-1)
b36 = sch.cache_read(block=b0, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b36, loop=l29, preserve_unit_loops=True, index=-1)
l37, l38, l39, l40, l41, l42 = sch.get_loops(block=b36)
l43 = sch.fuse(l41, l42, preserve_unit_iters=True)
v44 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b36, ann_key="meta_schedule.cooperative_fetch", ann_val=v44)
b45 = sch.cache_read(block=b0, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b45, loop=l29, preserve_unit_loops=True, index=-1)
l46, l47, l48, l49, l50, l51 = sch.get_loops(block=b45)
l52 = sch.fuse(l50, l51, preserve_unit_iters=True)
v53 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b45, ann_key="meta_schedule.cooperative_fetch", ann_val=v53)
sch.reverse_compute_inline(block=b1)
v54 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=1)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v54)
sch.enter_postproc()
sch.unannotate(block_or_loop=b36, ann_key="meta_schedule.cooperative_fetch")
l55, l56, l57, l58, l59 = sch.get_loops(block=b36)
l60, l61, l62 = sch.split(loop=l59, factors=[None, 40, 2], preserve_unit_iters=True)
sch.vectorize(loop=l62)
sch.bind(loop=l61, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b45, ann_key="meta_schedule.cooperative_fetch")
l63, l64, l65, l66, l67 = sch.get_loops(block=b45)
l68, l69 = sch.split(loop=l67, factors=[None, 40], preserve_unit_iters=True)
sch.bind(loop=l69, thread_axis="threadIdx.x")
b70 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b70, ann_key="meta_schedule.unroll_explicit")
b71, b72, b73, b74 = sch.get_child_blocks(b70)
l75, l76, l77, l78, l79, l80, l81 = sch.get_loops(block=b71)
l82, l83, l84, l85, l86, l87 = sch.get_loops(block=b72)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b73)
sch.annotate(block_or_loop=l88, ann_key="pragma_auto_unroll_max_step", ann_val=16)
sch.annotate(block_or_loop=l88, ann_key="pragma_unroll_explicit", ann_val=1)
l98, l99, l100, l101, l102 = sch.get_loops(block=b74)
b103 = sch.get_block(name="T_matmul_NT", func_name="main")
l104, l105, l106, l107, l108, l109, l110, l111, l112, l113 = sch.get_loops(block=b103)
b114 = sch.decompose_reduction(block=b103, loop=l107)
2023-11-11 02:09:57 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #92: GFLOPs: 34.8075. Time: 29.4477 us. Best GFLOPs: 80.9361
2023-11-11 02:09:57 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #93: GFLOPs: 32.2766. Time: 31.7568 us. Best GFLOPs: 80.9361
2023-11-11 02:09:57 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #94: GFLOPs: 26.6670. Time: 38.4370 us. Best GFLOPs: 80.9361
2023-11-11 02:09:57 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #95: GFLOPs: 31.3309. Time: 32.7153 us. Best GFLOPs: 80.9361
2023-11-11 02:09:57 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #96: GFLOPs: 31.3307. Time: 32.7155 us. Best GFLOPs: 80.9361
2023-11-11 02:09:57 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #97: GFLOPs: 34.8084. Time: 29.4469 us. Best GFLOPs: 80.9361
2023-11-11 02:09:57 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #98: GFLOPs: 20.6943. Time: 49.5304 us. Best GFLOPs: 80.9361
2023-11-11 02:09:57 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #99: GFLOPs: 20.6892. Time: 49.5427 us. Best GFLOPs: 80.9361
2023-11-11 02:09:57 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #100: GFLOPs: 34.8062. Time: 29.4487 us. Best GFLOPs: 80.9361
2023-11-11 02:09:57 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #101: GFLOPs: 34.8076. Time: 29.4476 us. Best GFLOPs: 80.9361
2023-11-11 02:09:57 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #102: GFLOPs: 20.6946. Time: 49.5298 us. Best GFLOPs: 80.9361
2023-11-11 02:09:57 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #103: GFLOPs: 27.2622. Time: 37.5978 us. Best GFLOPs: 80.9361
2023-11-11 02:09:57 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #104: GFLOPs: 27.2615. Time: 37.5989 us. Best GFLOPs: 80.9361
2023-11-11 02:09:57 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #105: GFLOPs: 27.8305. Time: 36.8301 us. Best GFLOPs: 80.9361
2023-11-11 02:09:57 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #106: GFLOPs: 32.2776. Time: 31.7557 us. Best GFLOPs: 80.9361
2023-11-11 02:09:57 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #107: GFLOPs: 32.2813. Time: 31.7522 us. Best GFLOPs: 80.9361
2023-11-11 02:09:57 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #108: GFLOPs: 40.5355. Time: 25.2865 us. Best GFLOPs: 80.9361
2023-11-11 02:09:57 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #109: GFLOPs: 40.5329. Time: 25.2881 us. Best GFLOPs: 80.9361
2023-11-11 02:09:57 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #110: GFLOPs: 20.8189. Time: 49.2342 us. Best GFLOPs: 80.9361
2023-11-11 02:09:57 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #111: GFLOPs: 20.7042. Time: 49.5070 us. Best GFLOPs: 80.9361
2023-11-11 02:09:57 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #112: GFLOPs: 20.7042. Time: 49.5070 us. Best GFLOPs: 80.9361
2023-11-11 02:09:57 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #113: GFLOPs: 85.6331. Time: 11.9697 us. Best GFLOPs: 85.6331
2023-11-11 02:09:57 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #114: GFLOPs: 27.3978. Time: 37.4118 us. Best GFLOPs: 85.6331
2023-11-11 02:09:57 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #115: GFLOPs: 91.3908. Time: 11.2156 us. Best GFLOPs: 91.3908
2023-11-11 02:09:57 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #116: GFLOPs: 64.7503. Time: 15.8300 us. Best GFLOPs: 91.3908
2023-11-11 02:09:57 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #117: GFLOPs: 93.7699. Time: 10.9310 us. Best GFLOPs: 93.7699
2023-11-11 02:09:57 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #118: GFLOPs: 37.4314. Time: 27.3834 us. Best GFLOPs: 93.7699
2023-11-11 02:09:57 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #119: GFLOPs: 22.2844. Time: 45.9962 us. Best GFLOPs: 93.7699
2023-11-11 02:09:57 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #120: GFLOPs: 96.0504. Time: 10.6715 us. Best GFLOPs: 96.0504
2023-11-11 02:09:57 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #121: GFLOPs: 37.4312. Time: 27.3836 us. Best GFLOPs: 96.0504
2023-11-11 02:09:57 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #122: GFLOPs: 91.9979. Time: 11.1416 us. Best GFLOPs: 96.0504
2023-11-11 02:09:57 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #123: GFLOPs: 92.0858. Time: 11.1309 us. Best GFLOPs: 96.0504
2023-11-11 02:09:57 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #124: GFLOPs: 94.7164. Time: 10.8218 us. Best GFLOPs: 96.0504
2023-11-11 02:09:57 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #125: GFLOPs: 14.1259. Time: 72.5615 us. Best GFLOPs: 96.0504
2023-11-11 02:09:57 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #126: GFLOPs: 3.3156. Time: 309.1440 us. Best GFLOPs: 96.0504
2023-11-11 02:09:57 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #127: GFLOPs: 3.7989. Time: 269.8129 us. Best GFLOPs: 96.0504
2023-11-11 04:26:44 [INFO] [evolutionary_search.cc:713] Generating candidates......
2023-11-11 04:26:44 [INFO] [evolutionary_search.cc:715] Picked top 96 candidate(s) from database
2023-11-11 04:26:45 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310a405238)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310e76ec68)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310dc33cd8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310be916f8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56310f803f18)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310eeb7a68)]: 411 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310e771ba8)]: 0 failure(s)
2023-11-11 04:26:46 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310a405238)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310e76ec68)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310dc33cd8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310be916f8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56310f803f18)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310eeb7a68)]: 819 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310e771ba8)]: 0 failure(s)
2023-11-11 04:26:46 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310a405238)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310e76ec68)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310dc33cd8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310be916f8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56310f803f18)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310eeb7a68)]: 1229 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310e771ba8)]: 0 failure(s)
2023-11-11 04:26:47 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310a405238)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310e76ec68)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310dc33cd8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310be916f8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56310f803f18)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310eeb7a68)]: 1637 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310e771ba8)]: 0 failure(s)
2023-11-11 04:26:47 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310a405238)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310e76ec68)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310dc33cd8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310be916f8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56310f803f18)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310eeb7a68)]: 2046 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310e771ba8)]: 0 failure(s)
2023-11-11 04:26:48 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310a405238)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310e76ec68)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310dc33cd8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310be916f8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56310f803f18)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310eeb7a68)]: 2458 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310e771ba8)]: 0 failure(s)
2023-11-11 04:26:49 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310a405238)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310e76ec68)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310dc33cd8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310be916f8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56310f803f18)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310eeb7a68)]: 2868 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310e771ba8)]: 0 failure(s)
2023-11-11 04:26:49 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310a405238)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310e76ec68)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310dc33cd8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310be916f8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56310f803f18)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310eeb7a68)]: 3275 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310e771ba8)]: 0 failure(s)
2023-11-11 04:26:49 [INFO] [evolutionary_search.cc:723] Sampled 53 candidate(s)
2023-11-11 04:26:51 [INFO] [evolutionary_search.cc:621] Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310a405238)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310e76ec68)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310dc33cd8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310be916f8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56310f803f18)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310eeb7a68)]: 116 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310e771ba8)]: 0 failure(s)
2023-11-11 04:26:53 [INFO] [evolutionary_search.cc:621] Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310a405238)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310e76ec68)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310dc33cd8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310be916f8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56310f803f18)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310eeb7a68)]: 107 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310e771ba8)]: 0 failure(s)
2023-11-11 04:26:56 [INFO] [evolutionary_search.cc:621] Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310a405238)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310e76ec68)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310dc33cd8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310be916f8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56310f803f18)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310eeb7a68)]: 131 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310e771ba8)]: 0 failure(s)
2023-11-11 04:26:58 [INFO] [evolutionary_search.cc:621] Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310a405238)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310e76ec68)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310dc33cd8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310be916f8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56310f803f18)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310eeb7a68)]: 151 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310e771ba8)]: 0 failure(s)
2023-11-11 04:26:59 [INFO] [evolutionary_search.cc:649] Scores of the best 64 candidates:
[1 : 16]:	1.6359  1.6359  1.6024  1.5076  1.5076  1.4979  1.4979  1.4345  1.0785  1.0785  1.0323  1.0118  1.0071  1.0068  1.0068  1.0068
[17 : 32]:	1.0053  1.0053  1.0040  1.0040  1.0036  0.9989  0.9989  0.9971  0.9971  0.9971  0.9752  0.9748  0.9748  0.9745  0.9735  0.9735
[33 : 48]:	0.9732  0.9732  0.9704  0.9704  0.9701  0.9701  0.9692  0.9668  0.9582  0.9579  0.9577  0.9573  0.9573  0.9566  0.9557  0.9541
[49 : 64]:	0.9505  0.9489  0.9486  0.9404  0.9398  0.9349  0.9299  0.9296  0.9296  0.9294  0.9270  0.9209  0.9139  0.9133  0.9102  0.9102
2023-11-11 04:27:00 [INFO] [evolutionary_search.cc:727] Got 64 candidate(s) with evolutionary search
2023-11-11 04:27:00 [INFO] [evolutionary_search.cc:730] Sending 64 candidates(s) for measurement
2023-11-11 04:27:25 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #128: GFLOPs: 17.2464. Time: 59.4328 us. Best GFLOPs: 96.0504
2023-11-11 04:27:25 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #129: GFLOPs: 18.0516. Time: 56.7815 us. Best GFLOPs: 96.0504
2023-11-11 04:27:25 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #130: GFLOPs: 18.0503. Time: 56.7858 us. Best GFLOPs: 96.0504
2023-11-11 04:27:25 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #131: GFLOPs: 18.0489. Time: 56.7901 us. Best GFLOPs: 96.0504
2023-11-11 04:27:25 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #132: GFLOPs: 17.2464. Time: 59.4326 us. Best GFLOPs: 96.0504
2023-11-11 04:27:25 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #133: GFLOPs: 18.0549. Time: 56.7713 us. Best GFLOPs: 96.0504
2023-11-11 04:27:25 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #134: GFLOPs: 17.2463. Time: 59.4331 us. Best GFLOPs: 96.0504
2023-11-11 04:27:25 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #135: GFLOPs: 11.7224. Time: 87.4394 us. Best GFLOPs: 96.0504
2023-11-11 04:27:25 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #136: GFLOPs: 16.7935. Time: 61.0355 us. Best GFLOPs: 96.0504
2023-11-11 04:27:25 [INFO] [task_scheduler.cc:121] [Task #28: fused_nn_dense_add] Trial #137: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/home2/xiachunwei/Software/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/home2/xiachunwei/Software/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 159, in _worker_func
    costs: List[float] = f_run_evaluator(
  File "/home2/xiachunwei/Software/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 387, in default_run_evaluator
    return run_evaluator_common(rt_mod, device, evaluator_config, repeated_args)
  File "/home2/xiachunwei/Software/tvm/python/tvm/meta_schedule/runner/utils.py", line 117, in run_evaluator_common
    profile_result = evaluator(*args)
  File "/home2/xiachunwei/Software/tvm/python/tvm/runtime/module.py", line 397, in evaluator
    blob = feval(*args)
  File "/home2/xiachunwei/Software/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/home2/xiachunwei/Software/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  1: tvm::runtime::profiling::WrapTimeEvaluator(tvm::runtime::PackedFunc, DLDevice, int, int, int, int, int, int, int, tvm::runtime::PackedFunc)::{lambda(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*) [clone .isra.779]
  0: tvm::runtime::CUDADeviceAPI::StreamSync(DLDevice, void*)
  File "/home2/xiachunwei/Software/tvm/src/runtime/cuda/cuda_device_api.cc", line 212
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(512)), "float32"), p1: T.Buffer((T.int64(1000), T.int64(512)), "float32"), p2: T.Buffer((T.int64(1), T.int64(1000)), "float32"), T_add: T.Buffer((T.int64(1), T.int64(1000)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        T_matmul_NT_local = T.alloc_buffer((T.int64(1), T.int64(1000)), scope="local")
        p0_shared = T.alloc_buffer((T.int64(1), T.int64(512)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(1000), T.int64(512)), scope="shared")
        for i_0_j_0_fused in T.thread_binding(T.int64(25), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 1024, "pragma_unroll_explicit": 1}):
            for i_1_j_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for i_2_j_2_fused in T.thread_binding(T.int64(40), thread="threadIdx.x"):
                    for i_3_init, j_3_init, i_4_init, j_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_init"):
                            v_i = T.axis.spatial(T.int64(1), i_3_init + i_4_init)
                            v_j = T.axis.spatial(T.int64(1000), i_0_j_0_fused * T.int64(40) + i_2_j_2_fused + j_3_init + j_4_init)
                            T.reads()
                            T.writes(T_matmul_NT_local[v_i, v_j])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            T_matmul_NT_local[v_i, v_j] = T.float32(0)
                    for k_0_fused in T.serial(T.int64(512), annotations={"software_pipeline_async_stages": [0], "software_pipeline_order": [0, 1, 2], "software_pipeline_stage": [0, 0, 2]}):
                        for ax0_ax1_fused_0 in range(T.int64(1)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(40), thread="threadIdx.x"):
                                with T.block("p0_shared"):
                                    v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                    v1 = T.axis.spatial(T.int64(512), k_0_fused)
                                    T.where(ax0_ax1_fused_0 * T.int64(40) + ax0_ax1_fused_1 < T.int64(1))
                                    T.reads(p0[v0, v1])
                                    T.writes(p0_shared[v0, v1])
                                    p0_shared[v0, v1] = p0[v0, v1]
                        for ax0_ax1_fused_0 in range(T.int64(1)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(40), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("p1_shared"):
                                        v0 = T.axis.spatial(T.int64(1000), i_0_j_0_fused * T.int64(40) + (ax0_ax1_fused_0 * T.int64(160) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2))
                                        v1 = T.axis.spatial(T.int64(512), k_0_fused)
                                        T.where((ax0_ax1_fused_0 * T.int64(40) + ax0_ax1_fused_1) * T.int64(4) + ax0_ax1_fused_2 < T.int64(40))
                                        T.reads(p1[v0, v1])
                                        T.writes(p1_shared[v0, v1])
                                        p1_shared[v0, v1] = p1[v0, v1]
                        for k_1, i_3, j_3, k_2, i_4, j_4 in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                            with T.block("T_matmul_NT_update"):
                                v_i = T.axis.spatial(T.int64(1), i_3 + i_4)
                                v_j = T.axis.spatial(T.int64(1000), i_0_j_0_fused * T.int64(40) + i_2_j_2_fused + j_3 + j_4)
                                v_k = T.axis.reduce(T.int64(512), k_0_fused + k_1 + k_2)
                                T.reads(T_matmul_NT_local[v_i, v_j], p0_shared[v_i, v_k], p1_shared[v_j, v_k])
                                T.writes(T_matmul_NT_local[v_i, v_j])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                T_matmul_NT_local[v_i, v_j] = T_matmul_NT_local[v_i, v_j] + p0_shared[v_i, v_k] * p1_shared[v_j, v_k]
                    for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(1000), i_0_j_0_fused * T.int64(40) + i_2_j_2_fused + ax1)
                            T.reads(T_matmul_NT_local[v0, v1], p2[v0, v1])
                            T.writes(T_add[v0, v1])
                            T_add[v0, v1] = T_matmul_NT_local[v0, v1] + p2[v0, v1]
b0 = sch.get_block(name="T_matmul_NT", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l3, l4, l5 = sch.get_loops(block=b0)
v6, v7, v8, v9, v10 = sch.sample_perfect_tile(loop=l3, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l11, l12, l13, l14, l15 = sch.split(loop=l3, factors=[v6, v7, v8, v9, v10], preserve_unit_iters=True)
v16, v17, v18, v19, v20 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[25, 1, 40, 1, 1])
l21, l22, l23, l24, l25 = sch.split(loop=l4, factors=[v16, v17, v18, v19, v20], preserve_unit_iters=True)
v26, v27, v28 = sch.sample_perfect_tile(loop=l5, n=3, max_innermost_factor=64, decision=[512, 1, 1])
l29, l30, l31 = sch.split(loop=l5, factors=[v26, v27, v28], preserve_unit_iters=True)
sch.reorder(l11, l21, l12, l22, l13, l23, l29, l30, l14, l24, l31, l15, l25)
l32 = sch.fuse(l11, l21, preserve_unit_iters=True)
sch.bind(loop=l32, thread_axis="blockIdx.x")
l33 = sch.fuse(l12, l22, preserve_unit_iters=True)
sch.bind(loop=l33, thread_axis="vthread.x")
l34 = sch.fuse(l13, l23, preserve_unit_iters=True)
sch.bind(loop=l34, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b35 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b35, loop=l34, preserve_unit_loops=True, index=-1)
b36 = sch.cache_read(block=b0, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b36, loop=l29, preserve_unit_loops=True, index=-1)
l37, l38, l39, l40, l41, l42 = sch.get_loops(block=b36)
l43 = sch.fuse(l41, l42, preserve_unit_iters=True)
v44 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b36, ann_key="meta_schedule.cooperative_fetch", ann_val=v44)
b45 = sch.cache_read(block=b0, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b45, loop=l29, preserve_unit_loops=True, index=-1)
l46, l47, l48, l49, l50, l51 = sch.get_loops(block=b45)
l52 = sch.fuse(l50, l51, preserve_unit_iters=True)
v53 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b45, ann_key="meta_schedule.cooperative_fetch", ann_val=v53)
l54 = sch.fuse(l29, preserve_unit_iters=True)
sch.annotate(block_or_loop=l54, ann_key="software_pipeline_stage", ann_val=[0, 0, 2])
sch.annotate(block_or_loop=l54, ann_key="software_pipeline_order", ann_val=[0, 1, 2])
sch.annotate(block_or_loop=l54, ann_key="software_pipeline_async_stages", ann_val=[0])
sch.reverse_compute_inline(block=b1)
v55 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v55)
sch.enter_postproc()
sch.unannotate(block_or_loop=b36, ann_key="meta_schedule.cooperative_fetch")
l56, l57, l58, l59, l60 = sch.get_loops(block=b36)
l61, l62 = sch.split(loop=l60, factors=[None, 40], preserve_unit_iters=True)
sch.bind(loop=l62, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b45, ann_key="meta_schedule.cooperative_fetch")
l63, l64, l65, l66, l67 = sch.get_loops(block=b45)
l68, l69, l70 = sch.split(loop=l67, factors=[None, 40, 4], preserve_unit_iters=True)
sch.vectorize(loop=l70)
sch.bind(loop=l69, thread_axis="threadIdx.x")
b71 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b71, ann_key="meta_schedule.unroll_explicit")
b72, b73, b74, b75 = sch.get_child_blocks(b71)
l76, l77, l78, l79, l80, l81 = sch.get_loops(block=b72)
l82, l83, l84, l85, l86, l87, l88 = sch.get_loops(block=b73)
l89, l90, l91, l92, l93, l94, l95, l96, l97, l98 = sch.get_loops(block=b74)
sch.annotate(block_or_loop=l89, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l89, ann_key="pragma_unroll_explicit", ann_val=1)
l99, l100, l101, l102, l103 = sch.get_loops(block=b75)
b104 = sch.get_block(name="T_matmul_NT", func_name="main")
l105, l106, l107, l108, l109, l110, l111, l112, l113, l114 = sch.get_loops(block=b104)
b115 = sch.decompose_reduction(block=b104, loop=l108)
2023-11-11 04:27:25 [INFO] [task_scheduler.cc:121] [Task #28: fused_nn_dense_add] Trial #138: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/home2/xiachunwei/Software/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/home2/xiachunwei/Software/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/home2/xiachunwei/Software/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/home2/xiachunwei/Software/tvm/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/home2/xiachunwei/Software/tvm/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/home2/xiachunwei/Software/tvm/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/home2/xiachunwei/Software/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/home2/xiachunwei/Software/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  4: _ZN3tvm7runtime13PackedFun
  3: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  2: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  0: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  File "/home2/xiachunwei/Software/tvm/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(512)), "float32"), p1: T.Buffer((T.int64(1000), T.int64(512)), "float32"), p2: T.Buffer((T.int64(1), T.int64(1000)), "float32"), T_add: T.Buffer((T.int64(1), T.int64(1000)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        T_matmul_NT_local = T.alloc_buffer((T.int64(1), T.int64(1000)), scope="local")
        p0_shared = T.alloc_buffer((T.int64(1), T.int64(512)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(1000), T.int64(512)), scope="shared")
        for i_0_j_0_fused in T.thread_binding(T.int64(25), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 1024, "pragma_unroll_explicit": 1}):
            for i_1_j_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for i_2_j_2_fused in T.thread_binding(T.int64(40), thread="threadIdx.x"):
                    for i_3_init, j_3_init, i_4_init, j_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_init"):
                            v_i = T.axis.spatial(T.int64(1), i_3_init + i_4_init)
                            v_j = T.axis.spatial(T.int64(1000), i_0_j_0_fused * T.int64(40) + i_2_j_2_fused + j_3_init + j_4_init)
                            T.reads()
                            T.writes(T_matmul_NT_local[v_i, v_j])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            T_matmul_NT_local[v_i, v_j] = T.float32(0)
                    for k_0_fused in T.serial(T.int64(512), annotations={"software_pipeline_async_stages": [0], "software_pipeline_order": [0, 1, 2], "software_pipeline_stage": [0, 0, 2]}):
                        for ax0_ax1_fused_0 in range(T.int64(1)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(40), thread="threadIdx.x"):
                                with T.block("p0_shared"):
                                    v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                    v1 = T.axis.spatial(T.int64(512), k_0_fused)
                                    T.where(ax0_ax1_fused_0 * T.int64(40) + ax0_ax1_fused_1 < T.int64(1))
                                    T.reads(p0[v0, v1])
                                    T.writes(p0_shared[v0, v1])
                                    p0_shared[v0, v1] = p0[v0, v1]
                        for ax0_ax1_fused_0 in range(T.int64(1)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(40), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("p1_shared"):
                                        v0 = T.axis.spatial(T.int64(1000), i_0_j_0_fused * T.int64(40) + (ax0_ax1_fused_0 * T.int64(80) + ax0_ax1_fused_1 * T.int64(2) + ax0_ax1_fused_2))
                                        v1 = T.axis.spatial(T.int64(512), k_0_fused)
                                        T.where((ax0_ax1_fused_0 * T.int64(40) + ax0_ax1_fused_1) * T.int64(2) + ax0_ax1_fused_2 < T.int64(40))
                                        T.reads(p1[v0, v1])
                                        T.writes(p1_shared[v0, v1])
                                        p1_shared[v0, v1] = p1[v0, v1]
                        for k_1, i_3, j_3, k_2, i_4, j_4 in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                            with T.block("T_matmul_NT_update"):
                                v_i = T.axis.spatial(T.int64(1), i_3 + i_4)
                                v_j = T.axis.spatial(T.int64(1000), i_0_j_0_fused * T.int64(40) + i_2_j_2_fused + j_3 + j_4)
                                v_k = T.axis.reduce(T.int64(512), k_0_fused + k_1 + k_2)
                                T.reads(T_matmul_NT_local[v_i, v_j], p0_shared[v_i, v_k], p1_shared[v_j, v_k])
                                T.writes(T_matmul_NT_local[v_i, v_j])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                T_matmul_NT_local[v_i, v_j] = T_matmul_NT_local[v_i, v_j] + p0_shared[v_i, v_k] * p1_shared[v_j, v_k]
                    for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(1000), i_0_j_0_fused * T.int64(40) + i_2_j_2_fused + ax1)
                            T.reads(T_matmul_NT_local[v0, v1], p2[v0, v1])
                            T.writes(T_add[v0, v1])
                            T_add[v0, v1] = T_matmul_NT_local[v0, v1] + p2[v0, v1]
b0 = sch.get_block(name="T_matmul_NT", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l3, l4, l5 = sch.get_loops(block=b0)
v6, v7, v8, v9, v10 = sch.sample_perfect_tile(loop=l3, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l11, l12, l13, l14, l15 = sch.split(loop=l3, factors=[v6, v7, v8, v9, v10], preserve_unit_iters=True)
v16, v17, v18, v19, v20 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[25, 1, 40, 1, 1])
l21, l22, l23, l24, l25 = sch.split(loop=l4, factors=[v16, v17, v18, v19, v20], preserve_unit_iters=True)
v26, v27, v28 = sch.sample_perfect_tile(loop=l5, n=3, max_innermost_factor=64, decision=[512, 1, 1])
l29, l30, l31 = sch.split(loop=l5, factors=[v26, v27, v28], preserve_unit_iters=True)
sch.reorder(l11, l21, l12, l22, l13, l23, l29, l30, l14, l24, l31, l15, l25)
l32 = sch.fuse(l11, l21, preserve_unit_iters=True)
sch.bind(loop=l32, thread_axis="blockIdx.x")
l33 = sch.fuse(l12, l22, preserve_unit_iters=True)
sch.bind(loop=l33, thread_axis="vthread.x")
l34 = sch.fuse(l13, l23, preserve_unit_iters=True)
sch.bind(loop=l34, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b35 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b35, loop=l34, preserve_unit_loops=True, index=-1)
b36 = sch.cache_read(block=b0, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b36, loop=l29, preserve_unit_loops=True, index=-1)
l37, l38, l39, l40, l41, l42 = sch.get_loops(block=b36)
l43 = sch.fuse(l41, l42, preserve_unit_iters=True)
v44 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b36, ann_key="meta_schedule.cooperative_fetch", ann_val=v44)
b45 = sch.cache_read(block=b0, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b45, loop=l29, preserve_unit_loops=True, index=-1)
l46, l47, l48, l49, l50, l51 = sch.get_loops(block=b45)
l52 = sch.fuse(l50, l51, preserve_unit_iters=True)
v53 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b45, ann_key="meta_schedule.cooperative_fetch", ann_val=v53)
l54 = sch.fuse(l29, preserve_unit_iters=True)
sch.annotate(block_or_loop=l54, ann_key="software_pipeline_stage", ann_val=[0, 0, 2])
sch.annotate(block_or_loop=l54, ann_key="software_pipeline_order", ann_val=[0, 1, 2])
sch.annotate(block_or_loop=l54, ann_key="software_pipeline_async_stages", ann_val=[0])
sch.reverse_compute_inline(block=b1)
v55 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v55)
sch.enter_postproc()
sch.unannotate(block_or_loop=b36, ann_key="meta_schedule.cooperative_fetch")
l56, l57, l58, l59, l60 = sch.get_loops(block=b36)
l61, l62 = sch.split(loop=l60, factors=[None, 40], preserve_unit_iters=True)
sch.bind(loop=l62, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b45, ann_key="meta_schedule.cooperative_fetch")
l63, l64, l65, l66, l67 = sch.get_loops(block=b45)
l68, l69, l70 = sch.split(loop=l67, factors=[None, 40, 2], preserve_unit_iters=True)
sch.vectorize(loop=l70)
sch.bind(loop=l69, thread_axis="threadIdx.x")
b71 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b71, ann_key="meta_schedule.unroll_explicit")
b72, b73, b74, b75 = sch.get_child_blocks(b71)
l76, l77, l78, l79, l80, l81 = sch.get_loops(block=b72)
l82, l83, l84, l85, l86, l87, l88 = sch.get_loops(block=b73)
l89, l90, l91, l92, l93, l94, l95, l96, l97, l98 = sch.get_loops(block=b74)
sch.annotate(block_or_loop=l89, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l89, ann_key="pragma_unroll_explicit", ann_val=1)
l99, l100, l101, l102, l103 = sch.get_loops(block=b75)
b104 = sch.get_block(name="T_matmul_NT", func_name="main")
l105, l106, l107, l108, l109, l110, l111, l112, l113, l114 = sch.get_loops(block=b104)
b115 = sch.decompose_reduction(block=b104, loop=l108)
2023-11-11 04:27:25 [INFO] [task_scheduler.cc:121] [Task #28: fused_nn_dense_add] Trial #139: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/home2/xiachunwei/Software/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/home2/xiachunwei/Software/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/home2/xiachunwei/Software/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/home2/xiachunwei/Software/tvm/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/home2/xiachunwei/Software/tvm/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/home2/xiachunwei/Software/tvm/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/home2/xiachunwei/Software/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/home2/xiachunwei/Software/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  4: _ZN3tvm7runtime13PackedFun
  3: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  2: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  0: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  File "/home2/xiachunwei/Software/tvm/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(512)), "float32"), p1: T.Buffer((T.int64(1000), T.int64(512)), "float32"), p2: T.Buffer((T.int64(1), T.int64(1000)), "float32"), T_add: T.Buffer((T.int64(1), T.int64(1000)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        T_matmul_NT_local = T.alloc_buffer((T.int64(1), T.int64(1000)), scope="local")
        p0_shared = T.alloc_buffer((T.int64(1), T.int64(512)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(1000), T.int64(512)), scope="shared")
        for i_0_j_0_fused in T.thread_binding(T.int64(25), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 16, "pragma_unroll_explicit": 1}):
            for i_1_j_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for i_2_j_2_fused in T.thread_binding(T.int64(40), thread="threadIdx.x"):
                    for i_3_init, j_3_init, i_4_init, j_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_init"):
                            v_i = T.axis.spatial(T.int64(1), i_3_init + i_4_init)
                            v_j = T.axis.spatial(T.int64(1000), i_0_j_0_fused * T.int64(40) + i_2_j_2_fused + j_3_init + j_4_init)
                            T.reads()
                            T.writes(T_matmul_NT_local[v_i, v_j])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            T_matmul_NT_local[v_i, v_j] = T.float32(0)
                    for k_0_fused in T.serial(T.int64(512), annotations={"software_pipeline_async_stages": [0], "software_pipeline_order": [0, 1, 2], "software_pipeline_stage": [0, 0, 2]}):
                        for ax0_ax1_fused_0 in range(T.int64(1)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(40), thread="threadIdx.x"):
                                with T.block("p0_shared"):
                                    v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                    v1 = T.axis.spatial(T.int64(512), k_0_fused)
                                    T.where(ax0_ax1_fused_0 * T.int64(40) + ax0_ax1_fused_1 < T.int64(1))
                                    T.reads(p0[v0, v1])
                                    T.writes(p0_shared[v0, v1])
                                    p0_shared[v0, v1] = p0[v0, v1]
                        for ax0_ax1_fused_0 in range(T.int64(1)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(40), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("p1_shared"):
                                        v0 = T.axis.spatial(T.int64(1000), i_0_j_0_fused * T.int64(40) + (ax0_ax1_fused_0 * T.int64(160) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2))
                                        v1 = T.axis.spatial(T.int64(512), k_0_fused)
                                        T.where((ax0_ax1_fused_0 * T.int64(40) + ax0_ax1_fused_1) * T.int64(4) + ax0_ax1_fused_2 < T.int64(40))
                                        T.reads(p1[v0, v1])
                                        T.writes(p1_shared[v0, v1])
                                        p1_shared[v0, v1] = p1[v0, v1]
                        for k_1, i_3, j_3, k_2, i_4, j_4 in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                            with T.block("T_matmul_NT_update"):
                                v_i = T.axis.spatial(T.int64(1), i_3 + i_4)
                                v_j = T.axis.spatial(T.int64(1000), i_0_j_0_fused * T.int64(40) + i_2_j_2_fused + j_3 + j_4)
                                v_k = T.axis.reduce(T.int64(512), k_0_fused + k_1 + k_2)
                                T.reads(T_matmul_NT_local[v_i, v_j], p0_shared[v_i, v_k], p1_shared[v_j, v_k])
                                T.writes(T_matmul_NT_local[v_i, v_j])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                T_matmul_NT_local[v_i, v_j] = T_matmul_NT_local[v_i, v_j] + p0_shared[v_i, v_k] * p1_shared[v_j, v_k]
                    for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(1000), i_0_j_0_fused * T.int64(40) + i_2_j_2_fused + ax1)
                            T.reads(T_matmul_NT_local[v0, v1], p2[v0, v1])
                            T.writes(T_add[v0, v1])
                            T_add[v0, v1] = T_matmul_NT_local[v0, v1] + p2[v0, v1]
b0 = sch.get_block(name="T_matmul_NT", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l3, l4, l5 = sch.get_loops(block=b0)
v6, v7, v8, v9, v10 = sch.sample_perfect_tile(loop=l3, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l11, l12, l13, l14, l15 = sch.split(loop=l3, factors=[v6, v7, v8, v9, v10], preserve_unit_iters=True)
v16, v17, v18, v19, v20 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[25, 1, 40, 1, 1])
l21, l22, l23, l24, l25 = sch.split(loop=l4, factors=[v16, v17, v18, v19, v20], preserve_unit_iters=True)
v26, v27, v28 = sch.sample_perfect_tile(loop=l5, n=3, max_innermost_factor=64, decision=[512, 1, 1])
l29, l30, l31 = sch.split(loop=l5, factors=[v26, v27, v28], preserve_unit_iters=True)
sch.reorder(l11, l21, l12, l22, l13, l23, l29, l30, l14, l24, l31, l15, l25)
l32 = sch.fuse(l11, l21, preserve_unit_iters=True)
sch.bind(loop=l32, thread_axis="blockIdx.x")
l33 = sch.fuse(l12, l22, preserve_unit_iters=True)
sch.bind(loop=l33, thread_axis="vthread.x")
l34 = sch.fuse(l13, l23, preserve_unit_iters=True)
sch.bind(loop=l34, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b35 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b35, loop=l34, preserve_unit_loops=True, index=-1)
b36 = sch.cache_read(block=b0, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b36, loop=l29, preserve_unit_loops=True, index=-1)
l37, l38, l39, l40, l41, l42 = sch.get_loops(block=b36)
l43 = sch.fuse(l41, l42, preserve_unit_iters=True)
v44 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b36, ann_key="meta_schedule.cooperative_fetch", ann_val=v44)
b45 = sch.cache_read(block=b0, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b45, loop=l29, preserve_unit_loops=True, index=-1)
l46, l47, l48, l49, l50, l51 = sch.get_loops(block=b45)
l52 = sch.fuse(l50, l51, preserve_unit_iters=True)
v53 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b45, ann_key="meta_schedule.cooperative_fetch", ann_val=v53)
l54 = sch.fuse(l29, preserve_unit_iters=True)
sch.annotate(block_or_loop=l54, ann_key="software_pipeline_stage", ann_val=[0, 0, 2])
sch.annotate(block_or_loop=l54, ann_key="software_pipeline_order", ann_val=[0, 1, 2])
sch.annotate(block_or_loop=l54, ann_key="software_pipeline_async_stages", ann_val=[0])
sch.reverse_compute_inline(block=b1)
v55 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=1)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v55)
sch.enter_postproc()
sch.unannotate(block_or_loop=b36, ann_key="meta_schedule.cooperative_fetch")
l56, l57, l58, l59, l60 = sch.get_loops(block=b36)
l61, l62 = sch.split(loop=l60, factors=[None, 40], preserve_unit_iters=True)
sch.bind(loop=l62, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b45, ann_key="meta_schedule.cooperative_fetch")
l63, l64, l65, l66, l67 = sch.get_loops(block=b45)
l68, l69, l70 = sch.split(loop=l67, factors=[None, 40, 4], preserve_unit_iters=True)
sch.vectorize(loop=l70)
sch.bind(loop=l69, thread_axis="threadIdx.x")
b71 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b71, ann_key="meta_schedule.unroll_explicit")
b72, b73, b74, b75 = sch.get_child_blocks(b71)
l76, l77, l78, l79, l80, l81 = sch.get_loops(block=b72)
l82, l83, l84, l85, l86, l87, l88 = sch.get_loops(block=b73)
l89, l90, l91, l92, l93, l94, l95, l96, l97, l98 = sch.get_loops(block=b74)
sch.annotate(block_or_loop=l89, ann_key="pragma_auto_unroll_max_step", ann_val=16)
sch.annotate(block_or_loop=l89, ann_key="pragma_unroll_explicit", ann_val=1)
l99, l100, l101, l102, l103 = sch.get_loops(block=b75)
b104 = sch.get_block(name="T_matmul_NT", func_name="main")
l105, l106, l107, l108, l109, l110, l111, l112, l113, l114 = sch.get_loops(block=b104)
b115 = sch.decompose_reduction(block=b104, loop=l108)
2023-11-11 04:27:25 [INFO] [task_scheduler.cc:121] [Task #28: fused_nn_dense_add] Trial #140: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/home2/xiachunwei/Software/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/home2/xiachunwei/Software/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/home2/xiachunwei/Software/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/home2/xiachunwei/Software/tvm/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/home2/xiachunwei/Software/tvm/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/home2/xiachunwei/Software/tvm/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/home2/xiachunwei/Software/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/home2/xiachunwei/Software/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  4: _ZN3tvm7runtime13PackedFun
  3: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  2: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  0: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  File "/home2/xiachunwei/Software/tvm/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(512)), "float32"), p1: T.Buffer((T.int64(1000), T.int64(512)), "float32"), p2: T.Buffer((T.int64(1), T.int64(1000)), "float32"), T_add: T.Buffer((T.int64(1), T.int64(1000)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        T_matmul_NT_local = T.alloc_buffer((T.int64(1), T.int64(1000)), scope="local")
        p0_shared = T.alloc_buffer((T.int64(1), T.int64(512)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(1000), T.int64(512)), scope="shared")
        for i_0_j_0_fused in T.thread_binding(T.int64(25), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 1024, "pragma_unroll_explicit": 1}):
            for i_1_j_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for i_2_j_2_fused in T.thread_binding(T.int64(40), thread="threadIdx.x"):
                    for i_3_init, j_3_init, i_4_init, j_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_init"):
                            v_i = T.axis.spatial(T.int64(1), i_3_init + i_4_init)
                            v_j = T.axis.spatial(T.int64(1000), i_0_j_0_fused * T.int64(40) + i_2_j_2_fused + j_3_init + j_4_init)
                            T.reads()
                            T.writes(T_matmul_NT_local[v_i, v_j])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            T_matmul_NT_local[v_i, v_j] = T.float32(0)
                    for k_0 in range(T.int64(2)):
                        for ax0_ax1_fused_0 in range(T.int64(4)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(40), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("p0_shared"):
                                        v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v1 = T.axis.spatial(T.int64(512), k_0 * T.int64(256) + (ax0_ax1_fused_0 * T.int64(80) + ax0_ax1_fused_1 * T.int64(2) + ax0_ax1_fused_2))
                                        T.where((ax0_ax1_fused_0 * T.int64(40) + ax0_ax1_fused_1) * T.int64(2) + ax0_ax1_fused_2 < T.int64(256))
                                        T.reads(p0[v0, v1])
                                        T.writes(p0_shared[v0, v1])
                                        p0_shared[v0, v1] = p0[v0, v1]
                        for ax0_ax1_fused_0 in range(T.int64(64)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(40), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("p1_shared"):
                                        v0 = T.axis.spatial(T.int64(1000), i_0_j_0_fused * T.int64(40) + (ax0_ax1_fused_0 * T.int64(160) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2) // T.int64(256))
                                        v1 = T.axis.spatial(T.int64(512), k_0 * T.int64(256) + (ax0_ax1_fused_0 * T.int64(160) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2) % T.int64(256))
                                        T.reads(p1[v0, v1])
                                        T.writes(p1_shared[v0, v1])
                                        p1_shared[v0, v1] = p1[v0, v1]
                        for k_1, i_3, j_3, k_2, i_4, j_4 in T.grid(T.int64(128), T.int64(1), T.int64(1), T.int64(2), T.int64(1), T.int64(1)):
                            with T.block("T_matmul_NT_update"):
                                v_i = T.axis.spatial(T.int64(1), i_3 + i_4)
                                v_j = T.axis.spatial(T.int64(1000), i_0_j_0_fused * T.int64(40) + i_2_j_2_fused + j_3 + j_4)
                                v_k = T.axis.reduce(T.int64(512), k_0 * T.int64(256) + k_1 * T.int64(2) + k_2)
                                T.reads(T_matmul_NT_local[v_i, v_j], p0_shared[v_i, v_k], p1_shared[v_j, v_k])
                                T.writes(T_matmul_NT_local[v_i, v_j])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                T_matmul_NT_local[v_i, v_j] = T_matmul_NT_local[v_i, v_j] + p0_shared[v_i, v_k] * p1_shared[v_j, v_k]
                    for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(1000), i_0_j_0_fused * T.int64(40) + i_2_j_2_fused + ax1)
                            T.reads(T_matmul_NT_local[v0, v1], p2[v0, v1])
                            T.writes(T_add[v0, v1])
                            T_add[v0, v1] = T_matmul_NT_local[v0, v1] + p2[v0, v1]
b0 = sch.get_block(name="T_matmul_NT", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l3, l4, l5 = sch.get_loops(block=b0)
v6, v7, v8, v9, v10 = sch.sample_perfect_tile(loop=l3, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l11, l12, l13, l14, l15 = sch.split(loop=l3, factors=[v6, v7, v8, v9, v10], preserve_unit_iters=True)
v16, v17, v18, v19, v20 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[25, 1, 40, 1, 1])
l21, l22, l23, l24, l25 = sch.split(loop=l4, factors=[v16, v17, v18, v19, v20], preserve_unit_iters=True)
v26, v27, v28 = sch.sample_perfect_tile(loop=l5, n=3, max_innermost_factor=64, decision=[2, 128, 2])
l29, l30, l31 = sch.split(loop=l5, factors=[v26, v27, v28], preserve_unit_iters=True)
sch.reorder(l11, l21, l12, l22, l13, l23, l29, l30, l14, l24, l31, l15, l25)
l32 = sch.fuse(l11, l21, preserve_unit_iters=True)
sch.bind(loop=l32, thread_axis="blockIdx.x")
l33 = sch.fuse(l12, l22, preserve_unit_iters=True)
sch.bind(loop=l33, thread_axis="vthread.x")
l34 = sch.fuse(l13, l23, preserve_unit_iters=True)
sch.bind(loop=l34, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b35 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b35, loop=l34, preserve_unit_loops=True, index=-1)
b36 = sch.cache_read(block=b0, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b36, loop=l29, preserve_unit_loops=True, index=-1)
l37, l38, l39, l40, l41, l42 = sch.get_loops(block=b36)
l43 = sch.fuse(l41, l42, preserve_unit_iters=True)
v44 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b36, ann_key="meta_schedule.cooperative_fetch", ann_val=v44)
b45 = sch.cache_read(block=b0, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b45, loop=l29, preserve_unit_loops=True, index=-1)
l46, l47, l48, l49, l50, l51 = sch.get_loops(block=b45)
l52 = sch.fuse(l50, l51, preserve_unit_iters=True)
v53 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b45, ann_key="meta_schedule.cooperative_fetch", ann_val=v53)
sch.reverse_compute_inline(block=b1)
v54 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v54)
sch.enter_postproc()
sch.unannotate(block_or_loop=b36, ann_key="meta_schedule.cooperative_fetch")
l55, l56, l57, l58, l59 = sch.get_loops(block=b36)
l60, l61, l62 = sch.split(loop=l59, factors=[None, 40, 2], preserve_unit_iters=True)
sch.vectorize(loop=l62)
sch.bind(loop=l61, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b45, ann_key="meta_schedule.cooperative_fetch")
l63, l64, l65, l66, l67 = sch.get_loops(block=b45)
l68, l69, l70 = sch.split(loop=l67, factors=[None, 40, 4], preserve_unit_iters=True)
sch.vectorize(loop=l70)
sch.bind(loop=l69, thread_axis="threadIdx.x")
b71 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b71, ann_key="meta_schedule.unroll_explicit")
b72, b73, b74, b75 = sch.get_child_blocks(b71)
l76, l77, l78, l79, l80, l81, l82 = sch.get_loops(block=b72)
l83, l84, l85, l86, l87, l88, l89 = sch.get_loops(block=b73)
l90, l91, l92, l93, l94, l95, l96, l97, l98, l99 = sch.get_loops(block=b74)
sch.annotate(block_or_loop=l90, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l90, ann_key="pragma_unroll_explicit", ann_val=1)
l100, l101, l102, l103, l104 = sch.get_loops(block=b75)
b105 = sch.get_block(name="T_matmul_NT", func_name="main")
l106, l107, l108, l109, l110, l111, l112, l113, l114, l115 = sch.get_loops(block=b105)
b116 = sch.decompose_reduction(block=b105, loop=l109)
2023-11-11 04:27:25 [INFO] [task_scheduler.cc:121] [Task #28: fused_nn_dense_add] Trial #141: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/home2/xiachunwei/Software/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/home2/xiachunwei/Software/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/home2/xiachunwei/Software/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/home2/xiachunwei/Software/tvm/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/home2/xiachunwei/Software/tvm/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/home2/xiachunwei/Software/tvm/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/home2/xiachunwei/Software/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/home2/xiachunwei/Software/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  4: _ZN3tvm7runtime13PackedFun
  3: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  2: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  0: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  File "/home2/xiachunwei/Software/tvm/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(512)), "float32"), p1: T.Buffer((T.int64(1000), T.int64(512)), "float32"), p2: T.Buffer((T.int64(1), T.int64(1000)), "float32"), T_add: T.Buffer((T.int64(1), T.int64(1000)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        T_matmul_NT_local = T.alloc_buffer((T.int64(1), T.int64(1000)), scope="local")
        p0_shared = T.alloc_buffer((T.int64(1), T.int64(512)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(1000), T.int64(512)), scope="shared")
        for i_0_j_0_fused in T.thread_binding(T.int64(25), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 1024, "pragma_unroll_explicit": 1}):
            for i_1_j_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for i_2_j_2_fused in T.thread_binding(T.int64(40), thread="threadIdx.x"):
                    for i_3_init, j_3_init, i_4_init, j_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_init"):
                            v_i = T.axis.spatial(T.int64(1), i_3_init + i_4_init)
                            v_j = T.axis.spatial(T.int64(1000), i_0_j_0_fused * T.int64(40) + i_2_j_2_fused + j_3_init + j_4_init)
                            T.reads()
                            T.writes(T_matmul_NT_local[v_i, v_j])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            T_matmul_NT_local[v_i, v_j] = T.float32(0)
                    for k_0 in range(T.int64(2)):
                        for ax0_ax1_fused_0 in range(T.int64(4)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(40), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("p0_shared"):
                                        v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v1 = T.axis.spatial(T.int64(512), k_0 * T.int64(256) + (ax0_ax1_fused_0 * T.int64(80) + ax0_ax1_fused_1 * T.int64(2) + ax0_ax1_fused_2))
                                        T.where((ax0_ax1_fused_0 * T.int64(40) + ax0_ax1_fused_1) * T.int64(2) + ax0_ax1_fused_2 < T.int64(256))
                                        T.reads(p0[v0, v1])
                                        T.writes(p0_shared[v0, v1])
                                        p0_shared[v0, v1] = p0[v0, v1]
                        for ax0_ax1_fused_0 in range(T.int64(64)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(40), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("p1_shared"):
                                        v0 = T.axis.spatial(T.int64(1000), i_0_j_0_fused * T.int64(40) + (ax0_ax1_fused_0 * T.int64(160) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2) // T.int64(256))
                                        v1 = T.axis.spatial(T.int64(512), k_0 * T.int64(256) + (ax0_ax1_fused_0 * T.int64(160) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2) % T.int64(256))
                                        T.reads(p1[v0, v1])
                                        T.writes(p1_shared[v0, v1])
                                        p1_shared[v0, v1] = p1[v0, v1]
                        for k_1, i_3, j_3, k_2, i_4, j_4 in T.grid(T.int64(32), T.int64(1), T.int64(1), T.int64(8), T.int64(1), T.int64(1)):
                            with T.block("T_matmul_NT_update"):
                                v_i = T.axis.spatial(T.int64(1), i_3 + i_4)
                                v_j = T.axis.spatial(T.int64(1000), i_0_j_0_fused * T.int64(40) + i_2_j_2_fused + j_3 + j_4)
                                v_k = T.axis.reduce(T.int64(512), k_0 * T.int64(256) + k_1 * T.int64(8) + k_2)
                                T.reads(T_matmul_NT_local[v_i, v_j], p0_shared[v_i, v_k], p1_shared[v_j, v_k])
                                T.writes(T_matmul_NT_local[v_i, v_j])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                T_matmul_NT_local[v_i, v_j] = T_matmul_NT_local[v_i, v_j] + p0_shared[v_i, v_k] * p1_shared[v_j, v_k]
                    for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(1000), i_0_j_0_fused * T.int64(40) + i_2_j_2_fused + ax1)
                            T.reads(T_matmul_NT_local[v0, v1], p2[v0, v1])
                            T.writes(T_add[v0, v1])
                            T_add[v0, v1] = T_matmul_NT_local[v0, v1] + p2[v0, v1]
b0 = sch.get_block(name="T_matmul_NT", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l3, l4, l5 = sch.get_loops(block=b0)
v6, v7, v8, v9, v10 = sch.sample_perfect_tile(loop=l3, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l11, l12, l13, l14, l15 = sch.split(loop=l3, factors=[v6, v7, v8, v9, v10], preserve_unit_iters=True)
v16, v17, v18, v19, v20 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[25, 1, 40, 1, 1])
l21, l22, l23, l24, l25 = sch.split(loop=l4, factors=[v16, v17, v18, v19, v20], preserve_unit_iters=True)
v26, v27, v28 = sch.sample_perfect_tile(loop=l5, n=3, max_innermost_factor=64, decision=[2, 32, 8])
l29, l30, l31 = sch.split(loop=l5, factors=[v26, v27, v28], preserve_unit_iters=True)
sch.reorder(l11, l21, l12, l22, l13, l23, l29, l30, l14, l24, l31, l15, l25)
l32 = sch.fuse(l11, l21, preserve_unit_iters=True)
sch.bind(loop=l32, thread_axis="blockIdx.x")
l33 = sch.fuse(l12, l22, preserve_unit_iters=True)
sch.bind(loop=l33, thread_axis="vthread.x")
l34 = sch.fuse(l13, l23, preserve_unit_iters=True)
sch.bind(loop=l34, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b35 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b35, loop=l34, preserve_unit_loops=True, index=-1)
b36 = sch.cache_read(block=b0, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b36, loop=l29, preserve_unit_loops=True, index=-1)
l37, l38, l39, l40, l41, l42 = sch.get_loops(block=b36)
l43 = sch.fuse(l41, l42, preserve_unit_iters=True)
v44 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b36, ann_key="meta_schedule.cooperative_fetch", ann_val=v44)
b45 = sch.cache_read(block=b0, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b45, loop=l29, preserve_unit_loops=True, index=-1)
l46, l47, l48, l49, l50, l51 = sch.get_loops(block=b45)
l52 = sch.fuse(l50, l51, preserve_unit_iters=True)
v53 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b45, ann_key="meta_schedule.cooperative_fetch", ann_val=v53)
sch.reverse_compute_inline(block=b1)
v54 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v54)
sch.enter_postproc()
sch.unannotate(block_or_loop=b36, ann_key="meta_schedule.cooperative_fetch")
l55, l56, l57, l58, l59 = sch.get_loops(block=b36)
l60, l61, l62 = sch.split(loop=l59, factors=[None, 40, 2], preserve_unit_iters=True)
sch.vectorize(loop=l62)
sch.bind(loop=l61, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b45, ann_key="meta_schedule.cooperative_fetch")
l63, l64, l65, l66, l67 = sch.get_loops(block=b45)
l68, l69, l70 = sch.split(loop=l67, factors=[None, 40, 4], preserve_unit_iters=True)
sch.vectorize(loop=l70)
sch.bind(loop=l69, thread_axis="threadIdx.x")
b71 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b71, ann_key="meta_schedule.unroll_explicit")
b72, b73, b74, b75 = sch.get_child_blocks(b71)
l76, l77, l78, l79, l80, l81, l82 = sch.get_loops(block=b72)
l83, l84, l85, l86, l87, l88, l89 = sch.get_loops(block=b73)
l90, l91, l92, l93, l94, l95, l96, l97, l98, l99 = sch.get_loops(block=b74)
sch.annotate(block_or_loop=l90, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l90, ann_key="pragma_unroll_explicit", ann_val=1)
l100, l101, l102, l103, l104 = sch.get_loops(block=b75)
b105 = sch.get_block(name="T_matmul_NT", func_name="main")
l106, l107, l108, l109, l110, l111, l112, l113, l114, l115 = sch.get_loops(block=b105)
b116 = sch.decompose_reduction(block=b105, loop=l109)
2023-11-11 04:27:25 [INFO] [task_scheduler.cc:121] [Task #28: fused_nn_dense_add] Trial #142: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/home2/xiachunwei/Software/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/home2/xiachunwei/Software/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/home2/xiachunwei/Software/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/home2/xiachunwei/Software/tvm/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/home2/xiachunwei/Software/tvm/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/home2/xiachunwei/Software/tvm/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/home2/xiachunwei/Software/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/home2/xiachunwei/Software/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  4: _ZN3tvm7runtime13PackedFun
  3: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  2: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  0: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  File "/home2/xiachunwei/Software/tvm/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(512)), "float32"), p1: T.Buffer((T.int64(1000), T.int64(512)), "float32"), p2: T.Buffer((T.int64(1), T.int64(1000)), "float32"), T_add: T.Buffer((T.int64(1), T.int64(1000)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        T_matmul_NT_local = T.alloc_buffer((T.int64(1), T.int64(1000)), scope="local")
        p0_shared = T.alloc_buffer((T.int64(1), T.int64(512)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(1000), T.int64(512)), scope="shared")
        for i_0_j_0_fused in T.thread_binding(T.int64(25), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 1024, "pragma_unroll_explicit": 1}):
            for i_1_j_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for i_2_j_2_fused in T.thread_binding(T.int64(40), thread="threadIdx.x"):
                    for i_3_init, j_3_init, i_4_init, j_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_init"):
                            v_i = T.axis.spatial(T.int64(1), i_3_init + i_4_init)
                            v_j = T.axis.spatial(T.int64(1000), i_0_j_0_fused * T.int64(40) + i_2_j_2_fused + j_3_init + j_4_init)
                            T.reads()
                            T.writes(T_matmul_NT_local[v_i, v_j])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            T_matmul_NT_local[v_i, v_j] = T.float32(0)
                    for k_0 in range(T.int64(2)):
                        for ax0_ax1_fused_0 in range(T.int64(4)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(40), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("p0_shared"):
                                        v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v1 = T.axis.spatial(T.int64(512), k_0 * T.int64(256) + (ax0_ax1_fused_0 * T.int64(80) + ax0_ax1_fused_1 * T.int64(2) + ax0_ax1_fused_2))
                                        T.where((ax0_ax1_fused_0 * T.int64(40) + ax0_ax1_fused_1) * T.int64(2) + ax0_ax1_fused_2 < T.int64(256))
                                        T.reads(p0[v0, v1])
                                        T.writes(p0_shared[v0, v1])
                                        p0_shared[v0, v1] = p0[v0, v1]
                        for ax0_ax1_fused_0 in range(T.int64(64)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(40), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("p1_shared"):
                                        v0 = T.axis.spatial(T.int64(1000), i_0_j_0_fused * T.int64(40) + (ax0_ax1_fused_0 * T.int64(160) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2) // T.int64(256))
                                        v1 = T.axis.spatial(T.int64(512), k_0 * T.int64(256) + (ax0_ax1_fused_0 * T.int64(160) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2) % T.int64(256))
                                        T.reads(p1[v0, v1])
                                        T.writes(p1_shared[v0, v1])
                                        p1_shared[v0, v1] = p1[v0, v1]
                        for k_1, i_3, j_3, k_2, i_4, j_4 in T.grid(T.int64(4), T.int64(1), T.int64(1), T.int64(64), T.int64(1), T.int64(1)):
                            with T.block("T_matmul_NT_update"):
                                v_i = T.axis.spatial(T.int64(1), i_3 + i_4)
                                v_j = T.axis.spatial(T.int64(1000), i_0_j_0_fused * T.int64(40) + i_2_j_2_fused + j_3 + j_4)
                                v_k = T.axis.reduce(T.int64(512), k_0 * T.int64(256) + k_1 * T.int64(64) + k_2)
                                T.reads(T_matmul_NT_local[v_i, v_j], p0_shared[v_i, v_k], p1_shared[v_j, v_k])
                                T.writes(T_matmul_NT_local[v_i, v_j])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                T_matmul_NT_local[v_i, v_j] = T_matmul_NT_local[v_i, v_j] + p0_shared[v_i, v_k] * p1_shared[v_j, v_k]
                    for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(1000), i_0_j_0_fused * T.int64(40) + i_2_j_2_fused + ax1)
                            T.reads(T_matmul_NT_local[v0, v1], p2[v0, v1])
                            T.writes(T_add[v0, v1])
                            T_add[v0, v1] = T_matmul_NT_local[v0, v1] + p2[v0, v1]
b0 = sch.get_block(name="T_matmul_NT", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l3, l4, l5 = sch.get_loops(block=b0)
v6, v7, v8, v9, v10 = sch.sample_perfect_tile(loop=l3, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l11, l12, l13, l14, l15 = sch.split(loop=l3, factors=[v6, v7, v8, v9, v10], preserve_unit_iters=True)
v16, v17, v18, v19, v20 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[25, 1, 40, 1, 1])
l21, l22, l23, l24, l25 = sch.split(loop=l4, factors=[v16, v17, v18, v19, v20], preserve_unit_iters=True)
v26, v27, v28 = sch.sample_perfect_tile(loop=l5, n=3, max_innermost_factor=64, decision=[2, 4, 64])
l29, l30, l31 = sch.split(loop=l5, factors=[v26, v27, v28], preserve_unit_iters=True)
sch.reorder(l11, l21, l12, l22, l13, l23, l29, l30, l14, l24, l31, l15, l25)
l32 = sch.fuse(l11, l21, preserve_unit_iters=True)
sch.bind(loop=l32, thread_axis="blockIdx.x")
l33 = sch.fuse(l12, l22, preserve_unit_iters=True)
sch.bind(loop=l33, thread_axis="vthread.x")
l34 = sch.fuse(l13, l23, preserve_unit_iters=True)
sch.bind(loop=l34, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b35 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b35, loop=l34, preserve_unit_loops=True, index=-1)
b36 = sch.cache_read(block=b0, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b36, loop=l29, preserve_unit_loops=True, index=-1)
l37, l38, l39, l40, l41, l42 = sch.get_loops(block=b36)
l43 = sch.fuse(l41, l42, preserve_unit_iters=True)
v44 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b36, ann_key="meta_schedule.cooperative_fetch", ann_val=v44)
b45 = sch.cache_read(block=b0, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b45, loop=l29, preserve_unit_loops=True, index=-1)
l46, l47, l48, l49, l50, l51 = sch.get_loops(block=b45)
l52 = sch.fuse(l50, l51, preserve_unit_iters=True)
v53 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b45, ann_key="meta_schedule.cooperative_fetch", ann_val=v53)
sch.reverse_compute_inline(block=b1)
v54 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v54)
sch.enter_postproc()
sch.unannotate(block_or_loop=b36, ann_key="meta_schedule.cooperative_fetch")
l55, l56, l57, l58, l59 = sch.get_loops(block=b36)
l60, l61, l62 = sch.split(loop=l59, factors=[None, 40, 2], preserve_unit_iters=True)
sch.vectorize(loop=l62)
sch.bind(loop=l61, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b45, ann_key="meta_schedule.cooperative_fetch")
l63, l64, l65, l66, l67 = sch.get_loops(block=b45)
l68, l69, l70 = sch.split(loop=l67, factors=[None, 40, 4], preserve_unit_iters=True)
sch.vectorize(loop=l70)
sch.bind(loop=l69, thread_axis="threadIdx.x")
b71 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b71, ann_key="meta_schedule.unroll_explicit")
b72, b73, b74, b75 = sch.get_child_blocks(b71)
l76, l77, l78, l79, l80, l81, l82 = sch.get_loops(block=b72)
l83, l84, l85, l86, l87, l88, l89 = sch.get_loops(block=b73)
l90, l91, l92, l93, l94, l95, l96, l97, l98, l99 = sch.get_loops(block=b74)
sch.annotate(block_or_loop=l90, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l90, ann_key="pragma_unroll_explicit", ann_val=1)
l100, l101, l102, l103, l104 = sch.get_loops(block=b75)
b105 = sch.get_block(name="T_matmul_NT", func_name="main")
l106, l107, l108, l109, l110, l111, l112, l113, l114, l115 = sch.get_loops(block=b105)
b116 = sch.decompose_reduction(block=b105, loop=l109)
2023-11-11 04:27:25 [INFO] [task_scheduler.cc:121] [Task #28: fused_nn_dense_add] Trial #143: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/home2/xiachunwei/Software/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/home2/xiachunwei/Software/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/home2/xiachunwei/Software/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/home2/xiachunwei/Software/tvm/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/home2/xiachunwei/Software/tvm/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/home2/xiachunwei/Software/tvm/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/home2/xiachunwei/Software/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/home2/xiachunwei/Software/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  4: _ZN3tvm7runtime13PackedFun
  3: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  2: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  0: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  File "/home2/xiachunwei/Software/tvm/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(512)), "float32"), p1: T.Buffer((T.int64(1000), T.int64(512)), "float32"), p2: T.Buffer((T.int64(1), T.int64(1000)), "float32"), T_add: T.Buffer((T.int64(1), T.int64(1000)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        T_matmul_NT_local = T.alloc_buffer((T.int64(1), T.int64(1000)), scope="local")
        p0_shared = T.alloc_buffer((T.int64(1), T.int64(512)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(1000), T.int64(512)), scope="shared")
        for i_0_j_0_fused in T.thread_binding(T.int64(25), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 1024, "pragma_unroll_explicit": 1}):
            for i_1_j_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for i_2_j_2_fused in T.thread_binding(T.int64(40), thread="threadIdx.x"):
                    for i_3_init, j_3_init, i_4_init, j_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_init"):
                            v_i = T.axis.spatial(T.int64(1), i_3_init + i_4_init)
                            v_j = T.axis.spatial(T.int64(1000), i_0_j_0_fused * T.int64(40) + i_2_j_2_fused + j_3_init + j_4_init)
                            T.reads()
                            T.writes(T_matmul_NT_local[v_i, v_j])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            T_matmul_NT_local[v_i, v_j] = T.float32(0)
                    for k_0 in range(T.int64(2)):
                        for ax0_ax1_fused_0 in range(T.int64(4)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(40), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("p0_shared"):
                                        v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v1 = T.axis.spatial(T.int64(512), k_0 * T.int64(256) + (ax0_ax1_fused_0 * T.int64(80) + ax0_ax1_fused_1 * T.int64(2) + ax0_ax1_fused_2))
                                        T.where((ax0_ax1_fused_0 * T.int64(40) + ax0_ax1_fused_1) * T.int64(2) + ax0_ax1_fused_2 < T.int64(256))
                                        T.reads(p0[v0, v1])
                                        T.writes(p0_shared[v0, v1])
                                        p0_shared[v0, v1] = p0[v0, v1]
                        for ax0_ax1_fused_0 in range(T.int64(64)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(40), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("p1_shared"):
                                        v0 = T.axis.spatial(T.int64(1000), i_0_j_0_fused * T.int64(40) + (ax0_ax1_fused_0 * T.int64(160) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2) // T.int64(256))
                                        v1 = T.axis.spatial(T.int64(512), k_0 * T.int64(256) + (ax0_ax1_fused_0 * T.int64(160) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2) % T.int64(256))
                                        T.reads(p1[v0, v1])
                                        T.writes(p1_shared[v0, v1])
                                        p1_shared[v0, v1] = p1[v0, v1]
                        for k_1, i_3, j_3, k_2, i_4, j_4 in T.grid(T.int64(16), T.int64(1), T.int64(1), T.int64(16), T.int64(1), T.int64(1)):
                            with T.block("T_matmul_NT_update"):
                                v_i = T.axis.spatial(T.int64(1), i_3 + i_4)
                                v_j = T.axis.spatial(T.int64(1000), i_0_j_0_fused * T.int64(40) + i_2_j_2_fused + j_3 + j_4)
                                v_k = T.axis.reduce(T.int64(512), k_0 * T.int64(256) + k_1 * T.int64(16) + k_2)
                                T.reads(T_matmul_NT_local[v_i, v_j], p0_shared[v_i, v_k], p1_shared[v_j, v_k])
                                T.writes(T_matmul_NT_local[v_i, v_j])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                T_matmul_NT_local[v_i, v_j] = T_matmul_NT_local[v_i, v_j] + p0_shared[v_i, v_k] * p1_shared[v_j, v_k]
                    for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(1000), i_0_j_0_fused * T.int64(40) + i_2_j_2_fused + ax1)
                            T.reads(T_matmul_NT_local[v0, v1], p2[v0, v1])
                            T.writes(T_add[v0, v1])
                            T_add[v0, v1] = T_matmul_NT_local[v0, v1] + p2[v0, v1]
b0 = sch.get_block(name="T_matmul_NT", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l3, l4, l5 = sch.get_loops(block=b0)
v6, v7, v8, v9, v10 = sch.sample_perfect_tile(loop=l3, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l11, l12, l13, l14, l15 = sch.split(loop=l3, factors=[v6, v7, v8, v9, v10], preserve_unit_iters=True)
v16, v17, v18, v19, v20 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[25, 1, 40, 1, 1])
l21, l22, l23, l24, l25 = sch.split(loop=l4, factors=[v16, v17, v18, v19, v20], preserve_unit_iters=True)
v26, v27, v28 = sch.sample_perfect_tile(loop=l5, n=3, max_innermost_factor=64, decision=[2, 16, 16])
l29, l30, l31 = sch.split(loop=l5, factors=[v26, v27, v28], preserve_unit_iters=True)
sch.reorder(l11, l21, l12, l22, l13, l23, l29, l30, l14, l24, l31, l15, l25)
l32 = sch.fuse(l11, l21, preserve_unit_iters=True)
sch.bind(loop=l32, thread_axis="blockIdx.x")
l33 = sch.fuse(l12, l22, preserve_unit_iters=True)
sch.bind(loop=l33, thread_axis="vthread.x")
l34 = sch.fuse(l13, l23, preserve_unit_iters=True)
sch.bind(loop=l34, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b35 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b35, loop=l34, preserve_unit_loops=True, index=-1)
b36 = sch.cache_read(block=b0, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b36, loop=l29, preserve_unit_loops=True, index=-1)
l37, l38, l39, l40, l41, l42 = sch.get_loops(block=b36)
l43 = sch.fuse(l41, l42, preserve_unit_iters=True)
v44 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b36, ann_key="meta_schedule.cooperative_fetch", ann_val=v44)
b45 = sch.cache_read(block=b0, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b45, loop=l29, preserve_unit_loops=True, index=-1)
l46, l47, l48, l49, l50, l51 = sch.get_loops(block=b45)
l52 = sch.fuse(l50, l51, preserve_unit_iters=True)
v53 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b45, ann_key="meta_schedule.cooperative_fetch", ann_val=v53)
sch.reverse_compute_inline(block=b1)
v54 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v54)
sch.enter_postproc()
sch.unannotate(block_or_loop=b36, ann_key="meta_schedule.cooperative_fetch")
l55, l56, l57, l58, l59 = sch.get_loops(block=b36)
l60, l61, l62 = sch.split(loop=l59, factors=[None, 40, 2], preserve_unit_iters=True)
sch.vectorize(loop=l62)
sch.bind(loop=l61, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b45, ann_key="meta_schedule.cooperative_fetch")
l63, l64, l65, l66, l67 = sch.get_loops(block=b45)
l68, l69, l70 = sch.split(loop=l67, factors=[None, 40, 4], preserve_unit_iters=True)
sch.vectorize(loop=l70)
sch.bind(loop=l69, thread_axis="threadIdx.x")
b71 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b71, ann_key="meta_schedule.unroll_explicit")
b72, b73, b74, b75 = sch.get_child_blocks(b71)
l76, l77, l78, l79, l80, l81, l82 = sch.get_loops(block=b72)
l83, l84, l85, l86, l87, l88, l89 = sch.get_loops(block=b73)
l90, l91, l92, l93, l94, l95, l96, l97, l98, l99 = sch.get_loops(block=b74)
sch.annotate(block_or_loop=l90, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l90, ann_key="pragma_unroll_explicit", ann_val=1)
l100, l101, l102, l103, l104 = sch.get_loops(block=b75)
b105 = sch.get_block(name="T_matmul_NT", func_name="main")
l106, l107, l108, l109, l110, l111, l112, l113, l114, l115 = sch.get_loops(block=b105)
b116 = sch.decompose_reduction(block=b105, loop=l109)
2023-11-11 04:27:25 [INFO] [task_scheduler.cc:121] [Task #28: fused_nn_dense_add] Trial #144: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/home2/xiachunwei/Software/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/home2/xiachunwei/Software/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/home2/xiachunwei/Software/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/home2/xiachunwei/Software/tvm/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/home2/xiachunwei/Software/tvm/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/home2/xiachunwei/Software/tvm/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/home2/xiachunwei/Software/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/home2/xiachunwei/Software/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  4: _ZN3tvm7runtime13PackedFun
  3: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  2: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  0: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  File "/home2/xiachunwei/Software/tvm/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(512)), "float32"), p1: T.Buffer((T.int64(1000), T.int64(512)), "float32"), p2: T.Buffer((T.int64(1), T.int64(1000)), "float32"), T_add: T.Buffer((T.int64(1), T.int64(1000)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        T_matmul_NT_local = T.alloc_buffer((T.int64(1), T.int64(1000)), scope="local")
        p0_shared = T.alloc_buffer((T.int64(1), T.int64(512)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(1000), T.int64(512)), scope="shared")
        for i_0_j_0_fused in T.thread_binding(T.int64(25), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 64, "pragma_unroll_explicit": 1}):
            for i_1_j_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for i_2_j_2_fused in T.thread_binding(T.int64(40), thread="threadIdx.x"):
                    for i_3_init, j_3_init, i_4_init, j_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_init"):
                            v_i = T.axis.spatial(T.int64(1), i_3_init + i_4_init)
                            v_j = T.axis.spatial(T.int64(1000), i_0_j_0_fused * T.int64(40) + i_2_j_2_fused + j_3_init + j_4_init)
                            T.reads()
                            T.writes(T_matmul_NT_local[v_i, v_j])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            T_matmul_NT_local[v_i, v_j] = T.float32(0)
                    for k_0_fused in T.serial(T.int64(512), annotations={"software_pipeline_async_stages": [0], "software_pipeline_order": [0, 1, 2], "software_pipeline_stage": [0, 0, 2]}):
                        for ax0_ax1_fused_0 in range(T.int64(1)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(40), thread="threadIdx.x"):
                                with T.block("p0_shared"):
                                    v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                    v1 = T.axis.spatial(T.int64(512), k_0_fused)
                                    T.where(ax0_ax1_fused_0 * T.int64(40) + ax0_ax1_fused_1 < T.int64(1))
                                    T.reads(p0[v0, v1])
                                    T.writes(p0_shared[v0, v1])
                                    p0_shared[v0, v1] = p0[v0, v1]
                        for ax0_ax1_fused_0 in range(T.int64(1)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(40), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("p1_shared"):
                                        v0 = T.axis.spatial(T.int64(1000), i_0_j_0_fused * T.int64(40) + (ax0_ax1_fused_0 * T.int64(160) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2))
                                        v1 = T.axis.spatial(T.int64(512), k_0_fused)
                                        T.where((ax0_ax1_fused_0 * T.int64(40) + ax0_ax1_fused_1) * T.int64(4) + ax0_ax1_fused_2 < T.int64(40))
                                        T.reads(p1[v0, v1])
                                        T.writes(p1_shared[v0, v1])
                                        p1_shared[v0, v1] = p1[v0, v1]
                        for k_1, i_3, j_3, k_2, i_4, j_4 in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                            with T.block("T_matmul_NT_update"):
                                v_i = T.axis.spatial(T.int64(1), i_3 + i_4)
                                v_j = T.axis.spatial(T.int64(1000), i_0_j_0_fused * T.int64(40) + i_2_j_2_fused + j_3 + j_4)
                                v_k = T.axis.reduce(T.int64(512), k_0_fused + k_1 + k_2)
                                T.reads(T_matmul_NT_local[v_i, v_j], p0_shared[v_i, v_k], p1_shared[v_j, v_k])
                                T.writes(T_matmul_NT_local[v_i, v_j])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                T_matmul_NT_local[v_i, v_j] = T_matmul_NT_local[v_i, v_j] + p0_shared[v_i, v_k] * p1_shared[v_j, v_k]
                    for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(1000), i_0_j_0_fused * T.int64(40) + i_2_j_2_fused + ax1)
                            T.reads(T_matmul_NT_local[v0, v1], p2[v0, v1])
                            T.writes(T_add[v0, v1])
                            T_add[v0, v1] = T_matmul_NT_local[v0, v1] + p2[v0, v1]
b0 = sch.get_block(name="T_matmul_NT", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l3, l4, l5 = sch.get_loops(block=b0)
v6, v7, v8, v9, v10 = sch.sample_perfect_tile(loop=l3, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l11, l12, l13, l14, l15 = sch.split(loop=l3, factors=[v6, v7, v8, v9, v10], preserve_unit_iters=True)
v16, v17, v18, v19, v20 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[25, 1, 40, 1, 1])
l21, l22, l23, l24, l25 = sch.split(loop=l4, factors=[v16, v17, v18, v19, v20], preserve_unit_iters=True)
v26, v27, v28 = sch.sample_perfect_tile(loop=l5, n=3, max_innermost_factor=64, decision=[512, 1, 1])
l29, l30, l31 = sch.split(loop=l5, factors=[v26, v27, v28], preserve_unit_iters=True)
sch.reorder(l11, l21, l12, l22, l13, l23, l29, l30, l14, l24, l31, l15, l25)
l32 = sch.fuse(l11, l21, preserve_unit_iters=True)
sch.bind(loop=l32, thread_axis="blockIdx.x")
l33 = sch.fuse(l12, l22, preserve_unit_iters=True)
sch.bind(loop=l33, thread_axis="vthread.x")
l34 = sch.fuse(l13, l23, preserve_unit_iters=True)
sch.bind(loop=l34, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b35 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b35, loop=l34, preserve_unit_loops=True, index=-1)
b36 = sch.cache_read(block=b0, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b36, loop=l29, preserve_unit_loops=True, index=-1)
l37, l38, l39, l40, l41, l42 = sch.get_loops(block=b36)
l43 = sch.fuse(l41, l42, preserve_unit_iters=True)
v44 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b36, ann_key="meta_schedule.cooperative_fetch", ann_val=v44)
b45 = sch.cache_read(block=b0, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b45, loop=l29, preserve_unit_loops=True, index=-1)
l46, l47, l48, l49, l50, l51 = sch.get_loops(block=b45)
l52 = sch.fuse(l50, l51, preserve_unit_iters=True)
v53 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b45, ann_key="meta_schedule.cooperative_fetch", ann_val=v53)
l54 = sch.fuse(l29, preserve_unit_iters=True)
sch.annotate(block_or_loop=l54, ann_key="software_pipeline_stage", ann_val=[0, 0, 2])
sch.annotate(block_or_loop=l54, ann_key="software_pipeline_order", ann_val=[0, 1, 2])
sch.annotate(block_or_loop=l54, ann_key="software_pipeline_async_stages", ann_val=[0])
sch.reverse_compute_inline(block=b1)
v55 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=2)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v55)
sch.enter_postproc()
sch.unannotate(block_or_loop=b36, ann_key="meta_schedule.cooperative_fetch")
l56, l57, l58, l59, l60 = sch.get_loops(block=b36)
l61, l62 = sch.split(loop=l60, factors=[None, 40], preserve_unit_iters=True)
sch.bind(loop=l62, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b45, ann_key="meta_schedule.cooperative_fetch")
l63, l64, l65, l66, l67 = sch.get_loops(block=b45)
l68, l69, l70 = sch.split(loop=l67, factors=[None, 40, 4], preserve_unit_iters=True)
sch.vectorize(loop=l70)
sch.bind(loop=l69, thread_axis="threadIdx.x")
b71 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b71, ann_key="meta_schedule.unroll_explicit")
b72, b73, b74, b75 = sch.get_child_blocks(b71)
l76, l77, l78, l79, l80, l81 = sch.get_loops(block=b72)
l82, l83, l84, l85, l86, l87, l88 = sch.get_loops(block=b73)
l89, l90, l91, l92, l93, l94, l95, l96, l97, l98 = sch.get_loops(block=b74)
sch.annotate(block_or_loop=l89, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l89, ann_key="pragma_unroll_explicit", ann_val=1)
l99, l100, l101, l102, l103 = sch.get_loops(block=b75)
b104 = sch.get_block(name="T_matmul_NT", func_name="main")
l105, l106, l107, l108, l109, l110, l111, l112, l113, l114 = sch.get_loops(block=b104)
b115 = sch.decompose_reduction(block=b104, loop=l108)
2023-11-11 04:27:25 [INFO] [task_scheduler.cc:121] [Task #28: fused_nn_dense_add] Trial #145: Error in running:
LocalRunner: An exception occurred
Subprocess terminated
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(512)), "float32"), p1: T.Buffer((T.int64(1000), T.int64(512)), "float32"), p2: T.Buffer((T.int64(1), T.int64(1000)), "float32"), T_add: T.Buffer((T.int64(1), T.int64(1000)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        T_matmul_NT_local = T.alloc_buffer((T.int64(1), T.int64(1000)), scope="local")
        p0_shared = T.alloc_buffer((T.int64(1), T.int64(512)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(1000), T.int64(512)), scope="shared")
        for i_0_j_0_fused in T.thread_binding(T.int64(25), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 64, "pragma_unroll_explicit": 1}):
            for i_1_j_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for i_2_j_2_fused in T.thread_binding(T.int64(40), thread="threadIdx.x"):
                    for i_3_init, j_3_init, i_4_init, j_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_init"):
                            v_i = T.axis.spatial(T.int64(1), i_3_init + i_4_init)
                            v_j = T.axis.spatial(T.int64(1000), i_0_j_0_fused * T.int64(40) + i_2_j_2_fused + j_3_init + j_4_init)
                            T.reads()
                            T.writes(T_matmul_NT_local[v_i, v_j])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            T_matmul_NT_local[v_i, v_j] = T.float32(0)
                    for k_0_fused in T.serial(T.int64(512), annotations={"software_pipeline_async_stages": [0], "software_pipeline_order": [0, 1, 2], "software_pipeline_stage": [0, 0, 3]}):
                        for ax0_ax1_fused_0 in range(T.int64(1)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(40), thread="threadIdx.x"):
                                with T.block("p0_shared"):
                                    v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                    v1 = T.axis.spatial(T.int64(512), k_0_fused)
                                    T.where(ax0_ax1_fused_0 * T.int64(40) + ax0_ax1_fused_1 < T.int64(1))
                                    T.reads(p0[v0, v1])
                                    T.writes(p0_shared[v0, v1])
                                    p0_shared[v0, v1] = p0[v0, v1]
                        for ax0_ax1_fused_0 in range(T.int64(1)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(40), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("p1_shared"):
                                        v0 = T.axis.spatial(T.int64(1000), i_0_j_0_fused * T.int64(40) + (ax0_ax1_fused_0 * T.int64(160) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2))
                                        v1 = T.axis.spatial(T.int64(512), k_0_fused)
                                        T.where((ax0_ax1_fused_0 * T.int64(40) + ax0_ax1_fused_1) * T.int64(4) + ax0_ax1_fused_2 < T.int64(40))
                                        T.reads(p1[v0, v1])
                                        T.writes(p1_shared[v0, v1])
                                        p1_shared[v0, v1] = p1[v0, v1]
                        for k_1, i_3, j_3, k_2, i_4, j_4 in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                            with T.block("T_matmul_NT_update"):
                                v_i = T.axis.spatial(T.int64(1), i_3 + i_4)
                                v_j = T.axis.spatial(T.int64(1000), i_0_j_0_fused * T.int64(40) + i_2_j_2_fused + j_3 + j_4)
                                v_k = T.axis.reduce(T.int64(512), k_0_fused + k_1 + k_2)
                                T.reads(T_matmul_NT_local[v_i, v_j], p0_shared[v_i, v_k], p1_shared[v_j, v_k])
                                T.writes(T_matmul_NT_local[v_i, v_j])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                T_matmul_NT_local[v_i, v_j] = T_matmul_NT_local[v_i, v_j] + p0_shared[v_i, v_k] * p1_shared[v_j, v_k]
                    for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(1000), i_0_j_0_fused * T.int64(40) + i_2_j_2_fused + ax1)
                            T.reads(T_matmul_NT_local[v0, v1], p2[v0, v1])
                            T.writes(T_add[v0, v1])
                            T_add[v0, v1] = T_matmul_NT_local[v0, v1] + p2[v0, v1]
b0 = sch.get_block(name="T_matmul_NT", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l3, l4, l5 = sch.get_loops(block=b0)
v6, v7, v8, v9, v10 = sch.sample_perfect_tile(loop=l3, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l11, l12, l13, l14, l15 = sch.split(loop=l3, factors=[v6, v7, v8, v9, v10], preserve_unit_iters=True)
v16, v17, v18, v19, v20 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[25, 1, 40, 1, 1])
l21, l22, l23, l24, l25 = sch.split(loop=l4, factors=[v16, v17, v18, v19, v20], preserve_unit_iters=True)
v26, v27, v28 = sch.sample_perfect_tile(loop=l5, n=3, max_innermost_factor=64, decision=[512, 1, 1])
l29, l30, l31 = sch.split(loop=l5, factors=[v26, v27, v28], preserve_unit_iters=True)
sch.reorder(l11, l21, l12, l22, l13, l23, l29, l30, l14, l24, l31, l15, l25)
l32 = sch.fuse(l11, l21, preserve_unit_iters=True)
sch.bind(loop=l32, thread_axis="blockIdx.x")
l33 = sch.fuse(l12, l22, preserve_unit_iters=True)
sch.bind(loop=l33, thread_axis="vthread.x")
l34 = sch.fuse(l13, l23, preserve_unit_iters=True)
sch.bind(loop=l34, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b35 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b35, loop=l34, preserve_unit_loops=True, index=-1)
b36 = sch.cache_read(block=b0, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b36, loop=l29, preserve_unit_loops=True, index=-1)
l37, l38, l39, l40, l41, l42 = sch.get_loops(block=b36)
l43 = sch.fuse(l41, l42, preserve_unit_iters=True)
v44 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b36, ann_key="meta_schedule.cooperative_fetch", ann_val=v44)
b45 = sch.cache_read(block=b0, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b45, loop=l29, preserve_unit_loops=True, index=-1)
l46, l47, l48, l49, l50, l51 = sch.get_loops(block=b45)
l52 = sch.fuse(l50, l51, preserve_unit_iters=True)
v53 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b45, ann_key="meta_schedule.cooperative_fetch", ann_val=v53)
l54 = sch.fuse(l29, preserve_unit_iters=True)
sch.annotate(block_or_loop=l54, ann_key="software_pipeline_stage", ann_val=[0, 0, 3])
sch.annotate(block_or_loop=l54, ann_key="software_pipeline_order", ann_val=[0, 1, 2])
sch.annotate(block_or_loop=l54, ann_key="software_pipeline_async_stages", ann_val=[0])
sch.reverse_compute_inline(block=b1)
v55 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=2)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v55)
sch.enter_postproc()
sch.unannotate(block_or_loop=b36, ann_key="meta_schedule.cooperative_fetch")
l56, l57, l58, l59, l60 = sch.get_loops(block=b36)
l61, l62 = sch.split(loop=l60, factors=[None, 40], preserve_unit_iters=True)
sch.bind(loop=l62, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b45, ann_key="meta_schedule.cooperative_fetch")
l63, l64, l65, l66, l67 = sch.get_loops(block=b45)
l68, l69, l70 = sch.split(loop=l67, factors=[None, 40, 4], preserve_unit_iters=True)
sch.vectorize(loop=l70)
sch.bind(loop=l69, thread_axis="threadIdx.x")
b71 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b71, ann_key="meta_schedule.unroll_explicit")
b72, b73, b74, b75 = sch.get_child_blocks(b71)
l76, l77, l78, l79, l80, l81 = sch.get_loops(block=b72)
l82, l83, l84, l85, l86, l87, l88 = sch.get_loops(block=b73)
l89, l90, l91, l92, l93, l94, l95, l96, l97, l98 = sch.get_loops(block=b74)
sch.annotate(block_or_loop=l89, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l89, ann_key="pragma_unroll_explicit", ann_val=1)
l99, l100, l101, l102, l103 = sch.get_loops(block=b75)
b104 = sch.get_block(name="T_matmul_NT", func_name="main")
l105, l106, l107, l108, l109, l110, l111, l112, l113, l114 = sch.get_loops(block=b104)
b115 = sch.decompose_reduction(block=b104, loop=l108)
2023-11-11 04:27:25 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #146: GFLOPs: 94.9240. Time: 10.7981 us. Best GFLOPs: 96.0504
2023-11-11 04:27:25 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #147: GFLOPs: 94.9459. Time: 10.7956 us. Best GFLOPs: 96.0504
2023-11-11 04:27:25 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #148: GFLOPs: 94.9390. Time: 10.7964 us. Best GFLOPs: 96.0504
2023-11-11 04:27:25 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #149: GFLOPs: 11.2564. Time: 91.0593 us. Best GFLOPs: 96.0504
2023-11-11 04:27:25 [INFO] [task_scheduler.cc:121] [Task #28: fused_nn_dense_add] Trial #150: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/home2/xiachunwei/Software/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/home2/xiachunwei/Software/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 159, in _worker_func
    costs: List[float] = f_run_evaluator(
  File "/home2/xiachunwei/Software/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 387, in default_run_evaluator
    return run_evaluator_common(rt_mod, device, evaluator_config, repeated_args)
  File "/home2/xiachunwei/Software/tvm/python/tvm/meta_schedule/runner/utils.py", line 117, in run_evaluator_common
    profile_result = evaluator(*args)
  File "/home2/xiachunwei/Software/tvm/python/tvm/runtime/module.py", line 397, in evaluator
    blob = feval(*args)
  File "/home2/xiachunwei/Software/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/home2/xiachunwei/Software/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  1: tvm::runtime::profiling::WrapTimeEvaluator(tvm::runtime::PackedFunc, DLDevice, int, int, int, int, int, int, int, tvm::runtime::PackedFunc)::{lambda(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*) [clone .isra.779]
  0: tvm::runtime::CUDADeviceAPI::StreamSync(DLDevice, void*)
  File "/home2/xiachunwei/Software/tvm/src/runtime/cuda/cuda_device_api.cc", line 212
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(512)), "float32"), p1: T.Buffer((T.int64(1000), T.int64(512)), "float32"), p2: T.Buffer((T.int64(1), T.int64(1000)), "float32"), T_add: T.Buffer((T.int64(1), T.int64(1000)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        T_matmul_NT_local = T.alloc_buffer((T.int64(1), T.int64(1000)), scope="local")
        p0_shared = T.alloc_buffer((T.int64(1), T.int64(512)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(1000), T.int64(512)), scope="shared")
        for i_0_j_0_fused in T.thread_binding(T.int64(25), thread="blockIdx.x"):
            for i_1_j_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for i_2_j_2_fused in T.thread_binding(T.int64(40), thread="threadIdx.x"):
                    for i_3_init, j_3_init, i_4_init, j_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_init"):
                            v_i = T.axis.spatial(T.int64(1), i_3_init + i_4_init)
                            v_j = T.axis.spatial(T.int64(1000), i_0_j_0_fused * T.int64(40) + i_2_j_2_fused + j_3_init + j_4_init)
                            T.reads()
                            T.writes(T_matmul_NT_local[v_i, v_j])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            T_matmul_NT_local[v_i, v_j] = T.float32(0)
                    for k_0_fused in T.serial(T.int64(512), annotations={"software_pipeline_async_stages": [0], "software_pipeline_order": [0, 1, 2], "software_pipeline_stage": [0, 0, 2]}):
                        for ax0_ax1_fused_0 in range(T.int64(1)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(40), thread="threadIdx.x"):
                                with T.block("p0_shared"):
                                    v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                    v1 = T.axis.spatial(T.int64(512), k_0_fused)
                                    T.where(ax0_ax1_fused_0 * T.int64(40) + ax0_ax1_fused_1 < T.int64(1))
                                    T.reads(p0[v0, v1])
                                    T.writes(p0_shared[v0, v1])
                                    p0_shared[v0, v1] = p0[v0, v1]
                        for ax0_ax1_fused_0 in range(T.int64(1)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(40), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("p1_shared"):
                                        v0 = T.axis.spatial(T.int64(1000), i_0_j_0_fused * T.int64(40) + (ax0_ax1_fused_0 * T.int64(160) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2))
                                        v1 = T.axis.spatial(T.int64(512), k_0_fused)
                                        T.where((ax0_ax1_fused_0 * T.int64(40) + ax0_ax1_fused_1) * T.int64(4) + ax0_ax1_fused_2 < T.int64(40))
                                        T.reads(p1[v0, v1])
                                        T.writes(p1_shared[v0, v1])
                                        p1_shared[v0, v1] = p1[v0, v1]
                        for k_1, i_3, j_3, k_2, i_4, j_4 in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                            with T.block("T_matmul_NT_update"):
                                v_i = T.axis.spatial(T.int64(1), i_3 + i_4)
                                v_j = T.axis.spatial(T.int64(1000), i_0_j_0_fused * T.int64(40) + i_2_j_2_fused + j_3 + j_4)
                                v_k = T.axis.reduce(T.int64(512), k_0_fused + k_1 + k_2)
                                T.reads(T_matmul_NT_local[v_i, v_j], p0_shared[v_i, v_k], p1_shared[v_j, v_k])
                                T.writes(T_matmul_NT_local[v_i, v_j])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                T_matmul_NT_local[v_i, v_j] = T_matmul_NT_local[v_i, v_j] + p0_shared[v_i, v_k] * p1_shared[v_j, v_k]
                    for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(1000), i_0_j_0_fused * T.int64(40) + i_2_j_2_fused + ax1)
                            T.reads(T_matmul_NT_local[v0, v1], p2[v0, v1])
                            T.writes(T_add[v0, v1])
                            T_add[v0, v1] = T_matmul_NT_local[v0, v1] + p2[v0, v1]
b0 = sch.get_block(name="T_matmul_NT", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l3, l4, l5 = sch.get_loops(block=b0)
v6, v7, v8, v9, v10 = sch.sample_perfect_tile(loop=l3, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l11, l12, l13, l14, l15 = sch.split(loop=l3, factors=[v6, v7, v8, v9, v10], preserve_unit_iters=True)
v16, v17, v18, v19, v20 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[25, 1, 40, 1, 1])
l21, l22, l23, l24, l25 = sch.split(loop=l4, factors=[v16, v17, v18, v19, v20], preserve_unit_iters=True)
v26, v27, v28 = sch.sample_perfect_tile(loop=l5, n=3, max_innermost_factor=64, decision=[512, 1, 1])
l29, l30, l31 = sch.split(loop=l5, factors=[v26, v27, v28], preserve_unit_iters=True)
sch.reorder(l11, l21, l12, l22, l13, l23, l29, l30, l14, l24, l31, l15, l25)
l32 = sch.fuse(l11, l21, preserve_unit_iters=True)
sch.bind(loop=l32, thread_axis="blockIdx.x")
l33 = sch.fuse(l12, l22, preserve_unit_iters=True)
sch.bind(loop=l33, thread_axis="vthread.x")
l34 = sch.fuse(l13, l23, preserve_unit_iters=True)
sch.bind(loop=l34, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b35 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b35, loop=l34, preserve_unit_loops=True, index=-1)
b36 = sch.cache_read(block=b0, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b36, loop=l29, preserve_unit_loops=True, index=-1)
l37, l38, l39, l40, l41, l42 = sch.get_loops(block=b36)
l43 = sch.fuse(l41, l42, preserve_unit_iters=True)
v44 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b36, ann_key="meta_schedule.cooperative_fetch", ann_val=v44)
b45 = sch.cache_read(block=b0, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b45, loop=l29, preserve_unit_loops=True, index=-1)
l46, l47, l48, l49, l50, l51 = sch.get_loops(block=b45)
l52 = sch.fuse(l50, l51, preserve_unit_iters=True)
v53 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b45, ann_key="meta_schedule.cooperative_fetch", ann_val=v53)
l54 = sch.fuse(l29, preserve_unit_iters=True)
sch.annotate(block_or_loop=l54, ann_key="software_pipeline_stage", ann_val=[0, 0, 2])
sch.annotate(block_or_loop=l54, ann_key="software_pipeline_order", ann_val=[0, 1, 2])
sch.annotate(block_or_loop=l54, ann_key="software_pipeline_async_stages", ann_val=[0])
sch.reverse_compute_inline(block=b1)
v55 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=0)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v55)
sch.enter_postproc()
sch.unannotate(block_or_loop=b36, ann_key="meta_schedule.cooperative_fetch")
l56, l57, l58, l59, l60 = sch.get_loops(block=b36)
l61, l62 = sch.split(loop=l60, factors=[None, 40], preserve_unit_iters=True)
sch.bind(loop=l62, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b45, ann_key="meta_schedule.cooperative_fetch")
l63, l64, l65, l66, l67 = sch.get_loops(block=b45)
l68, l69, l70 = sch.split(loop=l67, factors=[None, 40, 4], preserve_unit_iters=True)
sch.vectorize(loop=l70)
sch.bind(loop=l69, thread_axis="threadIdx.x")
b71 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b71, ann_key="meta_schedule.unroll_explicit")
b72, b73, b74, b75 = sch.get_child_blocks(b71)
l76, l77, l78, l79, l80, l81 = sch.get_loops(block=b72)
l82, l83, l84, l85, l86, l87, l88 = sch.get_loops(block=b73)
l89, l90, l91, l92, l93, l94, l95, l96, l97, l98 = sch.get_loops(block=b74)
l99, l100, l101, l102, l103 = sch.get_loops(block=b75)
b104 = sch.get_block(name="T_matmul_NT", func_name="main")
l105, l106, l107, l108, l109, l110, l111, l112, l113, l114 = sch.get_loops(block=b104)
b115 = sch.decompose_reduction(block=b104, loop=l108)
2023-11-11 04:27:25 [INFO] [task_scheduler.cc:121] [Task #28: fused_nn_dense_add] Trial #151: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/home2/xiachunwei/Software/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/home2/xiachunwei/Software/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/home2/xiachunwei/Software/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/home2/xiachunwei/Software/tvm/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/home2/xiachunwei/Software/tvm/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/home2/xiachunwei/Software/tvm/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/home2/xiachunwei/Software/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/home2/xiachunwei/Software/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  4: _ZN3tvm7runtime13PackedFun
  3: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  2: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  0: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  File "/home2/xiachunwei/Software/tvm/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(512)), "float32"), p1: T.Buffer((T.int64(1000), T.int64(512)), "float32"), p2: T.Buffer((T.int64(1), T.int64(1000)), "float32"), T_add: T.Buffer((T.int64(1), T.int64(1000)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        T_matmul_NT_local = T.alloc_buffer((T.int64(1), T.int64(1000)), scope="local")
        p0_shared = T.alloc_buffer((T.int64(1), T.int64(512)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(1000), T.int64(512)), scope="shared")
        for i_0_j_0_fused in T.thread_binding(T.int64(25), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 1024, "pragma_unroll_explicit": 1}):
            for i_1_j_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for i_2_j_2_fused in T.thread_binding(T.int64(40), thread="threadIdx.x"):
                    for i_3_init, j_3_init, i_4_init, j_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_init"):
                            v_i = T.axis.spatial(T.int64(1), i_3_init + i_4_init)
                            v_j = T.axis.spatial(T.int64(1000), i_0_j_0_fused * T.int64(40) + i_2_j_2_fused + j_3_init + j_4_init)
                            T.reads()
                            T.writes(T_matmul_NT_local[v_i, v_j])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            T_matmul_NT_local[v_i, v_j] = T.float32(0)
                    for k_0 in range(T.int64(2)):
                        for ax0_ax1_fused_0 in range(T.int64(7)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(40), thread="threadIdx.x"):
                                with T.block("p0_shared"):
                                    v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                    v1 = T.axis.spatial(T.int64(512), k_0 * T.int64(256) + (ax0_ax1_fused_0 * T.int64(40) + ax0_ax1_fused_1))
                                    T.where(ax0_ax1_fused_0 * T.int64(40) + ax0_ax1_fused_1 < T.int64(256))
                                    T.reads(p0[v0, v1])
                                    T.writes(p0_shared[v0, v1])
                                    p0_shared[v0, v1] = p0[v0, v1]
                        for ax0_ax1_fused_0 in range(T.int64(64)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(40), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("p1_shared"):
                                        v0 = T.axis.spatial(T.int64(1000), i_0_j_0_fused * T.int64(40) + (ax0_ax1_fused_0 * T.int64(160) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2) // T.int64(256))
                                        v1 = T.axis.spatial(T.int64(512), k_0 * T.int64(256) + (ax0_ax1_fused_0 * T.int64(160) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2) % T.int64(256))
                                        T.reads(p1[v0, v1])
                                        T.writes(p1_shared[v0, v1])
                                        p1_shared[v0, v1] = p1[v0, v1]
                        for k_1, i_3, j_3, k_2, i_4, j_4 in T.grid(T.int64(16), T.int64(1), T.int64(1), T.int64(16), T.int64(1), T.int64(1)):
                            with T.block("T_matmul_NT_update"):
                                v_i = T.axis.spatial(T.int64(1), i_3 + i_4)
                                v_j = T.axis.spatial(T.int64(1000), i_0_j_0_fused * T.int64(40) + i_2_j_2_fused + j_3 + j_4)
                                v_k = T.axis.reduce(T.int64(512), k_0 * T.int64(256) + k_1 * T.int64(16) + k_2)
                                T.reads(T_matmul_NT_local[v_i, v_j], p0_shared[v_i, v_k], p1_shared[v_j, v_k])
                                T.writes(T_matmul_NT_local[v_i, v_j])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                T_matmul_NT_local[v_i, v_j] = T_matmul_NT_local[v_i, v_j] + p0_shared[v_i, v_k] * p1_shared[v_j, v_k]
                    for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(1000), i_0_j_0_fused * T.int64(40) + i_2_j_2_fused + ax1)
                            T.reads(T_matmul_NT_local[v0, v1], p2[v0, v1])
                            T.writes(T_add[v0, v1])
                            T_add[v0, v1] = T_matmul_NT_local[v0, v1] + p2[v0, v1]
b0 = sch.get_block(name="T_matmul_NT", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l3, l4, l5 = sch.get_loops(block=b0)
v6, v7, v8, v9, v10 = sch.sample_perfect_tile(loop=l3, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l11, l12, l13, l14, l15 = sch.split(loop=l3, factors=[v6, v7, v8, v9, v10], preserve_unit_iters=True)
v16, v17, v18, v19, v20 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[25, 1, 40, 1, 1])
l21, l22, l23, l24, l25 = sch.split(loop=l4, factors=[v16, v17, v18, v19, v20], preserve_unit_iters=True)
v26, v27, v28 = sch.sample_perfect_tile(loop=l5, n=3, max_innermost_factor=64, decision=[2, 16, 16])
l29, l30, l31 = sch.split(loop=l5, factors=[v26, v27, v28], preserve_unit_iters=True)
sch.reorder(l11, l21, l12, l22, l13, l23, l29, l30, l14, l24, l31, l15, l25)
l32 = sch.fuse(l11, l21, preserve_unit_iters=True)
sch.bind(loop=l32, thread_axis="blockIdx.x")
l33 = sch.fuse(l12, l22, preserve_unit_iters=True)
sch.bind(loop=l33, thread_axis="vthread.x")
l34 = sch.fuse(l13, l23, preserve_unit_iters=True)
sch.bind(loop=l34, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b35 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b35, loop=l34, preserve_unit_loops=True, index=-1)
b36 = sch.cache_read(block=b0, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b36, loop=l29, preserve_unit_loops=True, index=-1)
l37, l38, l39, l40, l41, l42 = sch.get_loops(block=b36)
l43 = sch.fuse(l41, l42, preserve_unit_iters=True)
v44 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b36, ann_key="meta_schedule.cooperative_fetch", ann_val=v44)
b45 = sch.cache_read(block=b0, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b45, loop=l29, preserve_unit_loops=True, index=-1)
l46, l47, l48, l49, l50, l51 = sch.get_loops(block=b45)
l52 = sch.fuse(l50, l51, preserve_unit_iters=True)
v53 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b45, ann_key="meta_schedule.cooperative_fetch", ann_val=v53)
sch.reverse_compute_inline(block=b1)
v54 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v54)
sch.enter_postproc()
sch.unannotate(block_or_loop=b36, ann_key="meta_schedule.cooperative_fetch")
l55, l56, l57, l58, l59 = sch.get_loops(block=b36)
l60, l61 = sch.split(loop=l59, factors=[None, 40], preserve_unit_iters=True)
sch.bind(loop=l61, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b45, ann_key="meta_schedule.cooperative_fetch")
l62, l63, l64, l65, l66 = sch.get_loops(block=b45)
l67, l68, l69 = sch.split(loop=l66, factors=[None, 40, 4], preserve_unit_iters=True)
sch.vectorize(loop=l69)
sch.bind(loop=l68, thread_axis="threadIdx.x")
b70 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b70, ann_key="meta_schedule.unroll_explicit")
b71, b72, b73, b74 = sch.get_child_blocks(b70)
l75, l76, l77, l78, l79, l80 = sch.get_loops(block=b71)
l81, l82, l83, l84, l85, l86, l87 = sch.get_loops(block=b72)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b73)
sch.annotate(block_or_loop=l88, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l88, ann_key="pragma_unroll_explicit", ann_val=1)
l98, l99, l100, l101, l102 = sch.get_loops(block=b74)
b103 = sch.get_block(name="T_matmul_NT", func_name="main")
l104, l105, l106, l107, l108, l109, l110, l111, l112, l113 = sch.get_loops(block=b103)
b114 = sch.decompose_reduction(block=b103, loop=l107)
2023-11-11 04:27:25 [INFO] [task_scheduler.cc:121] [Task #28: fused_nn_dense_add] Trial #152: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/home2/xiachunwei/Software/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/home2/xiachunwei/Software/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/home2/xiachunwei/Software/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/home2/xiachunwei/Software/tvm/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/home2/xiachunwei/Software/tvm/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/home2/xiachunwei/Software/tvm/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/home2/xiachunwei/Software/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/home2/xiachunwei/Software/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  4: _ZN3tvm7runtime13PackedFun
  3: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  2: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  0: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  File "/home2/xiachunwei/Software/tvm/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(512)), "float32"), p1: T.Buffer((T.int64(1000), T.int64(512)), "float32"), p2: T.Buffer((T.int64(1), T.int64(1000)), "float32"), T_add: T.Buffer((T.int64(1), T.int64(1000)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        T_matmul_NT_local = T.alloc_buffer((T.int64(1), T.int64(1000)), scope="local")
        p0_shared = T.alloc_buffer((T.int64(1), T.int64(512)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(1000), T.int64(512)), scope="shared")
        for i_0_j_0_fused in T.thread_binding(T.int64(25), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 1024, "pragma_unroll_explicit": 1}):
            for i_1_j_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for i_2_j_2_fused in T.thread_binding(T.int64(40), thread="threadIdx.x"):
                    for i_3_init, j_3_init, i_4_init, j_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_init"):
                            v_i = T.axis.spatial(T.int64(1), i_3_init + i_4_init)
                            v_j = T.axis.spatial(T.int64(1000), i_0_j_0_fused * T.int64(40) + i_2_j_2_fused + j_3_init + j_4_init)
                            T.reads()
                            T.writes(T_matmul_NT_local[v_i, v_j])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            T_matmul_NT_local[v_i, v_j] = T.float32(0)
                    for k_0 in range(T.int64(2)):
                        for ax0_ax1_fused_0 in range(T.int64(7)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(40), thread="threadIdx.x"):
                                with T.block("p0_shared"):
                                    v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                    v1 = T.axis.spatial(T.int64(512), k_0 * T.int64(256) + (ax0_ax1_fused_0 * T.int64(40) + ax0_ax1_fused_1))
                                    T.where(ax0_ax1_fused_0 * T.int64(40) + ax0_ax1_fused_1 < T.int64(256))
                                    T.reads(p0[v0, v1])
                                    T.writes(p0_shared[v0, v1])
                                    p0_shared[v0, v1] = p0[v0, v1]
                        for ax0_ax1_fused_0 in range(T.int64(64)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(40), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("p1_shared"):
                                        v0 = T.axis.spatial(T.int64(1000), i_0_j_0_fused * T.int64(40) + (ax0_ax1_fused_0 * T.int64(160) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2) // T.int64(256))
                                        v1 = T.axis.spatial(T.int64(512), k_0 * T.int64(256) + (ax0_ax1_fused_0 * T.int64(160) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2) % T.int64(256))
                                        T.reads(p1[v0, v1])
                                        T.writes(p1_shared[v0, v1])
                                        p1_shared[v0, v1] = p1[v0, v1]
                        for k_1, i_3, j_3, k_2, i_4, j_4 in T.grid(T.int64(32), T.int64(1), T.int64(1), T.int64(8), T.int64(1), T.int64(1)):
                            with T.block("T_matmul_NT_update"):
                                v_i = T.axis.spatial(T.int64(1), i_3 + i_4)
                                v_j = T.axis.spatial(T.int64(1000), i_0_j_0_fused * T.int64(40) + i_2_j_2_fused + j_3 + j_4)
                                v_k = T.axis.reduce(T.int64(512), k_0 * T.int64(256) + k_1 * T.int64(8) + k_2)
                                T.reads(T_matmul_NT_local[v_i, v_j], p0_shared[v_i, v_k], p1_shared[v_j, v_k])
                                T.writes(T_matmul_NT_local[v_i, v_j])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                T_matmul_NT_local[v_i, v_j] = T_matmul_NT_local[v_i, v_j] + p0_shared[v_i, v_k] * p1_shared[v_j, v_k]
                    for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(1000), i_0_j_0_fused * T.int64(40) + i_2_j_2_fused + ax1)
                            T.reads(T_matmul_NT_local[v0, v1], p2[v0, v1])
                            T.writes(T_add[v0, v1])
                            T_add[v0, v1] = T_matmul_NT_local[v0, v1] + p2[v0, v1]
b0 = sch.get_block(name="T_matmul_NT", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l3, l4, l5 = sch.get_loops(block=b0)
v6, v7, v8, v9, v10 = sch.sample_perfect_tile(loop=l3, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l11, l12, l13, l14, l15 = sch.split(loop=l3, factors=[v6, v7, v8, v9, v10], preserve_unit_iters=True)
v16, v17, v18, v19, v20 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[25, 1, 40, 1, 1])
l21, l22, l23, l24, l25 = sch.split(loop=l4, factors=[v16, v17, v18, v19, v20], preserve_unit_iters=True)
v26, v27, v28 = sch.sample_perfect_tile(loop=l5, n=3, max_innermost_factor=64, decision=[2, 32, 8])
l29, l30, l31 = sch.split(loop=l5, factors=[v26, v27, v28], preserve_unit_iters=True)
sch.reorder(l11, l21, l12, l22, l13, l23, l29, l30, l14, l24, l31, l15, l25)
l32 = sch.fuse(l11, l21, preserve_unit_iters=True)
sch.bind(loop=l32, thread_axis="blockIdx.x")
l33 = sch.fuse(l12, l22, preserve_unit_iters=True)
sch.bind(loop=l33, thread_axis="vthread.x")
l34 = sch.fuse(l13, l23, preserve_unit_iters=True)
sch.bind(loop=l34, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b35 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b35, loop=l34, preserve_unit_loops=True, index=-1)
b36 = sch.cache_read(block=b0, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b36, loop=l29, preserve_unit_loops=True, index=-1)
l37, l38, l39, l40, l41, l42 = sch.get_loops(block=b36)
l43 = sch.fuse(l41, l42, preserve_unit_iters=True)
v44 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b36, ann_key="meta_schedule.cooperative_fetch", ann_val=v44)
b45 = sch.cache_read(block=b0, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b45, loop=l29, preserve_unit_loops=True, index=-1)
l46, l47, l48, l49, l50, l51 = sch.get_loops(block=b45)
l52 = sch.fuse(l50, l51, preserve_unit_iters=True)
v53 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b45, ann_key="meta_schedule.cooperative_fetch", ann_val=v53)
sch.reverse_compute_inline(block=b1)
v54 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v54)
sch.enter_postproc()
sch.unannotate(block_or_loop=b36, ann_key="meta_schedule.cooperative_fetch")
l55, l56, l57, l58, l59 = sch.get_loops(block=b36)
l60, l61 = sch.split(loop=l59, factors=[None, 40], preserve_unit_iters=True)
sch.bind(loop=l61, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b45, ann_key="meta_schedule.cooperative_fetch")
l62, l63, l64, l65, l66 = sch.get_loops(block=b45)
l67, l68, l69 = sch.split(loop=l66, factors=[None, 40, 4], preserve_unit_iters=True)
sch.vectorize(loop=l69)
sch.bind(loop=l68, thread_axis="threadIdx.x")
b70 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b70, ann_key="meta_schedule.unroll_explicit")
b71, b72, b73, b74 = sch.get_child_blocks(b70)
l75, l76, l77, l78, l79, l80 = sch.get_loops(block=b71)
l81, l82, l83, l84, l85, l86, l87 = sch.get_loops(block=b72)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b73)
sch.annotate(block_or_loop=l88, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l88, ann_key="pragma_unroll_explicit", ann_val=1)
l98, l99, l100, l101, l102 = sch.get_loops(block=b74)
b103 = sch.get_block(name="T_matmul_NT", func_name="main")
l104, l105, l106, l107, l108, l109, l110, l111, l112, l113 = sch.get_loops(block=b103)
b114 = sch.decompose_reduction(block=b103, loop=l107)
2023-11-11 04:27:25 [INFO] [task_scheduler.cc:121] [Task #28: fused_nn_dense_add] Trial #153: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/home2/xiachunwei/Software/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/home2/xiachunwei/Software/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/home2/xiachunwei/Software/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/home2/xiachunwei/Software/tvm/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/home2/xiachunwei/Software/tvm/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/home2/xiachunwei/Software/tvm/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/home2/xiachunwei/Software/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/home2/xiachunwei/Software/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  4: _ZN3tvm7runtime13PackedFun
  3: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  2: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  0: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  File "/home2/xiachunwei/Software/tvm/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(512)), "float32"), p1: T.Buffer((T.int64(1000), T.int64(512)), "float32"), p2: T.Buffer((T.int64(1), T.int64(1000)), "float32"), T_add: T.Buffer((T.int64(1), T.int64(1000)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        T_matmul_NT_local = T.alloc_buffer((T.int64(1), T.int64(1000)), scope="local")
        p0_shared = T.alloc_buffer((T.int64(1), T.int64(512)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(1000), T.int64(512)), scope="shared")
        for i_0_j_0_fused in T.thread_binding(T.int64(25), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 1024, "pragma_unroll_explicit": 1}):
            for i_1_j_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for i_2_j_2_fused in T.thread_binding(T.int64(40), thread="threadIdx.x"):
                    for i_3_init, j_3_init, i_4_init, j_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_init"):
                            v_i = T.axis.spatial(T.int64(1), i_3_init + i_4_init)
                            v_j = T.axis.spatial(T.int64(1000), i_0_j_0_fused * T.int64(40) + i_2_j_2_fused + j_3_init + j_4_init)
                            T.reads()
                            T.writes(T_matmul_NT_local[v_i, v_j])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            T_matmul_NT_local[v_i, v_j] = T.float32(0)
                    for k_0 in range(T.int64(2)):
                        for ax0_ax1_fused_0 in range(T.int64(7)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(40), thread="threadIdx.x"):
                                with T.block("p0_shared"):
                                    v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                    v1 = T.axis.spatial(T.int64(512), k_0 * T.int64(256) + (ax0_ax1_fused_0 * T.int64(40) + ax0_ax1_fused_1))
                                    T.where(ax0_ax1_fused_0 * T.int64(40) + ax0_ax1_fused_1 < T.int64(256))
                                    T.reads(p0[v0, v1])
                                    T.writes(p0_shared[v0, v1])
                                    p0_shared[v0, v1] = p0[v0, v1]
                        for ax0_ax1_fused_0 in range(T.int64(64)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(40), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("p1_shared"):
                                        v0 = T.axis.spatial(T.int64(1000), i_0_j_0_fused * T.int64(40) + (ax0_ax1_fused_0 * T.int64(160) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2) // T.int64(256))
                                        v1 = T.axis.spatial(T.int64(512), k_0 * T.int64(256) + (ax0_ax1_fused_0 * T.int64(160) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2) % T.int64(256))
                                        T.reads(p1[v0, v1])
                                        T.writes(p1_shared[v0, v1])
                                        p1_shared[v0, v1] = p1[v0, v1]
                        for k_1, i_3, j_3, k_2, i_4, j_4 in T.grid(T.int64(8), T.int64(1), T.int64(1), T.int64(32), T.int64(1), T.int64(1)):
                            with T.block("T_matmul_NT_update"):
                                v_i = T.axis.spatial(T.int64(1), i_3 + i_4)
                                v_j = T.axis.spatial(T.int64(1000), i_0_j_0_fused * T.int64(40) + i_2_j_2_fused + j_3 + j_4)
                                v_k = T.axis.reduce(T.int64(512), k_0 * T.int64(256) + k_1 * T.int64(32) + k_2)
                                T.reads(T_matmul_NT_local[v_i, v_j], p0_shared[v_i, v_k], p1_shared[v_j, v_k])
                                T.writes(T_matmul_NT_local[v_i, v_j])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                T_matmul_NT_local[v_i, v_j] = T_matmul_NT_local[v_i, v_j] + p0_shared[v_i, v_k] * p1_shared[v_j, v_k]
                    for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(1000), i_0_j_0_fused * T.int64(40) + i_2_j_2_fused + ax1)
                            T.reads(T_matmul_NT_local[v0, v1], p2[v0, v1])
                            T.writes(T_add[v0, v1])
                            T_add[v0, v1] = T_matmul_NT_local[v0, v1] + p2[v0, v1]
b0 = sch.get_block(name="T_matmul_NT", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l3, l4, l5 = sch.get_loops(block=b0)
v6, v7, v8, v9, v10 = sch.sample_perfect_tile(loop=l3, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l11, l12, l13, l14, l15 = sch.split(loop=l3, factors=[v6, v7, v8, v9, v10], preserve_unit_iters=True)
v16, v17, v18, v19, v20 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[25, 1, 40, 1, 1])
l21, l22, l23, l24, l25 = sch.split(loop=l4, factors=[v16, v17, v18, v19, v20], preserve_unit_iters=True)
v26, v27, v28 = sch.sample_perfect_tile(loop=l5, n=3, max_innermost_factor=64, decision=[2, 8, 32])
l29, l30, l31 = sch.split(loop=l5, factors=[v26, v27, v28], preserve_unit_iters=True)
sch.reorder(l11, l21, l12, l22, l13, l23, l29, l30, l14, l24, l31, l15, l25)
l32 = sch.fuse(l11, l21, preserve_unit_iters=True)
sch.bind(loop=l32, thread_axis="blockIdx.x")
l33 = sch.fuse(l12, l22, preserve_unit_iters=True)
sch.bind(loop=l33, thread_axis="vthread.x")
l34 = sch.fuse(l13, l23, preserve_unit_iters=True)
sch.bind(loop=l34, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b35 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b35, loop=l34, preserve_unit_loops=True, index=-1)
b36 = sch.cache_read(block=b0, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b36, loop=l29, preserve_unit_loops=True, index=-1)
l37, l38, l39, l40, l41, l42 = sch.get_loops(block=b36)
l43 = sch.fuse(l41, l42, preserve_unit_iters=True)
v44 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b36, ann_key="meta_schedule.cooperative_fetch", ann_val=v44)
b45 = sch.cache_read(block=b0, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b45, loop=l29, preserve_unit_loops=True, index=-1)
l46, l47, l48, l49, l50, l51 = sch.get_loops(block=b45)
l52 = sch.fuse(l50, l51, preserve_unit_iters=True)
v53 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b45, ann_key="meta_schedule.cooperative_fetch", ann_val=v53)
sch.reverse_compute_inline(block=b1)
v54 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v54)
sch.enter_postproc()
sch.unannotate(block_or_loop=b36, ann_key="meta_schedule.cooperative_fetch")
l55, l56, l57, l58, l59 = sch.get_loops(block=b36)
l60, l61 = sch.split(loop=l59, factors=[None, 40], preserve_unit_iters=True)
sch.bind(loop=l61, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b45, ann_key="meta_schedule.cooperative_fetch")
l62, l63, l64, l65, l66 = sch.get_loops(block=b45)
l67, l68, l69 = sch.split(loop=l66, factors=[None, 40, 4], preserve_unit_iters=True)
sch.vectorize(loop=l69)
sch.bind(loop=l68, thread_axis="threadIdx.x")
b70 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b70, ann_key="meta_schedule.unroll_explicit")
b71, b72, b73, b74 = sch.get_child_blocks(b70)
l75, l76, l77, l78, l79, l80 = sch.get_loops(block=b71)
l81, l82, l83, l84, l85, l86, l87 = sch.get_loops(block=b72)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b73)
sch.annotate(block_or_loop=l88, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l88, ann_key="pragma_unroll_explicit", ann_val=1)
l98, l99, l100, l101, l102 = sch.get_loops(block=b74)
b103 = sch.get_block(name="T_matmul_NT", func_name="main")
l104, l105, l106, l107, l108, l109, l110, l111, l112, l113 = sch.get_loops(block=b103)
b114 = sch.decompose_reduction(block=b103, loop=l107)
2023-11-11 04:27:25 [INFO] [task_scheduler.cc:121] [Task #28: fused_nn_dense_add] Trial #154: Error in running:
LocalRunner: An exception occurred
Subprocess terminated
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(512)), "float32"), p1: T.Buffer((T.int64(1000), T.int64(512)), "float32"), p2: T.Buffer((T.int64(1), T.int64(1000)), "float32"), T_add: T.Buffer((T.int64(1), T.int64(1000)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        T_matmul_NT_local = T.alloc_buffer((T.int64(1), T.int64(1000)), scope="local")
        p0_shared = T.alloc_buffer((T.int64(1), T.int64(512)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(1000), T.int64(512)), scope="shared")
        for i_0_j_0_fused in T.thread_binding(T.int64(25), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 512, "pragma_unroll_explicit": 1}):
            for i_1_j_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for i_2_j_2_fused in T.thread_binding(T.int64(40), thread="threadIdx.x"):
                    for i_3_init, j_3_init, i_4_init, j_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_init"):
                            v_i = T.axis.spatial(T.int64(1), i_3_init + i_4_init)
                            v_j = T.axis.spatial(T.int64(1000), i_0_j_0_fused * T.int64(40) + i_2_j_2_fused + j_3_init + j_4_init)
                            T.reads()
                            T.writes(T_matmul_NT_local[v_i, v_j])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            T_matmul_NT_local[v_i, v_j] = T.float32(0)
                    for k_0 in range(T.int64(2)):
                        for ax0_ax1_fused_0 in range(T.int64(4)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(40), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("p0_shared"):
                                        v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v1 = T.axis.spatial(T.int64(512), k_0 * T.int64(256) + (ax0_ax1_fused_0 * T.int64(80) + ax0_ax1_fused_1 * T.int64(2) + ax0_ax1_fused_2))
                                        T.where((ax0_ax1_fused_0 * T.int64(40) + ax0_ax1_fused_1) * T.int64(2) + ax0_ax1_fused_2 < T.int64(256))
                                        T.reads(p0[v0, v1])
                                        T.writes(p0_shared[v0, v1])
                                        p0_shared[v0, v1] = p0[v0, v1]
                        for ax0_ax1_fused_0 in range(T.int64(64)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(40), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("p1_shared"):
                                        v0 = T.axis.spatial(T.int64(1000), i_0_j_0_fused * T.int64(40) + (ax0_ax1_fused_0 * T.int64(160) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2) // T.int64(256))
                                        v1 = T.axis.spatial(T.int64(512), k_0 * T.int64(256) + (ax0_ax1_fused_0 * T.int64(160) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2) % T.int64(256))
                                        T.reads(p1[v0, v1])
                                        T.writes(p1_shared[v0, v1])
                                        p1_shared[v0, v1] = p1[v0, v1]
                        for k_1, i_3, j_3, k_2, i_4, j_4 in T.grid(T.int64(128), T.int64(1), T.int64(1), T.int64(2), T.int64(1), T.int64(1)):
                            with T.block("T_matmul_NT_update"):
                                v_i = T.axis.spatial(T.int64(1), i_3 + i_4)
                                v_j = T.axis.spatial(T.int64(1000), i_0_j_0_fused * T.int64(40) + i_2_j_2_fused + j_3 + j_4)
                                v_k = T.axis.reduce(T.int64(512), k_0 * T.int64(256) + k_1 * T.int64(2) + k_2)
                                T.reads(T_matmul_NT_local[v_i, v_j], p0_shared[v_i, v_k], p1_shared[v_j, v_k])
                                T.writes(T_matmul_NT_local[v_i, v_j])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                T_matmul_NT_local[v_i, v_j] = T_matmul_NT_local[v_i, v_j] + p0_shared[v_i, v_k] * p1_shared[v_j, v_k]
                    for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(1000), i_0_j_0_fused * T.int64(40) + i_2_j_2_fused + ax1)
                            T.reads(T_matmul_NT_local[v0, v1], p2[v0, v1])
                            T.writes(T_add[v0, v1])
                            T_add[v0, v1] = T_matmul_NT_local[v0, v1] + p2[v0, v1]
b0 = sch.get_block(name="T_matmul_NT", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l3, l4, l5 = sch.get_loops(block=b0)
v6, v7, v8, v9, v10 = sch.sample_perfect_tile(loop=l3, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l11, l12, l13, l14, l15 = sch.split(loop=l3, factors=[v6, v7, v8, v9, v10], preserve_unit_iters=True)
v16, v17, v18, v19, v20 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[25, 1, 40, 1, 1])
l21, l22, l23, l24, l25 = sch.split(loop=l4, factors=[v16, v17, v18, v19, v20], preserve_unit_iters=True)
v26, v27, v28 = sch.sample_perfect_tile(loop=l5, n=3, max_innermost_factor=64, decision=[2, 128, 2])
l29, l30, l31 = sch.split(loop=l5, factors=[v26, v27, v28], preserve_unit_iters=True)
sch.reorder(l11, l21, l12, l22, l13, l23, l29, l30, l14, l24, l31, l15, l25)
l32 = sch.fuse(l11, l21, preserve_unit_iters=True)
sch.bind(loop=l32, thread_axis="blockIdx.x")
l33 = sch.fuse(l12, l22, preserve_unit_iters=True)
sch.bind(loop=l33, thread_axis="vthread.x")
l34 = sch.fuse(l13, l23, preserve_unit_iters=True)
sch.bind(loop=l34, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b35 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b35, loop=l34, preserve_unit_loops=True, index=-1)
b36 = sch.cache_read(block=b0, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b36, loop=l29, preserve_unit_loops=True, index=-1)
l37, l38, l39, l40, l41, l42 = sch.get_loops(block=b36)
l43 = sch.fuse(l41, l42, preserve_unit_iters=True)
v44 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b36, ann_key="meta_schedule.cooperative_fetch", ann_val=v44)
b45 = sch.cache_read(block=b0, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b45, loop=l29, preserve_unit_loops=True, index=-1)
l46, l47, l48, l49, l50, l51 = sch.get_loops(block=b45)
l52 = sch.fuse(l50, l51, preserve_unit_iters=True)
v53 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b45, ann_key="meta_schedule.cooperative_fetch", ann_val=v53)
sch.reverse_compute_inline(block=b1)
v54 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v54)
sch.enter_postproc()
sch.unannotate(block_or_loop=b36, ann_key="meta_schedule.cooperative_fetch")
l55, l56, l57, l58, l59 = sch.get_loops(block=b36)
l60, l61, l62 = sch.split(loop=l59, factors=[None, 40, 2], preserve_unit_iters=True)
sch.vectorize(loop=l62)
sch.bind(loop=l61, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b45, ann_key="meta_schedule.cooperative_fetch")
l63, l64, l65, l66, l67 = sch.get_loops(block=b45)
l68, l69, l70 = sch.split(loop=l67, factors=[None, 40, 4], preserve_unit_iters=True)
sch.vectorize(loop=l70)
sch.bind(loop=l69, thread_axis="threadIdx.x")
b71 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b71, ann_key="meta_schedule.unroll_explicit")
b72, b73, b74, b75 = sch.get_child_blocks(b71)
l76, l77, l78, l79, l80, l81, l82 = sch.get_loops(block=b72)
l83, l84, l85, l86, l87, l88, l89 = sch.get_loops(block=b73)
l90, l91, l92, l93, l94, l95, l96, l97, l98, l99 = sch.get_loops(block=b74)
sch.annotate(block_or_loop=l90, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l90, ann_key="pragma_unroll_explicit", ann_val=1)
l100, l101, l102, l103, l104 = sch.get_loops(block=b75)
b105 = sch.get_block(name="T_matmul_NT", func_name="main")
l106, l107, l108, l109, l110, l111, l112, l113, l114, l115 = sch.get_loops(block=b105)
b116 = sch.decompose_reduction(block=b105, loop=l109)
2023-11-11 04:27:25 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #155: GFLOPs: 93.3958. Time: 10.9748 us. Best GFLOPs: 96.0504
2023-11-11 04:27:25 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #156: GFLOPs: 93.3921. Time: 10.9752 us. Best GFLOPs: 96.0504
2023-11-11 04:27:25 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #157: GFLOPs: 93.4653. Time: 10.9666 us. Best GFLOPs: 96.0504
2023-11-11 04:27:25 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #158: GFLOPs: 95.3070. Time: 10.7547 us. Best GFLOPs: 96.0504
2023-11-11 04:27:25 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #159: GFLOPs: 95.2987. Time: 10.7557 us. Best GFLOPs: 96.0504
2023-11-11 04:27:25 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #160: GFLOPs: 95.3001. Time: 10.7555 us. Best GFLOPs: 96.0504
2023-11-11 04:27:25 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #161: GFLOPs: 95.2983. Time: 10.7557 us. Best GFLOPs: 96.0504
2023-11-11 04:27:25 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #162: GFLOPs: 95.1432. Time: 10.7732 us. Best GFLOPs: 96.0504
2023-11-11 04:27:25 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #163: GFLOPs: 95.1104. Time: 10.7770 us. Best GFLOPs: 96.0504
2023-11-11 04:27:25 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #164: GFLOPs: 95.1336. Time: 10.7743 us. Best GFLOPs: 96.0504
2023-11-11 04:27:25 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #165: GFLOPs: 95.1351. Time: 10.7742 us. Best GFLOPs: 96.0504
2023-11-11 04:27:25 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #166: GFLOPs: 15.4951. Time: 66.1498 us. Best GFLOPs: 96.0504
2023-11-11 04:27:25 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #167: GFLOPs: 97.5888. Time: 10.5033 us. Best GFLOPs: 97.5888
2023-11-11 04:27:25 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #168: GFLOPs: 46.3036. Time: 22.1365 us. Best GFLOPs: 97.5888
2023-11-11 04:27:25 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #169: GFLOPs: 38.9402. Time: 26.3224 us. Best GFLOPs: 97.5888
2023-11-11 04:27:25 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #170: GFLOPs: 95.4150. Time: 10.7425 us. Best GFLOPs: 97.5888
2023-11-11 04:27:25 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #171: GFLOPs: 95.4084. Time: 10.7433 us. Best GFLOPs: 97.5888
2023-11-11 04:27:25 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #172: GFLOPs: 95.4139. Time: 10.7427 us. Best GFLOPs: 97.5888
2023-11-11 04:27:25 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #173: GFLOPs: 69.4598. Time: 14.7567 us. Best GFLOPs: 97.5888
2023-11-11 04:27:25 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #174: GFLOPs: 93.3935. Time: 10.9751 us. Best GFLOPs: 97.5888
2023-11-11 04:27:25 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #175: GFLOPs: 93.4342. Time: 10.9703 us. Best GFLOPs: 97.5888
2023-11-11 04:27:25 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #176: GFLOPs: 69.4995. Time: 14.7483 us. Best GFLOPs: 97.5888
2023-11-11 04:27:25 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #177: GFLOPs: 91.5095. Time: 11.2010 us. Best GFLOPs: 97.5888
2023-11-11 04:27:25 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #178: GFLOPs: 91.5226. Time: 11.1994 us. Best GFLOPs: 97.5888
2023-11-11 04:27:25 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #179: GFLOPs: 83.3502. Time: 12.2975 us. Best GFLOPs: 97.5888
2023-11-11 04:27:25 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #180: GFLOPs: 95.1344. Time: 10.7742 us. Best GFLOPs: 97.5888
2023-11-11 04:27:25 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #181: GFLOPs: 93.4398. Time: 10.9696 us. Best GFLOPs: 97.5888
2023-11-11 04:27:25 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #182: GFLOPs: 69.5154. Time: 14.7449 us. Best GFLOPs: 97.5888
2023-11-11 04:27:25 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #183: GFLOPs: 69.5158. Time: 14.7448 us. Best GFLOPs: 97.5888
2023-11-11 04:27:25 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #184: GFLOPs: 69.4833. Time: 14.7518 us. Best GFLOPs: 97.5888
2023-11-11 04:27:25 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #185: GFLOPs: 91.5082. Time: 11.2012 us. Best GFLOPs: 97.5888
2023-11-11 04:27:25 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #186: GFLOPs: 95.4395. Time: 10.7398 us. Best GFLOPs: 97.5888
2023-11-11 04:27:25 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #187: GFLOPs: 69.6412. Time: 14.7183 us. Best GFLOPs: 97.5888
2023-11-11 04:27:25 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #188: GFLOPs: 88.0183. Time: 11.6453 us. Best GFLOPs: 97.5888
2023-11-11 04:27:25 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #189: GFLOPs: 5.4731. Time: 187.2809 us. Best GFLOPs: 97.5888
2023-11-11 04:27:25 [INFO] [task_scheduler.cc:121] [Task #28: fused_nn_dense_add] Trial #190: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/home2/xiachunwei/Software/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/home2/xiachunwei/Software/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 159, in _worker_func
    costs: List[float] = f_run_evaluator(
  File "/home2/xiachunwei/Software/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 387, in default_run_evaluator
    return run_evaluator_common(rt_mod, device, evaluator_config, repeated_args)
  File "/home2/xiachunwei/Software/tvm/python/tvm/meta_schedule/runner/utils.py", line 117, in run_evaluator_common
    profile_result = evaluator(*args)
  File "/home2/xiachunwei/Software/tvm/python/tvm/runtime/module.py", line 397, in evaluator
    blob = feval(*args)
  File "/home2/xiachunwei/Software/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/home2/xiachunwei/Software/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  1: tvm::runtime::profiling::WrapTimeEvaluator(tvm::runtime::PackedFunc, DLDevice, int, int, int, int, int, int, int, tvm::runtime::PackedFunc)::{lambda(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*) [clone .isra.779]
  0: tvm::runtime::CUDADeviceAPI::StreamSync(DLDevice, void*)
  File "/home2/xiachunwei/Software/tvm/src/runtime/cuda/cuda_device_api.cc", line 212
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(512)), "float32"), p1: T.Buffer((T.int64(1000), T.int64(512)), "float32"), p2: T.Buffer((T.int64(1), T.int64(1000)), "float32"), T_add: T.Buffer((T.int64(1), T.int64(1000)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        T_matmul_NT_local = T.alloc_buffer((T.int64(1), T.int64(1000)), scope="local")
        p0_shared = T.alloc_buffer((T.int64(1), T.int64(512)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(1000), T.int64(512)), scope="shared")
        for i_0_j_0_fused in T.thread_binding(T.int64(1), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 16, "pragma_unroll_explicit": 1}):
            for i_1_j_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for i_2_j_2_fused in T.thread_binding(T.int64(50), thread="threadIdx.x"):
                    for i_3_init, j_3_init, i_4_init, j_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(20)):
                        with T.block("T_matmul_NT_init"):
                            v_i = T.axis.spatial(T.int64(1), i_3_init + i_4_init)
                            v_j = T.axis.spatial(T.int64(1000), i_2_j_2_fused * T.int64(20) + j_3_init * T.int64(20) + j_4_init)
                            T.reads()
                            T.writes(T_matmul_NT_local[v_i, v_j])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            T_matmul_NT_local[v_i, v_j] = T.float32(0)
                    for k_0 in range(T.int64(256)):
                        for ax0_ax1_fused_0 in range(T.int64(1)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(50), thread="threadIdx.x"):
                                with T.block("p0_shared"):
                                    v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                    v1 = T.axis.spatial(T.int64(512), k_0 * T.int64(2) + (ax0_ax1_fused_0 * T.int64(50) + ax0_ax1_fused_1))
                                    T.where(ax0_ax1_fused_0 * T.int64(50) + ax0_ax1_fused_1 < T.int64(2))
                                    T.reads(p0[v0, v1])
                                    T.writes(p0_shared[v0, v1])
                                    p0_shared[v0, v1] = p0[v0, v1]
                        for ax0_ax1_fused_0 in range(T.int64(10)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(50), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("p1_shared"):
                                        v0 = T.axis.spatial(T.int64(1000), (ax0_ax1_fused_0 * T.int64(200) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2) // T.int64(2))
                                        v1 = T.axis.spatial(T.int64(512), k_0 * T.int64(2) + (ax0_ax1_fused_0 * T.int64(200) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2) % T.int64(2))
                                        T.reads(p1[v0, v1])
                                        T.writes(p1_shared[v0, v1])
                                        p1_shared[v0, v1] = p1[v0, v1]
                        for k_1, i_3, j_3, k_2, i_4, j_4 in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(1), T.int64(20)):
                            with T.block("T_matmul_NT_update"):
                                v_i = T.axis.spatial(T.int64(1), i_3 + i_4)
                                v_j = T.axis.spatial(T.int64(1000), i_2_j_2_fused * T.int64(20) + j_3 * T.int64(20) + j_4)
                                v_k = T.axis.reduce(T.int64(512), k_0 * T.int64(2) + k_1 * T.int64(2) + k_2)
                                T.reads(T_matmul_NT_local[v_i, v_j], p0_shared[v_i, v_k], p1_shared[v_j, v_k])
                                T.writes(T_matmul_NT_local[v_i, v_j])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                T_matmul_NT_local[v_i, v_j] = T_matmul_NT_local[v_i, v_j] + p0_shared[v_i, v_k] * p1_shared[v_j, v_k]
                    for ax0, ax1 in T.grid(T.int64(1), T.int64(20)):
                        with T.block("T_matmul_NT_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(1000), i_2_j_2_fused * T.int64(20) + ax1)
                            T.reads(T_matmul_NT_local[v0, v1], p2[v0, v1])
                            T.writes(T_add[v0, v1])
                            T_add[v0, v1] = T_matmul_NT_local[v0, v1] + p2[v0, v1]
b0 = sch.get_block(name="T_matmul_NT", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l3, l4, l5 = sch.get_loops(block=b0)
v6, v7, v8, v9, v10 = sch.sample_perfect_tile(loop=l3, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l11, l12, l13, l14, l15 = sch.split(loop=l3, factors=[v6, v7, v8, v9, v10], preserve_unit_iters=True)
v16, v17, v18, v19, v20 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[1, 1, 50, 1, 20])
l21, l22, l23, l24, l25 = sch.split(loop=l4, factors=[v16, v17, v18, v19, v20], preserve_unit_iters=True)
v26, v27, v28 = sch.sample_perfect_tile(loop=l5, n=3, max_innermost_factor=64, decision=[256, 1, 2])
l29, l30, l31 = sch.split(loop=l5, factors=[v26, v27, v28], preserve_unit_iters=True)
sch.reorder(l11, l21, l12, l22, l13, l23, l29, l30, l14, l24, l31, l15, l25)
l32 = sch.fuse(l11, l21, preserve_unit_iters=True)
sch.bind(loop=l32, thread_axis="blockIdx.x")
l33 = sch.fuse(l12, l22, preserve_unit_iters=True)
sch.bind(loop=l33, thread_axis="vthread.x")
l34 = sch.fuse(l13, l23, preserve_unit_iters=True)
sch.bind(loop=l34, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b35 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b35, loop=l34, preserve_unit_loops=True, index=-1)
b36 = sch.cache_read(block=b0, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b36, loop=l29, preserve_unit_loops=True, index=-1)
l37, l38, l39, l40, l41, l42 = sch.get_loops(block=b36)
l43 = sch.fuse(l41, l42, preserve_unit_iters=True)
v44 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b36, ann_key="meta_schedule.cooperative_fetch", ann_val=v44)
b45 = sch.cache_read(block=b0, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b45, loop=l29, preserve_unit_loops=True, index=-1)
l46, l47, l48, l49, l50, l51 = sch.get_loops(block=b45)
l52 = sch.fuse(l50, l51, preserve_unit_iters=True)
v53 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b45, ann_key="meta_schedule.cooperative_fetch", ann_val=v53)
sch.reverse_compute_inline(block=b1)
v54 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=1)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v54)
sch.enter_postproc()
sch.unannotate(block_or_loop=b36, ann_key="meta_schedule.cooperative_fetch")
l55, l56, l57, l58, l59 = sch.get_loops(block=b36)
l60, l61 = sch.split(loop=l59, factors=[None, 50], preserve_unit_iters=True)
sch.bind(loop=l61, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b45, ann_key="meta_schedule.cooperative_fetch")
l62, l63, l64, l65, l66 = sch.get_loops(block=b45)
l67, l68, l69 = sch.split(loop=l66, factors=[None, 50, 4], preserve_unit_iters=True)
sch.vectorize(loop=l69)
sch.bind(loop=l68, thread_axis="threadIdx.x")
b70 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b70, ann_key="meta_schedule.unroll_explicit")
b71, b72, b73, b74 = sch.get_child_blocks(b70)
l75, l76, l77, l78, l79, l80 = sch.get_loops(block=b71)
l81, l82, l83, l84, l85, l86, l87 = sch.get_loops(block=b72)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b73)
sch.annotate(block_or_loop=l88, ann_key="pragma_auto_unroll_max_step", ann_val=16)
sch.annotate(block_or_loop=l88, ann_key="pragma_unroll_explicit", ann_val=1)
l98, l99, l100, l101, l102 = sch.get_loops(block=b74)
b103 = sch.get_block(name="T_matmul_NT", func_name="main")
l104, l105, l106, l107, l108, l109, l110, l111, l112, l113 = sch.get_loops(block=b103)
b114 = sch.decompose_reduction(block=b103, loop=l107)
2023-11-11 04:27:25 [INFO] [task_scheduler.cc:121] [Task #28: fused_nn_dense_add] Trial #191: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/home2/xiachunwei/Software/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/home2/xiachunwei/Software/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/home2/xiachunwei/Software/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/home2/xiachunwei/Software/tvm/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/home2/xiachunwei/Software/tvm/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/home2/xiachunwei/Software/tvm/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/home2/xiachunwei/Software/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/home2/xiachunwei/Software/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  4: _ZN3tvm7runtime13PackedFun
  3: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  2: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  0: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  File "/home2/xiachunwei/Software/tvm/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(512)), "float32"), p1: T.Buffer((T.int64(1000), T.int64(512)), "float32"), p2: T.Buffer((T.int64(1), T.int64(1000)), "float32"), T_add: T.Buffer((T.int64(1), T.int64(1000)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        T_matmul_NT_local = T.alloc_buffer((T.int64(1), T.int64(1000)), scope="local")
        p0_shared = T.alloc_buffer((T.int64(1), T.int64(512)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(1000), T.int64(512)), scope="shared")
        for i_0_j_0_fused in T.thread_binding(T.int64(1), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 16, "pragma_unroll_explicit": 1}):
            for i_1_j_1_fused in T.thread_binding(T.int64(5), thread="vthread.x"):
                for i_2_j_2_fused in T.thread_binding(T.int64(40), thread="threadIdx.x"):
                    for i_3_init, j_3_init, i_4_init, j_4_init in T.grid(T.int64(1), T.int64(5), T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_init"):
                            v_i = T.axis.spatial(T.int64(1), i_3_init + i_4_init)
                            v_j = T.axis.spatial(T.int64(1000), i_1_j_1_fused * T.int64(200) + i_2_j_2_fused * T.int64(5) + j_3_init + j_4_init)
                            T.reads()
                            T.writes(T_matmul_NT_local[v_i, v_j])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            T_matmul_NT_local[v_i, v_j] = T.float32(0)
                    for k_0_fused in T.serial(T.int64(128), annotations={"software_pipeline_async_stages": [0], "software_pipeline_order": [0, 1, 2], "software_pipeline_stage": [0, 0, 2]}):
                        for ax0_ax1_fused_0 in range(T.int64(1)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(40), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("p0_shared"):
                                        v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v1 = T.axis.spatial(T.int64(512), k_0_fused * T.int64(4) + (ax0_ax1_fused_0 * T.int64(160) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2))
                                        T.where((ax0_ax1_fused_0 * T.int64(40) + ax0_ax1_fused_1) * T.int64(4) + ax0_ax1_fused_2 < T.int64(4))
                                        T.reads(p0[v0, v1])
                                        T.writes(p0_shared[v0, v1])
                                        p0_shared[v0, v1] = p0[v0, v1]
                        for ax0_ax1_fused_0 in range(T.int64(100)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(40), thread="threadIdx.x"):
                                with T.block("p1_shared"):
                                    v0 = T.axis.spatial(T.int64(1000), (ax0_ax1_fused_0 * T.int64(40) + ax0_ax1_fused_1) // T.int64(4))
                                    v1 = T.axis.spatial(T.int64(512), k_0_fused * T.int64(4) + (ax0_ax1_fused_0 * T.int64(40) + ax0_ax1_fused_1) % T.int64(4))
                                    T.reads(p1[v0, v1])
                                    T.writes(p1_shared[v0, v1])
                                    p1_shared[v0, v1] = p1[v0, v1]
                        for k_1, i_3, j_3, k_2, i_4, j_4 in T.grid(T.int64(1), T.int64(1), T.int64(5), T.int64(4), T.int64(1), T.int64(1)):
                            with T.block("T_matmul_NT_update"):
                                v_i = T.axis.spatial(T.int64(1), i_3 + i_4)
                                v_j = T.axis.spatial(T.int64(1000), i_1_j_1_fused * T.int64(200) + i_2_j_2_fused * T.int64(5) + j_3 + j_4)
                                v_k = T.axis.reduce(T.int64(512), k_0_fused * T.int64(4) + k_1 * T.int64(4) + k_2)
                                T.reads(T_matmul_NT_local[v_i, v_j], p0_shared[v_i, v_k], p1_shared[v_j, v_k])
                                T.writes(T_matmul_NT_local[v_i, v_j])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                T_matmul_NT_local[v_i, v_j] = T_matmul_NT_local[v_i, v_j] + p0_shared[v_i, v_k] * p1_shared[v_j, v_k]
                    for ax0, ax1 in T.grid(T.int64(1), T.int64(5)):
                        with T.block("T_matmul_NT_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(1000), i_1_j_1_fused * T.int64(200) + i_2_j_2_fused * T.int64(5) + ax1)
                            T.reads(T_matmul_NT_local[v0, v1], p2[v0, v1])
                            T.writes(T_add[v0, v1])
                            T_add[v0, v1] = T_matmul_NT_local[v0, v1] + p2[v0, v1]
b0 = sch.get_block(name="T_matmul_NT", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l3, l4, l5 = sch.get_loops(block=b0)
v6, v7, v8, v9, v10 = sch.sample_perfect_tile(loop=l3, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l11, l12, l13, l14, l15 = sch.split(loop=l3, factors=[v6, v7, v8, v9, v10], preserve_unit_iters=True)
v16, v17, v18, v19, v20 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[1, 5, 40, 5, 1])
l21, l22, l23, l24, l25 = sch.split(loop=l4, factors=[v16, v17, v18, v19, v20], preserve_unit_iters=True)
v26, v27, v28 = sch.sample_perfect_tile(loop=l5, n=3, max_innermost_factor=64, decision=[128, 1, 4])
l29, l30, l31 = sch.split(loop=l5, factors=[v26, v27, v28], preserve_unit_iters=True)
sch.reorder(l11, l21, l12, l22, l13, l23, l29, l30, l14, l24, l31, l15, l25)
l32 = sch.fuse(l11, l21, preserve_unit_iters=True)
sch.bind(loop=l32, thread_axis="blockIdx.x")
l33 = sch.fuse(l12, l22, preserve_unit_iters=True)
sch.bind(loop=l33, thread_axis="vthread.x")
l34 = sch.fuse(l13, l23, preserve_unit_iters=True)
sch.bind(loop=l34, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b35 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b35, loop=l34, preserve_unit_loops=True, index=-1)
b36 = sch.cache_read(block=b0, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b36, loop=l29, preserve_unit_loops=True, index=-1)
l37, l38, l39, l40, l41, l42 = sch.get_loops(block=b36)
l43 = sch.fuse(l41, l42, preserve_unit_iters=True)
v44 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b36, ann_key="meta_schedule.cooperative_fetch", ann_val=v44)
b45 = sch.cache_read(block=b0, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b45, loop=l29, preserve_unit_loops=True, index=-1)
l46, l47, l48, l49, l50, l51 = sch.get_loops(block=b45)
l52 = sch.fuse(l50, l51, preserve_unit_iters=True)
v53 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b45, ann_key="meta_schedule.cooperative_fetch", ann_val=v53)
l54 = sch.fuse(l29, preserve_unit_iters=True)
sch.annotate(block_or_loop=l54, ann_key="software_pipeline_stage", ann_val=[0, 0, 2])
sch.annotate(block_or_loop=l54, ann_key="software_pipeline_order", ann_val=[0, 1, 2])
sch.annotate(block_or_loop=l54, ann_key="software_pipeline_async_stages", ann_val=[0])
sch.reverse_compute_inline(block=b1)
v55 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=1)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v55)
sch.enter_postproc()
sch.unannotate(block_or_loop=b36, ann_key="meta_schedule.cooperative_fetch")
l56, l57, l58, l59, l60 = sch.get_loops(block=b36)
l61, l62, l63 = sch.split(loop=l60, factors=[None, 40, 4], preserve_unit_iters=True)
sch.vectorize(loop=l63)
sch.bind(loop=l62, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b45, ann_key="meta_schedule.cooperative_fetch")
l64, l65, l66, l67, l68 = sch.get_loops(block=b45)
l69, l70 = sch.split(loop=l68, factors=[None, 40], preserve_unit_iters=True)
sch.bind(loop=l70, thread_axis="threadIdx.x")
b71 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b71, ann_key="meta_schedule.unroll_explicit")
b72, b73, b74, b75 = sch.get_child_blocks(b71)
l76, l77, l78, l79, l80, l81, l82 = sch.get_loops(block=b72)
l83, l84, l85, l86, l87, l88 = sch.get_loops(block=b73)
l89, l90, l91, l92, l93, l94, l95, l96, l97, l98 = sch.get_loops(block=b74)
sch.annotate(block_or_loop=l89, ann_key="pragma_auto_unroll_max_step", ann_val=16)
sch.annotate(block_or_loop=l89, ann_key="pragma_unroll_explicit", ann_val=1)
l99, l100, l101, l102, l103 = sch.get_loops(block=b75)
b104 = sch.get_block(name="T_matmul_NT", func_name="main")
l105, l106, l107, l108, l109, l110, l111, l112, l113, l114 = sch.get_loops(block=b104)
b115 = sch.decompose_reduction(block=b104, loop=l108)
2023-11-11 06:07:53 [INFO] [evolutionary_search.cc:713] Generating candidates......
2023-11-11 06:07:54 [INFO] [evolutionary_search.cc:715] Picked top 102 candidate(s) from database
2023-11-11 06:07:54 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310a405238)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310e76ec68)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310dc33cd8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310be916f8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56310f803f18)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310eeb7a68)]: 399 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310e771ba8)]: 0 failure(s)
2023-11-11 06:07:55 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310a405238)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310e76ec68)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310dc33cd8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310be916f8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56310f803f18)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310eeb7a68)]: 800 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310e771ba8)]: 0 failure(s)
2023-11-11 06:07:56 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310a405238)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310e76ec68)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310dc33cd8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310be916f8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56310f803f18)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310eeb7a68)]: 1204 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310e771ba8)]: 0 failure(s)
2023-11-11 06:07:56 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310a405238)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310e76ec68)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310dc33cd8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310be916f8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56310f803f18)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310eeb7a68)]: 1604 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310e771ba8)]: 0 failure(s)
2023-11-11 06:07:57 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310a405238)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310e76ec68)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310dc33cd8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310be916f8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56310f803f18)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310eeb7a68)]: 2011 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310e771ba8)]: 0 failure(s)
2023-11-11 06:07:58 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310a405238)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310e76ec68)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310dc33cd8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310be916f8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56310f803f18)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310eeb7a68)]: 2415 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310e771ba8)]: 0 failure(s)
2023-11-11 06:07:58 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310a405238)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310e76ec68)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310dc33cd8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310be916f8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56310f803f18)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310eeb7a68)]: 2820 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310e771ba8)]: 0 failure(s)
2023-11-11 06:07:58 [INFO] [evolutionary_search.cc:723] Sampled 50 candidate(s)
2023-11-11 06:08:00 [INFO] [evolutionary_search.cc:621] Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310a405238)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310e76ec68)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310dc33cd8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310be916f8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56310f803f18)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310eeb7a68)]: 141 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310e771ba8)]: 0 failure(s)
2023-11-11 06:08:03 [INFO] [evolutionary_search.cc:621] Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310a405238)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310e76ec68)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310dc33cd8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310be916f8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56310f803f18)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310eeb7a68)]: 150 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310e771ba8)]: 0 failure(s)
2023-11-11 06:08:05 [INFO] [evolutionary_search.cc:621] Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310a405238)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310e76ec68)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310dc33cd8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310be916f8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56310f803f18)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310eeb7a68)]: 161 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310e771ba8)]: 0 failure(s)
2023-11-11 06:08:08 [INFO] [evolutionary_search.cc:621] Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310a405238)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310e76ec68)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310dc33cd8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310be916f8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56310f803f18)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310eeb7a68)]: 170 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310e771ba8)]: 0 failure(s)
2023-11-11 06:08:09 [INFO] [evolutionary_search.cc:649] Scores of the best 64 candidates:
[1 : 16]:	1.2531  1.2445  1.2401  1.1308  1.1305  1.1305  1.1305  1.1301  1.1301  1.1118  1.1113  1.0509  1.0509  1.0509  1.0509  1.0376
[17 : 32]:	1.0086  0.9841  0.9835  0.9824  0.9808  0.9799  0.9797  0.9796  0.9796  0.9791  0.9791  0.9791  0.9791  0.9790  0.9790  0.9788
[33 : 48]:	0.9787  0.9780  0.9779  0.9759  0.9754  0.9751  0.9751  0.9747  0.9743  0.9717  0.9684  0.9680  0.9680  0.9680  0.9668  0.9659
[49 : 64]:	0.9642  0.9606  0.9592  0.9589  0.9568  0.9560  0.9550  0.9530  0.9498  0.9495  0.9480  0.9480  0.9479  0.9479  0.9468  0.9464
2023-11-11 06:08:09 [INFO] [evolutionary_search.cc:727] Got 64 candidate(s) with evolutionary search
2023-11-11 06:08:09 [INFO] [evolutionary_search.cc:730] Sending 64 candidates(s) for measurement
2023-11-11 06:08:37 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #192: GFLOPs: 64.4316. Time: 15.9083 us. Best GFLOPs: 97.5888
2023-11-11 06:08:37 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #193: GFLOPs: 64.4429. Time: 15.9055 us. Best GFLOPs: 97.5888
2023-11-11 06:08:37 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #194: GFLOPs: 55.8322. Time: 18.3586 us. Best GFLOPs: 97.5888
2023-11-11 06:08:37 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #195: GFLOPs: 55.1566. Time: 18.5835 us. Best GFLOPs: 97.5888
2023-11-11 06:08:37 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #196: GFLOPs: 55.3431. Time: 18.5208 us. Best GFLOPs: 97.5888
2023-11-11 06:08:37 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #197: GFLOPs: 55.1548. Time: 18.5841 us. Best GFLOPs: 97.5888
2023-11-11 06:08:37 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #198: GFLOPs: 55.3461. Time: 18.5198 us. Best GFLOPs: 97.5888
2023-11-11 06:08:37 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #199: GFLOPs: 55.1540. Time: 18.5843 us. Best GFLOPs: 97.5888
2023-11-11 06:08:37 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #200: GFLOPs: 55.1552. Time: 18.5839 us. Best GFLOPs: 97.5888
2023-11-11 06:08:37 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #201: GFLOPs: 55.3465. Time: 18.5197 us. Best GFLOPs: 97.5888
2023-11-11 06:08:37 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #202: GFLOPs: 55.1555. Time: 18.5838 us. Best GFLOPs: 97.5888
2023-11-11 06:08:37 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #203: GFLOPs: 78.8771. Time: 12.9949 us. Best GFLOPs: 97.5888
2023-11-11 06:08:37 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #204: GFLOPs: 55.3473. Time: 18.5194 us. Best GFLOPs: 97.5888
2023-11-11 06:08:37 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #205: GFLOPs: 55.1543. Time: 18.5842 us. Best GFLOPs: 97.5888
2023-11-11 06:08:37 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #206: GFLOPs: 55.3517. Time: 18.5179 us. Best GFLOPs: 97.5888
2023-11-11 06:08:37 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #207: GFLOPs: 55.1567. Time: 18.5834 us. Best GFLOPs: 97.5888
2023-11-11 06:08:37 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #208: GFLOPs: 38.8417. Time: 26.3892 us. Best GFLOPs: 97.5888
2023-11-11 06:08:37 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #209: GFLOPs: 60.9606. Time: 16.8141 us. Best GFLOPs: 97.5888
2023-11-11 06:08:37 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #210: GFLOPs: 96.9273. Time: 10.5749 us. Best GFLOPs: 97.5888
2023-11-11 06:08:37 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #211: GFLOPs: 96.9544. Time: 10.5720 us. Best GFLOPs: 97.5888
2023-11-11 06:08:37 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #212: GFLOPs: 96.9877. Time: 10.5684 us. Best GFLOPs: 97.5888
2023-11-11 06:08:37 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #213: GFLOPs: 96.9641. Time: 10.5709 us. Best GFLOPs: 97.5888
2023-11-11 06:08:37 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #214: GFLOPs: 73.7471. Time: 13.8989 us. Best GFLOPs: 97.5888
2023-11-11 06:08:37 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #215: GFLOPs: 96.9901. Time: 10.5681 us. Best GFLOPs: 97.5888
2023-11-11 06:08:37 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #216: GFLOPs: 96.9988. Time: 10.5671 us. Best GFLOPs: 97.5888
2023-11-11 06:08:37 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #217: GFLOPs: 61.7508. Time: 16.5990 us. Best GFLOPs: 97.5888
2023-11-11 06:08:37 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #218: GFLOPs: 61.7417. Time: 16.6014 us. Best GFLOPs: 97.5888
2023-11-11 06:08:37 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #219: GFLOPs: 50.7675. Time: 20.1901 us. Best GFLOPs: 97.5888
2023-11-11 06:08:37 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #220: GFLOPs: 95.1919. Time: 10.7677 us. Best GFLOPs: 97.5888
2023-11-11 06:08:37 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #221: GFLOPs: 95.2476. Time: 10.7614 us. Best GFLOPs: 97.5888
2023-11-11 06:08:37 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #222: GFLOPs: 95.2407. Time: 10.7622 us. Best GFLOPs: 97.5888
2023-11-11 06:08:37 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #223: GFLOPs: 96.9800. Time: 10.5692 us. Best GFLOPs: 97.5888
2023-11-11 06:08:37 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #224: GFLOPs: 96.9766. Time: 10.5696 us. Best GFLOPs: 97.5888
2023-11-11 06:08:37 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #225: GFLOPs: 61.7460. Time: 16.6003 us. Best GFLOPs: 97.5888
2023-11-11 06:08:37 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #226: GFLOPs: 61.7485. Time: 16.5996 us. Best GFLOPs: 97.5888
2023-11-11 06:08:37 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #227: GFLOPs: 67.4351. Time: 15.1998 us. Best GFLOPs: 97.5888
2023-11-11 06:08:37 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #228: GFLOPs: 96.3638. Time: 10.6368 us. Best GFLOPs: 97.5888
2023-11-11 06:08:37 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #229: GFLOPs: 96.8927. Time: 10.5787 us. Best GFLOPs: 97.5888
2023-11-11 06:08:37 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #230: GFLOPs: 96.8906. Time: 10.5789 us. Best GFLOPs: 97.5888
2023-11-11 06:08:37 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #231: GFLOPs: 67.4301. Time: 15.2009 us. Best GFLOPs: 97.5888
2023-11-11 06:08:37 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #232: GFLOPs: 96.2598. Time: 10.6483 us. Best GFLOPs: 97.5888
2023-11-11 06:08:37 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #233: GFLOPs: 67.2763. Time: 15.2357 us. Best GFLOPs: 97.5888
2023-11-11 06:08:37 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #234: GFLOPs: 3.2284. Time: 317.4944 us. Best GFLOPs: 97.5888
2023-11-11 06:08:37 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #235: GFLOPs: 64.4461. Time: 15.9048 us. Best GFLOPs: 97.5888
2023-11-11 06:08:37 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #236: GFLOPs: 89.5329. Time: 11.4483 us. Best GFLOPs: 97.5888
2023-11-11 06:08:37 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #237: GFLOPs: 64.4461. Time: 15.9048 us. Best GFLOPs: 97.5888
2023-11-11 06:08:37 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #238: GFLOPs: 64.4447. Time: 15.9051 us. Best GFLOPs: 97.5888
2023-11-11 06:08:37 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #239: GFLOPs: 3.2285. Time: 317.4884 us. Best GFLOPs: 97.5888
2023-11-11 06:08:37 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #240: GFLOPs: 60.9785. Time: 16.8092 us. Best GFLOPs: 97.5888
2023-11-11 06:08:37 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #241: GFLOPs: 38.8415. Time: 26.3893 us. Best GFLOPs: 97.5888
2023-11-11 06:08:37 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #242: GFLOPs: 61.7339. Time: 16.6035 us. Best GFLOPs: 97.5888
2023-11-11 06:08:37 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #243: GFLOPs: 36.1201. Time: 28.3775 us. Best GFLOPs: 97.5888
2023-11-11 06:08:37 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #244: GFLOPs: 61.7294. Time: 16.6047 us. Best GFLOPs: 97.5888
2023-11-11 06:08:37 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #245: GFLOPs: 67.4296. Time: 15.2010 us. Best GFLOPs: 97.5888
2023-11-11 06:08:37 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #246: GFLOPs: 61.7455. Time: 16.6004 us. Best GFLOPs: 97.5888
2023-11-11 06:08:37 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #247: GFLOPs: 67.2755. Time: 15.2358 us. Best GFLOPs: 97.5888
2023-11-11 06:08:37 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #248: GFLOPs: 94.7675. Time: 10.8159 us. Best GFLOPs: 97.5888
2023-11-11 06:08:37 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #249: GFLOPs: 74.8421. Time: 13.6955 us. Best GFLOPs: 97.5888
2023-11-11 06:08:37 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #250: GFLOPs: 64.4465. Time: 15.9047 us. Best GFLOPs: 97.5888
2023-11-11 06:08:37 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #251: GFLOPs: 94.7582. Time: 10.8170 us. Best GFLOPs: 97.5888
2023-11-11 06:08:37 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #252: GFLOPs: 94.7766. Time: 10.8149 us. Best GFLOPs: 97.5888
2023-11-11 06:08:37 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #253: GFLOPs: 16.0402. Time: 63.9018 us. Best GFLOPs: 97.5888
2023-11-11 06:08:37 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #254: GFLOPs: 11.4105. Time: 89.8295 us. Best GFLOPs: 97.5888
2023-11-11 06:08:37 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #255: GFLOPs: 10.4739. Time: 97.8625 us. Best GFLOPs: 97.5888
2023-11-11 07:33:06 [INFO] [evolutionary_search.cc:713] Generating candidates......
2023-11-11 07:33:07 [INFO] [evolutionary_search.cc:715] Picked top 102 candidate(s) from database
2023-11-11 07:33:07 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310a405238)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310e76ec68)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310dc33cd8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310be916f8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56310f803f18)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310eeb7a68)]: 403 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310e771ba8)]: 0 failure(s)
2023-11-11 07:33:08 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310a405238)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310e76ec68)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310dc33cd8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310be916f8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56310f803f18)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310eeb7a68)]: 803 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310e771ba8)]: 0 failure(s)
2023-11-11 07:33:09 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310a405238)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310e76ec68)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310dc33cd8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310be916f8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56310f803f18)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310eeb7a68)]: 1204 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310e771ba8)]: 0 failure(s)
2023-11-11 07:33:09 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310a405238)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310e76ec68)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310dc33cd8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310be916f8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56310f803f18)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310eeb7a68)]: 1606 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310e771ba8)]: 0 failure(s)
2023-11-11 07:33:10 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310a405238)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310e76ec68)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310dc33cd8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310be916f8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56310f803f18)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310eeb7a68)]: 2011 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310e771ba8)]: 0 failure(s)
2023-11-11 07:33:11 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310a405238)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310e76ec68)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310dc33cd8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310be916f8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56310f803f18)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310eeb7a68)]: 2412 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310e771ba8)]: 0 failure(s)
2023-11-11 07:33:11 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310a405238)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310e76ec68)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310dc33cd8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310be916f8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56310f803f18)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310eeb7a68)]: 2807 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310e771ba8)]: 0 failure(s)
2023-11-11 07:33:11 [INFO] [evolutionary_search.cc:723] Sampled 63 candidate(s)
2023-11-11 07:33:14 [INFO] [evolutionary_search.cc:621] Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310a405238)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310e76ec68)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310dc33cd8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310be916f8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56310f803f18)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310eeb7a68)]: 179 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310e771ba8)]: 0 failure(s)
2023-11-11 07:33:16 [INFO] [evolutionary_search.cc:621] Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310a405238)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310e76ec68)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310dc33cd8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310be916f8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56310f803f18)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310eeb7a68)]: 172 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310e771ba8)]: 0 failure(s)
2023-11-11 07:33:19 [INFO] [evolutionary_search.cc:621] Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310a405238)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310e76ec68)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310dc33cd8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310be916f8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56310f803f18)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310eeb7a68)]: 165 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310e771ba8)]: 0 failure(s)
2023-11-11 07:33:22 [INFO] [evolutionary_search.cc:621] Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310a405238)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310e76ec68)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310dc33cd8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310be916f8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56310f803f18)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310eeb7a68)]: 188 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310e771ba8)]: 0 failure(s)
2023-11-11 07:33:23 [INFO] [evolutionary_search.cc:649] Scores of the best 64 candidates:
[1 : 16]:	0.9613  0.9583  0.9577  0.9563  0.9513  0.9509  0.9498  0.9496  0.9461  0.9427  0.9426  0.9422  0.9411  0.9363  0.9348  0.9343
[17 : 32]:	0.9339  0.9330  0.9319  0.9316  0.9281  0.9266  0.9209  0.9192  0.9188  0.9182  0.9177  0.9174  0.9140  0.9104  0.9100  0.9089
[33 : 48]:	0.9048  0.9048  0.9047  0.9045  0.9044  0.9043  0.9040  0.9036  0.9030  0.9025  0.9022  0.8995  0.8988  0.8988  0.8985  0.8984
[49 : 64]:	0.8980  0.8979  0.8974  0.8972  0.8961  0.8955  0.8951  0.8947  0.8938  0.8936  0.8921  0.8910  0.8903  0.8845  0.8791  0.8763
2023-11-11 07:33:23 [INFO] [evolutionary_search.cc:727] Got 64 candidate(s) with evolutionary search
2023-11-11 07:33:23 [INFO] [evolutionary_search.cc:730] Sending 64 candidates(s) for measurement
2023-11-11 07:33:50 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #256: GFLOPs: 93.4050. Time: 10.9737 us. Best GFLOPs: 97.5888
2023-11-11 07:33:50 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #257: GFLOPs: 93.3350. Time: 10.9819 us. Best GFLOPs: 97.5888
2023-11-11 07:33:50 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #258: GFLOPs: 93.3416. Time: 10.9812 us. Best GFLOPs: 97.5888
2023-11-11 07:33:50 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #259: GFLOPs: 93.3370. Time: 10.9817 us. Best GFLOPs: 97.5888
2023-11-11 07:33:50 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #260: GFLOPs: 91.3987. Time: 11.2146 us. Best GFLOPs: 97.5888
2023-11-11 07:33:50 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #261: GFLOPs: 91.3876. Time: 11.2160 us. Best GFLOPs: 97.5888
2023-11-11 07:33:50 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #262: GFLOPs: 91.3889. Time: 11.2158 us. Best GFLOPs: 97.5888
2023-11-11 07:33:50 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #263: GFLOPs: 91.3869. Time: 11.2160 us. Best GFLOPs: 97.5888
2023-11-11 07:33:50 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #264: GFLOPs: 91.3882. Time: 11.2159 us. Best GFLOPs: 97.5888
2023-11-11 07:33:50 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #265: GFLOPs: 74.2817. Time: 13.7988 us. Best GFLOPs: 97.5888
2023-11-11 07:33:50 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #266: GFLOPs: 91.4082. Time: 11.2134 us. Best GFLOPs: 97.5888
2023-11-11 07:33:50 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #267: GFLOPs: 91.4138. Time: 11.2128 us. Best GFLOPs: 97.5888
2023-11-11 07:33:50 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #268: GFLOPs: 91.4000. Time: 11.2144 us. Best GFLOPs: 97.5888
2023-11-11 07:33:50 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #269: GFLOPs: 93.4323. Time: 10.9705 us. Best GFLOPs: 97.5888
2023-11-11 07:33:50 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #270: GFLOPs: 92.9440. Time: 11.0281 us. Best GFLOPs: 97.5888
2023-11-11 07:33:50 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #271: GFLOPs: 74.2791. Time: 13.7993 us. Best GFLOPs: 97.5888
2023-11-11 07:33:50 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #272: GFLOPs: 91.4952. Time: 11.2028 us. Best GFLOPs: 97.5888
2023-11-11 07:33:50 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #273: GFLOPs: 91.4759. Time: 11.2051 us. Best GFLOPs: 97.5888
2023-11-11 07:33:50 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #274: GFLOPs: 91.4736. Time: 11.2054 us. Best GFLOPs: 97.5888
2023-11-11 07:33:50 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #275: GFLOPs: 91.4912. Time: 11.2033 us. Best GFLOPs: 97.5888
2023-11-11 07:33:50 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #276: GFLOPs: 91.4981. Time: 11.2024 us. Best GFLOPs: 97.5888
2023-11-11 07:33:50 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #277: GFLOPs: 91.4932. Time: 11.2030 us. Best GFLOPs: 97.5888
2023-11-11 07:33:50 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #278: GFLOPs: 60.6198. Time: 16.9087 us. Best GFLOPs: 97.5888
2023-11-11 07:33:50 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #279: GFLOPs: 86.4183. Time: 11.8609 us. Best GFLOPs: 97.5888
2023-11-11 07:33:50 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #280: GFLOPs: 86.2315. Time: 11.8866 us. Best GFLOPs: 97.5888
2023-11-11 07:33:50 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #281: GFLOPs: 72.6952. Time: 14.1000 us. Best GFLOPs: 97.5888
2023-11-11 07:33:50 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #282: GFLOPs: 86.2254. Time: 11.8874 us. Best GFLOPs: 97.5888
2023-11-11 07:33:50 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #283: GFLOPs: 86.4097. Time: 11.8621 us. Best GFLOPs: 97.5888
2023-11-11 07:33:50 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #284: GFLOPs: 86.2395. Time: 11.8855 us. Best GFLOPs: 97.5888
2023-11-11 07:33:50 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #285: GFLOPs: 87.9722. Time: 11.6514 us. Best GFLOPs: 97.5888
2023-11-11 07:33:50 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #286: GFLOPs: 87.9741. Time: 11.6512 us. Best GFLOPs: 97.5888
2023-11-11 07:33:50 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #287: GFLOPs: 87.9652. Time: 11.6523 us. Best GFLOPs: 97.5888
2023-11-11 07:33:50 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #288: GFLOPs: 66.0712. Time: 15.5136 us. Best GFLOPs: 97.5888
2023-11-11 07:33:50 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #289: GFLOPs: 66.0617. Time: 15.5158 us. Best GFLOPs: 97.5888
2023-11-11 07:33:50 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #290: GFLOPs: 54.8248. Time: 18.6959 us. Best GFLOPs: 97.5888
2023-11-11 07:33:50 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #291: GFLOPs: 86.2281. Time: 11.8871 us. Best GFLOPs: 97.5888
2023-11-11 07:33:50 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #292: GFLOPs: 72.3509. Time: 14.1671 us. Best GFLOPs: 97.5888
2023-11-11 07:33:50 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #293: GFLOPs: 54.8901. Time: 18.6737 us. Best GFLOPs: 97.5888
2023-11-11 07:33:50 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #294: GFLOPs: 86.2018. Time: 11.8907 us. Best GFLOPs: 97.5888
2023-11-11 07:33:50 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #295: GFLOPs: 86.2150. Time: 11.8889 us. Best GFLOPs: 97.5888
2023-11-11 07:33:50 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #296: GFLOPs: 54.8086. Time: 18.7014 us. Best GFLOPs: 97.5888
2023-11-11 07:33:50 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #297: GFLOPs: 86.2248. Time: 11.8875 us. Best GFLOPs: 97.5888
2023-11-11 07:33:50 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #298: GFLOPs: 86.2093. Time: 11.8897 us. Best GFLOPs: 97.5888
2023-11-11 07:33:50 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #299: GFLOPs: 54.8913. Time: 18.6733 us. Best GFLOPs: 97.5888
2023-11-11 07:33:50 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #300: GFLOPs: 54.9003. Time: 18.6702 us. Best GFLOPs: 97.5888
2023-11-11 07:33:50 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #301: GFLOPs: 86.2217. Time: 11.8880 us. Best GFLOPs: 97.5888
2023-11-11 07:33:50 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #302: GFLOPs: 2.0482. Time: 500.4441 us. Best GFLOPs: 97.5888
2023-11-11 07:33:50 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #303: GFLOPs: 54.9031. Time: 18.6693 us. Best GFLOPs: 97.5888
2023-11-11 07:33:50 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #304: GFLOPs: 54.8945. Time: 18.6722 us. Best GFLOPs: 97.5888
2023-11-11 07:33:50 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #305: GFLOPs: 72.6572. Time: 14.1074 us. Best GFLOPs: 97.5888
2023-11-11 07:33:50 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #306: GFLOPs: 54.9040. Time: 18.6689 us. Best GFLOPs: 97.5888
2023-11-11 07:33:50 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #307: GFLOPs: 86.2266. Time: 11.8873 us. Best GFLOPs: 97.5888
2023-11-11 07:33:50 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #308: GFLOPs: 54.8572. Time: 18.6849 us. Best GFLOPs: 97.5888
2023-11-11 07:33:50 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #309: GFLOPs: 54.9029. Time: 18.6693 us. Best GFLOPs: 97.5888
2023-11-11 07:33:50 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #310: GFLOPs: 54.8541. Time: 18.6859 us. Best GFLOPs: 97.5888
2023-11-11 07:33:50 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #311: GFLOPs: 67.1344. Time: 15.2679 us. Best GFLOPs: 97.5888
2023-11-11 07:33:50 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #312: GFLOPs: 54.8553. Time: 18.6855 us. Best GFLOPs: 97.5888
2023-11-11 07:33:50 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #313: GFLOPs: 54.9040. Time: 18.6689 us. Best GFLOPs: 97.5888
2023-11-11 07:33:50 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #314: GFLOPs: 54.9055. Time: 18.6684 us. Best GFLOPs: 97.5888
2023-11-11 07:33:50 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #315: GFLOPs: 66.0712. Time: 15.5136 us. Best GFLOPs: 97.5888
2023-11-11 07:33:50 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #316: GFLOPs: 54.8560. Time: 18.6853 us. Best GFLOPs: 97.5888
2023-11-11 07:33:50 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #317: GFLOPs: 25.3099. Time: 40.4980 us. Best GFLOPs: 97.5888
2023-11-11 07:33:50 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #318: GFLOPs: 17.6996. Time: 57.9109 us. Best GFLOPs: 97.5888
2023-11-11 07:33:50 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #319: GFLOPs: 5.0730. Time: 202.0483 us. Best GFLOPs: 97.5888
2023-11-11 09:14:50 [INFO] [evolutionary_search.cc:713] Generating candidates......
2023-11-11 09:14:50 [INFO] [evolutionary_search.cc:715] Picked top 102 candidate(s) from database
2023-11-11 09:14:51 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310a405238)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310e76ec68)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310dc33cd8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310be916f8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56310f803f18)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310eeb7a68)]: 405 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310e771ba8)]: 0 failure(s)
2023-11-11 09:14:52 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310a405238)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310e76ec68)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310dc33cd8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310be916f8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56310f803f18)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310eeb7a68)]: 811 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310e771ba8)]: 0 failure(s)
2023-11-11 09:14:52 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310a405238)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310e76ec68)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310dc33cd8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310be916f8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56310f803f18)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310eeb7a68)]: 1214 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310e771ba8)]: 0 failure(s)
2023-11-11 09:14:53 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310a405238)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310e76ec68)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310dc33cd8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310be916f8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56310f803f18)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310eeb7a68)]: 1617 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310e771ba8)]: 0 failure(s)
2023-11-11 09:14:53 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310a405238)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310e76ec68)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310dc33cd8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310be916f8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56310f803f18)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310eeb7a68)]: 2019 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310e771ba8)]: 0 failure(s)
2023-11-11 09:14:54 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310a405238)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310e76ec68)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310dc33cd8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310be916f8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56310f803f18)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310eeb7a68)]: 2425 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310e771ba8)]: 0 failure(s)
2023-11-11 09:14:55 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310a405238)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310e76ec68)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310dc33cd8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310be916f8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56310f803f18)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310eeb7a68)]: 2828 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310e771ba8)]: 0 failure(s)
2023-11-11 09:14:55 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310a405238)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310e76ec68)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310dc33cd8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310be916f8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56310f803f18)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310eeb7a68)]: 3230 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310e771ba8)]: 0 failure(s)
2023-11-11 09:14:55 [INFO] [evolutionary_search.cc:723] Sampled 50 candidate(s)
2023-11-11 09:14:57 [INFO] [evolutionary_search.cc:621] Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310a405238)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310e76ec68)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310dc33cd8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310be916f8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56310f803f18)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310eeb7a68)]: 165 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310e771ba8)]: 0 failure(s)
2023-11-11 09:15:00 [INFO] [evolutionary_search.cc:621] Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310a405238)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310e76ec68)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310dc33cd8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310be916f8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56310f803f18)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310eeb7a68)]: 193 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310e771ba8)]: 0 failure(s)
2023-11-11 09:15:03 [INFO] [evolutionary_search.cc:621] Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310a405238)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310e76ec68)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310dc33cd8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310be916f8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56310f803f18)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310eeb7a68)]: 174 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310e771ba8)]: 0 failure(s)
2023-11-11 09:15:05 [INFO] [evolutionary_search.cc:621] Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310a405238)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310e76ec68)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310dc33cd8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310be916f8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56310f803f18)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310eeb7a68)]: 178 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310e771ba8)]: 0 failure(s)
2023-11-11 09:15:07 [INFO] [evolutionary_search.cc:649] Scores of the best 64 candidates:
[1 : 16]:	0.9594  0.9594  0.9451  0.9239  0.9223  0.9222  0.9205  0.8928  0.8918  0.8905  0.8904  0.8419  0.8150  0.8052  0.8041  0.8012
[17 : 32]:	0.8009  0.8005  0.8004  0.8004  0.8004  0.8003  0.8003  0.8003  0.8001  0.8001  0.8001  0.8000  0.8000  0.8000  0.7996  0.7996
[33 : 48]:	0.7996  0.7995  0.7995  0.7995  0.7859  0.7735  0.7719  0.7719  0.7642  0.7633  0.7633  0.7632  0.7600  0.7585  0.7585  0.7532
[49 : 64]:	0.7507  0.7449  0.7446  0.7437  0.7437  0.7437  0.7433  0.7425  0.7425  0.7412  0.7407  0.7387  0.7379  0.7379  0.7379  0.7378
2023-11-11 09:15:07 [INFO] [evolutionary_search.cc:727] Got 64 candidate(s) with evolutionary search
2023-11-11 09:15:07 [INFO] [evolutionary_search.cc:730] Sending 64 candidates(s) for measurement
2023-11-11 09:15:37 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #320: GFLOPs: 93.3667. Time: 10.9782 us. Best GFLOPs: 97.5888
2023-11-11 09:15:37 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #321: GFLOPs: 93.5788. Time: 10.9533 us. Best GFLOPs: 97.5888
2023-11-11 09:15:37 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #322: GFLOPs: 91.3967. Time: 11.2148 us. Best GFLOPs: 97.5888
2023-11-11 09:15:37 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #323: GFLOPs: 91.4945. Time: 11.2029 us. Best GFLOPs: 97.5888
2023-11-11 09:15:37 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #324: GFLOPs: 93.4568. Time: 10.9676 us. Best GFLOPs: 97.5888
2023-11-11 09:15:37 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #325: GFLOPs: 93.6698. Time: 10.9427 us. Best GFLOPs: 97.5888
2023-11-11 09:15:37 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #326: GFLOPs: 91.4143. Time: 11.2127 us. Best GFLOPs: 97.5888
2023-11-11 09:15:37 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #327: GFLOPs: 87.9785. Time: 11.6506 us. Best GFLOPs: 97.5888
2023-11-11 09:15:37 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #328: GFLOPs: 87.8908. Time: 11.6622 us. Best GFLOPs: 97.5888
2023-11-11 09:15:37 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #329: GFLOPs: 87.0726. Time: 11.7718 us. Best GFLOPs: 97.5888
2023-11-11 09:15:37 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #330: GFLOPs: 86.1967. Time: 11.8914 us. Best GFLOPs: 97.5888
2023-11-11 09:15:37 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #331: GFLOPs: 78.8930. Time: 12.9923 us. Best GFLOPs: 97.5888
2023-11-11 09:15:37 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #332: GFLOPs: 73.8044. Time: 13.8881 us. Best GFLOPs: 97.5888
2023-11-11 09:15:37 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #333: GFLOPs: 74.4314. Time: 13.7711 us. Best GFLOPs: 97.5888
2023-11-11 09:15:37 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #334: GFLOPs: 59.2730. Time: 17.2929 us. Best GFLOPs: 97.5888
2023-11-11 09:15:37 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #335: GFLOPs: 66.1457. Time: 15.4961 us. Best GFLOPs: 97.5888
2023-11-11 09:15:37 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #336: GFLOPs: 67.1300. Time: 15.2689 us. Best GFLOPs: 97.5888
2023-11-11 09:15:37 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #337: GFLOPs: 65.3243. Time: 15.6909 us. Best GFLOPs: 97.5888
2023-11-11 09:15:37 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #338: GFLOPs: 66.1462. Time: 15.4960 us. Best GFLOPs: 97.5888
2023-11-11 09:15:37 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #339: GFLOPs: 66.1494. Time: 15.4952 us. Best GFLOPs: 97.5888
2023-11-11 09:15:37 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #340: GFLOPs: 66.1420. Time: 15.4970 us. Best GFLOPs: 97.5888
2023-11-11 09:15:37 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #341: GFLOPs: 66.1494. Time: 15.4952 us. Best GFLOPs: 97.5888
2023-11-11 09:15:37 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #342: GFLOPs: 66.1457. Time: 15.4961 us. Best GFLOPs: 97.5888
2023-11-11 09:15:37 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #343: GFLOPs: 66.1420. Time: 15.4970 us. Best GFLOPs: 97.5888
2023-11-11 09:15:37 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #344: GFLOPs: 67.1188. Time: 15.2714 us. Best GFLOPs: 97.5888
2023-11-11 09:15:37 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #345: GFLOPs: 67.1302. Time: 15.2688 us. Best GFLOPs: 97.5888
2023-11-11 09:15:37 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #346: GFLOPs: 67.1305. Time: 15.2688 us. Best GFLOPs: 97.5888
2023-11-11 09:15:37 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #347: GFLOPs: 67.1300. Time: 15.2689 us. Best GFLOPs: 97.5888
2023-11-11 09:15:37 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #348: GFLOPs: 67.1277. Time: 15.2694 us. Best GFLOPs: 97.5888
2023-11-11 09:15:37 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #349: GFLOPs: 67.1304. Time: 15.2688 us. Best GFLOPs: 97.5888
2023-11-11 09:15:37 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #350: GFLOPs: 65.3730. Time: 15.6793 us. Best GFLOPs: 97.5888
2023-11-11 09:15:37 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #351: GFLOPs: 65.3131. Time: 15.6936 us. Best GFLOPs: 97.5888
2023-11-11 09:15:37 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #352: GFLOPs: 65.3964. Time: 15.6736 us. Best GFLOPs: 97.5888
2023-11-11 09:15:37 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #353: GFLOPs: 65.2898. Time: 15.6992 us. Best GFLOPs: 97.5888
2023-11-11 09:15:37 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #354: GFLOPs: 65.3698. Time: 15.6800 us. Best GFLOPs: 97.5888
2023-11-11 09:15:37 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #355: GFLOPs: 65.2952. Time: 15.6979 us. Best GFLOPs: 97.5888
2023-11-11 09:15:37 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #356: GFLOPs: 59.3084. Time: 17.2825 us. Best GFLOPs: 97.5888
2023-11-11 09:15:37 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #357: GFLOPs: 88.9355. Time: 11.5252 us. Best GFLOPs: 97.5888
2023-11-11 09:15:37 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #358: GFLOPs: 93.3072. Time: 10.9852 us. Best GFLOPs: 97.5888
2023-11-11 09:15:37 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #359: GFLOPs: 74.2927. Time: 13.7968 us. Best GFLOPs: 97.5888
2023-11-11 09:15:37 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #360: GFLOPs: 73.8685. Time: 13.8760 us. Best GFLOPs: 97.5888
2023-11-11 09:15:37 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #361: GFLOPs: 93.4779. Time: 10.9652 us. Best GFLOPs: 97.5888
2023-11-11 09:15:37 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #362: GFLOPs: 73.6956. Time: 13.9086 us. Best GFLOPs: 97.5888
2023-11-11 09:15:37 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #363: GFLOPs: 49.4950. Time: 20.7091 us. Best GFLOPs: 97.5888
2023-11-11 09:15:37 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #364: GFLOPs: 73.8256. Time: 13.8841 us. Best GFLOPs: 97.5888
2023-11-11 09:15:37 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #365: GFLOPs: 59.1490. Time: 17.3291 us. Best GFLOPs: 97.5888
2023-11-11 09:15:37 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #366: GFLOPs: 59.2982. Time: 17.2855 us. Best GFLOPs: 97.5888
2023-11-11 09:15:37 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #367: GFLOPs: 80.2166. Time: 12.7779 us. Best GFLOPs: 97.5888
2023-11-11 09:15:37 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #368: GFLOPs: 67.1445. Time: 15.2656 us. Best GFLOPs: 97.5888
2023-11-11 09:15:37 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #369: GFLOPs: 88.9507. Time: 11.5232 us. Best GFLOPs: 97.5888
2023-11-11 09:15:37 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #370: GFLOPs: 69.2233. Time: 14.8071 us. Best GFLOPs: 97.5888
2023-11-11 09:15:37 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #371: GFLOPs: 69.3434. Time: 14.7815 us. Best GFLOPs: 97.5888
2023-11-11 09:15:37 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #372: GFLOPs: 69.3978. Time: 14.7699 us. Best GFLOPs: 97.5888
2023-11-11 09:15:37 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #373: GFLOPs: 69.0385. Time: 14.8468 us. Best GFLOPs: 97.5888
2023-11-11 09:15:37 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #374: GFLOPs: 74.2853. Time: 13.7981 us. Best GFLOPs: 97.5888
2023-11-11 09:15:37 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #375: GFLOPs: 69.3613. Time: 14.7777 us. Best GFLOPs: 97.5888
2023-11-11 09:15:37 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #376: GFLOPs: 69.1934. Time: 14.8135 us. Best GFLOPs: 97.5888
2023-11-11 09:15:37 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #377: GFLOPs: 63.5943. Time: 16.1178 us. Best GFLOPs: 97.5888
2023-11-11 09:15:37 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #378: GFLOPs: 67.1324. Time: 15.2683 us. Best GFLOPs: 97.5888
2023-11-11 09:15:37 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #379: GFLOPs: 66.1416. Time: 15.4971 us. Best GFLOPs: 97.5888
2023-11-11 09:15:37 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #380: GFLOPs: 66.1443. Time: 15.4964 us. Best GFLOPs: 97.5888
2023-11-11 09:15:37 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #381: GFLOPs: 10.4725. Time: 97.8750 us. Best GFLOPs: 97.5888
2023-11-11 09:15:37 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #382: GFLOPs: 15.5166. Time: 66.0583 us. Best GFLOPs: 97.5888
2023-11-11 09:15:37 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #383: GFLOPs: 9.1360. Time: 112.1938 us. Best GFLOPs: 97.5888
2023-11-11 10:30:59 [INFO] [evolutionary_search.cc:713] Generating candidates......
2023-11-11 10:30:59 [INFO] [evolutionary_search.cc:715] Picked top 102 candidate(s) from database
2023-11-11 10:31:00 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310a405238)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310e76ec68)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310dc33cd8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310be916f8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56310f803f18)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310eeb7a68)]: 401 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310e771ba8)]: 0 failure(s)
2023-11-11 10:31:01 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310a405238)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310e76ec68)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310dc33cd8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310be916f8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56310f803f18)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310eeb7a68)]: 807 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310e771ba8)]: 0 failure(s)
2023-11-11 10:31:01 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310a405238)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310e76ec68)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310dc33cd8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310be916f8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56310f803f18)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310eeb7a68)]: 1211 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310e771ba8)]: 0 failure(s)
2023-11-11 10:31:02 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310a405238)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310e76ec68)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310dc33cd8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310be916f8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56310f803f18)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310eeb7a68)]: 1612 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310e771ba8)]: 0 failure(s)
2023-11-11 10:31:03 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310a405238)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310e76ec68)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310dc33cd8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310be916f8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56310f803f18)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310eeb7a68)]: 2012 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310e771ba8)]: 0 failure(s)
2023-11-11 10:31:03 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310a405238)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310e76ec68)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310dc33cd8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310be916f8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56310f803f18)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310eeb7a68)]: 2415 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310e771ba8)]: 0 failure(s)
2023-11-11 10:31:04 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310a405238)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310e76ec68)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310dc33cd8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310be916f8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56310f803f18)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310eeb7a68)]: 2819 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310e771ba8)]: 0 failure(s)
2023-11-11 10:31:04 [INFO] [evolutionary_search.cc:723] Sampled 51 candidate(s)
2023-11-11 10:31:06 [INFO] [evolutionary_search.cc:621] Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310a405238)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310e76ec68)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310dc33cd8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310be916f8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56310f803f18)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310eeb7a68)]: 178 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310e771ba8)]: 0 failure(s)
2023-11-11 10:31:09 [INFO] [evolutionary_search.cc:621] Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310a405238)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310e76ec68)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310dc33cd8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310be916f8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56310f803f18)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310eeb7a68)]: 164 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310e771ba8)]: 0 failure(s)
2023-11-11 10:31:12 [INFO] [evolutionary_search.cc:621] Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310a405238)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310e76ec68)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310dc33cd8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310be916f8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56310f803f18)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310eeb7a68)]: 161 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310e771ba8)]: 0 failure(s)
2023-11-11 10:31:15 [INFO] [evolutionary_search.cc:621] Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310a405238)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310e76ec68)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310dc33cd8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310be916f8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56310f803f18)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310eeb7a68)]: 169 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310e771ba8)]: 0 failure(s)
2023-11-11 10:31:16 [INFO] [evolutionary_search.cc:649] Scores of the best 64 candidates:
[1 : 16]:	1.0306  1.0301  1.0297  1.0246  1.0246  1.0246  1.0151  1.0141  1.0139  1.0109  1.0109  1.0094  1.0071  1.0063  1.0030  1.0025
[17 : 32]:	1.0014  0.9990  0.9954  0.9951  0.9925  0.9925  0.9844  0.9844  0.9838  0.9835  0.9831  0.9695  0.9695  0.9466  0.9420  0.9420
[33 : 48]:	0.9420  0.9363  0.9351  0.9283  0.9283  0.9217  0.9125  0.9113  0.9100  0.9100  0.9100  0.9079  0.9075  0.9068  0.9068  0.8939
[49 : 64]:	0.8866  0.8793  0.8706  0.8682  0.8569  0.8408  0.8408  0.8327  0.8294  0.8244  0.8230  0.8230  0.8159  0.8088  0.7974  0.7965
2023-11-11 10:31:16 [INFO] [evolutionary_search.cc:727] Got 64 candidate(s) with evolutionary search
2023-11-11 10:31:16 [INFO] [evolutionary_search.cc:730] Sending 64 candidates(s) for measurement
2023-11-11 10:31:44 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #384: GFLOPs: 80.4732. Time: 12.7372 us. Best GFLOPs: 97.5888
2023-11-11 10:31:44 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #385: GFLOPs: 80.4576. Time: 12.7396 us. Best GFLOPs: 97.5888
2023-11-11 10:31:44 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #386: GFLOPs: 79.7726. Time: 12.8490 us. Best GFLOPs: 97.5888
2023-11-11 10:31:44 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #387: GFLOPs: 82.5389. Time: 12.4184 us. Best GFLOPs: 97.5888
2023-11-11 10:31:44 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #388: GFLOPs: 82.5767. Time: 12.4127 us. Best GFLOPs: 97.5888
2023-11-11 10:31:44 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #389: GFLOPs: 82.5361. Time: 12.4188 us. Best GFLOPs: 97.5888
2023-11-11 10:31:44 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #390: GFLOPs: 79.7219. Time: 12.8572 us. Best GFLOPs: 97.5888
2023-11-11 10:31:44 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #391: GFLOPs: 82.1675. Time: 12.4745 us. Best GFLOPs: 97.5888
2023-11-11 10:31:44 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #392: GFLOPs: 82.1514. Time: 12.4770 us. Best GFLOPs: 97.5888
2023-11-11 10:31:44 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #393: GFLOPs: 82.1377. Time: 12.4790 us. Best GFLOPs: 97.5888
2023-11-11 10:31:44 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #394: GFLOPs: 82.1681. Time: 12.4744 us. Best GFLOPs: 97.5888
2023-11-11 10:31:44 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #395: GFLOPs: 82.1856. Time: 12.4718 us. Best GFLOPs: 97.5888
2023-11-11 10:31:44 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #396: GFLOPs: 82.3593. Time: 12.4455 us. Best GFLOPs: 97.5888
2023-11-11 10:31:44 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #397: GFLOPs: 82.1838. Time: 12.4720 us. Best GFLOPs: 97.5888
2023-11-11 10:31:44 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #398: GFLOPs: 82.1873. Time: 12.4715 us. Best GFLOPs: 97.5888
2023-11-11 10:31:44 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #399: GFLOPs: 82.1681. Time: 12.4744 us. Best GFLOPs: 97.5888
2023-11-11 10:31:44 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #400: GFLOPs: 80.3874. Time: 12.7508 us. Best GFLOPs: 97.5888
2023-11-11 10:31:44 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #401: GFLOPs: 85.7865. Time: 11.9483 us. Best GFLOPs: 97.5888
2023-11-11 10:31:44 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #402: GFLOPs: 85.7618. Time: 11.9517 us. Best GFLOPs: 97.5888
2023-11-11 10:31:44 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #403: GFLOPs: 82.3237. Time: 12.4508 us. Best GFLOPs: 97.5888
2023-11-11 10:31:44 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #404: GFLOPs: 82.3142. Time: 12.4523 us. Best GFLOPs: 97.5888
2023-11-11 10:31:44 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #405: GFLOPs: 82.3323. Time: 12.4495 us. Best GFLOPs: 97.5888
2023-11-11 10:31:44 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #406: GFLOPs: 85.7792. Time: 11.9493 us. Best GFLOPs: 97.5888
2023-11-11 10:31:44 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #407: GFLOPs: 85.7889. Time: 11.9479 us. Best GFLOPs: 97.5888
2023-11-11 10:31:44 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #408: GFLOPs: 85.7690. Time: 11.9507 us. Best GFLOPs: 97.5888
2023-11-11 10:31:44 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #409: GFLOPs: 85.7714. Time: 11.9504 us. Best GFLOPs: 97.5888
2023-11-11 10:31:44 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #410: GFLOPs: 80.5001. Time: 12.7329 us. Best GFLOPs: 97.5888
2023-11-11 10:31:44 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #411: GFLOPs: 88.1506. Time: 11.6278 us. Best GFLOPs: 97.5888
2023-11-11 10:31:44 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #412: GFLOPs: 88.2079. Time: 11.6203 us. Best GFLOPs: 97.5888
2023-11-11 10:31:44 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #413: GFLOPs: 67.8824. Time: 15.0996 us. Best GFLOPs: 97.5888
2023-11-11 10:31:44 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #414: GFLOPs: 67.8816. Time: 15.0998 us. Best GFLOPs: 97.5888
2023-11-11 10:31:44 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #415: GFLOPs: 87.6980. Time: 11.6878 us. Best GFLOPs: 97.5888
2023-11-11 10:31:44 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #416: GFLOPs: 87.6957. Time: 11.6881 us. Best GFLOPs: 97.5888
2023-11-11 10:31:44 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #417: GFLOPs: 36.0226. Time: 28.4544 us. Best GFLOPs: 97.5888
2023-11-11 10:31:44 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #418: GFLOPs: 67.8749. Time: 15.1013 us. Best GFLOPs: 97.5888
2023-11-11 10:31:44 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #419: GFLOPs: 58.6931. Time: 17.4637 us. Best GFLOPs: 97.5888
2023-11-11 10:31:44 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #420: GFLOPs: 58.6911. Time: 17.4643 us. Best GFLOPs: 97.5888
2023-11-11 10:31:44 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #421: GFLOPs: 36.0285. Time: 28.4497 us. Best GFLOPs: 97.5888
2023-11-11 10:31:44 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #422: GFLOPs: 65.5450. Time: 15.6381 us. Best GFLOPs: 97.5888
2023-11-11 10:31:44 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #423: GFLOPs: 65.5948. Time: 15.6262 us. Best GFLOPs: 97.5888
2023-11-11 10:31:44 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #424: GFLOPs: 65.5595. Time: 15.6346 us. Best GFLOPs: 97.5888
2023-11-11 10:31:44 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #425: GFLOPs: 65.6057. Time: 15.6236 us. Best GFLOPs: 97.5888
2023-11-11 10:31:44 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #426: GFLOPs: 65.5373. Time: 15.6399 us. Best GFLOPs: 97.5888
2023-11-11 10:31:44 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #427: GFLOPs: 65.6030. Time: 15.6243 us. Best GFLOPs: 97.5888
2023-11-11 10:31:44 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #428: GFLOPs: 65.5563. Time: 15.6354 us. Best GFLOPs: 97.5888
2023-11-11 10:31:44 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #429: GFLOPs: 36.0824. Time: 28.4072 us. Best GFLOPs: 97.5888
2023-11-11 10:31:44 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #430: GFLOPs: 36.0665. Time: 28.4198 us. Best GFLOPs: 97.5888
2023-11-11 10:31:44 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #431: GFLOPs: 36.0166. Time: 28.4591 us. Best GFLOPs: 97.5888
2023-11-11 10:31:44 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #432: GFLOPs: 83.9606. Time: 12.2081 us. Best GFLOPs: 97.5888
2023-11-11 10:31:44 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #433: GFLOPs: 36.0142. Time: 28.4610 us. Best GFLOPs: 97.5888
2023-11-11 10:31:44 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #434: GFLOPs: 72.9895. Time: 14.0431 us. Best GFLOPs: 97.5888
2023-11-11 10:31:44 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #435: GFLOPs: 85.7816. Time: 11.9489 us. Best GFLOPs: 97.5888
2023-11-11 10:31:44 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #436: GFLOPs: 36.1764. Time: 28.3334 us. Best GFLOPs: 97.5888
2023-11-11 10:31:44 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #437: GFLOPs: 87.9990. Time: 11.6479 us. Best GFLOPs: 97.5888
2023-11-11 10:31:44 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #438: GFLOPs: 88.0679. Time: 11.6387 us. Best GFLOPs: 97.5888
2023-11-11 10:31:44 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #439: GFLOPs: 36.0836. Time: 28.4063 us. Best GFLOPs: 97.5888
2023-11-11 10:31:44 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #440: GFLOPs: 73.0513. Time: 14.0312 us. Best GFLOPs: 97.5888
2023-11-11 10:31:44 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #441: GFLOPs: 92.3366. Time: 11.1007 us. Best GFLOPs: 97.5888
2023-11-11 10:31:44 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #442: GFLOPs: 65.6089. Time: 15.6229 us. Best GFLOPs: 97.5888
2023-11-11 10:31:44 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #443: GFLOPs: 87.7590. Time: 11.6797 us. Best GFLOPs: 97.5888
2023-11-11 10:31:44 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #444: GFLOPs: 80.9471. Time: 12.6626 us. Best GFLOPs: 97.5888
2023-11-11 10:31:44 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #445: GFLOPs: 7.3354. Time: 139.7330 us. Best GFLOPs: 97.5888
2023-11-11 10:31:44 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #446: GFLOPs: 5.7930. Time: 176.9380 us. Best GFLOPs: 97.5888
2023-11-11 10:31:44 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #447: GFLOPs: 13.0882. Time: 78.3151 us. Best GFLOPs: 97.5888
2023-11-11 11:08:24 [INFO] [evolutionary_search.cc:713] Generating candidates......
2023-11-11 11:08:24 [INFO] [evolutionary_search.cc:715] Picked top 102 candidate(s) from database
2023-11-11 11:08:25 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310a405238)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310e76ec68)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310dc33cd8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310be916f8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56310f803f18)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310eeb7a68)]: 396 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310e771ba8)]: 0 failure(s)
2023-11-11 11:08:26 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310a405238)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310e76ec68)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310dc33cd8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310be916f8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56310f803f18)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310eeb7a68)]: 794 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310e771ba8)]: 0 failure(s)
2023-11-11 11:08:26 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310a405238)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310e76ec68)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310dc33cd8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310be916f8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56310f803f18)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310eeb7a68)]: 1202 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310e771ba8)]: 0 failure(s)
2023-11-11 11:08:27 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310a405238)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310e76ec68)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310dc33cd8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310be916f8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56310f803f18)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310eeb7a68)]: 1601 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310e771ba8)]: 0 failure(s)
2023-11-11 11:08:28 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310a405238)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310e76ec68)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310dc33cd8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310be916f8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56310f803f18)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310eeb7a68)]: 2003 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310e771ba8)]: 0 failure(s)
2023-11-11 11:08:28 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310a405238)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310e76ec68)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310dc33cd8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310be916f8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56310f803f18)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310eeb7a68)]: 2407 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310e771ba8)]: 0 failure(s)
2023-11-11 11:08:28 [INFO] [evolutionary_search.cc:723] Sampled 53 candidate(s)
2023-11-11 11:08:30 [INFO] [evolutionary_search.cc:621] Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310a405238)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310e76ec68)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310dc33cd8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310be916f8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56310f803f18)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310eeb7a68)]: 209 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310e771ba8)]: 0 failure(s)
2023-11-11 11:08:33 [INFO] [evolutionary_search.cc:621] Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310a405238)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310e76ec68)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310dc33cd8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310be916f8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56310f803f18)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310eeb7a68)]: 173 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310e771ba8)]: 0 failure(s)
2023-11-11 11:08:36 [INFO] [evolutionary_search.cc:621] Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310a405238)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310e76ec68)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310dc33cd8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310be916f8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56310f803f18)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310eeb7a68)]: 161 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310e771ba8)]: 0 failure(s)
2023-11-11 11:08:39 [INFO] [evolutionary_search.cc:621] Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310a405238)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310e76ec68)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310dc33cd8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310be916f8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56310f803f18)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310eeb7a68)]: 179 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310e771ba8)]: 0 failure(s)
2023-11-11 11:08:40 [INFO] [evolutionary_search.cc:649] Scores of the best 64 candidates:
[1 : 16]:	1.0370  1.0353  1.0322  1.0213  1.0199  1.0183  1.0177  1.0151  1.0135  1.0119  1.0109  1.0097  1.0040  0.9977  0.9925  0.9907
[17 : 32]:	0.9879  0.9876  0.9869  0.9856  0.9844  0.9841  0.9831  0.9815  0.9784  0.9695  0.9420  0.9374  0.9372  0.9355  0.9334  0.9246
[33 : 48]:	0.9237  0.9217  0.9217  0.9188  0.9068  0.9053  0.9040  0.8866  0.8864  0.8831  0.8831  0.8803  0.8793  0.8793  0.8765  0.8747
[49 : 64]:	0.8682  0.8682  0.8616  0.8609  0.8583  0.8408  0.8379  0.8321  0.8306  0.8293  0.8245  0.8245  0.8215  0.8215  0.8215  0.8168
2023-11-11 11:08:40 [INFO] [evolutionary_search.cc:727] Got 64 candidate(s) with evolutionary search
2023-11-11 11:08:40 [INFO] [evolutionary_search.cc:730] Sending 64 candidates(s) for measurement
2023-11-11 11:09:08 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #448: GFLOPs: 82.1104. Time: 12.4832 us. Best GFLOPs: 97.5888
2023-11-11 11:09:08 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #449: GFLOPs: 85.4483. Time: 11.9956 us. Best GFLOPs: 97.5888
2023-11-11 11:09:08 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #450: GFLOPs: 85.4578. Time: 11.9942 us. Best GFLOPs: 97.5888
2023-11-11 11:09:08 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #451: GFLOPs: 86.7259. Time: 11.8189 us. Best GFLOPs: 97.5888
2023-11-11 11:09:08 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #452: GFLOPs: 83.5463. Time: 12.2686 us. Best GFLOPs: 97.5888
2023-11-11 11:09:08 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #453: GFLOPs: 86.8887. Time: 11.7967 us. Best GFLOPs: 97.5888
2023-11-11 11:09:08 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #454: GFLOPs: 80.6142. Time: 12.7149 us. Best GFLOPs: 97.5888
2023-11-11 11:09:08 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #455: GFLOPs: 80.6080. Time: 12.7159 us. Best GFLOPs: 97.5888
2023-11-11 11:09:08 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #456: GFLOPs: 86.7183. Time: 11.8199 us. Best GFLOPs: 97.5888
2023-11-11 11:09:08 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #457: GFLOPs: 86.9082. Time: 11.7941 us. Best GFLOPs: 97.5888
2023-11-11 11:09:08 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #458: GFLOPs: 83.8226. Time: 12.2282 us. Best GFLOPs: 97.5888
2023-11-11 11:09:08 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #459: GFLOPs: 86.7220. Time: 11.8194 us. Best GFLOPs: 97.5888
2023-11-11 11:09:08 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #460: GFLOPs: 82.0452. Time: 12.4931 us. Best GFLOPs: 97.5888
2023-11-11 11:09:08 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #461: GFLOPs: 81.5130. Time: 12.5747 us. Best GFLOPs: 97.5888
2023-11-11 11:09:08 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #462: GFLOPs: 83.5476. Time: 12.2685 us. Best GFLOPs: 97.5888
2023-11-11 11:09:08 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #463: GFLOPs: 86.7696. Time: 11.8129 us. Best GFLOPs: 97.5888
2023-11-11 11:09:08 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #464: GFLOPs: 83.5463. Time: 12.2686 us. Best GFLOPs: 97.5888
2023-11-11 11:09:08 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #465: GFLOPs: 86.7629. Time: 11.8138 us. Best GFLOPs: 97.5888
2023-11-11 11:09:08 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #466: GFLOPs: 88.3048. Time: 11.6075 us. Best GFLOPs: 97.5888
2023-11-11 11:09:08 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #467: GFLOPs: 81.5313. Time: 12.5719 us. Best GFLOPs: 97.5888
2023-11-11 11:09:08 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #468: GFLOPs: 88.3065. Time: 11.6073 us. Best GFLOPs: 97.5888
2023-11-11 11:09:08 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #469: GFLOPs: 88.2959. Time: 11.6087 us. Best GFLOPs: 97.5888
2023-11-11 11:09:08 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #470: GFLOPs: 81.5779. Time: 12.5647 us. Best GFLOPs: 97.5888
2023-11-11 11:09:08 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #471: GFLOPs: 88.2965. Time: 11.6086 us. Best GFLOPs: 97.5888
2023-11-11 11:09:08 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #472: GFLOPs: 81.5474. Time: 12.5694 us. Best GFLOPs: 97.5888
2023-11-11 11:09:08 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #473: GFLOPs: 88.3040. Time: 11.6076 us. Best GFLOPs: 97.5888
2023-11-11 11:09:08 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #474: GFLOPs: 68.7595. Time: 14.9070 us. Best GFLOPs: 97.5888
2023-11-11 11:09:08 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #475: GFLOPs: 68.7209. Time: 14.9154 us. Best GFLOPs: 97.5888
2023-11-11 11:09:08 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #476: GFLOPs: 59.6678. Time: 17.1784 us. Best GFLOPs: 97.5888
2023-11-11 11:09:08 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #477: GFLOPs: 68.7501. Time: 14.9091 us. Best GFLOPs: 97.5888
2023-11-11 11:09:08 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #478: GFLOPs: 59.6434. Time: 17.1855 us. Best GFLOPs: 97.5888
2023-11-11 11:09:08 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #479: GFLOPs: 66.6720. Time: 15.3738 us. Best GFLOPs: 97.5888
2023-11-11 11:09:08 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #480: GFLOPs: 59.6150. Time: 17.1937 us. Best GFLOPs: 97.5888
2023-11-11 11:09:08 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #481: GFLOPs: 36.3086. Time: 28.2302 us. Best GFLOPs: 97.5888
2023-11-11 11:09:08 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #482: GFLOPs: 36.3094. Time: 28.2296 us. Best GFLOPs: 97.5888
2023-11-11 11:09:08 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #483: GFLOPs: 36.3077. Time: 28.2309 us. Best GFLOPs: 97.5888
2023-11-11 11:09:08 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #484: GFLOPs: 36.0253. Time: 28.4523 us. Best GFLOPs: 97.5888
2023-11-11 11:09:08 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #485: GFLOPs: 66.6647. Time: 15.3754 us. Best GFLOPs: 97.5888
2023-11-11 11:09:08 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #486: GFLOPs: 36.0241. Time: 28.4532 us. Best GFLOPs: 97.5888
2023-11-11 11:09:08 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #487: GFLOPs: 85.0728. Time: 12.0485 us. Best GFLOPs: 97.5888
2023-11-11 11:09:08 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #488: GFLOPs: 83.5498. Time: 12.2681 us. Best GFLOPs: 97.5888
2023-11-11 11:09:08 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #489: GFLOPs: 88.3521. Time: 11.6013 us. Best GFLOPs: 97.5888
2023-11-11 11:09:08 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #490: GFLOPs: 88.3440. Time: 11.6024 us. Best GFLOPs: 97.5888
2023-11-11 11:09:08 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #491: GFLOPs: 88.3427. Time: 11.6025 us. Best GFLOPs: 97.5888
2023-11-11 11:09:08 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #492: GFLOPs: 36.3809. Time: 28.1741 us. Best GFLOPs: 97.5888
2023-11-11 11:09:08 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #493: GFLOPs: 36.3803. Time: 28.1746 us. Best GFLOPs: 97.5888
2023-11-11 11:09:08 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #494: GFLOPs: 36.3823. Time: 28.1731 us. Best GFLOPs: 97.5888
2023-11-11 11:09:08 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #495: GFLOPs: 36.3832. Time: 28.1723 us. Best GFLOPs: 97.5888
2023-11-11 11:09:08 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #496: GFLOPs: 87.5932. Time: 11.7018 us. Best GFLOPs: 97.5888
2023-11-11 11:09:08 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #497: GFLOPs: 87.6168. Time: 11.6987 us. Best GFLOPs: 97.5888
2023-11-11 11:09:08 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #498: GFLOPs: 82.1519. Time: 12.4769 us. Best GFLOPs: 97.5888
2023-11-11 11:09:08 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #499: GFLOPs: 74.1678. Time: 13.8200 us. Best GFLOPs: 97.5888
2023-11-11 11:09:08 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #500: GFLOPs: 82.3144. Time: 12.4523 us. Best GFLOPs: 97.5888
2023-11-11 11:09:08 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #501: GFLOPs: 88.6839. Time: 11.5579 us. Best GFLOPs: 97.5888
2023-11-11 11:09:08 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #502: GFLOPs: 88.6721. Time: 11.5594 us. Best GFLOPs: 97.5888
2023-11-11 11:09:08 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #503: GFLOPs: 36.3239. Time: 28.2183 us. Best GFLOPs: 97.5888
2023-11-11 11:09:08 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #504: GFLOPs: 74.2043. Time: 13.8132 us. Best GFLOPs: 97.5888
2023-11-11 11:09:08 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #505: GFLOPs: 36.3241. Time: 28.2182 us. Best GFLOPs: 97.5888
2023-11-11 11:09:08 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #506: GFLOPs: 36.0626. Time: 28.4228 us. Best GFLOPs: 97.5888
2023-11-11 11:09:08 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #507: GFLOPs: 36.0271. Time: 28.4508 us. Best GFLOPs: 97.5888
2023-11-11 11:09:08 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #508: GFLOPs: 36.0245. Time: 28.4529 us. Best GFLOPs: 97.5888
2023-11-11 11:09:08 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #509: GFLOPs: 5.9472. Time: 172.3491 us. Best GFLOPs: 97.5888
2023-11-11 11:09:08 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #510: GFLOPs: 4.8090. Time: 213.1430 us. Best GFLOPs: 97.5888
2023-11-11 11:09:08 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #511: GFLOPs: 2.9331. Time: 349.4617 us. Best GFLOPs: 97.5888
2023-11-11 11:31:05 [INFO] [evolutionary_search.cc:713] Generating candidates......
2023-11-11 11:31:05 [INFO] [evolutionary_search.cc:715] Picked top 102 candidate(s) from database
2023-11-11 11:31:06 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310a405238)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310e76ec68)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310dc33cd8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310be916f8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56310f803f18)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310eeb7a68)]: 407 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310e771ba8)]: 0 failure(s)
2023-11-11 11:31:07 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310a405238)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310e76ec68)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310dc33cd8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310be916f8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56310f803f18)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310eeb7a68)]: 812 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310e771ba8)]: 0 failure(s)
2023-11-11 11:31:07 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310a405238)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310e76ec68)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310dc33cd8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310be916f8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56310f803f18)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310eeb7a68)]: 1216 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310e771ba8)]: 0 failure(s)
2023-11-11 11:31:08 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310a405238)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310e76ec68)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310dc33cd8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310be916f8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56310f803f18)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310eeb7a68)]: 1620 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310e771ba8)]: 0 failure(s)
2023-11-11 11:31:08 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310a405238)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310e76ec68)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310dc33cd8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310be916f8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56310f803f18)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310eeb7a68)]: 2024 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310e771ba8)]: 0 failure(s)
2023-11-11 11:31:09 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310a405238)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310e76ec68)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310dc33cd8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310be916f8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56310f803f18)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310eeb7a68)]: 2425 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310e771ba8)]: 0 failure(s)
2023-11-11 11:31:10 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310a405238)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310e76ec68)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310dc33cd8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310be916f8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56310f803f18)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310eeb7a68)]: 2825 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310e771ba8)]: 0 failure(s)
2023-11-11 11:31:10 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310a405238)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310e76ec68)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310dc33cd8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310be916f8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56310f803f18)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310eeb7a68)]: 3229 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310e771ba8)]: 0 failure(s)
2023-11-11 11:31:10 [INFO] [evolutionary_search.cc:723] Sampled 51 candidate(s)
2023-11-11 11:31:12 [INFO] [evolutionary_search.cc:621] Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310a405238)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310e76ec68)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310dc33cd8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310be916f8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56310f803f18)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310eeb7a68)]: 194 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310e771ba8)]: 0 failure(s)
2023-11-11 11:31:15 [INFO] [evolutionary_search.cc:621] Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310a405238)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310e76ec68)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310dc33cd8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310be916f8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56310f803f18)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310eeb7a68)]: 174 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310e771ba8)]: 0 failure(s)
2023-11-11 11:31:18 [INFO] [evolutionary_search.cc:621] Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310a405238)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310e76ec68)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310dc33cd8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310be916f8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56310f803f18)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310eeb7a68)]: 191 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310e771ba8)]: 0 failure(s)
2023-11-11 11:31:21 [INFO] [evolutionary_search.cc:621] Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310a405238)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310e76ec68)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310dc33cd8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310be916f8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56310f803f18)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310eeb7a68)]: 178 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310e771ba8)]: 0 failure(s)
2023-11-11 11:31:22 [INFO] [evolutionary_search.cc:649] Scores of the best 64 candidates:
[1 : 16]:	1.0392  1.0339  1.0306  1.0289  1.0289  1.0281  1.0276  1.0255  1.0182  1.0160  1.0114  1.0111  1.0102  1.0102  1.0030  0.9956
[17 : 32]:	0.9861  0.9843  0.9797  0.9720  0.9667  0.9566  0.9451  0.9429  0.9420  0.9392  0.9374  0.9338  0.9331  0.9214  0.9190  0.9130
[33 : 48]:	0.9079  0.8977  0.8969  0.8913  0.8864  0.8856  0.8831  0.8828  0.8784  0.8654  0.8610  0.8609  0.8583  0.8554  0.8321  0.8306
[49 : 64]:	0.8294  0.8293  0.8272  0.8245  0.8171  0.8138  0.8107  0.8088  0.8072  0.8011  0.8011  0.7993  0.7991  0.7974  0.7974  0.7974
2023-11-11 11:31:22 [INFO] [evolutionary_search.cc:727] Got 64 candidate(s) with evolutionary search
2023-11-11 11:31:22 [INFO] [evolutionary_search.cc:730] Sending 64 candidates(s) for measurement
2023-11-11 11:31:50 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #512: GFLOPs: 81.9186. Time: 12.5124 us. Best GFLOPs: 97.5888
2023-11-11 11:31:50 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #513: GFLOPs: 80.2211. Time: 12.7772 us. Best GFLOPs: 97.5888
2023-11-11 11:31:50 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #514: GFLOPs: 80.2228. Time: 12.7769 us. Best GFLOPs: 97.5888
2023-11-11 11:31:50 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #515: GFLOPs: 85.7243. Time: 11.9569 us. Best GFLOPs: 97.5888
2023-11-11 11:31:50 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #516: GFLOPs: 85.7209. Time: 11.9574 us. Best GFLOPs: 97.5888
2023-11-11 11:31:50 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #517: GFLOPs: 85.7265. Time: 11.9566 us. Best GFLOPs: 97.5888
2023-11-11 11:31:50 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #518: GFLOPs: 81.9540. Time: 12.5070 us. Best GFLOPs: 97.5888
2023-11-11 11:31:50 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #519: GFLOPs: 83.8553. Time: 12.2234 us. Best GFLOPs: 97.5888
2023-11-11 11:31:50 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #520: GFLOPs: 80.5718. Time: 12.7216 us. Best GFLOPs: 97.5888
2023-11-11 11:31:50 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #521: GFLOPs: 80.1486. Time: 12.7888 us. Best GFLOPs: 97.5888
2023-11-11 11:31:50 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #522: GFLOPs: 85.3610. Time: 12.0078 us. Best GFLOPs: 97.5888
2023-11-11 11:31:50 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #523: GFLOPs: 85.3398. Time: 12.0108 us. Best GFLOPs: 97.5888
2023-11-11 11:31:50 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #524: GFLOPs: 85.3437. Time: 12.0103 us. Best GFLOPs: 97.5888
2023-11-11 11:31:50 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #525: GFLOPs: 85.3381. Time: 12.0111 us. Best GFLOPs: 97.5888
2023-11-11 11:31:50 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #526: GFLOPs: 83.5937. Time: 12.2617 us. Best GFLOPs: 97.5888
2023-11-11 11:31:50 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #527: GFLOPs: 81.8855. Time: 12.5175 us. Best GFLOPs: 97.5888
2023-11-11 11:31:50 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #528: GFLOPs: 80.2154. Time: 12.7781 us. Best GFLOPs: 97.5888
2023-11-11 11:31:50 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #529: GFLOPs: 85.2934. Time: 12.0173 us. Best GFLOPs: 97.5888
2023-11-11 11:31:50 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #530: GFLOPs: 87.5897. Time: 11.7023 us. Best GFLOPs: 97.5888
2023-11-11 11:31:50 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #531: GFLOPs: 87.1212. Time: 11.7652 us. Best GFLOPs: 97.5888
2023-11-11 11:31:50 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #532: GFLOPs: 87.0932. Time: 11.7690 us. Best GFLOPs: 97.5888
2023-11-11 11:31:50 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #533: GFLOPs: 89.5336. Time: 11.4482 us. Best GFLOPs: 97.5888
2023-11-11 11:31:50 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #534: GFLOPs: 68.0698. Time: 15.0581 us. Best GFLOPs: 97.5888
2023-11-11 11:31:50 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #535: GFLOPs: 59.3782. Time: 17.2622 us. Best GFLOPs: 97.5888
2023-11-11 11:31:50 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #536: GFLOPs: 89.5311. Time: 11.4485 us. Best GFLOPs: 97.5888
2023-11-11 11:31:50 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #537: GFLOPs: 89.5324. Time: 11.4484 us. Best GFLOPs: 97.5888
2023-11-11 11:31:50 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #538: GFLOPs: 89.5309. Time: 11.4486 us. Best GFLOPs: 97.5888
2023-11-11 11:31:50 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #539: GFLOPs: 59.3766. Time: 17.2627 us. Best GFLOPs: 97.5888
2023-11-11 11:31:50 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #540: GFLOPs: 59.3779. Time: 17.2623 us. Best GFLOPs: 97.5888
2023-11-11 11:31:50 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #541: GFLOPs: 36.1182. Time: 28.3790 us. Best GFLOPs: 97.5888
2023-11-11 11:31:50 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #542: GFLOPs: 66.1312. Time: 15.4995 us. Best GFLOPs: 97.5888
2023-11-11 11:31:50 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #543: GFLOPs: 66.0865. Time: 15.5100 us. Best GFLOPs: 97.5888
2023-11-11 11:31:50 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #544: GFLOPs: 66.1268. Time: 15.5005 us. Best GFLOPs: 97.5888
2023-11-11 11:31:50 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #545: GFLOPs: 87.5879. Time: 11.7025 us. Best GFLOPs: 97.5888
2023-11-11 11:31:50 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #546: GFLOPs: 83.5269. Time: 12.2715 us. Best GFLOPs: 97.5888
2023-11-11 11:31:50 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #547: GFLOPs: 83.5281. Time: 12.2713 us. Best GFLOPs: 97.5888
2023-11-11 11:31:50 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #548: GFLOPs: 84.0001. Time: 12.2024 us. Best GFLOPs: 97.5888
2023-11-11 11:31:50 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #549: GFLOPs: 87.5859. Time: 11.7028 us. Best GFLOPs: 97.5888
2023-11-11 11:31:50 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #550: GFLOPs: 87.5848. Time: 11.7029 us. Best GFLOPs: 97.5888
2023-11-11 11:31:50 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #551: GFLOPs: 87.5809. Time: 11.7035 us. Best GFLOPs: 97.5888
2023-11-11 11:31:50 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #552: GFLOPs: 87.5783. Time: 11.7038 us. Best GFLOPs: 97.5888
2023-11-11 11:31:50 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #553: GFLOPs: 87.5885. Time: 11.7024 us. Best GFLOPs: 97.5888
2023-11-11 11:31:50 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #554: GFLOPs: 83.9520. Time: 12.2094 us. Best GFLOPs: 97.5888
2023-11-11 11:31:50 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #555: GFLOPs: 93.6232. Time: 10.9481 us. Best GFLOPs: 97.5888
2023-11-11 11:31:50 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #556: GFLOPs: 82.2674. Time: 12.4594 us. Best GFLOPs: 97.5888
2023-11-11 11:31:50 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #557: GFLOPs: 87.0695. Time: 11.7722 us. Best GFLOPs: 97.5888
2023-11-11 11:31:50 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #558: GFLOPs: 36.0295. Time: 28.4489 us. Best GFLOPs: 97.5888
2023-11-11 11:31:50 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #559: GFLOPs: 93.0779. Time: 11.0123 us. Best GFLOPs: 97.5888
2023-11-11 11:31:50 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #560: GFLOPs: 42.9984. Time: 23.8381 us. Best GFLOPs: 97.5888
2023-11-11 11:31:50 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #561: GFLOPs: 36.0309. Time: 28.4478 us. Best GFLOPs: 97.5888
2023-11-11 11:31:50 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #562: GFLOPs: 93.6666. Time: 10.9431 us. Best GFLOPs: 97.5888
2023-11-11 11:31:50 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #563: GFLOPs: 36.1180. Time: 28.3792 us. Best GFLOPs: 97.5888
2023-11-11 11:31:50 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #564: GFLOPs: 90.9946. Time: 11.2644 us. Best GFLOPs: 97.5888
2023-11-11 11:31:50 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #565: GFLOPs: 53.1655. Time: 19.2794 us. Best GFLOPs: 97.5888
2023-11-11 11:31:50 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #566: GFLOPs: 41.5516. Time: 24.6681 us. Best GFLOPs: 97.5888
2023-11-11 11:31:50 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #567: GFLOPs: 73.8318. Time: 13.8829 us. Best GFLOPs: 97.5888
2023-11-11 11:31:50 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #568: GFLOPs: 93.6574. Time: 10.9441 us. Best GFLOPs: 97.5888
2023-11-11 11:31:50 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #569: GFLOPs: 73.2256. Time: 13.9978 us. Best GFLOPs: 97.5888
2023-11-11 11:31:50 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #570: GFLOPs: 73.0492. Time: 14.0316 us. Best GFLOPs: 97.5888
2023-11-11 11:31:50 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #571: GFLOPs: 36.0504. Time: 28.4324 us. Best GFLOPs: 97.5888
2023-11-11 11:31:50 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #572: GFLOPs: 53.1622. Time: 19.2806 us. Best GFLOPs: 97.5888
2023-11-11 11:31:50 [INFO] [task_scheduler.cc:121] [Task #28: fused_nn_dense_add] Trial #573: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/home2/xiachunwei/Software/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/home2/xiachunwei/Software/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 159, in _worker_func
    costs: List[float] = f_run_evaluator(
  File "/home2/xiachunwei/Software/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 387, in default_run_evaluator
    return run_evaluator_common(rt_mod, device, evaluator_config, repeated_args)
  File "/home2/xiachunwei/Software/tvm/python/tvm/meta_schedule/runner/utils.py", line 117, in run_evaluator_common
    profile_result = evaluator(*args)
  File "/home2/xiachunwei/Software/tvm/python/tvm/runtime/module.py", line 397, in evaluator
    blob = feval(*args)
  File "/home2/xiachunwei/Software/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/home2/xiachunwei/Software/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  1: tvm::runtime::profiling::WrapTimeEvaluator(tvm::runtime::PackedFunc, DLDevice, int, int, int, int, int, int, int, tvm::runtime::PackedFunc)::{lambda(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*) [clone .isra.779]
  0: tvm::runtime::CUDADeviceAPI::StreamSync(DLDevice, void*)
  File "/home2/xiachunwei/Software/tvm/src/runtime/cuda/cuda_device_api.cc", line 212
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(512)), "float32"), p1: T.Buffer((T.int64(1000), T.int64(512)), "float32"), p2: T.Buffer((T.int64(1), T.int64(1000)), "float32"), T_add: T.Buffer((T.int64(1), T.int64(1000)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        T_matmul_NT_local = T.alloc_buffer((T.int64(1), T.int64(1000)), scope="local")
        p0_shared = T.alloc_buffer((T.int64(1), T.int64(512)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(1000), T.int64(512)), scope="shared")
        for i_0_j_0_fused in T.thread_binding(T.int64(1), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 512, "pragma_unroll_explicit": 1}):
            for i_1_j_1_fused in T.thread_binding(T.int64(10), thread="vthread.x"):
                for i_2_j_2_fused in T.thread_binding(T.int64(50), thread="threadIdx.x"):
                    for i_3_init, j_3_init, i_4_init, j_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(2)):
                        with T.block("T_matmul_NT_init"):
                            v_i = T.axis.spatial(T.int64(1), i_3_init + i_4_init)
                            v_j = T.axis.spatial(T.int64(1000), i_1_j_1_fused * T.int64(100) + i_2_j_2_fused * T.int64(2) + j_3_init * T.int64(2) + j_4_init)
                            T.reads()
                            T.writes(T_matmul_NT_local[v_i, v_j])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            T_matmul_NT_local[v_i, v_j] = T.float32(0)
                    for k_0_fused in T.serial(T.int64(512), annotations={"software_pipeline_async_stages": [0], "software_pipeline_order": [0, 1, 2], "software_pipeline_stage": [0, 0, 2]}):
                        for ax0_ax1_fused_0 in range(T.int64(1)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(50), thread="threadIdx.x"):
                                with T.block("p0_shared"):
                                    v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                    v1 = T.axis.spatial(T.int64(512), k_0_fused)
                                    T.where(ax0_ax1_fused_0 * T.int64(50) + ax0_ax1_fused_1 < T.int64(1))
                                    T.reads(p0[v0, v1])
                                    T.writes(p0_shared[v0, v1])
                                    p0_shared[v0, v1] = p0[v0, v1]
                        for ax0_ax1_fused_0 in range(T.int64(10)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(50), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("p1_shared"):
                                        v0 = T.axis.spatial(T.int64(1000), ax0_ax1_fused_0 * T.int64(100) + ax0_ax1_fused_1 * T.int64(2) + ax0_ax1_fused_2)
                                        v1 = T.axis.spatial(T.int64(512), k_0_fused)
                                        T.reads(p1[v0, v1])
                                        T.writes(p1_shared[v0, v1])
                                        p1_shared[v0, v1] = p1[v0, v1]
                        for k_1, i_3, j_3, k_2, i_4, j_4 in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(2)):
                            with T.block("T_matmul_NT_update"):
                                v_i = T.axis.spatial(T.int64(1), i_3 + i_4)
                                v_j = T.axis.spatial(T.int64(1000), i_1_j_1_fused * T.int64(100) + i_2_j_2_fused * T.int64(2) + j_3 * T.int64(2) + j_4)
                                v_k = T.axis.reduce(T.int64(512), k_0_fused + k_1 + k_2)
                                T.reads(T_matmul_NT_local[v_i, v_j], p0_shared[v_i, v_k], p1_shared[v_j, v_k])
                                T.writes(T_matmul_NT_local[v_i, v_j])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                T_matmul_NT_local[v_i, v_j] = T_matmul_NT_local[v_i, v_j] + p0_shared[v_i, v_k] * p1_shared[v_j, v_k]
                    for ax0, ax1 in T.grid(T.int64(1), T.int64(2)):
                        with T.block("T_matmul_NT_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(1000), i_1_j_1_fused * T.int64(100) + i_2_j_2_fused * T.int64(2) + ax1)
                            T.reads(T_matmul_NT_local[v0, v1], p2[v0, v1])
                            T.writes(T_add[v0, v1])
                            T_add[v0, v1] = T_matmul_NT_local[v0, v1] + p2[v0, v1]
b0 = sch.get_block(name="T_matmul_NT", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l3, l4, l5 = sch.get_loops(block=b0)
v6, v7, v8, v9, v10 = sch.sample_perfect_tile(loop=l3, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l11, l12, l13, l14, l15 = sch.split(loop=l3, factors=[v6, v7, v8, v9, v10], preserve_unit_iters=True)
v16, v17, v18, v19, v20 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[1, 10, 50, 1, 2])
l21, l22, l23, l24, l25 = sch.split(loop=l4, factors=[v16, v17, v18, v19, v20], preserve_unit_iters=True)
v26, v27, v28 = sch.sample_perfect_tile(loop=l5, n=3, max_innermost_factor=64, decision=[512, 1, 1])
l29, l30, l31 = sch.split(loop=l5, factors=[v26, v27, v28], preserve_unit_iters=True)
sch.reorder(l11, l21, l12, l22, l13, l23, l29, l30, l14, l24, l31, l15, l25)
l32 = sch.fuse(l11, l21, preserve_unit_iters=True)
sch.bind(loop=l32, thread_axis="blockIdx.x")
l33 = sch.fuse(l12, l22, preserve_unit_iters=True)
sch.bind(loop=l33, thread_axis="vthread.x")
l34 = sch.fuse(l13, l23, preserve_unit_iters=True)
sch.bind(loop=l34, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b35 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b35, loop=l34, preserve_unit_loops=True, index=-1)
b36 = sch.cache_read(block=b0, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b36, loop=l29, preserve_unit_loops=True, index=-1)
l37, l38, l39, l40, l41, l42 = sch.get_loops(block=b36)
l43 = sch.fuse(l41, l42, preserve_unit_iters=True)
v44 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b36, ann_key="meta_schedule.cooperative_fetch", ann_val=v44)
b45 = sch.cache_read(block=b0, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b45, loop=l29, preserve_unit_loops=True, index=-1)
l46, l47, l48, l49, l50, l51 = sch.get_loops(block=b45)
l52 = sch.fuse(l50, l51, preserve_unit_iters=True)
v53 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b45, ann_key="meta_schedule.cooperative_fetch", ann_val=v53)
l54 = sch.fuse(l29, preserve_unit_iters=True)
sch.annotate(block_or_loop=l54, ann_key="software_pipeline_stage", ann_val=[0, 0, 2])
sch.annotate(block_or_loop=l54, ann_key="software_pipeline_order", ann_val=[0, 1, 2])
sch.annotate(block_or_loop=l54, ann_key="software_pipeline_async_stages", ann_val=[0])
sch.reverse_compute_inline(block=b1)
v55 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v55)
sch.enter_postproc()
sch.unannotate(block_or_loop=b36, ann_key="meta_schedule.cooperative_fetch")
l56, l57, l58, l59, l60 = sch.get_loops(block=b36)
l61, l62 = sch.split(loop=l60, factors=[None, 50], preserve_unit_iters=True)
sch.bind(loop=l62, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b45, ann_key="meta_schedule.cooperative_fetch")
l63, l64, l65, l66, l67 = sch.get_loops(block=b45)
l68, l69, l70 = sch.split(loop=l67, factors=[None, 50, 2], preserve_unit_iters=True)
sch.vectorize(loop=l70)
sch.bind(loop=l69, thread_axis="threadIdx.x")
b71 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b71, ann_key="meta_schedule.unroll_explicit")
b72, b73, b74, b75 = sch.get_child_blocks(b71)
l76, l77, l78, l79, l80, l81 = sch.get_loops(block=b72)
l82, l83, l84, l85, l86, l87, l88 = sch.get_loops(block=b73)
l89, l90, l91, l92, l93, l94, l95, l96, l97, l98 = sch.get_loops(block=b74)
sch.annotate(block_or_loop=l89, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l89, ann_key="pragma_unroll_explicit", ann_val=1)
l99, l100, l101, l102, l103 = sch.get_loops(block=b75)
b104 = sch.get_block(name="T_matmul_NT", func_name="main")
l105, l106, l107, l108, l109, l110, l111, l112, l113, l114 = sch.get_loops(block=b104)
b115 = sch.decompose_reduction(block=b104, loop=l108)
2023-11-11 11:31:50 [INFO] [task_scheduler.cc:121] [Task #28: fused_nn_dense_add] Trial #574: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/home2/xiachunwei/Software/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/home2/xiachunwei/Software/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/home2/xiachunwei/Software/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/home2/xiachunwei/Software/tvm/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/home2/xiachunwei/Software/tvm/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/home2/xiachunwei/Software/tvm/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/home2/xiachunwei/Software/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/home2/xiachunwei/Software/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  4: _ZN3tvm7runtime13PackedFun
  3: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  2: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  0: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  File "/home2/xiachunwei/Software/tvm/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(512)), "float32"), p1: T.Buffer((T.int64(1000), T.int64(512)), "float32"), p2: T.Buffer((T.int64(1), T.int64(1000)), "float32"), T_add: T.Buffer((T.int64(1), T.int64(1000)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        T_matmul_NT_local = T.alloc_buffer((T.int64(1), T.int64(1000)), scope="local")
        p0_shared = T.alloc_buffer((T.int64(1), T.int64(512)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(1000), T.int64(512)), scope="shared")
        for i_0_j_0_fused in T.thread_binding(T.int64(5), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 512, "pragma_unroll_explicit": 1}):
            for i_1_j_1_fused in T.thread_binding(T.int64(4), thread="vthread.x"):
                for i_2_j_2_fused in T.thread_binding(T.int64(50), thread="threadIdx.x"):
                    for i_3_init, j_3_init, i_4_init, j_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_init"):
                            v_i = T.axis.spatial(T.int64(1), i_3_init + i_4_init)
                            v_j = T.axis.spatial(T.int64(1000), i_0_j_0_fused * T.int64(200) + i_1_j_1_fused * T.int64(50) + i_2_j_2_fused + j_3_init + j_4_init)
                            T.reads()
                            T.writes(T_matmul_NT_local[v_i, v_j])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            T_matmul_NT_local[v_i, v_j] = T.float32(0)
                    for k_0_fused in T.serial(T.int64(32), annotations={"software_pipeline_async_stages": [0], "software_pipeline_order": [0, 1, 2], "software_pipeline_stage": [0, 0, 2]}):
                        for ax0_ax1_fused_0 in range(T.int64(1)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(50), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("p0_shared"):
                                        v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v1 = T.axis.spatial(T.int64(512), k_0_fused * T.int64(16) + (ax0_ax1_fused_0 * T.int64(200) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2))
                                        T.where((ax0_ax1_fused_0 * T.int64(50) + ax0_ax1_fused_1) * T.int64(4) + ax0_ax1_fused_2 < T.int64(16))
                                        T.reads(p0[v0, v1])
                                        T.writes(p0_shared[v0, v1])
                                        p0_shared[v0, v1] = p0[v0, v1]
                        for ax0_ax1_fused_0 in range(T.int64(64)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(50), thread="threadIdx.x"):
                                with T.block("p1_shared"):
                                    v0 = T.axis.spatial(T.int64(1000), i_0_j_0_fused * T.int64(200) + (ax0_ax1_fused_0 * T.int64(50) + ax0_ax1_fused_1) // T.int64(16))
                                    v1 = T.axis.spatial(T.int64(512), k_0_fused * T.int64(16) + (ax0_ax1_fused_0 * T.int64(50) + ax0_ax1_fused_1) % T.int64(16))
                                    T.reads(p1[v0, v1])
                                    T.writes(p1_shared[v0, v1])
                                    p1_shared[v0, v1] = p1[v0, v1]
                        for k_1, i_3, j_3, k_2, i_4, j_4 in T.grid(T.int64(2), T.int64(1), T.int64(1), T.int64(8), T.int64(1), T.int64(1)):
                            with T.block("T_matmul_NT_update"):
                                v_i = T.axis.spatial(T.int64(1), i_3 + i_4)
                                v_j = T.axis.spatial(T.int64(1000), i_0_j_0_fused * T.int64(200) + i_1_j_1_fused * T.int64(50) + i_2_j_2_fused + j_3 + j_4)
                                v_k = T.axis.reduce(T.int64(512), k_0_fused * T.int64(16) + k_1 * T.int64(8) + k_2)
                                T.reads(T_matmul_NT_local[v_i, v_j], p0_shared[v_i, v_k], p1_shared[v_j, v_k])
                                T.writes(T_matmul_NT_local[v_i, v_j])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                T_matmul_NT_local[v_i, v_j] = T_matmul_NT_local[v_i, v_j] + p0_shared[v_i, v_k] * p1_shared[v_j, v_k]
                    for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(1000), i_0_j_0_fused * T.int64(200) + i_1_j_1_fused * T.int64(50) + i_2_j_2_fused + ax1)
                            T.reads(T_matmul_NT_local[v0, v1], p2[v0, v1])
                            T.writes(T_add[v0, v1])
                            T_add[v0, v1] = T_matmul_NT_local[v0, v1] + p2[v0, v1]
b0 = sch.get_block(name="T_matmul_NT", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l3, l4, l5 = sch.get_loops(block=b0)
v6, v7, v8, v9, v10 = sch.sample_perfect_tile(loop=l3, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l11, l12, l13, l14, l15 = sch.split(loop=l3, factors=[v6, v7, v8, v9, v10], preserve_unit_iters=True)
v16, v17, v18, v19, v20 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[5, 4, 50, 1, 1])
l21, l22, l23, l24, l25 = sch.split(loop=l4, factors=[v16, v17, v18, v19, v20], preserve_unit_iters=True)
v26, v27, v28 = sch.sample_perfect_tile(loop=l5, n=3, max_innermost_factor=64, decision=[32, 2, 8])
l29, l30, l31 = sch.split(loop=l5, factors=[v26, v27, v28], preserve_unit_iters=True)
sch.reorder(l11, l21, l12, l22, l13, l23, l29, l30, l14, l24, l31, l15, l25)
l32 = sch.fuse(l11, l21, preserve_unit_iters=True)
sch.bind(loop=l32, thread_axis="blockIdx.x")
l33 = sch.fuse(l12, l22, preserve_unit_iters=True)
sch.bind(loop=l33, thread_axis="vthread.x")
l34 = sch.fuse(l13, l23, preserve_unit_iters=True)
sch.bind(loop=l34, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b35 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b35, loop=l34, preserve_unit_loops=True, index=-1)
b36 = sch.cache_read(block=b0, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b36, loop=l29, preserve_unit_loops=True, index=-1)
l37, l38, l39, l40, l41, l42 = sch.get_loops(block=b36)
l43 = sch.fuse(l41, l42, preserve_unit_iters=True)
v44 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b36, ann_key="meta_schedule.cooperative_fetch", ann_val=v44)
b45 = sch.cache_read(block=b0, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b45, loop=l29, preserve_unit_loops=True, index=-1)
l46, l47, l48, l49, l50, l51 = sch.get_loops(block=b45)
l52 = sch.fuse(l50, l51, preserve_unit_iters=True)
v53 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b45, ann_key="meta_schedule.cooperative_fetch", ann_val=v53)
l54 = sch.fuse(l29, preserve_unit_iters=True)
sch.annotate(block_or_loop=l54, ann_key="software_pipeline_stage", ann_val=[0, 0, 2])
sch.annotate(block_or_loop=l54, ann_key="software_pipeline_order", ann_val=[0, 1, 2])
sch.annotate(block_or_loop=l54, ann_key="software_pipeline_async_stages", ann_val=[0])
sch.reverse_compute_inline(block=b1)
v55 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v55)
sch.enter_postproc()
sch.unannotate(block_or_loop=b36, ann_key="meta_schedule.cooperative_fetch")
l56, l57, l58, l59, l60 = sch.get_loops(block=b36)
l61, l62, l63 = sch.split(loop=l60, factors=[None, 50, 4], preserve_unit_iters=True)
sch.vectorize(loop=l63)
sch.bind(loop=l62, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b45, ann_key="meta_schedule.cooperative_fetch")
l64, l65, l66, l67, l68 = sch.get_loops(block=b45)
l69, l70 = sch.split(loop=l68, factors=[None, 50], preserve_unit_iters=True)
sch.bind(loop=l70, thread_axis="threadIdx.x")
b71 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b71, ann_key="meta_schedule.unroll_explicit")
b72, b73, b74, b75 = sch.get_child_blocks(b71)
l76, l77, l78, l79, l80, l81, l82 = sch.get_loops(block=b72)
l83, l84, l85, l86, l87, l88 = sch.get_loops(block=b73)
l89, l90, l91, l92, l93, l94, l95, l96, l97, l98 = sch.get_loops(block=b74)
sch.annotate(block_or_loop=l89, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l89, ann_key="pragma_unroll_explicit", ann_val=1)
l99, l100, l101, l102, l103 = sch.get_loops(block=b75)
b104 = sch.get_block(name="T_matmul_NT", func_name="main")
l105, l106, l107, l108, l109, l110, l111, l112, l113, l114 = sch.get_loops(block=b104)
b115 = sch.decompose_reduction(block=b104, loop=l108)
2023-11-11 11:31:50 [INFO] [task_scheduler.cc:121] [Task #28: fused_nn_dense_add] Trial #575: Error in running:
LocalRunner: An exception occurred
Subprocess terminated
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(512)), "float32"), p1: T.Buffer((T.int64(1000), T.int64(512)), "float32"), p2: T.Buffer((T.int64(1), T.int64(1000)), "float32"), T_add: T.Buffer((T.int64(1), T.int64(1000)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        T_matmul_NT_local = T.alloc_buffer((T.int64(1), T.int64(1000)), scope="local")
        p0_shared = T.alloc_buffer((T.int64(1), T.int64(512)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(1000), T.int64(512)), scope="shared")
        for i_0_j_0_fused in T.thread_binding(T.int64(1), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 64, "pragma_unroll_explicit": 1}):
            for i_1_j_1_fused in T.thread_binding(T.int64(2), thread="vthread.x"):
                for i_2_j_2_fused in T.thread_binding(T.int64(500), thread="threadIdx.x"):
                    for i_3_init, j_3_init, i_4_init, j_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_init"):
                            v_i = T.axis.spatial(T.int64(1), i_3_init + i_4_init)
                            v_j = T.axis.spatial(T.int64(1000), i_1_j_1_fused * T.int64(500) + i_2_j_2_fused + j_3_init + j_4_init)
                            T.reads()
                            T.writes(T_matmul_NT_local[v_i, v_j])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            T_matmul_NT_local[v_i, v_j] = T.float32(0)
                    for k_0 in range(T.int64(64)):
                        for ax0_ax1_fused_0 in range(T.int64(1)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(500), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("p0_shared"):
                                        v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v1 = T.axis.spatial(T.int64(512), k_0 * T.int64(8) + (ax0_ax1_fused_0 * T.int64(1000) + ax0_ax1_fused_1 * T.int64(2) + ax0_ax1_fused_2))
                                        T.where((ax0_ax1_fused_0 * T.int64(500) + ax0_ax1_fused_1) * T.int64(2) + ax0_ax1_fused_2 < T.int64(8))
                                        T.reads(p0[v0, v1])
                                        T.writes(p0_shared[v0, v1])
                                        p0_shared[v0, v1] = p0[v0, v1]
                        for ax0_ax1_fused_0 in range(T.int64(8)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(500), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("p1_shared"):
                                        v0 = T.axis.spatial(T.int64(1000), (ax0_ax1_fused_0 * T.int64(1000) + ax0_ax1_fused_1 * T.int64(2) + ax0_ax1_fused_2) // T.int64(8))
                                        v1 = T.axis.spatial(T.int64(512), k_0 * T.int64(8) + (ax0_ax1_fused_0 * T.int64(1000) + ax0_ax1_fused_1 * T.int64(2) + ax0_ax1_fused_2) % T.int64(8))
                                        T.reads(p1[v0, v1])
                                        T.writes(p1_shared[v0, v1])
                                        p1_shared[v0, v1] = p1[v0, v1]
                        for k_1, i_3, j_3, k_2, i_4, j_4 in T.grid(T.int64(2), T.int64(1), T.int64(1), T.int64(4), T.int64(1), T.int64(1)):
                            with T.block("T_matmul_NT_update"):
                                v_i = T.axis.spatial(T.int64(1), i_3 + i_4)
                                v_j = T.axis.spatial(T.int64(1000), i_1_j_1_fused * T.int64(500) + i_2_j_2_fused + j_3 + j_4)
                                v_k = T.axis.reduce(T.int64(512), k_0 * T.int64(8) + k_1 * T.int64(4) + k_2)
                                T.reads(T_matmul_NT_local[v_i, v_j], p0_shared[v_i, v_k], p1_shared[v_j, v_k])
                                T.writes(T_matmul_NT_local[v_i, v_j])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                T_matmul_NT_local[v_i, v_j] = T_matmul_NT_local[v_i, v_j] + p0_shared[v_i, v_k] * p1_shared[v_j, v_k]
                    for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(1000), i_1_j_1_fused * T.int64(500) + i_2_j_2_fused + ax1)
                            T.reads(T_matmul_NT_local[v0, v1], p2[v0, v1])
                            T.writes(T_add[v0, v1])
                            T_add[v0, v1] = T_matmul_NT_local[v0, v1] + p2[v0, v1]
b0 = sch.get_block(name="T_matmul_NT", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l3, l4, l5 = sch.get_loops(block=b0)
v6, v7, v8, v9, v10 = sch.sample_perfect_tile(loop=l3, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l11, l12, l13, l14, l15 = sch.split(loop=l3, factors=[v6, v7, v8, v9, v10], preserve_unit_iters=True)
v16, v17, v18, v19, v20 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[1, 2, 500, 1, 1])
l21, l22, l23, l24, l25 = sch.split(loop=l4, factors=[v16, v17, v18, v19, v20], preserve_unit_iters=True)
v26, v27, v28 = sch.sample_perfect_tile(loop=l5, n=3, max_innermost_factor=64, decision=[64, 2, 4])
l29, l30, l31 = sch.split(loop=l5, factors=[v26, v27, v28], preserve_unit_iters=True)
sch.reorder(l11, l21, l12, l22, l13, l23, l29, l30, l14, l24, l31, l15, l25)
l32 = sch.fuse(l11, l21, preserve_unit_iters=True)
sch.bind(loop=l32, thread_axis="blockIdx.x")
l33 = sch.fuse(l12, l22, preserve_unit_iters=True)
sch.bind(loop=l33, thread_axis="vthread.x")
l34 = sch.fuse(l13, l23, preserve_unit_iters=True)
sch.bind(loop=l34, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b35 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b35, loop=l34, preserve_unit_loops=True, index=-1)
b36 = sch.cache_read(block=b0, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b36, loop=l29, preserve_unit_loops=True, index=-1)
l37, l38, l39, l40, l41, l42 = sch.get_loops(block=b36)
l43 = sch.fuse(l41, l42, preserve_unit_iters=True)
v44 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b36, ann_key="meta_schedule.cooperative_fetch", ann_val=v44)
b45 = sch.cache_read(block=b0, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b45, loop=l29, preserve_unit_loops=True, index=-1)
l46, l47, l48, l49, l50, l51 = sch.get_loops(block=b45)
l52 = sch.fuse(l50, l51, preserve_unit_iters=True)
v53 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b45, ann_key="meta_schedule.cooperative_fetch", ann_val=v53)
sch.reverse_compute_inline(block=b1)
v54 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=2)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v54)
sch.enter_postproc()
sch.unannotate(block_or_loop=b36, ann_key="meta_schedule.cooperative_fetch")
l55, l56, l57, l58, l59 = sch.get_loops(block=b36)
l60, l61, l62 = sch.split(loop=l59, factors=[None, 500, 2], preserve_unit_iters=True)
sch.vectorize(loop=l62)
sch.bind(loop=l61, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b45, ann_key="meta_schedule.cooperative_fetch")
l63, l64, l65, l66, l67 = sch.get_loops(block=b45)
l68, l69, l70 = sch.split(loop=l67, factors=[None, 500, 2], preserve_unit_iters=True)
sch.vectorize(loop=l70)
sch.bind(loop=l69, thread_axis="threadIdx.x")
b71 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b71, ann_key="meta_schedule.unroll_explicit")
b72, b73, b74, b75 = sch.get_child_blocks(b71)
l76, l77, l78, l79, l80, l81, l82 = sch.get_loops(block=b72)
l83, l84, l85, l86, l87, l88, l89 = sch.get_loops(block=b73)
l90, l91, l92, l93, l94, l95, l96, l97, l98, l99 = sch.get_loops(block=b74)
sch.annotate(block_or_loop=l90, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l90, ann_key="pragma_unroll_explicit", ann_val=1)
l100, l101, l102, l103, l104 = sch.get_loops(block=b75)
b105 = sch.get_block(name="T_matmul_NT", func_name="main")
l106, l107, l108, l109, l110, l111, l112, l113, l114, l115 = sch.get_loops(block=b105)
b116 = sch.decompose_reduction(block=b105, loop=l109)
2023-11-11 11:44:09 [INFO] [evolutionary_search.cc:713] Generating candidates......
2023-11-11 11:44:10 [INFO] [evolutionary_search.cc:715] Picked top 102 candidate(s) from database
2023-11-11 11:44:10 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310a405238)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310e76ec68)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310dc33cd8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310be916f8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56310f803f18)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310eeb7a68)]: 399 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310e771ba8)]: 0 failure(s)
2023-11-11 11:44:11 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310a405238)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310e76ec68)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310dc33cd8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310be916f8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56310f803f18)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310eeb7a68)]: 801 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310e771ba8)]: 0 failure(s)
2023-11-11 11:44:12 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310a405238)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310e76ec68)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310dc33cd8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310be916f8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56310f803f18)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310eeb7a68)]: 1205 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310e771ba8)]: 0 failure(s)
2023-11-11 11:44:12 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310a405238)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310e76ec68)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310dc33cd8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310be916f8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56310f803f18)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310eeb7a68)]: 1610 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310e771ba8)]: 0 failure(s)
2023-11-11 11:44:13 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310a405238)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310e76ec68)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310dc33cd8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310be916f8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56310f803f18)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310eeb7a68)]: 2011 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310e771ba8)]: 0 failure(s)
2023-11-11 11:44:14 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310a405238)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310e76ec68)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310dc33cd8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310be916f8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56310f803f18)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310eeb7a68)]: 2419 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310e771ba8)]: 0 failure(s)
2023-11-11 11:44:14 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310a405238)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310e76ec68)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310dc33cd8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310be916f8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56310f803f18)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310eeb7a68)]: 2826 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310e771ba8)]: 0 failure(s)
2023-11-11 11:44:15 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310a405238)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310e76ec68)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310dc33cd8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310be916f8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56310f803f18)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310eeb7a68)]: 3225 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310e771ba8)]: 0 failure(s)
2023-11-11 11:44:15 [INFO] [evolutionary_search.cc:723] Sampled 55 candidate(s)
2023-11-11 11:44:17 [INFO] [evolutionary_search.cc:621] Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310a405238)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310e76ec68)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310dc33cd8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310be916f8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56310f803f18)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310eeb7a68)]: 184 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310e771ba8)]: 0 failure(s)
2023-11-11 11:44:20 [INFO] [evolutionary_search.cc:621] Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310a405238)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310e76ec68)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310dc33cd8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310be916f8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56310f803f18)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310eeb7a68)]: 169 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310e771ba8)]: 0 failure(s)
2023-11-11 11:44:23 [INFO] [evolutionary_search.cc:621] Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310a405238)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310e76ec68)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310dc33cd8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310be916f8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56310f803f18)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310eeb7a68)]: 177 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310e771ba8)]: 0 failure(s)
2023-11-11 11:44:26 [INFO] [evolutionary_search.cc:621] Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310a405238)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310e76ec68)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310dc33cd8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310be916f8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56310f803f18)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310eeb7a68)]: 156 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310e771ba8)]: 0 failure(s)
2023-11-11 11:44:27 [INFO] [evolutionary_search.cc:649] Scores of the best 64 candidates:
[1 : 16]:	1.0417  1.0271  1.0230  1.0152  1.0151  1.0094  1.0014  0.9843  0.9831  0.9648  0.9446  0.9446  0.9420  0.9420  0.9389  0.9355
[17 : 32]:	0.9338  0.9242  0.9170  0.9072  0.9021  0.8969  0.8864  0.8819  0.8636  0.8610  0.8583  0.8564  0.8361  0.8221  0.8172  0.8169
[33 : 48]:	0.8142  0.8088  0.8084  0.8044  0.8011  0.8011  0.7974  0.7974  0.7962  0.7962  0.7961  0.7927  0.7915  0.7868  0.7862  0.7852
[49 : 64]:	0.7852  0.7815  0.7785  0.7773  0.7773  0.7741  0.7741  0.7729  0.7729  0.7729  0.7729  0.7695  0.7682  0.7676  0.7657  0.7645
2023-11-11 11:44:27 [INFO] [evolutionary_search.cc:727] Got 64 candidate(s) with evolutionary search
2023-11-11 11:44:27 [INFO] [evolutionary_search.cc:730] Sending 64 candidates(s) for measurement
2023-11-11 11:44:55 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #576: GFLOPs: 81.5712. Time: 12.5657 us. Best GFLOPs: 97.5888
2023-11-11 11:44:55 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #577: GFLOPs: 83.0377. Time: 12.3438 us. Best GFLOPs: 97.5888
2023-11-11 11:44:55 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #578: GFLOPs: 86.9109. Time: 11.7937 us. Best GFLOPs: 97.5888
2023-11-11 11:44:55 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #579: GFLOPs: 86.8737. Time: 11.7987 us. Best GFLOPs: 97.5888
2023-11-11 11:44:55 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #580: GFLOPs: 80.0416. Time: 12.8058 us. Best GFLOPs: 97.5888
2023-11-11 11:44:55 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #581: GFLOPs: 86.1503. Time: 11.8978 us. Best GFLOPs: 97.5888
2023-11-11 11:44:55 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #582: GFLOPs: 81.5537. Time: 12.5684 us. Best GFLOPs: 97.5888
2023-11-11 11:44:55 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #583: GFLOPs: 86.4832. Time: 11.8520 us. Best GFLOPs: 97.5888
2023-11-11 11:44:55 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #584: GFLOPs: 81.5664. Time: 12.5665 us. Best GFLOPs: 97.5888
2023-11-11 11:44:55 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #585: GFLOPs: 87.8307. Time: 11.6702 us. Best GFLOPs: 97.5888
2023-11-11 11:44:55 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #586: GFLOPs: 69.2738. Time: 14.7964 us. Best GFLOPs: 97.5888
2023-11-11 11:44:55 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #587: GFLOPs: 89.5273. Time: 11.4490 us. Best GFLOPs: 97.5888
2023-11-11 11:44:55 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #588: GFLOPs: 69.2683. Time: 14.7975 us. Best GFLOPs: 97.5888
2023-11-11 11:44:55 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #589: GFLOPs: 69.2684. Time: 14.7975 us. Best GFLOPs: 97.5888
2023-11-11 11:44:55 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #590: GFLOPs: 69.2684. Time: 14.7975 us. Best GFLOPs: 97.5888
2023-11-11 11:44:55 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #591: GFLOPs: 69.2692. Time: 14.7973 us. Best GFLOPs: 97.5888
2023-11-11 11:44:55 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #592: GFLOPs: 59.4318. Time: 17.2467 us. Best GFLOPs: 97.5888
2023-11-11 11:44:55 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #593: GFLOPs: 36.3815. Time: 28.1736 us. Best GFLOPs: 97.5888
2023-11-11 11:44:55 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #594: GFLOPs: 36.3815. Time: 28.1736 us. Best GFLOPs: 97.5888
2023-11-11 11:44:55 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #595: GFLOPs: 66.2139. Time: 15.4801 us. Best GFLOPs: 97.5888
2023-11-11 11:44:55 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #596: GFLOPs: 36.0930. Time: 28.3988 us. Best GFLOPs: 97.5888
2023-11-11 11:44:55 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #597: GFLOPs: 84.6502. Time: 12.1087 us. Best GFLOPs: 97.5888
2023-11-11 11:44:55 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #598: GFLOPs: 84.5202. Time: 12.1273 us. Best GFLOPs: 97.5888
2023-11-11 11:44:55 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #599: GFLOPs: 36.3811. Time: 28.1740 us. Best GFLOPs: 97.5888
2023-11-11 11:44:55 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #600: GFLOPs: 87.8405. Time: 11.6689 us. Best GFLOPs: 97.5888
2023-11-11 11:44:55 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #601: GFLOPs: 83.0742. Time: 12.3384 us. Best GFLOPs: 97.5888
2023-11-11 11:44:55 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #602: GFLOPs: 82.9853. Time: 12.3516 us. Best GFLOPs: 97.5888
2023-11-11 11:44:55 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #603: GFLOPs: 83.0788. Time: 12.3377 us. Best GFLOPs: 97.5888
2023-11-11 11:44:55 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #604: GFLOPs: 88.8254. Time: 11.5395 us. Best GFLOPs: 97.5888
2023-11-11 11:44:55 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #605: GFLOPs: 36.0967. Time: 28.3959 us. Best GFLOPs: 97.5888
2023-11-11 11:44:55 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #606: GFLOPs: 93.6121. Time: 10.9494 us. Best GFLOPs: 97.5888
2023-11-11 11:44:55 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #607: GFLOPs: 36.0936. Time: 28.3984 us. Best GFLOPs: 97.5888
2023-11-11 11:44:55 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #608: GFLOPs: 36.3813. Time: 28.1738 us. Best GFLOPs: 97.5888
2023-11-11 11:44:55 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #609: GFLOPs: 93.2788. Time: 10.9886 us. Best GFLOPs: 97.5888
2023-11-11 11:44:55 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #610: GFLOPs: 93.3971. Time: 10.9746 us. Best GFLOPs: 97.5888
2023-11-11 11:44:55 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #611: GFLOPs: 89.4400. Time: 11.4602 us. Best GFLOPs: 97.5888
2023-11-11 11:44:55 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #612: GFLOPs: 93.2017. Time: 10.9977 us. Best GFLOPs: 97.5888
2023-11-11 11:44:55 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #613: GFLOPs: 93.2041. Time: 10.9974 us. Best GFLOPs: 97.5888
2023-11-11 11:44:55 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #614: GFLOPs: 91.4691. Time: 11.2060 us. Best GFLOPs: 97.5888
2023-11-11 11:44:55 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #615: GFLOPs: 93.2021. Time: 10.9976 us. Best GFLOPs: 97.5888
2023-11-11 11:44:55 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #616: GFLOPs: 36.3590. Time: 28.1911 us. Best GFLOPs: 97.5888
2023-11-11 11:44:55 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #617: GFLOPs: 36.2288. Time: 28.2924 us. Best GFLOPs: 97.5888
2023-11-11 11:44:55 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #618: GFLOPs: 93.1279. Time: 11.0064 us. Best GFLOPs: 97.5888
2023-11-11 11:44:55 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #619: GFLOPs: 67.2676. Time: 15.2376 us. Best GFLOPs: 97.5888
2023-11-11 11:44:55 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #620: GFLOPs: 73.7301. Time: 13.9021 us. Best GFLOPs: 97.5888
2023-11-11 11:44:55 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #621: GFLOPs: 73.7182. Time: 13.9043 us. Best GFLOPs: 97.5888
2023-11-11 11:44:55 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #622: GFLOPs: 53.3441. Time: 19.2149 us. Best GFLOPs: 97.5888
2023-11-11 11:44:55 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #623: GFLOPs: 73.7193. Time: 13.9041 us. Best GFLOPs: 97.5888
2023-11-11 11:44:55 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #624: GFLOPs: 73.7187. Time: 13.9042 us. Best GFLOPs: 97.5888
2023-11-11 11:44:55 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #625: GFLOPs: 73.7213. Time: 13.9037 us. Best GFLOPs: 97.5888
2023-11-11 11:44:55 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #626: GFLOPs: 95.0897. Time: 10.7793 us. Best GFLOPs: 97.5888
2023-11-11 11:44:55 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #627: GFLOPs: 73.3822. Time: 13.9680 us. Best GFLOPs: 97.5888
2023-11-11 11:44:55 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #628: GFLOPs: 43.2038. Time: 23.7248 us. Best GFLOPs: 97.5888
2023-11-11 11:44:55 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #629: GFLOPs: 89.5298. Time: 11.4487 us. Best GFLOPs: 97.5888
2023-11-11 11:44:55 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #630: GFLOPs: 68.2640. Time: 15.0152 us. Best GFLOPs: 97.5888
2023-11-11 11:44:55 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #631: GFLOPs: 93.1470. Time: 11.0041 us. Best GFLOPs: 97.5888
2023-11-11 11:44:55 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #632: GFLOPs: 73.3511. Time: 13.9739 us. Best GFLOPs: 97.5888
2023-11-11 11:44:55 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #633: GFLOPs: 73.3735. Time: 13.9696 us. Best GFLOPs: 97.5888
2023-11-11 11:44:55 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #634: GFLOPs: 42.0209. Time: 24.3926 us. Best GFLOPs: 97.5888
2023-11-11 11:44:55 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #635: GFLOPs: 67.2696. Time: 15.2372 us. Best GFLOPs: 97.5888
2023-11-11 11:44:55 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #636: GFLOPs: 73.3669. Time: 13.9709 us. Best GFLOPs: 97.5888
2023-11-11 11:44:55 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #637: GFLOPs: 5.7345. Time: 178.7417 us. Best GFLOPs: 97.5888
2023-11-11 11:44:55 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #638: GFLOPs: 8.1919. Time: 125.1238 us. Best GFLOPs: 97.5888
2023-11-11 11:44:55 [INFO] [task_scheduler.cc:121] [Task #28: fused_nn_dense_add] Trial #639: Error in building:
LocalBuilder: An exception occurred
Traceback (most recent call last):
  File "/home2/xiachunwei/Software/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/home2/xiachunwei/Software/tvm/python/tvm/meta_schedule/builder/local_builder.py", line 165, in <lambda>
    lambda x: _worker_func(*x),
  File "/home2/xiachunwei/Software/tvm/python/tvm/meta_schedule/builder/local_builder.py", line 231, in _worker_func
    rt_mod: Module = f_build(mod, target, _deserialize_params(params))
  File "/home2/xiachunwei/Software/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/home2/xiachunwei/Software/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
  File "/home2/xiachunwei/Software/tvm/python/tvm/meta_schedule/builder/local_builder.py", line 261, in default_build
    return tvm_build(mod, target=target)
  File "/home2/xiachunwei/Software/tvm/python/tvm/driver/build_module.py", line 281, in build
    rt_mod_host = _driver_ffi.tir_to_runtime(annotated_mods, target_host)
  File "/home2/xiachunwei/Software/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/home2/xiachunwei/Software/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  218: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::TypedPackedFunc<tvm::runtime::Module (tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)>::AssignTypedLambda<tvm::{lambda(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)#6}>(tvm::{lambda(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)#6}, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}> >::Call(tvm::runtime::PackedFuncObj const*, tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)
  217: tvm::TIRToRuntime(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target const&)
  216: tvm::SplitMixedModule(tvm::IRModule, tvm::Target const&, tvm::Target const&)
  215: tvm::ApplyPasses(tvm::IRModule, tvm::transform::Sequential)
  214: tvm::transform::Pass::operator()(tvm::IRModule) const
  213: tvm::transform::Pass::operator()(tvm::IRModule, tvm::transform::PassContext const&) const
  212: tvm::transform::SequentialNode::operator()(tvm::IRModule, tvm::transform::PassContext const&) const
  211: tvm::transform::Pass::operator()(tvm::IRModule, tvm::transform::PassContext const&) const
  210: tvm::tir::transform::PrimFuncPassNode::operator()(tvm::IRModule, tvm::transform::PassContext const&) const
  209: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::TypedPackedFunc<tvm::tir::PrimFunc (tvm::tir::PrimFunc, tvm::IRModule, tvm::transform::PassContext)>::AssignTypedLambda<tvm::tir::transform::LowerTVMBuiltin()::{lambda(tvm::tir::PrimFunc, tvm::IRModule, tvm::transform::PassContext)#1}>(tvm::tir::transform::LowerTVMBuiltin()::{lambda(tvm::tir::PrimFunc, tvm::IRModule, tvm::transform::PassContext)#1})::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}> >::Call(tvm::runtime::PackedFuncObj const*, tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)
  208: tvm::tir::BuiltinLower::VisitBodyAndRealizeAlloca(tvm::tir::Stmt)
  207: tvm::tir::BuiltinLower::GetMaxStack(tvm::tir::Stmt)
  206: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  205: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7r
  204: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  203: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7r
  202: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  201: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7r
  200: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  199: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  198: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7r
  197: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  196: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  195: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7r
  194: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  193: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  192: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7r
  191: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  190: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  189: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7r
  188: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  187: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  186: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7r
  185: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  184: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  183: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7r
  182: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  181: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  180: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7r
  179: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  178: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  177: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7r
  176: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  175: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  174: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7r
  173: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::AttrStmtNode const*)
  172: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  171: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7r
  170: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  169: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  168: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7r
  167: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  166: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7r
  165: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  164: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  163: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7r
  162: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  161: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7r
  160: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  159: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  158: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7r
  157: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  156: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  155: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7r
  154: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::AttrStmtNode const*)
  153: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  152: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7r
  151: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  150: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  149: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7r
  148: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  147: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7r
  146: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  145: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  144: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7r
  143: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  142: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7r
  141: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  140: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  139: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7r
  138: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::AttrStmtNode const*)
  137: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  136: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7r
  135: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  134: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  133: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7r
  132: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  131: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7r
  130: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  129: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  128: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7r
  127: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  126: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7r
  125: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  124: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  123: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7r
  122: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::AttrStmtNode const*)
  121: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  120: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7r
  119: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  118: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  117: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7r
  116: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  115: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7r
  114: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  113: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  112: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7r
  111: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  110: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7r
  109: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  108: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7r
  107: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  106: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7r
  105: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  104: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7r
  103: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  102: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7r
  101: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::AttrStmtNode const*)
  100: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  99: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7r
  98: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::AttrStmtNode const*)
  97: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  96: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7r
  95: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  94: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7r
  93: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  92: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7r
  91: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  90: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7r
  89: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  88: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7r
  87: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  86: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runt
  85: tvm::runtime::ObjectPtr<tvm::runtime::Object> tvm::runtime::Array<tvm::tir::Stmt, void>::MapHelper<tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1}, tvm::tir::Stmt>(tvm::runtime::ObjectPtr<tvm::runtime::Object>, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1})
  84: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  83: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7r
  82: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  81: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7r
  80: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  79: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7r
  78: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  77: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7r
  76: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  75: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7r
  74: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  73: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7r
  72: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  71: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7r
  70: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  69: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runt
  68: tvm::runtime::ObjectPtr<tvm::runtime::Object> tvm::runtime::Array<tvm::tir::Stmt, void>::MapHelper<tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1}, tvm::tir::Stmt>(tvm::runtime::ObjectPtr<tvm::runtime::Object>, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1})
  67: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  66: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7r
  65: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  64: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7r
  63: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  62: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7r
  61: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  60: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7r
  59: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  58: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7r
  57: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  56: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7r
  55: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  54: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7r
  53: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  52: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7r
  51: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  50: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runt
  49: tvm::runtime::ObjectPtr<tvm::runtime::Object> tvm::runtime::Array<tvm::tir::Stmt, void>::MapHelper<tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1}, tvm::tir::Stmt>(tvm::runtime::ObjectPtr<tvm::runtime::Object>, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1})
  48: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  47: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7r
  46: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  45: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7r
  44: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  43: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7r
  42: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  41: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7r
  40: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  39: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7r
  38: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  37: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7r
  36: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  35: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7r
  34: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  33: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7r
  32: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  31: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runt
  30: tvm::runtime::ObjectPtr<tvm::runtime::Object> tvm::runtime::Array<tvm::tir::Stmt, void>::MapHelper<tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1}, tvm::tir::Stmt>(tvm::runtime::ObjectPtr<tvm::runtime::Object>, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1})
  29: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  28: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7r
  27: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  26: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7r
  25: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  24: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7r
  23: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  22: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7r
  21: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  20: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7r
  19: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  18: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7r
  17: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  16: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7r
  15: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  14: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runt
  13: tvm::runtime::ObjectPtr<tvm::runtime::Object> tvm::runtime::Array<tvm::tir::Stmt, void>::MapHelper<tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1}, tvm::tir::Stmt>(tvm::runtime::ObjectPtr<tvm::runtime::Object>, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1})
  12: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  11: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7r
  10: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::AttrStmtNode const*)
  9: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  8: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7r
  7: tvm::tir::BuiltinLower::VisitStmt_(tvm::tir::LetStmtNode const*)
  6: tvm::tir::BuiltinLower::VisitStmt(tvm::tir::Stmt const&)
  5: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runt
  4: tvm::tir::StmtExprMutator::VisitExpr(tvm::PrimExpr const&)
  3: _ZZN3tvm3tir11ExprFunctorIFNS_8PrimExprERKS2_EE10InitVTableEvENUlRKNS
  2: tvm::tir::BuiltinLower::VisitExpr_(tvm::tir::CallNode const*)
  1: tvm::tir::BuiltinLower::MakeCallPacked(tvm::tir::CallNode const*, bool)
  0: tvm::tir::APIType(tvm::runtime::DataType)
  File "/home2/xiachunwei/Software/tvm/src/tir/transforms/ir_utils.h", line 157
InternalError: Check failed: t.lanes() == 1 (4 vs. 1) : Cannot pass vector type through packed API.

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(512)), "float32"), p1: T.Buffer((T.int64(1000), T.int64(512)), "float32"), p2: T.Buffer((T.int64(1), T.int64(1000)), "float32"), T_add: T.Buffer((T.int64(1), T.int64(1000)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        T_matmul_NT_local = T.alloc_buffer((T.int64(1), T.int64(1000)), scope="local")
        p0_shared = T.alloc_buffer((T.int64(1), T.int64(512)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(1000), T.int64(512)), scope="shared")
        for i_0_j_0_fused in T.thread_binding(T.int64(5), thread="blockIdx.x"):
            for i_1_j_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for i_2_j_2_fused in T.thread_binding(T.int64(100), thread="threadIdx.x"):
                    for i_3_init, j_3_init, i_4_init, j_4_init in T.grid(T.int64(1), T.int64(2), T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_init"):
                            v_i = T.axis.spatial(T.int64(1), i_3_init + i_4_init)
                            v_j = T.axis.spatial(T.int64(1000), i_0_j_0_fused * T.int64(200) + i_2_j_2_fused * T.int64(2) + j_3_init + j_4_init)
                            T.reads()
                            T.writes(T_matmul_NT_local[v_i, v_j])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            T_matmul_NT_local[v_i, v_j] = T.float32(0)
                    for k_0_fused in T.serial(T.int64(256), annotations={"software_pipeline_async_stages": [0], "software_pipeline_order": [0, 1, 2], "software_pipeline_stage": [0, 0, 3]}):
                        for ax0_ax1_fused_0 in range(T.int64(1)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(100), thread="threadIdx.x"):
                                with T.block("p0_shared"):
                                    v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                    v1 = T.axis.spatial(T.int64(512), k_0_fused * T.int64(2) + (ax0_ax1_fused_0 * T.int64(100) + ax0_ax1_fused_1))
                                    T.where(ax0_ax1_fused_0 * T.int64(100) + ax0_ax1_fused_1 < T.int64(2))
                                    T.reads(p0[v0, v1])
                                    T.writes(p0_shared[v0, v1])
                                    p0_shared[v0, v1] = p0[v0, v1]
                        for ax0_ax1_fused_0 in range(T.int64(1)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(100), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("p1_shared"):
                                        v0 = T.axis.spatial(T.int64(1000), i_0_j_0_fused * T.int64(200) + (ax0_ax1_fused_0 * T.int64(400) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2) // T.int64(2))
                                        v1 = T.axis.spatial(T.int64(512), k_0_fused * T.int64(2) + (ax0_ax1_fused_0 * T.int64(400) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2) % T.int64(2))
                                        T.reads(p1[v0, v1])
                                        T.writes(p1_shared[v0, v1])
                                        p1_shared[v0, v1] = p1[v0, v1]
                        for k_1, i_3, j_3, k_2, i_4, j_4 in T.grid(T.int64(2), T.int64(1), T.int64(2), T.int64(1), T.int64(1), T.int64(1)):
                            with T.block("T_matmul_NT_update"):
                                v_i = T.axis.spatial(T.int64(1), i_3 + i_4)
                                v_j = T.axis.spatial(T.int64(1000), i_0_j_0_fused * T.int64(200) + i_2_j_2_fused * T.int64(2) + j_3 + j_4)
                                v_k = T.axis.reduce(T.int64(512), k_0_fused * T.int64(2) + k_1 + k_2)
                                T.reads(T_matmul_NT_local[v_i, v_j], p0_shared[v_i, v_k], p1_shared[v_j, v_k])
                                T.writes(T_matmul_NT_local[v_i, v_j])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                T_matmul_NT_local[v_i, v_j] = T_matmul_NT_local[v_i, v_j] + p0_shared[v_i, v_k] * p1_shared[v_j, v_k]
                    for ax0, ax1 in T.grid(T.int64(1), T.int64(2)):
                        with T.block("T_matmul_NT_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(1000), i_0_j_0_fused * T.int64(200) + i_2_j_2_fused * T.int64(2) + ax1)
                            T.reads(T_matmul_NT_local[v0, v1], p2[v0, v1])
                            T.writes(T_add[v0, v1])
                            T_add[v0, v1] = T_matmul_NT_local[v0, v1] + p2[v0, v1]
b0 = sch.get_block(name="T_matmul_NT", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l3, l4, l5 = sch.get_loops(block=b0)
v6, v7, v8, v9, v10 = sch.sample_perfect_tile(loop=l3, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l11, l12, l13, l14, l15 = sch.split(loop=l3, factors=[v6, v7, v8, v9, v10], preserve_unit_iters=True)
v16, v17, v18, v19, v20 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[5, 1, 100, 2, 1])
l21, l22, l23, l24, l25 = sch.split(loop=l4, factors=[v16, v17, v18, v19, v20], preserve_unit_iters=True)
v26, v27, v28 = sch.sample_perfect_tile(loop=l5, n=3, max_innermost_factor=64, decision=[256, 2, 1])
l29, l30, l31 = sch.split(loop=l5, factors=[v26, v27, v28], preserve_unit_iters=True)
sch.reorder(l11, l21, l12, l22, l13, l23, l29, l30, l14, l24, l31, l15, l25)
l32 = sch.fuse(l11, l21, preserve_unit_iters=True)
sch.bind(loop=l32, thread_axis="blockIdx.x")
l33 = sch.fuse(l12, l22, preserve_unit_iters=True)
sch.bind(loop=l33, thread_axis="vthread.x")
l34 = sch.fuse(l13, l23, preserve_unit_iters=True)
sch.bind(loop=l34, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b35 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b35, loop=l34, preserve_unit_loops=True, index=-1)
b36 = sch.cache_read(block=b0, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b36, loop=l29, preserve_unit_loops=True, index=-1)
l37, l38, l39, l40, l41, l42 = sch.get_loops(block=b36)
l43 = sch.fuse(l41, l42, preserve_unit_iters=True)
v44 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b36, ann_key="meta_schedule.cooperative_fetch", ann_val=v44)
b45 = sch.cache_read(block=b0, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b45, loop=l29, preserve_unit_loops=True, index=-1)
l46, l47, l48, l49, l50, l51 = sch.get_loops(block=b45)
l52 = sch.fuse(l50, l51, preserve_unit_iters=True)
v53 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b45, ann_key="meta_schedule.cooperative_fetch", ann_val=v53)
l54 = sch.fuse(l29, preserve_unit_iters=True)
sch.annotate(block_or_loop=l54, ann_key="software_pipeline_stage", ann_val=[0, 0, 3])
sch.annotate(block_or_loop=l54, ann_key="software_pipeline_order", ann_val=[0, 1, 2])
sch.annotate(block_or_loop=l54, ann_key="software_pipeline_async_stages", ann_val=[0])
sch.reverse_compute_inline(block=b1)
v55 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=0)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v55)
sch.enter_postproc()
sch.unannotate(block_or_loop=b36, ann_key="meta_schedule.cooperative_fetch")
l56, l57, l58, l59, l60 = sch.get_loops(block=b36)
l61, l62 = sch.split(loop=l60, factors=[None, 100], preserve_unit_iters=True)
sch.bind(loop=l62, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b45, ann_key="meta_schedule.cooperative_fetch")
l63, l64, l65, l66, l67 = sch.get_loops(block=b45)
l68, l69, l70 = sch.split(loop=l67, factors=[None, 100, 4], preserve_unit_iters=True)
sch.vectorize(loop=l70)
sch.bind(loop=l69, thread_axis="threadIdx.x")
b71 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b71, ann_key="meta_schedule.unroll_explicit")
b72, b73, b74, b75 = sch.get_child_blocks(b71)
l76, l77, l78, l79, l80, l81 = sch.get_loops(block=b72)
l82, l83, l84, l85, l86, l87, l88 = sch.get_loops(block=b73)
l89, l90, l91, l92, l93, l94, l95, l96, l97, l98 = sch.get_loops(block=b74)
l99, l100, l101, l102, l103 = sch.get_loops(block=b75)
b104 = sch.get_block(name="T_matmul_NT", func_name="main")
l105, l106, l107, l108, l109, l110, l111, l112, l113, l114 = sch.get_loops(block=b104)
b115 = sch.decompose_reduction(block=b104, loop=l108)
2023-11-11 11:47:28 [INFO] [evolutionary_search.cc:713] Generating candidates......
2023-11-11 11:47:28 [INFO] [evolutionary_search.cc:715] Picked top 102 candidate(s) from database
2023-11-11 11:47:29 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310a405238)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310e76ec68)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310dc33cd8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310be916f8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56310f803f18)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310eeb7a68)]: 402 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310e771ba8)]: 0 failure(s)
2023-11-11 11:47:30 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310a405238)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310e76ec68)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310dc33cd8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310be916f8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56310f803f18)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310eeb7a68)]: 806 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310e771ba8)]: 0 failure(s)
2023-11-11 11:47:30 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310a405238)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310e76ec68)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310dc33cd8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310be916f8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56310f803f18)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310eeb7a68)]: 1211 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310e771ba8)]: 0 failure(s)
2023-11-11 11:47:31 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310a405238)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310e76ec68)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310dc33cd8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310be916f8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56310f803f18)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310eeb7a68)]: 1616 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310e771ba8)]: 0 failure(s)
2023-11-11 11:47:31 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310a405238)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310e76ec68)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310dc33cd8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310be916f8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56310f803f18)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310eeb7a68)]: 2017 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310e771ba8)]: 0 failure(s)
2023-11-11 11:47:32 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310a405238)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310e76ec68)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310dc33cd8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310be916f8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56310f803f18)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310eeb7a68)]: 2421 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310e771ba8)]: 0 failure(s)
2023-11-11 11:47:33 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310a405238)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310e76ec68)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310dc33cd8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310be916f8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56310f803f18)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310eeb7a68)]: 2825 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310e771ba8)]: 0 failure(s)
2023-11-11 11:47:33 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310a405238)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310e76ec68)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310dc33cd8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310be916f8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56310f803f18)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310eeb7a68)]: 3233 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310e771ba8)]: 0 failure(s)
2023-11-11 11:47:34 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310a405238)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310e76ec68)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310dc33cd8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310be916f8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56310f803f18)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310eeb7a68)]: 3637 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310e771ba8)]: 0 failure(s)
2023-11-11 11:47:34 [INFO] [evolutionary_search.cc:723] Sampled 53 candidate(s)
2023-11-11 11:47:36 [INFO] [evolutionary_search.cc:621] Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310a405238)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310e76ec68)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310dc33cd8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310be916f8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56310f803f18)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310eeb7a68)]: 182 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310e771ba8)]: 0 failure(s)
2023-11-11 11:47:39 [INFO] [evolutionary_search.cc:621] Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310a405238)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310e76ec68)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310dc33cd8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310be916f8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56310f803f18)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310eeb7a68)]: 165 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310e771ba8)]: 0 failure(s)
2023-11-11 11:47:42 [INFO] [evolutionary_search.cc:621] Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310a405238)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310e76ec68)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310dc33cd8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310be916f8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56310f803f18)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310eeb7a68)]: 177 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310e771ba8)]: 0 failure(s)
2023-11-11 11:47:45 [INFO] [evolutionary_search.cc:621] Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310a405238)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310e76ec68)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310dc33cd8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310be916f8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56310f803f18)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310eeb7a68)]: 163 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310e771ba8)]: 0 failure(s)
2023-11-11 11:47:46 [INFO] [evolutionary_search.cc:649] Scores of the best 64 candidates:
[1 : 16]:	1.0646  1.0346  1.0298  1.0278  1.0166  1.0158  1.0038  1.0022  0.9807  0.9679  0.9670  0.9670  0.9546  0.9546  0.9546  0.9508
[17 : 32]:	0.9403  0.9309  0.9177  0.9169  0.9144  0.9136  0.8790  0.8614  0.8433  0.8324  0.8215  0.8011  0.7974  0.7965  0.7962  0.7938
[33 : 48]:	0.7915  0.7915  0.7915  0.7915  0.7905  0.7852  0.7852  0.7815  0.7806  0.7768  0.7750  0.7746  0.7729  0.7729  0.7729  0.7715
[49 : 64]:	0.7703  0.7695  0.7695  0.7695  0.7691  0.7691  0.7682  0.7676  0.7645  0.7614  0.7612  0.7612  0.7612  0.7610  0.7607  0.7607
2023-11-11 11:47:46 [INFO] [evolutionary_search.cc:727] Got 64 candidate(s) with evolutionary search
2023-11-11 11:47:46 [INFO] [evolutionary_search.cc:730] Sending 63 candidates(s) for measurement
2023-11-11 11:52:37 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #640: GFLOPs: 30.7967. Time: 33.2828 us. Best GFLOPs: 97.5888
2023-11-11 11:52:37 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #641: GFLOPs: 30.8003. Time: 33.2789 us. Best GFLOPs: 97.5888
2023-11-11 11:52:37 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #642: GFLOPs: 80.6253. Time: 12.7131 us. Best GFLOPs: 97.5888
2023-11-11 11:52:37 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #643: GFLOPs: 33.5698. Time: 30.5334 us. Best GFLOPs: 97.5888
2023-11-11 11:52:37 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #644: GFLOPs: 85.3443. Time: 12.0102 us. Best GFLOPs: 97.5888
2023-11-11 11:52:37 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #645: GFLOPs: 29.5827. Time: 34.6486 us. Best GFLOPs: 97.5888
2023-11-11 11:52:37 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #646: GFLOPs: 30.8002. Time: 33.2790 us. Best GFLOPs: 97.5888
2023-11-11 11:52:37 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #647: GFLOPs: 83.9198. Time: 12.2140 us. Best GFLOPs: 97.5888
2023-11-11 11:52:37 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #648: GFLOPs: 29.5837. Time: 34.6475 us. Best GFLOPs: 97.5888
2023-11-11 11:52:37 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #649: GFLOPs: 33.5791. Time: 30.5250 us. Best GFLOPs: 97.5888
2023-11-11 11:52:37 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #650: GFLOPs: 33.5736. Time: 30.5299 us. Best GFLOPs: 97.5888
2023-11-11 11:52:37 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #651: GFLOPs: 33.5736. Time: 30.5300 us. Best GFLOPs: 97.5888
2023-11-11 11:52:37 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #652: GFLOPs: 29.5839. Time: 34.6473 us. Best GFLOPs: 97.5888
2023-11-11 11:52:37 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #653: GFLOPs: 29.5792. Time: 34.6527 us. Best GFLOPs: 97.5888
2023-11-11 11:52:37 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #654: GFLOPs: 29.5727. Time: 34.6603 us. Best GFLOPs: 97.5888
2023-11-11 11:52:37 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #655: GFLOPs: 34.8655. Time: 29.3987 us. Best GFLOPs: 97.5888
2023-11-11 11:52:37 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #656: GFLOPs: 59.4085. Time: 17.2534 us. Best GFLOPs: 97.5888
2023-11-11 11:52:37 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #657: GFLOPs: 59.4123. Time: 17.2523 us. Best GFLOPs: 97.5888
2023-11-11 11:52:37 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #658: GFLOPs: 35.1510. Time: 29.1599 us. Best GFLOPs: 97.5888
2023-11-11 11:52:37 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #659: GFLOPs: 35.1510. Time: 29.1599 us. Best GFLOPs: 97.5888
2023-11-11 11:52:37 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #660: GFLOPs: 66.4658. Time: 15.4215 us. Best GFLOPs: 97.5888
2023-11-11 11:52:37 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #661: GFLOPs: 34.9555. Time: 29.3230 us. Best GFLOPs: 97.5888
2023-11-11 11:52:37 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #662: GFLOPs: 82.3418. Time: 12.4481 us. Best GFLOPs: 97.5888
2023-11-11 11:52:37 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #663: GFLOPs: 82.3751. Time: 12.4431 us. Best GFLOPs: 97.5888
2023-11-11 11:52:37 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #664: GFLOPs: 88.9298. Time: 11.5260 us. Best GFLOPs: 97.5888
2023-11-11 11:52:37 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #665: GFLOPs: 36.2563. Time: 28.2709 us. Best GFLOPs: 97.5888
2023-11-11 11:52:37 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #666: GFLOPs: 36.0135. Time: 28.4615 us. Best GFLOPs: 97.5888
2023-11-11 11:52:37 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #667: GFLOPs: 92.6218. Time: 11.0665 us. Best GFLOPs: 97.5888
2023-11-11 11:52:37 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #668: GFLOPs: 80.9343. Time: 12.6646 us. Best GFLOPs: 97.5888
2023-11-11 11:52:37 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #669: GFLOPs: 90.9125. Time: 11.2746 us. Best GFLOPs: 97.5888
2023-11-11 11:52:37 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #670: GFLOPs: 36.0207. Time: 28.4559 us. Best GFLOPs: 97.5888
2023-11-11 11:52:37 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #671: GFLOPs: 36.0188. Time: 28.4574 us. Best GFLOPs: 97.5888
2023-11-11 11:52:37 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #672: GFLOPs: 74.2284. Time: 13.8087 us. Best GFLOPs: 97.5888
2023-11-11 11:52:37 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #673: GFLOPs: 74.2192. Time: 13.8104 us. Best GFLOPs: 97.5888
2023-11-11 11:52:37 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #674: GFLOPs: 94.5474. Time: 10.8411 us. Best GFLOPs: 97.5888
2023-11-11 11:52:37 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #675: GFLOPs: 94.5407. Time: 10.8419 us. Best GFLOPs: 97.5888
2023-11-11 11:52:37 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #676: GFLOPs: 92.7015. Time: 11.0570 us. Best GFLOPs: 97.5888
2023-11-11 11:52:37 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #677: GFLOPs: 93.2254. Time: 10.9949 us. Best GFLOPs: 97.5888
2023-11-11 11:52:37 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #678: GFLOPs: 93.1845. Time: 10.9997 us. Best GFLOPs: 97.5888
2023-11-11 11:52:37 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #679: GFLOPs: 93.2221. Time: 10.9952 us. Best GFLOPs: 97.5888
2023-11-11 11:52:37 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #680: GFLOPs: 93.2017. Time: 10.9977 us. Best GFLOPs: 97.5888
2023-11-11 11:52:37 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #681: GFLOPs: 88.9329. Time: 11.5255 us. Best GFLOPs: 97.5888
2023-11-11 11:52:37 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #682: GFLOPs: 93.6027. Time: 10.9505 us. Best GFLOPs: 97.5888
2023-11-11 11:52:37 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #683: GFLOPs: 73.0787. Time: 14.0260 us. Best GFLOPs: 97.5888
2023-11-11 11:52:37 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #684: GFLOPs: 92.1930. Time: 11.1180 us. Best GFLOPs: 97.5888
2023-11-11 11:52:37 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #685: GFLOPs: 92.1887. Time: 11.1185 us. Best GFLOPs: 97.5888
2023-11-11 11:52:37 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #686: GFLOPs: 66.4552. Time: 15.4239 us. Best GFLOPs: 97.5888
2023-11-11 11:52:37 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #687: GFLOPs: 88.7258. Time: 11.5524 us. Best GFLOPs: 97.5888
2023-11-11 11:52:37 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #688: GFLOPs: 92.6084. Time: 11.0681 us. Best GFLOPs: 97.5888
2023-11-11 11:52:37 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #689: GFLOPs: 88.9171. Time: 11.5276 us. Best GFLOPs: 97.5888
2023-11-11 11:52:37 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #690: GFLOPs: 88.9060. Time: 11.5290 us. Best GFLOPs: 97.5888
2023-11-11 11:52:37 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #691: GFLOPs: 66.5054. Time: 15.4123 us. Best GFLOPs: 97.5888
2023-11-11 11:52:37 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #692: GFLOPs: 92.1958. Time: 11.1176 us. Best GFLOPs: 97.5888
2023-11-11 11:52:37 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #693: GFLOPs: 72.8975. Time: 14.0608 us. Best GFLOPs: 97.5888
2023-11-11 11:52:37 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #694: GFLOPs: 92.1963. Time: 11.1176 us. Best GFLOPs: 97.5888
2023-11-11 11:52:37 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #695: GFLOPs: 80.9407. Time: 12.6636 us. Best GFLOPs: 97.5888
2023-11-11 11:52:37 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #696: GFLOPs: 41.9404. Time: 24.4394 us. Best GFLOPs: 97.5888
2023-11-11 11:52:37 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #697: GFLOPs: 88.9334. Time: 11.5255 us. Best GFLOPs: 97.5888
2023-11-11 11:52:37 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #698: GFLOPs: 74.2231. Time: 13.8097 us. Best GFLOPs: 97.5888
2023-11-11 11:52:37 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #699: GFLOPs: 74.2252. Time: 13.8093 us. Best GFLOPs: 97.5888
2023-11-11 11:52:37 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #700: GFLOPs: 93.2936. Time: 10.9868 us. Best GFLOPs: 97.5888
2023-11-11 11:52:37 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #701: GFLOPs: 16.1494. Time: 63.4698 us. Best GFLOPs: 97.5888
2023-11-11 11:52:37 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #702: GFLOPs: 13.3686. Time: 76.6724 us. Best GFLOPs: 97.5888
2023-11-11 11:54:37 [INFO] [evolutionary_search.cc:713] Generating candidates......
2023-11-11 11:54:37 [INFO] [evolutionary_search.cc:715] Picked top 102 candidate(s) from database
2023-11-11 11:54:38 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310a405238)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310e76ec68)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310dc33cd8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310be916f8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56310f803f18)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310eeb7a68)]: 405 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310e771ba8)]: 0 failure(s)
2023-11-11 11:54:38 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310a405238)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310e76ec68)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310dc33cd8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310be916f8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56310f803f18)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310eeb7a68)]: 806 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310e771ba8)]: 0 failure(s)
2023-11-11 11:54:39 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310a405238)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310e76ec68)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310dc33cd8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310be916f8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56310f803f18)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310eeb7a68)]: 1209 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310e771ba8)]: 0 failure(s)
2023-11-11 11:54:40 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310a405238)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310e76ec68)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310dc33cd8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310be916f8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56310f803f18)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310eeb7a68)]: 1604 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310e771ba8)]: 0 failure(s)
2023-11-11 11:54:40 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310a405238)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310e76ec68)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310dc33cd8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310be916f8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56310f803f18)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310eeb7a68)]: 2007 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310e771ba8)]: 0 failure(s)
2023-11-11 11:54:41 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310a405238)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310e76ec68)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310dc33cd8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310be916f8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56310f803f18)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310eeb7a68)]: 2412 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310e771ba8)]: 0 failure(s)
2023-11-11 11:54:42 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310a405238)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310e76ec68)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310dc33cd8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310be916f8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56310f803f18)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310eeb7a68)]: 2821 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310e771ba8)]: 0 failure(s)
2023-11-11 11:54:42 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310a405238)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310e76ec68)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310dc33cd8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310be916f8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56310f803f18)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310eeb7a68)]: 3222 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310e771ba8)]: 0 failure(s)
2023-11-11 11:54:42 [INFO] [evolutionary_search.cc:723] Sampled 58 candidate(s)
2023-11-11 11:54:45 [INFO] [evolutionary_search.cc:621] Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310a405238)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310e76ec68)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310dc33cd8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310be916f8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56310f803f18)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310eeb7a68)]: 181 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310e771ba8)]: 0 failure(s)
2023-11-11 11:54:48 [INFO] [evolutionary_search.cc:621] Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310a405238)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310e76ec68)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310dc33cd8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310be916f8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56310f803f18)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310eeb7a68)]: 179 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310e771ba8)]: 0 failure(s)
2023-11-11 11:54:51 [INFO] [evolutionary_search.cc:621] Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310a405238)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310e76ec68)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310dc33cd8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310be916f8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56310f803f18)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310eeb7a68)]: 187 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310e771ba8)]: 0 failure(s)
2023-11-11 11:54:54 [INFO] [evolutionary_search.cc:621] Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310a405238)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310e76ec68)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310dc33cd8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310be916f8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56310f803f18)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310eeb7a68)]: 156 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310e771ba8)]: 0 failure(s)
2023-11-11 11:54:55 [INFO] [evolutionary_search.cc:649] Scores of the best 64 candidates:
[1 : 16]:	0.9711  0.9604  0.9604  0.9550  0.9550  0.9531  0.9516  0.9516  0.9444  0.9389  0.9383  0.9356  0.9336  0.9334  0.9293  0.9242
[17 : 32]:	0.9242  0.8969  0.8943  0.8922  0.8915  0.8902  0.8795  0.8795  0.8795  0.8791  0.8791  0.8782  0.8768  0.8761  0.8747  0.8715
[33 : 48]:	0.8705  0.8692  0.8692  0.8654  0.8654  0.8625  0.8619  0.8555  0.8493  0.8446  0.8390  0.8383  0.8365  0.8318  0.8318  0.8257
[49 : 64]:	0.8257  0.8254  0.8208  0.8207  0.8203  0.8185  0.8168  0.8154  0.8129  0.8127  0.8084  0.8084  0.8075  0.8037  0.7999  0.7978
2023-11-11 11:54:55 [INFO] [evolutionary_search.cc:727] Got 64 candidate(s) with evolutionary search
2023-11-11 11:54:55 [INFO] [evolutionary_search.cc:730] Sending 64 candidates(s) for measurement
2023-11-11 11:55:21 [INFO] [task_scheduler.cc:121] [Task #28: fused_nn_dense_add] Trial #703: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/home2/xiachunwei/Software/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/home2/xiachunwei/Software/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/home2/xiachunwei/Software/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/home2/xiachunwei/Software/tvm/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/home2/xiachunwei/Software/tvm/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/home2/xiachunwei/Software/tvm/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/home2/xiachunwei/Software/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/home2/xiachunwei/Software/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  4: _ZN3tvm7runtime13PackedFun
  3: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  2: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  0: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  File "/home2/xiachunwei/Software/tvm/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(512)), "float32"), p1: T.Buffer((T.int64(1000), T.int64(512)), "float32"), p2: T.Buffer((T.int64(1), T.int64(1000)), "float32"), T_add: T.Buffer((T.int64(1), T.int64(1000)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        T_matmul_NT_local = T.alloc_buffer((T.int64(1), T.int64(1000)), scope="local")
        p0_shared = T.alloc_buffer((T.int64(1), T.int64(512)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(1000), T.int64(512)), scope="shared")
        for i_0_j_0_fused in T.thread_binding(T.int64(25), thread="blockIdx.x"):
            for i_1_j_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for i_2_j_2_fused in T.thread_binding(T.int64(40), thread="threadIdx.x"):
                    for i_3_init, j_3_init, i_4_init, j_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_init"):
                            v_i = T.axis.spatial(T.int64(1), i_3_init + i_4_init)
                            v_j = T.axis.spatial(T.int64(1000), i_0_j_0_fused * T.int64(40) + i_2_j_2_fused + j_3_init + j_4_init)
                            T.reads()
                            T.writes(T_matmul_NT_local[v_i, v_j])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            T_matmul_NT_local[v_i, v_j] = T.float32(0)
                    for k_0_fused in T.serial(T.int64(8), annotations={"software_pipeline_async_stages": [0], "software_pipeline_order": [0, 1, 2], "software_pipeline_stage": [0, 0, 2]}):
                        for ax0_ax1_fused_0 in range(T.int64(1)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(40), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("p0_shared"):
                                        v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v1 = T.axis.spatial(T.int64(512), k_0_fused * T.int64(64) + (ax0_ax1_fused_0 * T.int64(80) + ax0_ax1_fused_1 * T.int64(2) + ax0_ax1_fused_2))
                                        T.where((ax0_ax1_fused_0 * T.int64(40) + ax0_ax1_fused_1) * T.int64(2) + ax0_ax1_fused_2 < T.int64(64))
                                        T.reads(p0[v0, v1])
                                        T.writes(p0_shared[v0, v1])
                                        p0_shared[v0, v1] = p0[v0, v1]
                        for ax0_ax1_fused_0 in range(T.int64(16)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(40), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("p1_shared"):
                                        v0 = T.axis.spatial(T.int64(1000), i_0_j_0_fused * T.int64(40) + (ax0_ax1_fused_0 * T.int64(160) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2) // T.int64(64))
                                        v1 = T.axis.spatial(T.int64(512), k_0_fused * T.int64(64) + (ax0_ax1_fused_0 * T.int64(160) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2) % T.int64(64))
                                        T.reads(p1[v0, v1])
                                        T.writes(p1_shared[v0, v1])
                                        p1_shared[v0, v1] = p1[v0, v1]
                        for k_1, i_3, j_3, k_2, i_4, j_4 in T.grid(T.int64(2), T.int64(1), T.int64(1), T.int64(32), T.int64(1), T.int64(1)):
                            with T.block("T_matmul_NT_update"):
                                v_i = T.axis.spatial(T.int64(1), i_3 + i_4)
                                v_j = T.axis.spatial(T.int64(1000), i_0_j_0_fused * T.int64(40) + i_2_j_2_fused + j_3 + j_4)
                                v_k = T.axis.reduce(T.int64(512), k_0_fused * T.int64(64) + k_1 * T.int64(32) + k_2)
                                T.reads(T_matmul_NT_local[v_i, v_j], p0_shared[v_i, v_k], p1_shared[v_j, v_k])
                                T.writes(T_matmul_NT_local[v_i, v_j])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                T_matmul_NT_local[v_i, v_j] = T_matmul_NT_local[v_i, v_j] + p0_shared[v_i, v_k] * p1_shared[v_j, v_k]
                    for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(1000), i_0_j_0_fused * T.int64(40) + i_2_j_2_fused + ax1)
                            T.reads(T_matmul_NT_local[v0, v1], p2[v0, v1])
                            T.writes(T_add[v0, v1])
                            T_add[v0, v1] = T_matmul_NT_local[v0, v1] + p2[v0, v1]
b0 = sch.get_block(name="T_matmul_NT", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l3, l4, l5 = sch.get_loops(block=b0)
v6, v7, v8, v9, v10 = sch.sample_perfect_tile(loop=l3, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l11, l12, l13, l14, l15 = sch.split(loop=l3, factors=[v6, v7, v8, v9, v10], preserve_unit_iters=True)
v16, v17, v18, v19, v20 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[25, 1, 40, 1, 1])
l21, l22, l23, l24, l25 = sch.split(loop=l4, factors=[v16, v17, v18, v19, v20], preserve_unit_iters=True)
v26, v27, v28 = sch.sample_perfect_tile(loop=l5, n=3, max_innermost_factor=64, decision=[8, 2, 32])
l29, l30, l31 = sch.split(loop=l5, factors=[v26, v27, v28], preserve_unit_iters=True)
sch.reorder(l11, l21, l12, l22, l13, l23, l29, l30, l14, l24, l31, l15, l25)
l32 = sch.fuse(l11, l21, preserve_unit_iters=True)
sch.bind(loop=l32, thread_axis="blockIdx.x")
l33 = sch.fuse(l12, l22, preserve_unit_iters=True)
sch.bind(loop=l33, thread_axis="vthread.x")
l34 = sch.fuse(l13, l23, preserve_unit_iters=True)
sch.bind(loop=l34, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b35 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b35, loop=l34, preserve_unit_loops=True, index=-1)
b36 = sch.cache_read(block=b0, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b36, loop=l29, preserve_unit_loops=True, index=-1)
l37, l38, l39, l40, l41, l42 = sch.get_loops(block=b36)
l43 = sch.fuse(l41, l42, preserve_unit_iters=True)
v44 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b36, ann_key="meta_schedule.cooperative_fetch", ann_val=v44)
b45 = sch.cache_read(block=b0, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b45, loop=l29, preserve_unit_loops=True, index=-1)
l46, l47, l48, l49, l50, l51 = sch.get_loops(block=b45)
l52 = sch.fuse(l50, l51, preserve_unit_iters=True)
v53 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b45, ann_key="meta_schedule.cooperative_fetch", ann_val=v53)
l54 = sch.fuse(l29, preserve_unit_iters=True)
sch.annotate(block_or_loop=l54, ann_key="software_pipeline_stage", ann_val=[0, 0, 2])
sch.annotate(block_or_loop=l54, ann_key="software_pipeline_order", ann_val=[0, 1, 2])
sch.annotate(block_or_loop=l54, ann_key="software_pipeline_async_stages", ann_val=[0])
sch.reverse_compute_inline(block=b1)
v55 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=0)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v55)
sch.enter_postproc()
sch.unannotate(block_or_loop=b36, ann_key="meta_schedule.cooperative_fetch")
l56, l57, l58, l59, l60 = sch.get_loops(block=b36)
l61, l62, l63 = sch.split(loop=l60, factors=[None, 40, 2], preserve_unit_iters=True)
sch.vectorize(loop=l63)
sch.bind(loop=l62, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b45, ann_key="meta_schedule.cooperative_fetch")
l64, l65, l66, l67, l68 = sch.get_loops(block=b45)
l69, l70, l71 = sch.split(loop=l68, factors=[None, 40, 4], preserve_unit_iters=True)
sch.vectorize(loop=l71)
sch.bind(loop=l70, thread_axis="threadIdx.x")
b72 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b72, ann_key="meta_schedule.unroll_explicit")
b73, b74, b75, b76 = sch.get_child_blocks(b72)
l77, l78, l79, l80, l81, l82, l83 = sch.get_loops(block=b73)
l84, l85, l86, l87, l88, l89, l90 = sch.get_loops(block=b74)
l91, l92, l93, l94, l95, l96, l97, l98, l99, l100 = sch.get_loops(block=b75)
l101, l102, l103, l104, l105 = sch.get_loops(block=b76)
b106 = sch.get_block(name="T_matmul_NT", func_name="main")
l107, l108, l109, l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b106)
b117 = sch.decompose_reduction(block=b106, loop=l110)
2023-11-11 11:55:21 [INFO] [task_scheduler.cc:121] [Task #28: fused_nn_dense_add] Trial #704: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/home2/xiachunwei/Software/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/home2/xiachunwei/Software/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/home2/xiachunwei/Software/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/home2/xiachunwei/Software/tvm/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/home2/xiachunwei/Software/tvm/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/home2/xiachunwei/Software/tvm/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/home2/xiachunwei/Software/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/home2/xiachunwei/Software/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  4: _ZN3tvm7runtime13PackedFun
  3: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  2: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  0: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  File "/home2/xiachunwei/Software/tvm/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(512)), "float32"), p1: T.Buffer((T.int64(1000), T.int64(512)), "float32"), p2: T.Buffer((T.int64(1), T.int64(1000)), "float32"), T_add: T.Buffer((T.int64(1), T.int64(1000)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        T_matmul_NT_local = T.alloc_buffer((T.int64(1), T.int64(1000)), scope="local")
        p0_shared = T.alloc_buffer((T.int64(1), T.int64(512)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(1000), T.int64(512)), scope="shared")
        for i_0_j_0_fused in T.thread_binding(T.int64(25), thread="blockIdx.x"):
            for i_1_j_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for i_2_j_2_fused in T.thread_binding(T.int64(40), thread="threadIdx.x"):
                    for i_3_init, j_3_init, i_4_init, j_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_init"):
                            v_i = T.axis.spatial(T.int64(1), i_3_init + i_4_init)
                            v_j = T.axis.spatial(T.int64(1000), i_0_j_0_fused * T.int64(40) + i_2_j_2_fused + j_3_init + j_4_init)
                            T.reads()
                            T.writes(T_matmul_NT_local[v_i, v_j])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            T_matmul_NT_local[v_i, v_j] = T.float32(0)
                    for k_0_fused in T.serial(T.int64(8), annotations={"software_pipeline_async_stages": [0], "software_pipeline_order": [0, 1, 2], "software_pipeline_stage": [0, 0, 2]}):
                        for ax0_ax1_fused_0 in range(T.int64(1)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(40), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("p0_shared"):
                                        v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v1 = T.axis.spatial(T.int64(512), k_0_fused * T.int64(64) + (ax0_ax1_fused_0 * T.int64(80) + ax0_ax1_fused_1 * T.int64(2) + ax0_ax1_fused_2))
                                        T.where((ax0_ax1_fused_0 * T.int64(40) + ax0_ax1_fused_1) * T.int64(2) + ax0_ax1_fused_2 < T.int64(64))
                                        T.reads(p0[v0, v1])
                                        T.writes(p0_shared[v0, v1])
                                        p0_shared[v0, v1] = p0[v0, v1]
                        for ax0_ax1_fused_0 in range(T.int64(16)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(40), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("p1_shared"):
                                        v0 = T.axis.spatial(T.int64(1000), i_0_j_0_fused * T.int64(40) + (ax0_ax1_fused_0 * T.int64(160) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2) // T.int64(64))
                                        v1 = T.axis.spatial(T.int64(512), k_0_fused * T.int64(64) + (ax0_ax1_fused_0 * T.int64(160) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2) % T.int64(64))
                                        T.reads(p1[v0, v1])
                                        T.writes(p1_shared[v0, v1])
                                        p1_shared[v0, v1] = p1[v0, v1]
                        for k_1, i_3, j_3, k_2, i_4, j_4 in T.grid(T.int64(4), T.int64(1), T.int64(1), T.int64(16), T.int64(1), T.int64(1)):
                            with T.block("T_matmul_NT_update"):
                                v_i = T.axis.spatial(T.int64(1), i_3 + i_4)
                                v_j = T.axis.spatial(T.int64(1000), i_0_j_0_fused * T.int64(40) + i_2_j_2_fused + j_3 + j_4)
                                v_k = T.axis.reduce(T.int64(512), k_0_fused * T.int64(64) + k_1 * T.int64(16) + k_2)
                                T.reads(T_matmul_NT_local[v_i, v_j], p0_shared[v_i, v_k], p1_shared[v_j, v_k])
                                T.writes(T_matmul_NT_local[v_i, v_j])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                T_matmul_NT_local[v_i, v_j] = T_matmul_NT_local[v_i, v_j] + p0_shared[v_i, v_k] * p1_shared[v_j, v_k]
                    for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(1000), i_0_j_0_fused * T.int64(40) + i_2_j_2_fused + ax1)
                            T.reads(T_matmul_NT_local[v0, v1], p2[v0, v1])
                            T.writes(T_add[v0, v1])
                            T_add[v0, v1] = T_matmul_NT_local[v0, v1] + p2[v0, v1]
b0 = sch.get_block(name="T_matmul_NT", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l3, l4, l5 = sch.get_loops(block=b0)
v6, v7, v8, v9, v10 = sch.sample_perfect_tile(loop=l3, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l11, l12, l13, l14, l15 = sch.split(loop=l3, factors=[v6, v7, v8, v9, v10], preserve_unit_iters=True)
v16, v17, v18, v19, v20 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[25, 1, 40, 1, 1])
l21, l22, l23, l24, l25 = sch.split(loop=l4, factors=[v16, v17, v18, v19, v20], preserve_unit_iters=True)
v26, v27, v28 = sch.sample_perfect_tile(loop=l5, n=3, max_innermost_factor=64, decision=[8, 4, 16])
l29, l30, l31 = sch.split(loop=l5, factors=[v26, v27, v28], preserve_unit_iters=True)
sch.reorder(l11, l21, l12, l22, l13, l23, l29, l30, l14, l24, l31, l15, l25)
l32 = sch.fuse(l11, l21, preserve_unit_iters=True)
sch.bind(loop=l32, thread_axis="blockIdx.x")
l33 = sch.fuse(l12, l22, preserve_unit_iters=True)
sch.bind(loop=l33, thread_axis="vthread.x")
l34 = sch.fuse(l13, l23, preserve_unit_iters=True)
sch.bind(loop=l34, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b35 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b35, loop=l34, preserve_unit_loops=True, index=-1)
b36 = sch.cache_read(block=b0, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b36, loop=l29, preserve_unit_loops=True, index=-1)
l37, l38, l39, l40, l41, l42 = sch.get_loops(block=b36)
l43 = sch.fuse(l41, l42, preserve_unit_iters=True)
v44 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b36, ann_key="meta_schedule.cooperative_fetch", ann_val=v44)
b45 = sch.cache_read(block=b0, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b45, loop=l29, preserve_unit_loops=True, index=-1)
l46, l47, l48, l49, l50, l51 = sch.get_loops(block=b45)
l52 = sch.fuse(l50, l51, preserve_unit_iters=True)
v53 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b45, ann_key="meta_schedule.cooperative_fetch", ann_val=v53)
l54 = sch.fuse(l29, preserve_unit_iters=True)
sch.annotate(block_or_loop=l54, ann_key="software_pipeline_stage", ann_val=[0, 0, 2])
sch.annotate(block_or_loop=l54, ann_key="software_pipeline_order", ann_val=[0, 1, 2])
sch.annotate(block_or_loop=l54, ann_key="software_pipeline_async_stages", ann_val=[0])
sch.reverse_compute_inline(block=b1)
v55 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=0)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v55)
sch.enter_postproc()
sch.unannotate(block_or_loop=b36, ann_key="meta_schedule.cooperative_fetch")
l56, l57, l58, l59, l60 = sch.get_loops(block=b36)
l61, l62, l63 = sch.split(loop=l60, factors=[None, 40, 2], preserve_unit_iters=True)
sch.vectorize(loop=l63)
sch.bind(loop=l62, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b45, ann_key="meta_schedule.cooperative_fetch")
l64, l65, l66, l67, l68 = sch.get_loops(block=b45)
l69, l70, l71 = sch.split(loop=l68, factors=[None, 40, 4], preserve_unit_iters=True)
sch.vectorize(loop=l71)
sch.bind(loop=l70, thread_axis="threadIdx.x")
b72 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b72, ann_key="meta_schedule.unroll_explicit")
b73, b74, b75, b76 = sch.get_child_blocks(b72)
l77, l78, l79, l80, l81, l82, l83 = sch.get_loops(block=b73)
l84, l85, l86, l87, l88, l89, l90 = sch.get_loops(block=b74)
l91, l92, l93, l94, l95, l96, l97, l98, l99, l100 = sch.get_loops(block=b75)
l101, l102, l103, l104, l105 = sch.get_loops(block=b76)
b106 = sch.get_block(name="T_matmul_NT", func_name="main")
l107, l108, l109, l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b106)
b117 = sch.decompose_reduction(block=b106, loop=l110)
2023-11-11 11:55:21 [INFO] [task_scheduler.cc:121] [Task #28: fused_nn_dense_add] Trial #705: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/home2/xiachunwei/Software/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/home2/xiachunwei/Software/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/home2/xiachunwei/Software/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/home2/xiachunwei/Software/tvm/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/home2/xiachunwei/Software/tvm/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/home2/xiachunwei/Software/tvm/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/home2/xiachunwei/Software/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/home2/xiachunwei/Software/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  4: _ZN3tvm7runtime13PackedFun
  3: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  2: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  0: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  File "/home2/xiachunwei/Software/tvm/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(512)), "float32"), p1: T.Buffer((T.int64(1000), T.int64(512)), "float32"), p2: T.Buffer((T.int64(1), T.int64(1000)), "float32"), T_add: T.Buffer((T.int64(1), T.int64(1000)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        T_matmul_NT_local = T.alloc_buffer((T.int64(1), T.int64(1000)), scope="local")
        p0_shared = T.alloc_buffer((T.int64(1), T.int64(512)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(1000), T.int64(512)), scope="shared")
        for i_0_j_0_fused in T.thread_binding(T.int64(25), thread="blockIdx.x"):
            for i_1_j_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for i_2_j_2_fused in T.thread_binding(T.int64(40), thread="threadIdx.x"):
                    for i_3_init, j_3_init, i_4_init, j_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_init"):
                            v_i = T.axis.spatial(T.int64(1), i_3_init + i_4_init)
                            v_j = T.axis.spatial(T.int64(1000), i_0_j_0_fused * T.int64(40) + i_2_j_2_fused + j_3_init + j_4_init)
                            T.reads()
                            T.writes(T_matmul_NT_local[v_i, v_j])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            T_matmul_NT_local[v_i, v_j] = T.float32(0)
                    for k_0 in range(T.int64(8)):
                        for ax0_ax1_fused_0 in range(T.int64(1)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(40), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("p0_shared"):
                                        v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v1 = T.axis.spatial(T.int64(512), k_0 * T.int64(64) + (ax0_ax1_fused_0 * T.int64(80) + ax0_ax1_fused_1 * T.int64(2) + ax0_ax1_fused_2))
                                        T.where((ax0_ax1_fused_0 * T.int64(40) + ax0_ax1_fused_1) * T.int64(2) + ax0_ax1_fused_2 < T.int64(64))
                                        T.reads(p0[v0, v1])
                                        T.writes(p0_shared[v0, v1])
                                        p0_shared[v0, v1] = p0[v0, v1]
                        for ax0_ax1_fused_0 in range(T.int64(16)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(40), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("p1_shared"):
                                        v0 = T.axis.spatial(T.int64(1000), i_0_j_0_fused * T.int64(40) + (ax0_ax1_fused_0 * T.int64(160) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2) // T.int64(64))
                                        v1 = T.axis.spatial(T.int64(512), k_0 * T.int64(64) + (ax0_ax1_fused_0 * T.int64(160) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2) % T.int64(64))
                                        T.reads(p1[v0, v1])
                                        T.writes(p1_shared[v0, v1])
                                        p1_shared[v0, v1] = p1[v0, v1]
                        for k_1, i_3, j_3, k_2, i_4, j_4 in T.grid(T.int64(4), T.int64(1), T.int64(1), T.int64(16), T.int64(1), T.int64(1)):
                            with T.block("T_matmul_NT_update"):
                                v_i = T.axis.spatial(T.int64(1), i_3 + i_4)
                                v_j = T.axis.spatial(T.int64(1000), i_0_j_0_fused * T.int64(40) + i_2_j_2_fused + j_3 + j_4)
                                v_k = T.axis.reduce(T.int64(512), k_0 * T.int64(64) + k_1 * T.int64(16) + k_2)
                                T.reads(T_matmul_NT_local[v_i, v_j], p0_shared[v_i, v_k], p1_shared[v_j, v_k])
                                T.writes(T_matmul_NT_local[v_i, v_j])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                T_matmul_NT_local[v_i, v_j] = T_matmul_NT_local[v_i, v_j] + p0_shared[v_i, v_k] * p1_shared[v_j, v_k]
                    for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(1000), i_0_j_0_fused * T.int64(40) + i_2_j_2_fused + ax1)
                            T.reads(T_matmul_NT_local[v0, v1], p2[v0, v1])
                            T.writes(T_add[v0, v1])
                            T_add[v0, v1] = T_matmul_NT_local[v0, v1] + p2[v0, v1]
b0 = sch.get_block(name="T_matmul_NT", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l3, l4, l5 = sch.get_loops(block=b0)
v6, v7, v8, v9, v10 = sch.sample_perfect_tile(loop=l3, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l11, l12, l13, l14, l15 = sch.split(loop=l3, factors=[v6, v7, v8, v9, v10], preserve_unit_iters=True)
v16, v17, v18, v19, v20 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[25, 1, 40, 1, 1])
l21, l22, l23, l24, l25 = sch.split(loop=l4, factors=[v16, v17, v18, v19, v20], preserve_unit_iters=True)
v26, v27, v28 = sch.sample_perfect_tile(loop=l5, n=3, max_innermost_factor=64, decision=[8, 4, 16])
l29, l30, l31 = sch.split(loop=l5, factors=[v26, v27, v28], preserve_unit_iters=True)
sch.reorder(l11, l21, l12, l22, l13, l23, l29, l30, l14, l24, l31, l15, l25)
l32 = sch.fuse(l11, l21, preserve_unit_iters=True)
sch.bind(loop=l32, thread_axis="blockIdx.x")
l33 = sch.fuse(l12, l22, preserve_unit_iters=True)
sch.bind(loop=l33, thread_axis="vthread.x")
l34 = sch.fuse(l13, l23, preserve_unit_iters=True)
sch.bind(loop=l34, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b35 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b35, loop=l34, preserve_unit_loops=True, index=-1)
b36 = sch.cache_read(block=b0, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b36, loop=l29, preserve_unit_loops=True, index=-1)
l37, l38, l39, l40, l41, l42 = sch.get_loops(block=b36)
l43 = sch.fuse(l41, l42, preserve_unit_iters=True)
v44 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b36, ann_key="meta_schedule.cooperative_fetch", ann_val=v44)
b45 = sch.cache_read(block=b0, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b45, loop=l29, preserve_unit_loops=True, index=-1)
l46, l47, l48, l49, l50, l51 = sch.get_loops(block=b45)
l52 = sch.fuse(l50, l51, preserve_unit_iters=True)
v53 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b45, ann_key="meta_schedule.cooperative_fetch", ann_val=v53)
sch.reverse_compute_inline(block=b1)
v54 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=0)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v54)
sch.enter_postproc()
sch.unannotate(block_or_loop=b36, ann_key="meta_schedule.cooperative_fetch")
l55, l56, l57, l58, l59 = sch.get_loops(block=b36)
l60, l61, l62 = sch.split(loop=l59, factors=[None, 40, 2], preserve_unit_iters=True)
sch.vectorize(loop=l62)
sch.bind(loop=l61, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b45, ann_key="meta_schedule.cooperative_fetch")
l63, l64, l65, l66, l67 = sch.get_loops(block=b45)
l68, l69, l70 = sch.split(loop=l67, factors=[None, 40, 4], preserve_unit_iters=True)
sch.vectorize(loop=l70)
sch.bind(loop=l69, thread_axis="threadIdx.x")
b71 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b71, ann_key="meta_schedule.unroll_explicit")
b72, b73, b74, b75 = sch.get_child_blocks(b71)
l76, l77, l78, l79, l80, l81, l82 = sch.get_loops(block=b72)
l83, l84, l85, l86, l87, l88, l89 = sch.get_loops(block=b73)
l90, l91, l92, l93, l94, l95, l96, l97, l98, l99 = sch.get_loops(block=b74)
l100, l101, l102, l103, l104 = sch.get_loops(block=b75)
b105 = sch.get_block(name="T_matmul_NT", func_name="main")
l106, l107, l108, l109, l110, l111, l112, l113, l114, l115 = sch.get_loops(block=b105)
b116 = sch.decompose_reduction(block=b105, loop=l109)
2023-11-11 11:55:21 [INFO] [task_scheduler.cc:121] [Task #28: fused_nn_dense_add] Trial #706: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/home2/xiachunwei/Software/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/home2/xiachunwei/Software/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/home2/xiachunwei/Software/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/home2/xiachunwei/Software/tvm/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/home2/xiachunwei/Software/tvm/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/home2/xiachunwei/Software/tvm/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/home2/xiachunwei/Software/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/home2/xiachunwei/Software/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  4: _ZN3tvm7runtime13PackedFun
  3: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  2: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  0: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  File "/home2/xiachunwei/Software/tvm/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(512)), "float32"), p1: T.Buffer((T.int64(1000), T.int64(512)), "float32"), p2: T.Buffer((T.int64(1), T.int64(1000)), "float32"), T_add: T.Buffer((T.int64(1), T.int64(1000)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        T_matmul_NT_local = T.alloc_buffer((T.int64(1), T.int64(1000)), scope="local")
        p0_shared = T.alloc_buffer((T.int64(1), T.int64(512)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(1000), T.int64(512)), scope="shared")
        for i_0_j_0_fused in T.thread_binding(T.int64(25), thread="blockIdx.x"):
            for i_1_j_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for i_2_j_2_fused in T.thread_binding(T.int64(40), thread="threadIdx.x"):
                    for i_3_init, j_3_init, i_4_init, j_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_init"):
                            v_i = T.axis.spatial(T.int64(1), i_3_init + i_4_init)
                            v_j = T.axis.spatial(T.int64(1000), i_0_j_0_fused * T.int64(40) + i_2_j_2_fused + j_3_init + j_4_init)
                            T.reads()
                            T.writes(T_matmul_NT_local[v_i, v_j])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            T_matmul_NT_local[v_i, v_j] = T.float32(0)
                    for k_0_fused in T.serial(T.int64(8), annotations={"software_pipeline_async_stages": [0], "software_pipeline_order": [0, 1, 2], "software_pipeline_stage": [0, 0, 2]}):
                        for ax0_ax1_fused_0 in range(T.int64(2)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(40), thread="threadIdx.x"):
                                with T.block("p0_shared"):
                                    v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                    v1 = T.axis.spatial(T.int64(512), k_0_fused * T.int64(64) + (ax0_ax1_fused_0 * T.int64(40) + ax0_ax1_fused_1))
                                    T.where(ax0_ax1_fused_0 * T.int64(40) + ax0_ax1_fused_1 < T.int64(64))
                                    T.reads(p0[v0, v1])
                                    T.writes(p0_shared[v0, v1])
                                    p0_shared[v0, v1] = p0[v0, v1]
                        for ax0_ax1_fused_0 in range(T.int64(16)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(40), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("p1_shared"):
                                        v0 = T.axis.spatial(T.int64(1000), i_0_j_0_fused * T.int64(40) + (ax0_ax1_fused_0 * T.int64(160) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2) // T.int64(64))
                                        v1 = T.axis.spatial(T.int64(512), k_0_fused * T.int64(64) + (ax0_ax1_fused_0 * T.int64(160) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2) % T.int64(64))
                                        T.reads(p1[v0, v1])
                                        T.writes(p1_shared[v0, v1])
                                        p1_shared[v0, v1] = p1[v0, v1]
                        for k_1, i_3, j_3, k_2, i_4, j_4 in T.grid(T.int64(2), T.int64(1), T.int64(1), T.int64(32), T.int64(1), T.int64(1)):
                            with T.block("T_matmul_NT_update"):
                                v_i = T.axis.spatial(T.int64(1), i_3 + i_4)
                                v_j = T.axis.spatial(T.int64(1000), i_0_j_0_fused * T.int64(40) + i_2_j_2_fused + j_3 + j_4)
                                v_k = T.axis.reduce(T.int64(512), k_0_fused * T.int64(64) + k_1 * T.int64(32) + k_2)
                                T.reads(T_matmul_NT_local[v_i, v_j], p0_shared[v_i, v_k], p1_shared[v_j, v_k])
                                T.writes(T_matmul_NT_local[v_i, v_j])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                T_matmul_NT_local[v_i, v_j] = T_matmul_NT_local[v_i, v_j] + p0_shared[v_i, v_k] * p1_shared[v_j, v_k]
                    for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(1000), i_0_j_0_fused * T.int64(40) + i_2_j_2_fused + ax1)
                            T.reads(T_matmul_NT_local[v0, v1], p2[v0, v1])
                            T.writes(T_add[v0, v1])
                            T_add[v0, v1] = T_matmul_NT_local[v0, v1] + p2[v0, v1]
b0 = sch.get_block(name="T_matmul_NT", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l3, l4, l5 = sch.get_loops(block=b0)
v6, v7, v8, v9, v10 = sch.sample_perfect_tile(loop=l3, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l11, l12, l13, l14, l15 = sch.split(loop=l3, factors=[v6, v7, v8, v9, v10], preserve_unit_iters=True)
v16, v17, v18, v19, v20 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[25, 1, 40, 1, 1])
l21, l22, l23, l24, l25 = sch.split(loop=l4, factors=[v16, v17, v18, v19, v20], preserve_unit_iters=True)
v26, v27, v28 = sch.sample_perfect_tile(loop=l5, n=3, max_innermost_factor=64, decision=[8, 2, 32])
l29, l30, l31 = sch.split(loop=l5, factors=[v26, v27, v28], preserve_unit_iters=True)
sch.reorder(l11, l21, l12, l22, l13, l23, l29, l30, l14, l24, l31, l15, l25)
l32 = sch.fuse(l11, l21, preserve_unit_iters=True)
sch.bind(loop=l32, thread_axis="blockIdx.x")
l33 = sch.fuse(l12, l22, preserve_unit_iters=True)
sch.bind(loop=l33, thread_axis="vthread.x")
l34 = sch.fuse(l13, l23, preserve_unit_iters=True)
sch.bind(loop=l34, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b35 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b35, loop=l34, preserve_unit_loops=True, index=-1)
b36 = sch.cache_read(block=b0, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b36, loop=l29, preserve_unit_loops=True, index=-1)
l37, l38, l39, l40, l41, l42 = sch.get_loops(block=b36)
l43 = sch.fuse(l41, l42, preserve_unit_iters=True)
v44 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b36, ann_key="meta_schedule.cooperative_fetch", ann_val=v44)
b45 = sch.cache_read(block=b0, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b45, loop=l29, preserve_unit_loops=True, index=-1)
l46, l47, l48, l49, l50, l51 = sch.get_loops(block=b45)
l52 = sch.fuse(l50, l51, preserve_unit_iters=True)
v53 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b45, ann_key="meta_schedule.cooperative_fetch", ann_val=v53)
l54 = sch.fuse(l29, preserve_unit_iters=True)
sch.annotate(block_or_loop=l54, ann_key="software_pipeline_stage", ann_val=[0, 0, 2])
sch.annotate(block_or_loop=l54, ann_key="software_pipeline_order", ann_val=[0, 1, 2])
sch.annotate(block_or_loop=l54, ann_key="software_pipeline_async_stages", ann_val=[0])
sch.reverse_compute_inline(block=b1)
v55 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=0)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v55)
sch.enter_postproc()
sch.unannotate(block_or_loop=b36, ann_key="meta_schedule.cooperative_fetch")
l56, l57, l58, l59, l60 = sch.get_loops(block=b36)
l61, l62 = sch.split(loop=l60, factors=[None, 40], preserve_unit_iters=True)
sch.bind(loop=l62, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b45, ann_key="meta_schedule.cooperative_fetch")
l63, l64, l65, l66, l67 = sch.get_loops(block=b45)
l68, l69, l70 = sch.split(loop=l67, factors=[None, 40, 4], preserve_unit_iters=True)
sch.vectorize(loop=l70)
sch.bind(loop=l69, thread_axis="threadIdx.x")
b71 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b71, ann_key="meta_schedule.unroll_explicit")
b72, b73, b74, b75 = sch.get_child_blocks(b71)
l76, l77, l78, l79, l80, l81 = sch.get_loops(block=b72)
l82, l83, l84, l85, l86, l87, l88 = sch.get_loops(block=b73)
l89, l90, l91, l92, l93, l94, l95, l96, l97, l98 = sch.get_loops(block=b74)
l99, l100, l101, l102, l103 = sch.get_loops(block=b75)
b104 = sch.get_block(name="T_matmul_NT", func_name="main")
l105, l106, l107, l108, l109, l110, l111, l112, l113, l114 = sch.get_loops(block=b104)
b115 = sch.decompose_reduction(block=b104, loop=l108)
2023-11-11 11:55:21 [INFO] [task_scheduler.cc:121] [Task #28: fused_nn_dense_add] Trial #707: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/home2/xiachunwei/Software/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/home2/xiachunwei/Software/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/home2/xiachunwei/Software/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/home2/xiachunwei/Software/tvm/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/home2/xiachunwei/Software/tvm/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/home2/xiachunwei/Software/tvm/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/home2/xiachunwei/Software/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/home2/xiachunwei/Software/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  4: _ZN3tvm7runtime13PackedFun
  3: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  2: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  0: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  File "/home2/xiachunwei/Software/tvm/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(512)), "float32"), p1: T.Buffer((T.int64(1000), T.int64(512)), "float32"), p2: T.Buffer((T.int64(1), T.int64(1000)), "float32"), T_add: T.Buffer((T.int64(1), T.int64(1000)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        T_matmul_NT_local = T.alloc_buffer((T.int64(1), T.int64(1000)), scope="local")
        p0_shared = T.alloc_buffer((T.int64(1), T.int64(512)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(1000), T.int64(512)), scope="shared")
        for i_0_j_0_fused in T.thread_binding(T.int64(25), thread="blockIdx.x"):
            for i_1_j_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for i_2_j_2_fused in T.thread_binding(T.int64(40), thread="threadIdx.x"):
                    for i_3_init, j_3_init, i_4_init, j_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_init"):
                            v_i = T.axis.spatial(T.int64(1), i_3_init + i_4_init)
                            v_j = T.axis.spatial(T.int64(1000), i_0_j_0_fused * T.int64(40) + i_2_j_2_fused + j_3_init + j_4_init)
                            T.reads()
                            T.writes(T_matmul_NT_local[v_i, v_j])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            T_matmul_NT_local[v_i, v_j] = T.float32(0)
                    for k_0_fused in T.serial(T.int64(8), annotations={"software_pipeline_async_stages": [0], "software_pipeline_order": [0, 1, 2], "software_pipeline_stage": [0, 0, 2]}):
                        for ax0_ax1_fused_0 in range(T.int64(1)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(40), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("p0_shared"):
                                        v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v1 = T.axis.spatial(T.int64(512), k_0_fused * T.int64(64) + (ax0_ax1_fused_0 * T.int64(80) + ax0_ax1_fused_1 * T.int64(2) + ax0_ax1_fused_2))
                                        T.where((ax0_ax1_fused_0 * T.int64(40) + ax0_ax1_fused_1) * T.int64(2) + ax0_ax1_fused_2 < T.int64(64))
                                        T.reads(p0[v0, v1])
                                        T.writes(p0_shared[v0, v1])
                                        p0_shared[v0, v1] = p0[v0, v1]
                        for ax0_ax1_fused_0 in range(T.int64(16)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(40), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("p1_shared"):
                                        v0 = T.axis.spatial(T.int64(1000), i_0_j_0_fused * T.int64(40) + (ax0_ax1_fused_0 * T.int64(160) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2) // T.int64(64))
                                        v1 = T.axis.spatial(T.int64(512), k_0_fused * T.int64(64) + (ax0_ax1_fused_0 * T.int64(160) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2) % T.int64(64))
                                        T.reads(p1[v0, v1])
                                        T.writes(p1_shared[v0, v1])
                                        p1_shared[v0, v1] = p1[v0, v1]
                        for k_1, i_3, j_3, k_2, i_4, j_4 in T.grid(T.int64(8), T.int64(1), T.int64(1), T.int64(8), T.int64(1), T.int64(1)):
                            with T.block("T_matmul_NT_update"):
                                v_i = T.axis.spatial(T.int64(1), i_3 + i_4)
                                v_j = T.axis.spatial(T.int64(1000), i_0_j_0_fused * T.int64(40) + i_2_j_2_fused + j_3 + j_4)
                                v_k = T.axis.reduce(T.int64(512), k_0_fused * T.int64(64) + k_1 * T.int64(8) + k_2)
                                T.reads(T_matmul_NT_local[v_i, v_j], p0_shared[v_i, v_k], p1_shared[v_j, v_k])
                                T.writes(T_matmul_NT_local[v_i, v_j])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                T_matmul_NT_local[v_i, v_j] = T_matmul_NT_local[v_i, v_j] + p0_shared[v_i, v_k] * p1_shared[v_j, v_k]
                    for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(1000), i_0_j_0_fused * T.int64(40) + i_2_j_2_fused + ax1)
                            T.reads(T_matmul_NT_local[v0, v1], p2[v0, v1])
                            T.writes(T_add[v0, v1])
                            T_add[v0, v1] = T_matmul_NT_local[v0, v1] + p2[v0, v1]
b0 = sch.get_block(name="T_matmul_NT", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l3, l4, l5 = sch.get_loops(block=b0)
v6, v7, v8, v9, v10 = sch.sample_perfect_tile(loop=l3, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l11, l12, l13, l14, l15 = sch.split(loop=l3, factors=[v6, v7, v8, v9, v10], preserve_unit_iters=True)
v16, v17, v18, v19, v20 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[25, 1, 40, 1, 1])
l21, l22, l23, l24, l25 = sch.split(loop=l4, factors=[v16, v17, v18, v19, v20], preserve_unit_iters=True)
v26, v27, v28 = sch.sample_perfect_tile(loop=l5, n=3, max_innermost_factor=64, decision=[8, 8, 8])
l29, l30, l31 = sch.split(loop=l5, factors=[v26, v27, v28], preserve_unit_iters=True)
sch.reorder(l11, l21, l12, l22, l13, l23, l29, l30, l14, l24, l31, l15, l25)
l32 = sch.fuse(l11, l21, preserve_unit_iters=True)
sch.bind(loop=l32, thread_axis="blockIdx.x")
l33 = sch.fuse(l12, l22, preserve_unit_iters=True)
sch.bind(loop=l33, thread_axis="vthread.x")
l34 = sch.fuse(l13, l23, preserve_unit_iters=True)
sch.bind(loop=l34, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b35 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b35, loop=l34, preserve_unit_loops=True, index=-1)
b36 = sch.cache_read(block=b0, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b36, loop=l29, preserve_unit_loops=True, index=-1)
l37, l38, l39, l40, l41, l42 = sch.get_loops(block=b36)
l43 = sch.fuse(l41, l42, preserve_unit_iters=True)
v44 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b36, ann_key="meta_schedule.cooperative_fetch", ann_val=v44)
b45 = sch.cache_read(block=b0, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b45, loop=l29, preserve_unit_loops=True, index=-1)
l46, l47, l48, l49, l50, l51 = sch.get_loops(block=b45)
l52 = sch.fuse(l50, l51, preserve_unit_iters=True)
v53 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b45, ann_key="meta_schedule.cooperative_fetch", ann_val=v53)
l54 = sch.fuse(l29, preserve_unit_iters=True)
sch.annotate(block_or_loop=l54, ann_key="software_pipeline_stage", ann_val=[0, 0, 2])
sch.annotate(block_or_loop=l54, ann_key="software_pipeline_order", ann_val=[0, 1, 2])
sch.annotate(block_or_loop=l54, ann_key="software_pipeline_async_stages", ann_val=[0])
sch.reverse_compute_inline(block=b1)
v55 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=0)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v55)
sch.enter_postproc()
sch.unannotate(block_or_loop=b36, ann_key="meta_schedule.cooperative_fetch")
l56, l57, l58, l59, l60 = sch.get_loops(block=b36)
l61, l62, l63 = sch.split(loop=l60, factors=[None, 40, 2], preserve_unit_iters=True)
sch.vectorize(loop=l63)
sch.bind(loop=l62, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b45, ann_key="meta_schedule.cooperative_fetch")
l64, l65, l66, l67, l68 = sch.get_loops(block=b45)
l69, l70, l71 = sch.split(loop=l68, factors=[None, 40, 4], preserve_unit_iters=True)
sch.vectorize(loop=l71)
sch.bind(loop=l70, thread_axis="threadIdx.x")
b72 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b72, ann_key="meta_schedule.unroll_explicit")
b73, b74, b75, b76 = sch.get_child_blocks(b72)
l77, l78, l79, l80, l81, l82, l83 = sch.get_loops(block=b73)
l84, l85, l86, l87, l88, l89, l90 = sch.get_loops(block=b74)
l91, l92, l93, l94, l95, l96, l97, l98, l99, l100 = sch.get_loops(block=b75)
l101, l102, l103, l104, l105 = sch.get_loops(block=b76)
b106 = sch.get_block(name="T_matmul_NT", func_name="main")
l107, l108, l109, l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b106)
b117 = sch.decompose_reduction(block=b106, loop=l110)
2023-11-11 11:55:21 [INFO] [task_scheduler.cc:121] [Task #28: fused_nn_dense_add] Trial #708: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/home2/xiachunwei/Software/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/home2/xiachunwei/Software/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/home2/xiachunwei/Software/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/home2/xiachunwei/Software/tvm/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/home2/xiachunwei/Software/tvm/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/home2/xiachunwei/Software/tvm/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/home2/xiachunwei/Software/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/home2/xiachunwei/Software/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  4: _ZN3tvm7runtime13PackedFun
  3: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  2: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  0: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  File "/home2/xiachunwei/Software/tvm/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(512)), "float32"), p1: T.Buffer((T.int64(1000), T.int64(512)), "float32"), p2: T.Buffer((T.int64(1), T.int64(1000)), "float32"), T_add: T.Buffer((T.int64(1), T.int64(1000)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        T_matmul_NT_local = T.alloc_buffer((T.int64(1), T.int64(1000)), scope="local")
        p0_shared = T.alloc_buffer((T.int64(1), T.int64(512)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(1000), T.int64(512)), scope="shared")
        for i_0_j_0_fused in T.thread_binding(T.int64(25), thread="blockIdx.x"):
            for i_1_j_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for i_2_j_2_fused in T.thread_binding(T.int64(40), thread="threadIdx.x"):
                    for i_3_init, j_3_init, i_4_init, j_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_init"):
                            v_i = T.axis.spatial(T.int64(1), i_3_init + i_4_init)
                            v_j = T.axis.spatial(T.int64(1000), i_0_j_0_fused * T.int64(40) + i_2_j_2_fused + j_3_init + j_4_init)
                            T.reads()
                            T.writes(T_matmul_NT_local[v_i, v_j])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            T_matmul_NT_local[v_i, v_j] = T.float32(0)
                    for k_0_fused in T.serial(T.int64(8), annotations={"software_pipeline_async_stages": [0], "software_pipeline_order": [0, 1, 2], "software_pipeline_stage": [0, 0, 2]}):
                        for ax0_ax1_fused_0 in range(T.int64(1)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(40), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("p0_shared"):
                                        v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v1 = T.axis.spatial(T.int64(512), k_0_fused * T.int64(64) + (ax0_ax1_fused_0 * T.int64(160) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2))
                                        T.where((ax0_ax1_fused_0 * T.int64(40) + ax0_ax1_fused_1) * T.int64(4) + ax0_ax1_fused_2 < T.int64(64))
                                        T.reads(p0[v0, v1])
                                        T.writes(p0_shared[v0, v1])
                                        p0_shared[v0, v1] = p0[v0, v1]
                        for ax0_ax1_fused_0 in range(T.int64(16)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(40), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("p1_shared"):
                                        v0 = T.axis.spatial(T.int64(1000), i_0_j_0_fused * T.int64(40) + (ax0_ax1_fused_0 * T.int64(160) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2) // T.int64(64))
                                        v1 = T.axis.spatial(T.int64(512), k_0_fused * T.int64(64) + (ax0_ax1_fused_0 * T.int64(160) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2) % T.int64(64))
                                        T.reads(p1[v0, v1])
                                        T.writes(p1_shared[v0, v1])
                                        p1_shared[v0, v1] = p1[v0, v1]
                        for k_1, i_3, j_3, k_2, i_4, j_4 in T.grid(T.int64(2), T.int64(1), T.int64(1), T.int64(32), T.int64(1), T.int64(1)):
                            with T.block("T_matmul_NT_update"):
                                v_i = T.axis.spatial(T.int64(1), i_3 + i_4)
                                v_j = T.axis.spatial(T.int64(1000), i_0_j_0_fused * T.int64(40) + i_2_j_2_fused + j_3 + j_4)
                                v_k = T.axis.reduce(T.int64(512), k_0_fused * T.int64(64) + k_1 * T.int64(32) + k_2)
                                T.reads(T_matmul_NT_local[v_i, v_j], p0_shared[v_i, v_k], p1_shared[v_j, v_k])
                                T.writes(T_matmul_NT_local[v_i, v_j])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                T_matmul_NT_local[v_i, v_j] = T_matmul_NT_local[v_i, v_j] + p0_shared[v_i, v_k] * p1_shared[v_j, v_k]
                    for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(1000), i_0_j_0_fused * T.int64(40) + i_2_j_2_fused + ax1)
                            T.reads(T_matmul_NT_local[v0, v1], p2[v0, v1])
                            T.writes(T_add[v0, v1])
                            T_add[v0, v1] = T_matmul_NT_local[v0, v1] + p2[v0, v1]
b0 = sch.get_block(name="T_matmul_NT", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l3, l4, l5 = sch.get_loops(block=b0)
v6, v7, v8, v9, v10 = sch.sample_perfect_tile(loop=l3, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l11, l12, l13, l14, l15 = sch.split(loop=l3, factors=[v6, v7, v8, v9, v10], preserve_unit_iters=True)
v16, v17, v18, v19, v20 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[25, 1, 40, 1, 1])
l21, l22, l23, l24, l25 = sch.split(loop=l4, factors=[v16, v17, v18, v19, v20], preserve_unit_iters=True)
v26, v27, v28 = sch.sample_perfect_tile(loop=l5, n=3, max_innermost_factor=64, decision=[8, 2, 32])
l29, l30, l31 = sch.split(loop=l5, factors=[v26, v27, v28], preserve_unit_iters=True)
sch.reorder(l11, l21, l12, l22, l13, l23, l29, l30, l14, l24, l31, l15, l25)
l32 = sch.fuse(l11, l21, preserve_unit_iters=True)
sch.bind(loop=l32, thread_axis="blockIdx.x")
l33 = sch.fuse(l12, l22, preserve_unit_iters=True)
sch.bind(loop=l33, thread_axis="vthread.x")
l34 = sch.fuse(l13, l23, preserve_unit_iters=True)
sch.bind(loop=l34, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b35 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b35, loop=l34, preserve_unit_loops=True, index=-1)
b36 = sch.cache_read(block=b0, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b36, loop=l29, preserve_unit_loops=True, index=-1)
l37, l38, l39, l40, l41, l42 = sch.get_loops(block=b36)
l43 = sch.fuse(l41, l42, preserve_unit_iters=True)
v44 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b36, ann_key="meta_schedule.cooperative_fetch", ann_val=v44)
b45 = sch.cache_read(block=b0, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b45, loop=l29, preserve_unit_loops=True, index=-1)
l46, l47, l48, l49, l50, l51 = sch.get_loops(block=b45)
l52 = sch.fuse(l50, l51, preserve_unit_iters=True)
v53 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b45, ann_key="meta_schedule.cooperative_fetch", ann_val=v53)
l54 = sch.fuse(l29, preserve_unit_iters=True)
sch.annotate(block_or_loop=l54, ann_key="software_pipeline_stage", ann_val=[0, 0, 2])
sch.annotate(block_or_loop=l54, ann_key="software_pipeline_order", ann_val=[0, 1, 2])
sch.annotate(block_or_loop=l54, ann_key="software_pipeline_async_stages", ann_val=[0])
sch.reverse_compute_inline(block=b1)
v55 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=0)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v55)
sch.enter_postproc()
sch.unannotate(block_or_loop=b36, ann_key="meta_schedule.cooperative_fetch")
l56, l57, l58, l59, l60 = sch.get_loops(block=b36)
l61, l62, l63 = sch.split(loop=l60, factors=[None, 40, 4], preserve_unit_iters=True)
sch.vectorize(loop=l63)
sch.bind(loop=l62, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b45, ann_key="meta_schedule.cooperative_fetch")
l64, l65, l66, l67, l68 = sch.get_loops(block=b45)
l69, l70, l71 = sch.split(loop=l68, factors=[None, 40, 4], preserve_unit_iters=True)
sch.vectorize(loop=l71)
sch.bind(loop=l70, thread_axis="threadIdx.x")
b72 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b72, ann_key="meta_schedule.unroll_explicit")
b73, b74, b75, b76 = sch.get_child_blocks(b72)
l77, l78, l79, l80, l81, l82, l83 = sch.get_loops(block=b73)
l84, l85, l86, l87, l88, l89, l90 = sch.get_loops(block=b74)
l91, l92, l93, l94, l95, l96, l97, l98, l99, l100 = sch.get_loops(block=b75)
l101, l102, l103, l104, l105 = sch.get_loops(block=b76)
b106 = sch.get_block(name="T_matmul_NT", func_name="main")
l107, l108, l109, l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b106)
b117 = sch.decompose_reduction(block=b106, loop=l110)
2023-11-11 11:55:21 [INFO] [task_scheduler.cc:121] [Task #28: fused_nn_dense_add] Trial #709: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/home2/xiachunwei/Software/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/home2/xiachunwei/Software/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/home2/xiachunwei/Software/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/home2/xiachunwei/Software/tvm/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/home2/xiachunwei/Software/tvm/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/home2/xiachunwei/Software/tvm/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/home2/xiachunwei/Software/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/home2/xiachunwei/Software/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  4: _ZN3tvm7runtime13PackedFun
  3: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  2: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  0: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  File "/home2/xiachunwei/Software/tvm/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(512)), "float32"), p1: T.Buffer((T.int64(1000), T.int64(512)), "float32"), p2: T.Buffer((T.int64(1), T.int64(1000)), "float32"), T_add: T.Buffer((T.int64(1), T.int64(1000)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        T_matmul_NT_local = T.alloc_buffer((T.int64(1), T.int64(1000)), scope="local")
        p0_shared = T.alloc_buffer((T.int64(1), T.int64(512)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(1000), T.int64(512)), scope="shared")
        for i_0_j_0_fused in T.thread_binding(T.int64(25), thread="blockIdx.x"):
            for i_1_j_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for i_2_j_2_fused in T.thread_binding(T.int64(40), thread="threadIdx.x"):
                    for i_3_init, j_3_init, i_4_init, j_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_init"):
                            v_i = T.axis.spatial(T.int64(1), i_3_init + i_4_init)
                            v_j = T.axis.spatial(T.int64(1000), i_0_j_0_fused * T.int64(40) + i_2_j_2_fused + j_3_init + j_4_init)
                            T.reads()
                            T.writes(T_matmul_NT_local[v_i, v_j])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            T_matmul_NT_local[v_i, v_j] = T.float32(0)
                    for k_0_fused in T.serial(T.int64(8), annotations={"software_pipeline_async_stages": [0], "software_pipeline_order": [0, 1, 2], "software_pipeline_stage": [0, 0, 2]}):
                        for ax0_ax1_fused_0 in range(T.int64(1)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(40), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("p0_shared"):
                                        v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v1 = T.axis.spatial(T.int64(512), k_0_fused * T.int64(64) + (ax0_ax1_fused_0 * T.int64(80) + ax0_ax1_fused_1 * T.int64(2) + ax0_ax1_fused_2))
                                        T.where((ax0_ax1_fused_0 * T.int64(40) + ax0_ax1_fused_1) * T.int64(2) + ax0_ax1_fused_2 < T.int64(64))
                                        T.reads(p0[v0, v1])
                                        T.writes(p0_shared[v0, v1])
                                        p0_shared[v0, v1] = p0[v0, v1]
                        for ax0_ax1_fused_0 in range(T.int64(16)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(40), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("p1_shared"):
                                        v0 = T.axis.spatial(T.int64(1000), i_0_j_0_fused * T.int64(40) + (ax0_ax1_fused_0 * T.int64(160) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2) // T.int64(64))
                                        v1 = T.axis.spatial(T.int64(512), k_0_fused * T.int64(64) + (ax0_ax1_fused_0 * T.int64(160) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2) % T.int64(64))
                                        T.reads(p1[v0, v1])
                                        T.writes(p1_shared[v0, v1])
                                        p1_shared[v0, v1] = p1[v0, v1]
                        for k_1, i_3, j_3, k_2, i_4, j_4 in T.grid(T.int64(64), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                            with T.block("T_matmul_NT_update"):
                                v_i = T.axis.spatial(T.int64(1), i_3 + i_4)
                                v_j = T.axis.spatial(T.int64(1000), i_0_j_0_fused * T.int64(40) + i_2_j_2_fused + j_3 + j_4)
                                v_k = T.axis.reduce(T.int64(512), k_0_fused * T.int64(64) + k_1 + k_2)
                                T.reads(T_matmul_NT_local[v_i, v_j], p0_shared[v_i, v_k], p1_shared[v_j, v_k])
                                T.writes(T_matmul_NT_local[v_i, v_j])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                T_matmul_NT_local[v_i, v_j] = T_matmul_NT_local[v_i, v_j] + p0_shared[v_i, v_k] * p1_shared[v_j, v_k]
                    for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(1000), i_0_j_0_fused * T.int64(40) + i_2_j_2_fused + ax1)
                            T.reads(T_matmul_NT_local[v0, v1], p2[v0, v1])
                            T.writes(T_add[v0, v1])
                            T_add[v0, v1] = T_matmul_NT_local[v0, v1] + p2[v0, v1]
b0 = sch.get_block(name="T_matmul_NT", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l3, l4, l5 = sch.get_loops(block=b0)
v6, v7, v8, v9, v10 = sch.sample_perfect_tile(loop=l3, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l11, l12, l13, l14, l15 = sch.split(loop=l3, factors=[v6, v7, v8, v9, v10], preserve_unit_iters=True)
v16, v17, v18, v19, v20 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[25, 1, 40, 1, 1])
l21, l22, l23, l24, l25 = sch.split(loop=l4, factors=[v16, v17, v18, v19, v20], preserve_unit_iters=True)
v26, v27, v28 = sch.sample_perfect_tile(loop=l5, n=3, max_innermost_factor=64, decision=[8, 64, 1])
l29, l30, l31 = sch.split(loop=l5, factors=[v26, v27, v28], preserve_unit_iters=True)
sch.reorder(l11, l21, l12, l22, l13, l23, l29, l30, l14, l24, l31, l15, l25)
l32 = sch.fuse(l11, l21, preserve_unit_iters=True)
sch.bind(loop=l32, thread_axis="blockIdx.x")
l33 = sch.fuse(l12, l22, preserve_unit_iters=True)
sch.bind(loop=l33, thread_axis="vthread.x")
l34 = sch.fuse(l13, l23, preserve_unit_iters=True)
sch.bind(loop=l34, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b35 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b35, loop=l34, preserve_unit_loops=True, index=-1)
b36 = sch.cache_read(block=b0, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b36, loop=l29, preserve_unit_loops=True, index=-1)
l37, l38, l39, l40, l41, l42 = sch.get_loops(block=b36)
l43 = sch.fuse(l41, l42, preserve_unit_iters=True)
v44 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b36, ann_key="meta_schedule.cooperative_fetch", ann_val=v44)
b45 = sch.cache_read(block=b0, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b45, loop=l29, preserve_unit_loops=True, index=-1)
l46, l47, l48, l49, l50, l51 = sch.get_loops(block=b45)
l52 = sch.fuse(l50, l51, preserve_unit_iters=True)
v53 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b45, ann_key="meta_schedule.cooperative_fetch", ann_val=v53)
l54 = sch.fuse(l29, preserve_unit_iters=True)
sch.annotate(block_or_loop=l54, ann_key="software_pipeline_stage", ann_val=[0, 0, 2])
sch.annotate(block_or_loop=l54, ann_key="software_pipeline_order", ann_val=[0, 1, 2])
sch.annotate(block_or_loop=l54, ann_key="software_pipeline_async_stages", ann_val=[0])
sch.reverse_compute_inline(block=b1)
v55 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=0)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v55)
sch.enter_postproc()
sch.unannotate(block_or_loop=b36, ann_key="meta_schedule.cooperative_fetch")
l56, l57, l58, l59, l60 = sch.get_loops(block=b36)
l61, l62, l63 = sch.split(loop=l60, factors=[None, 40, 2], preserve_unit_iters=True)
sch.vectorize(loop=l63)
sch.bind(loop=l62, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b45, ann_key="meta_schedule.cooperative_fetch")
l64, l65, l66, l67, l68 = sch.get_loops(block=b45)
l69, l70, l71 = sch.split(loop=l68, factors=[None, 40, 4], preserve_unit_iters=True)
sch.vectorize(loop=l71)
sch.bind(loop=l70, thread_axis="threadIdx.x")
b72 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b72, ann_key="meta_schedule.unroll_explicit")
b73, b74, b75, b76 = sch.get_child_blocks(b72)
l77, l78, l79, l80, l81, l82, l83 = sch.get_loops(block=b73)
l84, l85, l86, l87, l88, l89, l90 = sch.get_loops(block=b74)
l91, l92, l93, l94, l95, l96, l97, l98, l99, l100 = sch.get_loops(block=b75)
l101, l102, l103, l104, l105 = sch.get_loops(block=b76)
b106 = sch.get_block(name="T_matmul_NT", func_name="main")
l107, l108, l109, l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b106)
b117 = sch.decompose_reduction(block=b106, loop=l110)
2023-11-11 11:55:21 [INFO] [task_scheduler.cc:121] [Task #28: fused_nn_dense_add] Trial #710: Error in running:
LocalRunner: An exception occurred
Subprocess terminated
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(512)), "float32"), p1: T.Buffer((T.int64(1000), T.int64(512)), "float32"), p2: T.Buffer((T.int64(1), T.int64(1000)), "float32"), T_add: T.Buffer((T.int64(1), T.int64(1000)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        T_matmul_NT_local = T.alloc_buffer((T.int64(1), T.int64(1000)), scope="local")
        p0_shared = T.alloc_buffer((T.int64(1), T.int64(512)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(1000), T.int64(512)), scope="shared")
        for i_0_j_0_fused in T.thread_binding(T.int64(25), thread="blockIdx.x"):
            for i_1_j_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for i_2_j_2_fused in T.thread_binding(T.int64(40), thread="threadIdx.x"):
                    for i_3_init, j_3_init, i_4_init, j_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_init"):
                            v_i = T.axis.spatial(T.int64(1), i_3_init + i_4_init)
                            v_j = T.axis.spatial(T.int64(1000), i_0_j_0_fused * T.int64(40) + i_2_j_2_fused + j_3_init + j_4_init)
                            T.reads()
                            T.writes(T_matmul_NT_local[v_i, v_j])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            T_matmul_NT_local[v_i, v_j] = T.float32(0)
                    for k_0_fused in T.serial(T.int64(8), annotations={"software_pipeline_async_stages": [0], "software_pipeline_order": [0, 1, 2], "software_pipeline_stage": [0, 0, 2]}):
                        for ax0_ax1_fused_0 in range(T.int64(1)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(40), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("p0_shared"):
                                        v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v1 = T.axis.spatial(T.int64(512), k_0_fused * T.int64(64) + (ax0_ax1_fused_0 * T.int64(80) + ax0_ax1_fused_1 * T.int64(2) + ax0_ax1_fused_2))
                                        T.where((ax0_ax1_fused_0 * T.int64(40) + ax0_ax1_fused_1) * T.int64(2) + ax0_ax1_fused_2 < T.int64(64))
                                        T.reads(p0[v0, v1])
                                        T.writes(p0_shared[v0, v1])
                                        p0_shared[v0, v1] = p0[v0, v1]
                        for ax0_ax1_fused_0 in range(T.int64(16)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(40), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("p1_shared"):
                                        v0 = T.axis.spatial(T.int64(1000), i_0_j_0_fused * T.int64(40) + (ax0_ax1_fused_0 * T.int64(160) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2) // T.int64(64))
                                        v1 = T.axis.spatial(T.int64(512), k_0_fused * T.int64(64) + (ax0_ax1_fused_0 * T.int64(160) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2) % T.int64(64))
                                        T.reads(p1[v0, v1])
                                        T.writes(p1_shared[v0, v1])
                                        p1_shared[v0, v1] = p1[v0, v1]
                        for k_1, i_3, j_3, k_2, i_4, j_4 in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(64), T.int64(1), T.int64(1)):
                            with T.block("T_matmul_NT_update"):
                                v_i = T.axis.spatial(T.int64(1), i_3 + i_4)
                                v_j = T.axis.spatial(T.int64(1000), i_0_j_0_fused * T.int64(40) + i_2_j_2_fused + j_3 + j_4)
                                v_k = T.axis.reduce(T.int64(512), k_0_fused * T.int64(64) + k_1 * T.int64(64) + k_2)
                                T.reads(T_matmul_NT_local[v_i, v_j], p0_shared[v_i, v_k], p1_shared[v_j, v_k])
                                T.writes(T_matmul_NT_local[v_i, v_j])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                T_matmul_NT_local[v_i, v_j] = T_matmul_NT_local[v_i, v_j] + p0_shared[v_i, v_k] * p1_shared[v_j, v_k]
                    for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(1000), i_0_j_0_fused * T.int64(40) + i_2_j_2_fused + ax1)
                            T.reads(T_matmul_NT_local[v0, v1], p2[v0, v1])
                            T.writes(T_add[v0, v1])
                            T_add[v0, v1] = T_matmul_NT_local[v0, v1] + p2[v0, v1]
b0 = sch.get_block(name="T_matmul_NT", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l3, l4, l5 = sch.get_loops(block=b0)
v6, v7, v8, v9, v10 = sch.sample_perfect_tile(loop=l3, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l11, l12, l13, l14, l15 = sch.split(loop=l3, factors=[v6, v7, v8, v9, v10], preserve_unit_iters=True)
v16, v17, v18, v19, v20 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[25, 1, 40, 1, 1])
l21, l22, l23, l24, l25 = sch.split(loop=l4, factors=[v16, v17, v18, v19, v20], preserve_unit_iters=True)
v26, v27, v28 = sch.sample_perfect_tile(loop=l5, n=3, max_innermost_factor=64, decision=[8, 1, 64])
l29, l30, l31 = sch.split(loop=l5, factors=[v26, v27, v28], preserve_unit_iters=True)
sch.reorder(l11, l21, l12, l22, l13, l23, l29, l30, l14, l24, l31, l15, l25)
l32 = sch.fuse(l11, l21, preserve_unit_iters=True)
sch.bind(loop=l32, thread_axis="blockIdx.x")
l33 = sch.fuse(l12, l22, preserve_unit_iters=True)
sch.bind(loop=l33, thread_axis="vthread.x")
l34 = sch.fuse(l13, l23, preserve_unit_iters=True)
sch.bind(loop=l34, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b35 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b35, loop=l34, preserve_unit_loops=True, index=-1)
b36 = sch.cache_read(block=b0, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b36, loop=l29, preserve_unit_loops=True, index=-1)
l37, l38, l39, l40, l41, l42 = sch.get_loops(block=b36)
l43 = sch.fuse(l41, l42, preserve_unit_iters=True)
v44 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b36, ann_key="meta_schedule.cooperative_fetch", ann_val=v44)
b45 = sch.cache_read(block=b0, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b45, loop=l29, preserve_unit_loops=True, index=-1)
l46, l47, l48, l49, l50, l51 = sch.get_loops(block=b45)
l52 = sch.fuse(l50, l51, preserve_unit_iters=True)
v53 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b45, ann_key="meta_schedule.cooperative_fetch", ann_val=v53)
l54 = sch.fuse(l29, preserve_unit_iters=True)
sch.annotate(block_or_loop=l54, ann_key="software_pipeline_stage", ann_val=[0, 0, 2])
sch.annotate(block_or_loop=l54, ann_key="software_pipeline_order", ann_val=[0, 1, 2])
sch.annotate(block_or_loop=l54, ann_key="software_pipeline_async_stages", ann_val=[0])
sch.reverse_compute_inline(block=b1)
v55 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=0)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v55)
sch.enter_postproc()
sch.unannotate(block_or_loop=b36, ann_key="meta_schedule.cooperative_fetch")
l56, l57, l58, l59, l60 = sch.get_loops(block=b36)
l61, l62, l63 = sch.split(loop=l60, factors=[None, 40, 2], preserve_unit_iters=True)
sch.vectorize(loop=l63)
sch.bind(loop=l62, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b45, ann_key="meta_schedule.cooperative_fetch")
l64, l65, l66, l67, l68 = sch.get_loops(block=b45)
l69, l70, l71 = sch.split(loop=l68, factors=[None, 40, 4], preserve_unit_iters=True)
sch.vectorize(loop=l71)
sch.bind(loop=l70, thread_axis="threadIdx.x")
b72 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b72, ann_key="meta_schedule.unroll_explicit")
b73, b74, b75, b76 = sch.get_child_blocks(b72)
l77, l78, l79, l80, l81, l82, l83 = sch.get_loops(block=b73)
l84, l85, l86, l87, l88, l89, l90 = sch.get_loops(block=b74)
l91, l92, l93, l94, l95, l96, l97, l98, l99, l100 = sch.get_loops(block=b75)
l101, l102, l103, l104, l105 = sch.get_loops(block=b76)
b106 = sch.get_block(name="T_matmul_NT", func_name="main")
l107, l108, l109, l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b106)
b117 = sch.decompose_reduction(block=b106, loop=l110)
2023-11-11 11:55:21 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #711: GFLOPs: 91.6599. Time: 11.1826 us. Best GFLOPs: 97.5888
2023-11-11 11:55:21 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #712: GFLOPs: 91.6413. Time: 11.1849 us. Best GFLOPs: 97.5888
2023-11-11 11:55:21 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #713: GFLOPs: 93.1957. Time: 10.9984 us. Best GFLOPs: 97.5888
2023-11-11 11:55:21 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #714: GFLOPs: 91.6309. Time: 11.1862 us. Best GFLOPs: 97.5888
2023-11-11 11:55:21 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #715: GFLOPs: 93.1951. Time: 10.9984 us. Best GFLOPs: 97.5888
2023-11-11 11:55:21 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #716: GFLOPs: 93.6154. Time: 10.9491 us. Best GFLOPs: 97.5888
2023-11-11 11:55:21 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #717: GFLOPs: 93.6178. Time: 10.9488 us. Best GFLOPs: 97.5888
2023-11-11 11:55:21 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #718: GFLOPs: 93.6033. Time: 10.9505 us. Best GFLOPs: 97.5888
2023-11-11 11:55:21 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #719: GFLOPs: 93.6048. Time: 10.9503 us. Best GFLOPs: 97.5888
2023-11-11 11:55:21 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #720: GFLOPs: 89.6210. Time: 11.4370 us. Best GFLOPs: 97.5888
2023-11-11 11:55:21 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #721: GFLOPs: 90.5967. Time: 11.3139 us. Best GFLOPs: 97.5888
2023-11-11 11:55:21 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #722: GFLOPs: 89.6365. Time: 11.4351 us. Best GFLOPs: 97.5888
2023-11-11 11:55:21 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #723: GFLOPs: 93.5795. Time: 10.9533 us. Best GFLOPs: 97.5888
2023-11-11 11:55:21 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #724: GFLOPs: 93.5742. Time: 10.9539 us. Best GFLOPs: 97.5888
2023-11-11 11:55:21 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #725: GFLOPs: 89.6165. Time: 11.4376 us. Best GFLOPs: 97.5888
2023-11-11 11:55:21 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #726: GFLOPs: 90.5500. Time: 11.3197 us. Best GFLOPs: 97.5888
2023-11-11 11:55:21 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #727: GFLOPs: 61.9871. Time: 16.5357 us. Best GFLOPs: 97.5888
2023-11-11 11:55:21 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #728: GFLOPs: 74.1881. Time: 13.8162 us. Best GFLOPs: 97.5888
2023-11-11 11:55:21 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #729: GFLOPs: 93.4124. Time: 10.9729 us. Best GFLOPs: 97.5888
2023-11-11 11:55:21 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #730: GFLOPs: 93.1865. Time: 10.9994 us. Best GFLOPs: 97.5888
2023-11-11 11:55:21 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #731: GFLOPs: 80.8121. Time: 12.6837 us. Best GFLOPs: 97.5888
2023-11-11 11:55:21 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #732: GFLOPs: 90.5456. Time: 11.3203 us. Best GFLOPs: 97.5888
2023-11-11 11:55:21 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #733: GFLOPs: 93.1812. Time: 11.0001 us. Best GFLOPs: 97.5888
2023-11-11 11:55:21 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #734: GFLOPs: 89.6262. Time: 11.4364 us. Best GFLOPs: 97.5888
2023-11-11 11:55:21 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #735: GFLOPs: 93.1786. Time: 11.0004 us. Best GFLOPs: 97.5888
2023-11-11 11:55:21 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #736: GFLOPs: 93.1831. Time: 10.9998 us. Best GFLOPs: 97.5888
2023-11-11 11:55:21 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #737: GFLOPs: 73.0948. Time: 14.0229 us. Best GFLOPs: 97.5888
2023-11-11 11:55:21 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #738: GFLOPs: 93.1865. Time: 10.9994 us. Best GFLOPs: 97.5888
2023-11-11 11:55:21 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #739: GFLOPs: 93.1818. Time: 11.0000 us. Best GFLOPs: 97.5888
2023-11-11 11:55:21 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #740: GFLOPs: 89.6265. Time: 11.4364 us. Best GFLOPs: 97.5888
2023-11-11 11:55:21 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #741: GFLOPs: 74.2025. Time: 13.8135 us. Best GFLOPs: 97.5888
2023-11-11 11:55:21 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #742: GFLOPs: 84.0959. Time: 12.1885 us. Best GFLOPs: 97.5888
2023-11-11 11:55:21 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #743: GFLOPs: 93.9430. Time: 10.9109 us. Best GFLOPs: 97.5888
2023-11-11 11:55:21 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #744: GFLOPs: 95.0863. Time: 10.7797 us. Best GFLOPs: 97.5888
2023-11-11 11:55:21 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #745: GFLOPs: 89.6146. Time: 11.4379 us. Best GFLOPs: 97.5888
2023-11-11 11:55:21 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #746: GFLOPs: 73.0963. Time: 14.0226 us. Best GFLOPs: 97.5888
2023-11-11 11:55:21 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #747: GFLOPs: 73.0927. Time: 14.0233 us. Best GFLOPs: 97.5888
2023-11-11 11:55:21 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #748: GFLOPs: 95.0863. Time: 10.7797 us. Best GFLOPs: 97.5888
2023-11-11 11:55:21 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #749: GFLOPs: 95.0883. Time: 10.7795 us. Best GFLOPs: 97.5888
2023-11-11 11:55:21 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #750: GFLOPs: 89.4125. Time: 11.4637 us. Best GFLOPs: 97.5888
2023-11-11 11:55:21 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #751: GFLOPs: 66.7079. Time: 15.3655 us. Best GFLOPs: 97.5888
2023-11-11 11:55:21 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #752: GFLOPs: 66.7065. Time: 15.3658 us. Best GFLOPs: 97.5888
2023-11-11 11:55:21 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #753: GFLOPs: 74.0808. Time: 13.8362 us. Best GFLOPs: 97.5888
2023-11-11 11:55:21 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #754: GFLOPs: 89.4438. Time: 11.4597 us. Best GFLOPs: 97.5888
2023-11-11 11:55:21 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #755: GFLOPs: 89.6172. Time: 11.4375 us. Best GFLOPs: 97.5888
2023-11-11 11:55:21 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #756: GFLOPs: 91.0231. Time: 11.2609 us. Best GFLOPs: 97.5888
2023-11-11 11:55:21 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #757: GFLOPs: 83.3011. Time: 12.3048 us. Best GFLOPs: 97.5888
2023-11-11 11:55:21 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #758: GFLOPs: 89.6262. Time: 11.4364 us. Best GFLOPs: 97.5888
2023-11-11 11:55:21 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #759: GFLOPs: 89.4387. Time: 11.4604 us. Best GFLOPs: 97.5888
2023-11-11 11:55:21 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #760: GFLOPs: 90.9914. Time: 11.2648 us. Best GFLOPs: 97.5888
2023-11-11 11:55:21 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #761: GFLOPs: 89.4505. Time: 11.4589 us. Best GFLOPs: 97.5888
2023-11-11 11:55:21 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #762: GFLOPs: 89.4426. Time: 11.4599 us. Best GFLOPs: 97.5888
2023-11-11 11:55:21 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #763: GFLOPs: 89.4355. Time: 11.4608 us. Best GFLOPs: 97.5888
2023-11-11 11:55:21 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #764: GFLOPs: 3.3971. Time: 301.7303 us. Best GFLOPs: 97.5888
2023-11-11 11:55:21 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #765: GFLOPs: 74.0881. Time: 13.8349 us. Best GFLOPs: 97.5888
2023-11-11 11:55:21 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #766: GFLOPs: 23.1826. Time: 44.2142 us. Best GFLOPs: 97.5888
2023-11-11 11:58:26 [INFO] [evolutionary_search.cc:713] Generating candidates......
2023-11-11 11:58:26 [INFO] [evolutionary_search.cc:715] Picked top 102 candidate(s) from database
2023-11-11 11:58:27 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310a405238)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310e76ec68)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310dc33cd8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310be916f8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56310f803f18)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310eeb7a68)]: 397 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310e771ba8)]: 0 failure(s)
2023-11-11 11:58:28 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310a405238)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310e76ec68)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310dc33cd8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310be916f8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56310f803f18)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310eeb7a68)]: 798 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310e771ba8)]: 0 failure(s)
2023-11-11 11:58:28 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310a405238)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310e76ec68)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310dc33cd8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310be916f8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56310f803f18)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310eeb7a68)]: 1198 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310e771ba8)]: 0 failure(s)
2023-11-11 11:58:29 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310a405238)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310e76ec68)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310dc33cd8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310be916f8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56310f803f18)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310eeb7a68)]: 1599 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310e771ba8)]: 0 failure(s)
2023-11-11 11:58:30 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310a405238)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310e76ec68)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310dc33cd8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310be916f8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56310f803f18)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310eeb7a68)]: 1998 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310e771ba8)]: 0 failure(s)
2023-11-11 11:58:30 [INFO] [evolutionary_search.cc:723] Sampled 52 candidate(s)
2023-11-11 11:58:32 [INFO] [evolutionary_search.cc:621] Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310a405238)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310e76ec68)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310dc33cd8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310be916f8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56310f803f18)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310eeb7a68)]: 178 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310e771ba8)]: 0 failure(s)
2023-11-11 11:58:35 [INFO] [evolutionary_search.cc:621] Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310a405238)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310e76ec68)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310dc33cd8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310be916f8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56310f803f18)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310eeb7a68)]: 185 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310e771ba8)]: 0 failure(s)
2023-11-11 11:58:38 [INFO] [evolutionary_search.cc:621] Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310a405238)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310e76ec68)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310dc33cd8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310be916f8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56310f803f18)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310eeb7a68)]: 153 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310e771ba8)]: 0 failure(s)
2023-11-11 11:58:41 [INFO] [evolutionary_search.cc:621] Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310a405238)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310e76ec68)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310dc33cd8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310be916f8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56310f803f18)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310eeb7a68)]: 165 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310e771ba8)]: 0 failure(s)
2023-11-11 11:58:42 [INFO] [evolutionary_search.cc:649] Scores of the best 64 candidates:
[1 : 16]:	0.9403  0.9370  0.9370  0.9356  0.9336  0.9280  0.8992  0.8956  0.8922  0.8864  0.8864  0.8864  0.8842  0.8808  0.8795  0.8761
[17 : 32]:	0.8680  0.8639  0.8587  0.8587  0.8562  0.8499  0.8444  0.8371  0.8354  0.8313  0.8262  0.8262  0.8207  0.8129  0.8092  0.8084
[33 : 48]:	0.8082  0.8075  0.8066  0.8051  0.7999  0.7998  0.7978  0.7976  0.7969  0.7964  0.7964  0.7933  0.7933  0.7933  0.7933  0.7887
[49 : 64]:	0.7887  0.7821  0.7796  0.7754  0.7752  0.7740  0.7723  0.7723  0.7723  0.7721  0.7677  0.7677  0.7677  0.7675  0.7668  0.7668
2023-11-11 11:58:42 [INFO] [evolutionary_search.cc:727] Got 64 candidate(s) with evolutionary search
2023-11-11 11:58:42 [INFO] [evolutionary_search.cc:730] Sending 64 candidates(s) for measurement
2023-11-11 11:59:10 [INFO] [task_scheduler.cc:121] [Task #28: fused_nn_dense_add] Trial #767: Error in running:
LocalRunner: An exception occurred
Subprocess terminated
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(512)), "float32"), p1: T.Buffer((T.int64(1000), T.int64(512)), "float32"), p2: T.Buffer((T.int64(1), T.int64(1000)), "float32"), T_add: T.Buffer((T.int64(1), T.int64(1000)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        T_matmul_NT_local = T.alloc_buffer((T.int64(1), T.int64(1000)), scope="local")
        p0_shared = T.alloc_buffer((T.int64(1), T.int64(512)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(1000), T.int64(512)), scope="shared")
        for i_0_j_0_fused in T.thread_binding(T.int64(25), thread="blockIdx.x"):
            for i_1_j_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for i_2_j_2_fused in T.thread_binding(T.int64(40), thread="threadIdx.x"):
                    for i_3_init, j_3_init, i_4_init, j_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_init"):
                            v_i = T.axis.spatial(T.int64(1), i_3_init + i_4_init)
                            v_j = T.axis.spatial(T.int64(1000), i_0_j_0_fused * T.int64(40) + i_2_j_2_fused + j_3_init + j_4_init)
                            T.reads()
                            T.writes(T_matmul_NT_local[v_i, v_j])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            T_matmul_NT_local[v_i, v_j] = T.float32(0)
                    for k_0_fused in T.serial(T.int64(8), annotations={"software_pipeline_async_stages": [0], "software_pipeline_order": [0, 1, 2], "software_pipeline_stage": [0, 0, 2]}):
                        for ax0_ax1_fused_0 in range(T.int64(2)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(40), thread="threadIdx.x"):
                                with T.block("p0_shared"):
                                    v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                    v1 = T.axis.spatial(T.int64(512), k_0_fused * T.int64(64) + (ax0_ax1_fused_0 * T.int64(40) + ax0_ax1_fused_1))
                                    T.where(ax0_ax1_fused_0 * T.int64(40) + ax0_ax1_fused_1 < T.int64(64))
                                    T.reads(p0[v0, v1])
                                    T.writes(p0_shared[v0, v1])
                                    p0_shared[v0, v1] = p0[v0, v1]
                        for ax0_ax1_fused_0 in range(T.int64(16)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(40), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("p1_shared"):
                                        v0 = T.axis.spatial(T.int64(1000), i_0_j_0_fused * T.int64(40) + (ax0_ax1_fused_0 * T.int64(160) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2) // T.int64(64))
                                        v1 = T.axis.spatial(T.int64(512), k_0_fused * T.int64(64) + (ax0_ax1_fused_0 * T.int64(160) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2) % T.int64(64))
                                        T.reads(p1[v0, v1])
                                        T.writes(p1_shared[v0, v1])
                                        p1_shared[v0, v1] = p1[v0, v1]
                        for k_1, i_3, j_3, k_2, i_4, j_4 in T.grid(T.int64(32), T.int64(1), T.int64(1), T.int64(2), T.int64(1), T.int64(1)):
                            with T.block("T_matmul_NT_update"):
                                v_i = T.axis.spatial(T.int64(1), i_3 + i_4)
                                v_j = T.axis.spatial(T.int64(1000), i_0_j_0_fused * T.int64(40) + i_2_j_2_fused + j_3 + j_4)
                                v_k = T.axis.reduce(T.int64(512), k_0_fused * T.int64(64) + k_1 * T.int64(2) + k_2)
                                T.reads(T_matmul_NT_local[v_i, v_j], p0_shared[v_i, v_k], p1_shared[v_j, v_k])
                                T.writes(T_matmul_NT_local[v_i, v_j])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                T_matmul_NT_local[v_i, v_j] = T_matmul_NT_local[v_i, v_j] + p0_shared[v_i, v_k] * p1_shared[v_j, v_k]
                    for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(1000), i_0_j_0_fused * T.int64(40) + i_2_j_2_fused + ax1)
                            T.reads(T_matmul_NT_local[v0, v1], p2[v0, v1])
                            T.writes(T_add[v0, v1])
                            T_add[v0, v1] = T_matmul_NT_local[v0, v1] + p2[v0, v1]
b0 = sch.get_block(name="T_matmul_NT", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l3, l4, l5 = sch.get_loops(block=b0)
v6, v7, v8, v9, v10 = sch.sample_perfect_tile(loop=l3, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l11, l12, l13, l14, l15 = sch.split(loop=l3, factors=[v6, v7, v8, v9, v10], preserve_unit_iters=True)
v16, v17, v18, v19, v20 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[25, 1, 40, 1, 1])
l21, l22, l23, l24, l25 = sch.split(loop=l4, factors=[v16, v17, v18, v19, v20], preserve_unit_iters=True)
v26, v27, v28 = sch.sample_perfect_tile(loop=l5, n=3, max_innermost_factor=64, decision=[8, 32, 2])
l29, l30, l31 = sch.split(loop=l5, factors=[v26, v27, v28], preserve_unit_iters=True)
sch.reorder(l11, l21, l12, l22, l13, l23, l29, l30, l14, l24, l31, l15, l25)
l32 = sch.fuse(l11, l21, preserve_unit_iters=True)
sch.bind(loop=l32, thread_axis="blockIdx.x")
l33 = sch.fuse(l12, l22, preserve_unit_iters=True)
sch.bind(loop=l33, thread_axis="vthread.x")
l34 = sch.fuse(l13, l23, preserve_unit_iters=True)
sch.bind(loop=l34, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b35 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b35, loop=l34, preserve_unit_loops=True, index=-1)
b36 = sch.cache_read(block=b0, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b36, loop=l29, preserve_unit_loops=True, index=-1)
l37, l38, l39, l40, l41, l42 = sch.get_loops(block=b36)
l43 = sch.fuse(l41, l42, preserve_unit_iters=True)
v44 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b36, ann_key="meta_schedule.cooperative_fetch", ann_val=v44)
b45 = sch.cache_read(block=b0, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b45, loop=l29, preserve_unit_loops=True, index=-1)
l46, l47, l48, l49, l50, l51 = sch.get_loops(block=b45)
l52 = sch.fuse(l50, l51, preserve_unit_iters=True)
v53 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b45, ann_key="meta_schedule.cooperative_fetch", ann_val=v53)
l54 = sch.fuse(l29, preserve_unit_iters=True)
sch.annotate(block_or_loop=l54, ann_key="software_pipeline_stage", ann_val=[0, 0, 2])
sch.annotate(block_or_loop=l54, ann_key="software_pipeline_order", ann_val=[0, 1, 2])
sch.annotate(block_or_loop=l54, ann_key="software_pipeline_async_stages", ann_val=[0])
sch.reverse_compute_inline(block=b1)
v55 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=0)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v55)
sch.enter_postproc()
sch.unannotate(block_or_loop=b36, ann_key="meta_schedule.cooperative_fetch")
l56, l57, l58, l59, l60 = sch.get_loops(block=b36)
l61, l62 = sch.split(loop=l60, factors=[None, 40], preserve_unit_iters=True)
sch.bind(loop=l62, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b45, ann_key="meta_schedule.cooperative_fetch")
l63, l64, l65, l66, l67 = sch.get_loops(block=b45)
l68, l69, l70 = sch.split(loop=l67, factors=[None, 40, 4], preserve_unit_iters=True)
sch.vectorize(loop=l70)
sch.bind(loop=l69, thread_axis="threadIdx.x")
b71 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b71, ann_key="meta_schedule.unroll_explicit")
b72, b73, b74, b75 = sch.get_child_blocks(b71)
l76, l77, l78, l79, l80, l81 = sch.get_loops(block=b72)
l82, l83, l84, l85, l86, l87, l88 = sch.get_loops(block=b73)
l89, l90, l91, l92, l93, l94, l95, l96, l97, l98 = sch.get_loops(block=b74)
l99, l100, l101, l102, l103 = sch.get_loops(block=b75)
b104 = sch.get_block(name="T_matmul_NT", func_name="main")
l105, l106, l107, l108, l109, l110, l111, l112, l113, l114 = sch.get_loops(block=b104)
b115 = sch.decompose_reduction(block=b104, loop=l108)
2023-11-11 11:59:10 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #768: GFLOPs: 94.3951. Time: 10.8586 us. Best GFLOPs: 97.5888
2023-11-11 11:59:10 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #769: GFLOPs: 92.5436. Time: 11.0759 us. Best GFLOPs: 97.5888
2023-11-11 11:59:10 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #770: GFLOPs: 92.0980. Time: 11.1295 us. Best GFLOPs: 97.5888
2023-11-11 11:59:10 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #771: GFLOPs: 92.5249. Time: 11.0781 us. Best GFLOPs: 97.5888
2023-11-11 11:59:10 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #772: GFLOPs: 94.3971. Time: 10.8584 us. Best GFLOPs: 97.5888
2023-11-11 11:59:10 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #773: GFLOPs: 94.1891. Time: 10.8824 us. Best GFLOPs: 97.5888
2023-11-11 11:59:10 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #774: GFLOPs: 94.1921. Time: 10.8820 us. Best GFLOPs: 97.5888
2023-11-11 11:59:10 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #775: GFLOPs: 88.9874. Time: 11.5185 us. Best GFLOPs: 97.5888
2023-11-11 11:59:10 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #776: GFLOPs: 94.1725. Time: 10.8843 us. Best GFLOPs: 97.5888
2023-11-11 11:59:10 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #777: GFLOPs: 94.1712. Time: 10.8844 us. Best GFLOPs: 97.5888
2023-11-11 11:59:10 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #778: GFLOPs: 73.9379. Time: 13.8630 us. Best GFLOPs: 97.5888
2023-11-11 11:59:10 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #779: GFLOPs: 88.9316. Time: 11.5257 us. Best GFLOPs: 97.5888
2023-11-11 11:59:10 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #780: GFLOPs: 90.7191. Time: 11.2986 us. Best GFLOPs: 97.5888
2023-11-11 11:59:10 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #781: GFLOPs: 88.9456. Time: 11.5239 us. Best GFLOPs: 97.5888
2023-11-11 11:59:10 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #782: GFLOPs: 90.7243. Time: 11.2980 us. Best GFLOPs: 97.5888
2023-11-11 11:59:10 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #783: GFLOPs: 88.9855. Time: 11.5187 us. Best GFLOPs: 97.5888
2023-11-11 11:59:10 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #784: GFLOPs: 89.0001. Time: 11.5168 us. Best GFLOPs: 97.5888
2023-11-11 11:59:10 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #785: GFLOPs: 88.9819. Time: 11.5192 us. Best GFLOPs: 97.5888
2023-11-11 11:59:10 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #786: GFLOPs: 88.9959. Time: 11.5174 us. Best GFLOPs: 97.5888
2023-11-11 11:59:10 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #787: GFLOPs: 84.1574. Time: 12.1796 us. Best GFLOPs: 97.5888
2023-11-11 11:59:10 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #788: GFLOPs: 85.7432. Time: 11.9543 us. Best GFLOPs: 97.5888
2023-11-11 11:59:10 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #789: GFLOPs: 94.3795. Time: 10.8604 us. Best GFLOPs: 97.5888
2023-11-11 11:59:10 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #790: GFLOPs: 85.7271. Time: 11.9565 us. Best GFLOPs: 97.5888
2023-11-11 11:59:10 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #791: GFLOPs: 88.9469. Time: 11.5237 us. Best GFLOPs: 97.5888
2023-11-11 11:59:10 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #792: GFLOPs: 88.9526. Time: 11.5230 us. Best GFLOPs: 97.5888
2023-11-11 11:59:10 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #793: GFLOPs: 88.9500. Time: 11.5233 us. Best GFLOPs: 97.5888
2023-11-11 11:59:10 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #794: GFLOPs: 88.9488. Time: 11.5235 us. Best GFLOPs: 97.5888
2023-11-11 11:59:10 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #795: GFLOPs: 66.4256. Time: 15.4308 us. Best GFLOPs: 97.5888
2023-11-11 11:59:10 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #796: GFLOPs: 88.9919. Time: 11.5179 us. Best GFLOPs: 97.5888
2023-11-11 11:59:10 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #797: GFLOPs: 90.7301. Time: 11.2972 us. Best GFLOPs: 97.5888
2023-11-11 11:59:10 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #798: GFLOPs: 88.9912. Time: 11.5180 us. Best GFLOPs: 97.5888
2023-11-11 11:59:10 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #799: GFLOPs: 88.9976. Time: 11.5172 us. Best GFLOPs: 97.5888
2023-11-11 11:59:10 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #800: GFLOPs: 88.9988. Time: 11.5170 us. Best GFLOPs: 97.5888
2023-11-11 11:59:10 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #801: GFLOPs: 66.4301. Time: 15.4297 us. Best GFLOPs: 97.5888
2023-11-11 11:59:10 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #802: GFLOPs: 90.7429. Time: 11.2957 us. Best GFLOPs: 97.5888
2023-11-11 11:59:10 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #803: GFLOPs: 90.7339. Time: 11.2968 us. Best GFLOPs: 97.5888
2023-11-11 11:59:10 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #804: GFLOPs: 73.4163. Time: 13.9615 us. Best GFLOPs: 97.5888
2023-11-11 11:59:10 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #805: GFLOPs: 88.9887. Time: 11.5183 us. Best GFLOPs: 97.5888
2023-11-11 11:59:10 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #806: GFLOPs: 88.9868. Time: 11.5186 us. Best GFLOPs: 97.5888
2023-11-11 11:59:10 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #807: GFLOPs: 88.9837. Time: 11.5190 us. Best GFLOPs: 97.5888
2023-11-11 11:59:10 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #808: GFLOPs: 94.4052. Time: 10.8574 us. Best GFLOPs: 97.5888
2023-11-11 11:59:10 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #809: GFLOPs: 73.8235. Time: 13.8845 us. Best GFLOPs: 97.5888
2023-11-11 11:59:10 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #810: GFLOPs: 73.8240. Time: 13.8844 us. Best GFLOPs: 97.5888
2023-11-11 11:59:10 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #811: GFLOPs: 73.8266. Time: 13.8839 us. Best GFLOPs: 97.5888
2023-11-11 11:59:10 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #812: GFLOPs: 94.3971. Time: 10.8584 us. Best GFLOPs: 97.5888
2023-11-11 11:59:10 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #813: GFLOPs: 94.3924. Time: 10.8589 us. Best GFLOPs: 97.5888
2023-11-11 11:59:10 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #814: GFLOPs: 55.2460. Time: 18.5534 us. Best GFLOPs: 97.5888
2023-11-11 11:59:10 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #815: GFLOPs: 81.2668. Time: 12.6128 us. Best GFLOPs: 97.5888
2023-11-11 11:59:10 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #816: GFLOPs: 69.1091. Time: 14.8316 us. Best GFLOPs: 97.5888
2023-11-11 11:59:10 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #817: GFLOPs: 93.7512. Time: 10.9332 us. Best GFLOPs: 97.5888
2023-11-11 11:59:10 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #818: GFLOPs: 92.5284. Time: 11.0777 us. Best GFLOPs: 97.5888
2023-11-11 11:59:10 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #819: GFLOPs: 55.2461. Time: 18.5533 us. Best GFLOPs: 97.5888
2023-11-11 11:59:10 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #820: GFLOPs: 88.9412. Time: 11.5245 us. Best GFLOPs: 97.5888
2023-11-11 11:59:10 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #821: GFLOPs: 73.4168. Time: 13.9614 us. Best GFLOPs: 97.5888
2023-11-11 11:59:10 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #822: GFLOPs: 73.4105. Time: 13.9626 us. Best GFLOPs: 97.5888
2023-11-11 11:59:10 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #823: GFLOPs: 92.5466. Time: 11.0755 us. Best GFLOPs: 97.5888
2023-11-11 11:59:10 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #824: GFLOPs: 69.4054. Time: 14.7683 us. Best GFLOPs: 97.5888
2023-11-11 11:59:10 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #825: GFLOPs: 41.8544. Time: 24.4896 us. Best GFLOPs: 97.5888
2023-11-11 11:59:10 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #826: GFLOPs: 41.8463. Time: 24.4944 us. Best GFLOPs: 97.5888
2023-11-11 11:59:10 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #827: GFLOPs: 41.8452. Time: 24.4951 us. Best GFLOPs: 97.5888
2023-11-11 11:59:10 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #828: GFLOPs: 10.7378. Time: 95.4575 us. Best GFLOPs: 97.5888
2023-11-11 11:59:10 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #829: GFLOPs: 1.9844. Time: 516.5341 us. Best GFLOPs: 97.5888
2023-11-11 11:59:10 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #830: GFLOPs: 85.5982. Time: 11.9745 us. Best GFLOPs: 97.5888
2023-11-11 12:00:32 [INFO] [evolutionary_search.cc:713] Generating candidates......
2023-11-11 12:00:32 [INFO] [evolutionary_search.cc:715] Picked top 102 candidate(s) from database
2023-11-11 12:00:33 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310a405238)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310e76ec68)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310dc33cd8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310be916f8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56310f803f18)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310eeb7a68)]: 402 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310e771ba8)]: 0 failure(s)
2023-11-11 12:00:33 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310a405238)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310e76ec68)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310dc33cd8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310be916f8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56310f803f18)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310eeb7a68)]: 800 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310e771ba8)]: 0 failure(s)
2023-11-11 12:00:34 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310a405238)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310e76ec68)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310dc33cd8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310be916f8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56310f803f18)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310eeb7a68)]: 1205 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310e771ba8)]: 0 failure(s)
2023-11-11 12:00:35 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310a405238)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310e76ec68)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310dc33cd8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310be916f8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56310f803f18)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310eeb7a68)]: 1609 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310e771ba8)]: 0 failure(s)
2023-11-11 12:00:35 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310a405238)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310e76ec68)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310dc33cd8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310be916f8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56310f803f18)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310eeb7a68)]: 2010 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310e771ba8)]: 0 failure(s)
2023-11-11 12:00:36 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310a405238)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310e76ec68)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310dc33cd8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310be916f8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56310f803f18)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310eeb7a68)]: 2414 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310e771ba8)]: 0 failure(s)
2023-11-11 12:00:37 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310a405238)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310e76ec68)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310dc33cd8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310be916f8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56310f803f18)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310eeb7a68)]: 2820 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310e771ba8)]: 0 failure(s)
2023-11-11 12:00:37 [INFO] [evolutionary_search.cc:723] Sampled 50 candidate(s)
2023-11-11 12:00:39 [INFO] [evolutionary_search.cc:621] Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310a405238)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310e76ec68)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310dc33cd8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310be916f8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56310f803f18)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310eeb7a68)]: 207 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310e771ba8)]: 0 failure(s)
2023-11-11 12:00:42 [INFO] [evolutionary_search.cc:621] Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310a405238)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310e76ec68)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310dc33cd8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310be916f8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56310f803f18)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310eeb7a68)]: 194 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310e771ba8)]: 0 failure(s)
2023-11-11 12:00:45 [INFO] [evolutionary_search.cc:621] Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310a405238)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310e76ec68)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310dc33cd8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310be916f8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56310f803f18)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310eeb7a68)]: 192 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310e771ba8)]: 0 failure(s)
2023-11-11 12:00:48 [INFO] [evolutionary_search.cc:621] Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310a405238)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310e76ec68)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310dc33cd8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310be916f8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56310f803f18)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310eeb7a68)]: 180 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310e771ba8)]: 0 failure(s)
2023-11-11 12:00:49 [INFO] [evolutionary_search.cc:649] Scores of the best 64 candidates:
[1 : 16]:	0.9744  0.9424  0.9356  0.9336  0.9117  0.9010  0.8853  0.8849  0.8840  0.8619  0.8386  0.8300  0.8262  0.8254  0.8204  0.8201
[17 : 32]:	0.8129  0.8075  0.7999  0.7879  0.7879  0.7764  0.7740  0.7723  0.7723  0.7723  0.7721  0.7696  0.7668  0.7573  0.7564  0.7553
[33 : 48]:	0.7521  0.7521  0.7521  0.7509  0.7496  0.7493  0.7479  0.7479  0.7465  0.7460  0.7460  0.7460  0.7419  0.7411  0.7404  0.7371
[49 : 64]:	0.7337  0.7328  0.7328  0.7325  0.7311  0.7303  0.7296  0.7279  0.7278  0.7278  0.7235  0.7220  0.7204  0.7204  0.7156  0.7150
2023-11-11 12:00:50 [INFO] [evolutionary_search.cc:727] Got 64 candidate(s) with evolutionary search
2023-11-11 12:00:50 [INFO] [evolutionary_search.cc:730] Sending 64 candidates(s) for measurement
2023-11-11 12:01:18 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #831: GFLOPs: 3.7401. Time: 274.0541 us. Best GFLOPs: 97.5888
2023-11-11 12:01:18 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #832: GFLOPs: 92.5370. Time: 11.0767 us. Best GFLOPs: 97.5888
2023-11-11 12:01:18 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #833: GFLOPs: 90.6787. Time: 11.3036 us. Best GFLOPs: 97.5888
2023-11-11 12:01:18 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #834: GFLOPs: 90.7347. Time: 11.2967 us. Best GFLOPs: 97.5888
2023-11-11 12:01:18 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #835: GFLOPs: 88.9874. Time: 11.5185 us. Best GFLOPs: 97.5888
2023-11-11 12:01:18 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #836: GFLOPs: 88.9760. Time: 11.5200 us. Best GFLOPs: 97.5888
2023-11-11 12:01:18 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #837: GFLOPs: 87.3575. Time: 11.7334 us. Best GFLOPs: 97.5888
2023-11-11 12:01:18 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #838: GFLOPs: 90.7202. Time: 11.2985 us. Best GFLOPs: 97.5888
2023-11-11 12:01:18 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #839: GFLOPs: 73.5557. Time: 13.9350 us. Best GFLOPs: 97.5888
2023-11-11 12:01:18 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #840: GFLOPs: 94.4188. Time: 10.8559 us. Best GFLOPs: 97.5888
2023-11-11 12:01:18 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #841: GFLOPs: 85.7246. Time: 11.9569 us. Best GFLOPs: 97.5888
2023-11-11 12:01:18 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #842: GFLOPs: 88.9342. Time: 11.5254 us. Best GFLOPs: 97.5888
2023-11-11 12:01:18 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #843: GFLOPs: 88.9831. Time: 11.5190 us. Best GFLOPs: 97.5888
2023-11-11 12:01:18 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #844: GFLOPs: 88.9939. Time: 11.5176 us. Best GFLOPs: 97.5888
2023-11-11 12:01:18 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #845: GFLOPs: 81.5325. Time: 12.5717 us. Best GFLOPs: 97.5888
2023-11-11 12:01:18 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #846: GFLOPs: 88.9868. Time: 11.5186 us. Best GFLOPs: 97.5888
2023-11-11 12:01:18 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #847: GFLOPs: 66.4193. Time: 15.4323 us. Best GFLOPs: 97.5888
2023-11-11 12:01:18 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #848: GFLOPs: 88.9874. Time: 11.5185 us. Best GFLOPs: 97.5888
2023-11-11 12:01:18 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #849: GFLOPs: 90.7358. Time: 11.2965 us. Best GFLOPs: 97.5888
2023-11-11 12:01:18 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #850: GFLOPs: 73.8163. Time: 13.8858 us. Best GFLOPs: 97.5888
2023-11-11 12:01:18 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #851: GFLOPs: 94.3923. Time: 10.8589 us. Best GFLOPs: 97.5888
2023-11-11 12:01:18 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #852: GFLOPs: 69.1364. Time: 14.8258 us. Best GFLOPs: 97.5888
2023-11-11 12:01:18 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #853: GFLOPs: 67.5030. Time: 15.1845 us. Best GFLOPs: 97.5888
2023-11-11 12:01:18 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #854: GFLOPs: 92.5219. Time: 11.0785 us. Best GFLOPs: 97.5888
2023-11-11 12:01:18 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #855: GFLOPs: 92.5213. Time: 11.0785 us. Best GFLOPs: 97.5888
2023-11-11 12:01:18 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #856: GFLOPs: 92.5597. Time: 11.0739 us. Best GFLOPs: 97.5888
2023-11-11 12:01:18 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #857: GFLOPs: 69.1340. Time: 14.8263 us. Best GFLOPs: 97.5888
2023-11-11 12:01:18 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #858: GFLOPs: 66.5339. Time: 15.4057 us. Best GFLOPs: 97.5888
2023-11-11 12:01:18 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #859: GFLOPs: 92.5269. Time: 11.0779 us. Best GFLOPs: 97.5888
2023-11-11 12:01:18 [INFO] [task_scheduler.cc:121] [Task #28: fused_nn_dense_add] Trial #860: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/home2/xiachunwei/Software/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/home2/xiachunwei/Software/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 159, in _worker_func
    costs: List[float] = f_run_evaluator(
  File "/home2/xiachunwei/Software/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 387, in default_run_evaluator
    return run_evaluator_common(rt_mod, device, evaluator_config, repeated_args)
  File "/home2/xiachunwei/Software/tvm/python/tvm/meta_schedule/runner/utils.py", line 117, in run_evaluator_common
    profile_result = evaluator(*args)
  File "/home2/xiachunwei/Software/tvm/python/tvm/runtime/module.py", line 397, in evaluator
    blob = feval(*args)
  File "/home2/xiachunwei/Software/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/home2/xiachunwei/Software/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  1: tvm::runtime::profiling::WrapTimeEvaluator(tvm::runtime::PackedFunc, DLDevice, int, int, int, int, int, int, int, tvm::runtime::PackedFunc)::{lambda(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*) [clone .isra.779]
  0: tvm::runtime::CUDADeviceAPI::StreamSync(DLDevice, void*)
  File "/home2/xiachunwei/Software/tvm/src/runtime/cuda/cuda_device_api.cc", line 212
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(512)), "float32"), p1: T.Buffer((T.int64(1000), T.int64(512)), "float32"), p2: T.Buffer((T.int64(1), T.int64(1000)), "float32"), T_add: T.Buffer((T.int64(1), T.int64(1000)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        T_matmul_NT_local = T.alloc_buffer((T.int64(1), T.int64(1000)), scope="local")
        p0_shared = T.alloc_buffer((T.int64(1), T.int64(512)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(1000), T.int64(512)), scope="shared")
        for i_0_j_0_fused in T.thread_binding(T.int64(1), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 64, "pragma_unroll_explicit": 1}):
            for i_1_j_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for i_2_j_2_fused in T.thread_binding(T.int64(1000), thread="threadIdx.x"):
                    for i_3_init, j_3_init, i_4_init, j_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_init"):
                            v_i = T.axis.spatial(T.int64(1), i_3_init + i_4_init)
                            v_j = T.axis.spatial(T.int64(1000), i_2_j_2_fused + j_3_init + j_4_init)
                            T.reads()
                            T.writes(T_matmul_NT_local[v_i, v_j])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            T_matmul_NT_local[v_i, v_j] = T.float32(0)
                    for k_0 in range(T.int64(256)):
                        for ax0_ax1_fused_0 in range(T.int64(1)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(1000), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("p0_shared"):
                                        v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v1 = T.axis.spatial(T.int64(512), k_0 * T.int64(2) + (ax0_ax1_fused_0 * T.int64(2000) + ax0_ax1_fused_1 * T.int64(2) + ax0_ax1_fused_2))
                                        T.where((ax0_ax1_fused_0 * T.int64(1000) + ax0_ax1_fused_1) * T.int64(2) + ax0_ax1_fused_2 < T.int64(2))
                                        T.reads(p0[v0, v1])
                                        T.writes(p0_shared[v0, v1])
                                        p0_shared[v0, v1] = p0[v0, v1]
                        for ax0_ax1_fused_0 in range(T.int64(1)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(1000), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("p1_shared"):
                                        v0 = T.axis.spatial(T.int64(1000), (ax0_ax1_fused_0 * T.int64(4000) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2) // T.int64(2))
                                        v1 = T.axis.spatial(T.int64(512), k_0 * T.int64(2) + (ax0_ax1_fused_0 * T.int64(4000) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2) % T.int64(2))
                                        T.where((ax0_ax1_fused_0 * T.int64(1000) + ax0_ax1_fused_1) * T.int64(4) + ax0_ax1_fused_2 < T.int64(2000))
                                        T.reads(p1[v0, v1])
                                        T.writes(p1_shared[v0, v1])
                                        p1_shared[v0, v1] = p1[v0, v1]
                        for k_1, i_3, j_3, k_2, i_4, j_4 in T.grid(T.int64(2), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                            with T.block("T_matmul_NT_update"):
                                v_i = T.axis.spatial(T.int64(1), i_3 + i_4)
                                v_j = T.axis.spatial(T.int64(1000), i_2_j_2_fused + j_3 + j_4)
                                v_k = T.axis.reduce(T.int64(512), k_0 * T.int64(2) + k_1 + k_2)
                                T.reads(T_matmul_NT_local[v_i, v_j], p0_shared[v_i, v_k], p1_shared[v_j, v_k])
                                T.writes(T_matmul_NT_local[v_i, v_j])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                T_matmul_NT_local[v_i, v_j] = T_matmul_NT_local[v_i, v_j] + p0_shared[v_i, v_k] * p1_shared[v_j, v_k]
                    for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(1000), i_2_j_2_fused + ax1)
                            T.reads(T_matmul_NT_local[v0, v1], p2[v0, v1])
                            T.writes(T_add[v0, v1])
                            T_add[v0, v1] = T_matmul_NT_local[v0, v1] + p2[v0, v1]
b0 = sch.get_block(name="T_matmul_NT", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l3, l4, l5 = sch.get_loops(block=b0)
v6, v7, v8, v9, v10 = sch.sample_perfect_tile(loop=l3, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l11, l12, l13, l14, l15 = sch.split(loop=l3, factors=[v6, v7, v8, v9, v10], preserve_unit_iters=True)
v16, v17, v18, v19, v20 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[1, 1, 1000, 1, 1])
l21, l22, l23, l24, l25 = sch.split(loop=l4, factors=[v16, v17, v18, v19, v20], preserve_unit_iters=True)
v26, v27, v28 = sch.sample_perfect_tile(loop=l5, n=3, max_innermost_factor=64, decision=[256, 2, 1])
l29, l30, l31 = sch.split(loop=l5, factors=[v26, v27, v28], preserve_unit_iters=True)
sch.reorder(l11, l21, l12, l22, l13, l23, l29, l30, l14, l24, l31, l15, l25)
l32 = sch.fuse(l11, l21, preserve_unit_iters=True)
sch.bind(loop=l32, thread_axis="blockIdx.x")
l33 = sch.fuse(l12, l22, preserve_unit_iters=True)
sch.bind(loop=l33, thread_axis="vthread.x")
l34 = sch.fuse(l13, l23, preserve_unit_iters=True)
sch.bind(loop=l34, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b35 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b35, loop=l34, preserve_unit_loops=True, index=-1)
b36 = sch.cache_read(block=b0, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b36, loop=l29, preserve_unit_loops=True, index=-1)
l37, l38, l39, l40, l41, l42 = sch.get_loops(block=b36)
l43 = sch.fuse(l41, l42, preserve_unit_iters=True)
v44 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b36, ann_key="meta_schedule.cooperative_fetch", ann_val=v44)
b45 = sch.cache_read(block=b0, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b45, loop=l29, preserve_unit_loops=True, index=-1)
l46, l47, l48, l49, l50, l51 = sch.get_loops(block=b45)
l52 = sch.fuse(l50, l51, preserve_unit_iters=True)
v53 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b45, ann_key="meta_schedule.cooperative_fetch", ann_val=v53)
sch.reverse_compute_inline(block=b1)
v54 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=2)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v54)
sch.enter_postproc()
sch.unannotate(block_or_loop=b36, ann_key="meta_schedule.cooperative_fetch")
l55, l56, l57, l58, l59 = sch.get_loops(block=b36)
l60, l61, l62 = sch.split(loop=l59, factors=[None, 1000, 2], preserve_unit_iters=True)
sch.vectorize(loop=l62)
sch.bind(loop=l61, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b45, ann_key="meta_schedule.cooperative_fetch")
l63, l64, l65, l66, l67 = sch.get_loops(block=b45)
l68, l69, l70 = sch.split(loop=l67, factors=[None, 1000, 4], preserve_unit_iters=True)
sch.vectorize(loop=l70)
sch.bind(loop=l69, thread_axis="threadIdx.x")
b71 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b71, ann_key="meta_schedule.unroll_explicit")
b72, b73, b74, b75 = sch.get_child_blocks(b71)
l76, l77, l78, l79, l80, l81, l82 = sch.get_loops(block=b72)
l83, l84, l85, l86, l87, l88, l89 = sch.get_loops(block=b73)
l90, l91, l92, l93, l94, l95, l96, l97, l98, l99 = sch.get_loops(block=b74)
sch.annotate(block_or_loop=l90, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l90, ann_key="pragma_unroll_explicit", ann_val=1)
l100, l101, l102, l103, l104 = sch.get_loops(block=b75)
b105 = sch.get_block(name="T_matmul_NT", func_name="main")
l106, l107, l108, l109, l110, l111, l112, l113, l114, l115 = sch.get_loops(block=b105)
b116 = sch.decompose_reduction(block=b105, loop=l109)
2023-11-11 12:01:18 [INFO] [task_scheduler.cc:121] [Task #28: fused_nn_dense_add] Trial #861: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/home2/xiachunwei/Software/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/home2/xiachunwei/Software/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/home2/xiachunwei/Software/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/home2/xiachunwei/Software/tvm/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/home2/xiachunwei/Software/tvm/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/home2/xiachunwei/Software/tvm/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/home2/xiachunwei/Software/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/home2/xiachunwei/Software/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  4: _ZN3tvm7runtime13PackedFun
  3: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  2: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  0: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  File "/home2/xiachunwei/Software/tvm/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(512)), "float32"), p1: T.Buffer((T.int64(1000), T.int64(512)), "float32"), p2: T.Buffer((T.int64(1), T.int64(1000)), "float32"), T_add: T.Buffer((T.int64(1), T.int64(1000)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        T_matmul_NT_local = T.alloc_buffer((T.int64(1), T.int64(1000)), scope="local")
        p0_shared = T.alloc_buffer((T.int64(1), T.int64(512)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(1000), T.int64(512)), scope="shared")
        for i_0_j_0_fused in T.thread_binding(T.int64(25), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 16, "pragma_unroll_explicit": 1}):
            for i_1_j_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for i_2_j_2_fused in T.thread_binding(T.int64(40), thread="threadIdx.x"):
                    for i_3_init, j_3_init, i_4_init, j_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_init"):
                            v_i = T.axis.spatial(T.int64(1), i_3_init + i_4_init)
                            v_j = T.axis.spatial(T.int64(1000), i_0_j_0_fused * T.int64(40) + i_2_j_2_fused + j_3_init + j_4_init)
                            T.reads()
                            T.writes(T_matmul_NT_local[v_i, v_j])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            T_matmul_NT_local[v_i, v_j] = T.float32(0)
                    for k_0 in range(T.int64(2)):
                        for ax0_ax1_fused_0 in range(T.int64(2)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(40), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("p0_shared"):
                                        v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v1 = T.axis.spatial(T.int64(512), k_0 * T.int64(256) + (ax0_ax1_fused_0 * T.int64(160) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2))
                                        T.where((ax0_ax1_fused_0 * T.int64(40) + ax0_ax1_fused_1) * T.int64(4) + ax0_ax1_fused_2 < T.int64(256))
                                        T.reads(p0[v0, v1])
                                        T.writes(p0_shared[v0, v1])
                                        p0_shared[v0, v1] = p0[v0, v1]
                        for ax0_ax1_fused_0 in range(T.int64(64)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(40), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("p1_shared"):
                                        v0 = T.axis.spatial(T.int64(1000), i_0_j_0_fused * T.int64(40) + (ax0_ax1_fused_0 * T.int64(160) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2) // T.int64(256))
                                        v1 = T.axis.spatial(T.int64(512), k_0 * T.int64(256) + (ax0_ax1_fused_0 * T.int64(160) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2) % T.int64(256))
                                        T.reads(p1[v0, v1])
                                        T.writes(p1_shared[v0, v1])
                                        p1_shared[v0, v1] = p1[v0, v1]
                        for k_1, i_3, j_3, k_2, i_4, j_4 in T.grid(T.int64(16), T.int64(1), T.int64(1), T.int64(16), T.int64(1), T.int64(1)):
                            with T.block("T_matmul_NT_update"):
                                v_i = T.axis.spatial(T.int64(1), i_3 + i_4)
                                v_j = T.axis.spatial(T.int64(1000), i_0_j_0_fused * T.int64(40) + i_2_j_2_fused + j_3 + j_4)
                                v_k = T.axis.reduce(T.int64(512), k_0 * T.int64(256) + k_1 * T.int64(16) + k_2)
                                T.reads(T_matmul_NT_local[v_i, v_j], p0_shared[v_i, v_k], p1_shared[v_j, v_k])
                                T.writes(T_matmul_NT_local[v_i, v_j])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                T_matmul_NT_local[v_i, v_j] = T_matmul_NT_local[v_i, v_j] + p0_shared[v_i, v_k] * p1_shared[v_j, v_k]
                    for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(1000), i_0_j_0_fused * T.int64(40) + i_2_j_2_fused + ax1)
                            T.reads(T_matmul_NT_local[v0, v1], p2[v0, v1])
                            T.writes(T_add[v0, v1])
                            T_add[v0, v1] = T_matmul_NT_local[v0, v1] + p2[v0, v1]
b0 = sch.get_block(name="T_matmul_NT", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l3, l4, l5 = sch.get_loops(block=b0)
v6, v7, v8, v9, v10 = sch.sample_perfect_tile(loop=l3, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l11, l12, l13, l14, l15 = sch.split(loop=l3, factors=[v6, v7, v8, v9, v10], preserve_unit_iters=True)
v16, v17, v18, v19, v20 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[25, 1, 40, 1, 1])
l21, l22, l23, l24, l25 = sch.split(loop=l4, factors=[v16, v17, v18, v19, v20], preserve_unit_iters=True)
v26, v27, v28 = sch.sample_perfect_tile(loop=l5, n=3, max_innermost_factor=64, decision=[2, 16, 16])
l29, l30, l31 = sch.split(loop=l5, factors=[v26, v27, v28], preserve_unit_iters=True)
sch.reorder(l11, l21, l12, l22, l13, l23, l29, l30, l14, l24, l31, l15, l25)
l32 = sch.fuse(l11, l21, preserve_unit_iters=True)
sch.bind(loop=l32, thread_axis="blockIdx.x")
l33 = sch.fuse(l12, l22, preserve_unit_iters=True)
sch.bind(loop=l33, thread_axis="vthread.x")
l34 = sch.fuse(l13, l23, preserve_unit_iters=True)
sch.bind(loop=l34, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b35 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b35, loop=l34, preserve_unit_loops=True, index=-1)
b36 = sch.cache_read(block=b0, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b36, loop=l29, preserve_unit_loops=True, index=-1)
l37, l38, l39, l40, l41, l42 = sch.get_loops(block=b36)
l43 = sch.fuse(l41, l42, preserve_unit_iters=True)
v44 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b36, ann_key="meta_schedule.cooperative_fetch", ann_val=v44)
b45 = sch.cache_read(block=b0, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b45, loop=l29, preserve_unit_loops=True, index=-1)
l46, l47, l48, l49, l50, l51 = sch.get_loops(block=b45)
l52 = sch.fuse(l50, l51, preserve_unit_iters=True)
v53 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b45, ann_key="meta_schedule.cooperative_fetch", ann_val=v53)
sch.reverse_compute_inline(block=b1)
v54 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=1)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v54)
sch.enter_postproc()
sch.unannotate(block_or_loop=b36, ann_key="meta_schedule.cooperative_fetch")
l55, l56, l57, l58, l59 = sch.get_loops(block=b36)
l60, l61, l62 = sch.split(loop=l59, factors=[None, 40, 4], preserve_unit_iters=True)
sch.vectorize(loop=l62)
sch.bind(loop=l61, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b45, ann_key="meta_schedule.cooperative_fetch")
l63, l64, l65, l66, l67 = sch.get_loops(block=b45)
l68, l69, l70 = sch.split(loop=l67, factors=[None, 40, 4], preserve_unit_iters=True)
sch.vectorize(loop=l70)
sch.bind(loop=l69, thread_axis="threadIdx.x")
b71 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b71, ann_key="meta_schedule.unroll_explicit")
b72, b73, b74, b75 = sch.get_child_blocks(b71)
l76, l77, l78, l79, l80, l81, l82 = sch.get_loops(block=b72)
l83, l84, l85, l86, l87, l88, l89 = sch.get_loops(block=b73)
l90, l91, l92, l93, l94, l95, l96, l97, l98, l99 = sch.get_loops(block=b74)
sch.annotate(block_or_loop=l90, ann_key="pragma_auto_unroll_max_step", ann_val=16)
sch.annotate(block_or_loop=l90, ann_key="pragma_unroll_explicit", ann_val=1)
l100, l101, l102, l103, l104 = sch.get_loops(block=b75)
b105 = sch.get_block(name="T_matmul_NT", func_name="main")
l106, l107, l108, l109, l110, l111, l112, l113, l114, l115 = sch.get_loops(block=b105)
b116 = sch.decompose_reduction(block=b105, loop=l109)
2023-11-11 12:01:18 [INFO] [task_scheduler.cc:121] [Task #28: fused_nn_dense_add] Trial #862: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/home2/xiachunwei/Software/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/home2/xiachunwei/Software/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/home2/xiachunwei/Software/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/home2/xiachunwei/Software/tvm/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/home2/xiachunwei/Software/tvm/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/home2/xiachunwei/Software/tvm/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/home2/xiachunwei/Software/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/home2/xiachunwei/Software/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  4: _ZN3tvm7runtime13PackedFun
  3: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  2: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  0: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  File "/home2/xiachunwei/Software/tvm/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(512)), "float32"), p1: T.Buffer((T.int64(1000), T.int64(512)), "float32"), p2: T.Buffer((T.int64(1), T.int64(1000)), "float32"), T_add: T.Buffer((T.int64(1), T.int64(1000)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        T_matmul_NT_local = T.alloc_buffer((T.int64(1), T.int64(1000)), scope="local")
        p0_shared = T.alloc_buffer((T.int64(1), T.int64(512)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(1000), T.int64(512)), scope="shared")
        for i_0_j_0_fused in T.thread_binding(T.int64(25), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 1024, "pragma_unroll_explicit": 1}):
            for i_1_j_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for i_2_j_2_fused in T.thread_binding(T.int64(40), thread="threadIdx.x"):
                    for i_3_init, j_3_init, i_4_init, j_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_init"):
                            v_i = T.axis.spatial(T.int64(1), i_3_init + i_4_init)
                            v_j = T.axis.spatial(T.int64(1000), i_0_j_0_fused * T.int64(40) + i_2_j_2_fused + j_3_init + j_4_init)
                            T.reads()
                            T.writes(T_matmul_NT_local[v_i, v_j])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            T_matmul_NT_local[v_i, v_j] = T.float32(0)
                    for k_0_fused in T.serial(T.int64(8), annotations={"software_pipeline_async_stages": [0], "software_pipeline_order": [0, 1, 2], "software_pipeline_stage": [0, 0, 2]}):
                        for ax0_ax1_fused_0 in range(T.int64(2)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(40), thread="threadIdx.x"):
                                with T.block("p0_shared"):
                                    v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                    v1 = T.axis.spatial(T.int64(512), k_0_fused * T.int64(64) + (ax0_ax1_fused_0 * T.int64(40) + ax0_ax1_fused_1))
                                    T.where(ax0_ax1_fused_0 * T.int64(40) + ax0_ax1_fused_1 < T.int64(64))
                                    T.reads(p0[v0, v1])
                                    T.writes(p0_shared[v0, v1])
                                    p0_shared[v0, v1] = p0[v0, v1]
                        for ax0_ax1_fused_0 in range(T.int64(16)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(40), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("p1_shared"):
                                        v0 = T.axis.spatial(T.int64(1000), i_0_j_0_fused * T.int64(40) + (ax0_ax1_fused_0 * T.int64(160) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2) // T.int64(64))
                                        v1 = T.axis.spatial(T.int64(512), k_0_fused * T.int64(64) + (ax0_ax1_fused_0 * T.int64(160) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2) % T.int64(64))
                                        T.reads(p1[v0, v1])
                                        T.writes(p1_shared[v0, v1])
                                        p1_shared[v0, v1] = p1[v0, v1]
                        for k_1, i_3, j_3, k_2, i_4, j_4 in T.grid(T.int64(32), T.int64(1), T.int64(1), T.int64(2), T.int64(1), T.int64(1)):
                            with T.block("T_matmul_NT_update"):
                                v_i = T.axis.spatial(T.int64(1), i_3 + i_4)
                                v_j = T.axis.spatial(T.int64(1000), i_0_j_0_fused * T.int64(40) + i_2_j_2_fused + j_3 + j_4)
                                v_k = T.axis.reduce(T.int64(512), k_0_fused * T.int64(64) + k_1 * T.int64(2) + k_2)
                                T.reads(T_matmul_NT_local[v_i, v_j], p0_shared[v_i, v_k], p1_shared[v_j, v_k])
                                T.writes(T_matmul_NT_local[v_i, v_j])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                T_matmul_NT_local[v_i, v_j] = T_matmul_NT_local[v_i, v_j] + p0_shared[v_i, v_k] * p1_shared[v_j, v_k]
                    for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(1000), i_0_j_0_fused * T.int64(40) + i_2_j_2_fused + ax1)
                            T.reads(T_matmul_NT_local[v0, v1], p2[v0, v1])
                            T.writes(T_add[v0, v1])
                            T_add[v0, v1] = T_matmul_NT_local[v0, v1] + p2[v0, v1]
b0 = sch.get_block(name="T_matmul_NT", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l3, l4, l5 = sch.get_loops(block=b0)
v6, v7, v8, v9, v10 = sch.sample_perfect_tile(loop=l3, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l11, l12, l13, l14, l15 = sch.split(loop=l3, factors=[v6, v7, v8, v9, v10], preserve_unit_iters=True)
v16, v17, v18, v19, v20 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[25, 1, 40, 1, 1])
l21, l22, l23, l24, l25 = sch.split(loop=l4, factors=[v16, v17, v18, v19, v20], preserve_unit_iters=True)
v26, v27, v28 = sch.sample_perfect_tile(loop=l5, n=3, max_innermost_factor=64, decision=[8, 32, 2])
l29, l30, l31 = sch.split(loop=l5, factors=[v26, v27, v28], preserve_unit_iters=True)
sch.reorder(l11, l21, l12, l22, l13, l23, l29, l30, l14, l24, l31, l15, l25)
l32 = sch.fuse(l11, l21, preserve_unit_iters=True)
sch.bind(loop=l32, thread_axis="blockIdx.x")
l33 = sch.fuse(l12, l22, preserve_unit_iters=True)
sch.bind(loop=l33, thread_axis="vthread.x")
l34 = sch.fuse(l13, l23, preserve_unit_iters=True)
sch.bind(loop=l34, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b35 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b35, loop=l34, preserve_unit_loops=True, index=-1)
b36 = sch.cache_read(block=b0, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b36, loop=l29, preserve_unit_loops=True, index=-1)
l37, l38, l39, l40, l41, l42 = sch.get_loops(block=b36)
l43 = sch.fuse(l41, l42, preserve_unit_iters=True)
v44 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b36, ann_key="meta_schedule.cooperative_fetch", ann_val=v44)
b45 = sch.cache_read(block=b0, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b45, loop=l29, preserve_unit_loops=True, index=-1)
l46, l47, l48, l49, l50, l51 = sch.get_loops(block=b45)
l52 = sch.fuse(l50, l51, preserve_unit_iters=True)
v53 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b45, ann_key="meta_schedule.cooperative_fetch", ann_val=v53)
l54 = sch.fuse(l29, preserve_unit_iters=True)
sch.annotate(block_or_loop=l54, ann_key="software_pipeline_stage", ann_val=[0, 0, 2])
sch.annotate(block_or_loop=l54, ann_key="software_pipeline_order", ann_val=[0, 1, 2])
sch.annotate(block_or_loop=l54, ann_key="software_pipeline_async_stages", ann_val=[0])
sch.reverse_compute_inline(block=b1)
v55 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v55)
sch.enter_postproc()
sch.unannotate(block_or_loop=b36, ann_key="meta_schedule.cooperative_fetch")
l56, l57, l58, l59, l60 = sch.get_loops(block=b36)
l61, l62 = sch.split(loop=l60, factors=[None, 40], preserve_unit_iters=True)
sch.bind(loop=l62, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b45, ann_key="meta_schedule.cooperative_fetch")
l63, l64, l65, l66, l67 = sch.get_loops(block=b45)
l68, l69, l70 = sch.split(loop=l67, factors=[None, 40, 4], preserve_unit_iters=True)
sch.vectorize(loop=l70)
sch.bind(loop=l69, thread_axis="threadIdx.x")
b71 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b71, ann_key="meta_schedule.unroll_explicit")
b72, b73, b74, b75 = sch.get_child_blocks(b71)
l76, l77, l78, l79, l80, l81 = sch.get_loops(block=b72)
l82, l83, l84, l85, l86, l87, l88 = sch.get_loops(block=b73)
l89, l90, l91, l92, l93, l94, l95, l96, l97, l98 = sch.get_loops(block=b74)
sch.annotate(block_or_loop=l89, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l89, ann_key="pragma_unroll_explicit", ann_val=1)
l99, l100, l101, l102, l103 = sch.get_loops(block=b75)
b104 = sch.get_block(name="T_matmul_NT", func_name="main")
l105, l106, l107, l108, l109, l110, l111, l112, l113, l114 = sch.get_loops(block=b104)
b115 = sch.decompose_reduction(block=b104, loop=l108)
2023-11-11 12:01:18 [INFO] [task_scheduler.cc:121] [Task #28: fused_nn_dense_add] Trial #863: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/home2/xiachunwei/Software/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/home2/xiachunwei/Software/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/home2/xiachunwei/Software/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/home2/xiachunwei/Software/tvm/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/home2/xiachunwei/Software/tvm/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/home2/xiachunwei/Software/tvm/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/home2/xiachunwei/Software/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/home2/xiachunwei/Software/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  4: _ZN3tvm7runtime13PackedFun
  3: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  2: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  0: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  File "/home2/xiachunwei/Software/tvm/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(512)), "float32"), p1: T.Buffer((T.int64(1000), T.int64(512)), "float32"), p2: T.Buffer((T.int64(1), T.int64(1000)), "float32"), T_add: T.Buffer((T.int64(1), T.int64(1000)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        T_matmul_NT_local = T.alloc_buffer((T.int64(1), T.int64(1000)), scope="local")
        p0_shared = T.alloc_buffer((T.int64(1), T.int64(512)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(1000), T.int64(512)), scope="shared")
        for i_0_j_0_fused in T.thread_binding(T.int64(25), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 1024, "pragma_unroll_explicit": 1}):
            for i_1_j_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for i_2_j_2_fused in T.thread_binding(T.int64(40), thread="threadIdx.x"):
                    for i_3_init, j_3_init, i_4_init, j_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_init"):
                            v_i = T.axis.spatial(T.int64(1), i_3_init + i_4_init)
                            v_j = T.axis.spatial(T.int64(1000), i_0_j_0_fused * T.int64(40) + i_2_j_2_fused + j_3_init + j_4_init)
                            T.reads()
                            T.writes(T_matmul_NT_local[v_i, v_j])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            T_matmul_NT_local[v_i, v_j] = T.float32(0)
                    for k_0 in range(T.int64(8)):
                        for ax0_ax1_fused_0 in range(T.int64(2)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(40), thread="threadIdx.x"):
                                with T.block("p0_shared"):
                                    v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                    v1 = T.axis.spatial(T.int64(512), k_0 * T.int64(64) + (ax0_ax1_fused_0 * T.int64(40) + ax0_ax1_fused_1))
                                    T.where(ax0_ax1_fused_0 * T.int64(40) + ax0_ax1_fused_1 < T.int64(64))
                                    T.reads(p0[v0, v1])
                                    T.writes(p0_shared[v0, v1])
                                    p0_shared[v0, v1] = p0[v0, v1]
                        for ax0_ax1_fused_0 in range(T.int64(16)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(40), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("p1_shared"):
                                        v0 = T.axis.spatial(T.int64(1000), i_0_j_0_fused * T.int64(40) + (ax0_ax1_fused_0 * T.int64(160) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2) // T.int64(64))
                                        v1 = T.axis.spatial(T.int64(512), k_0 * T.int64(64) + (ax0_ax1_fused_0 * T.int64(160) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2) % T.int64(64))
                                        T.reads(p1[v0, v1])
                                        T.writes(p1_shared[v0, v1])
                                        p1_shared[v0, v1] = p1[v0, v1]
                        for k_1, i_3, j_3, k_2, i_4, j_4 in T.grid(T.int64(2), T.int64(1), T.int64(1), T.int64(32), T.int64(1), T.int64(1)):
                            with T.block("T_matmul_NT_update"):
                                v_i = T.axis.spatial(T.int64(1), i_3 + i_4)
                                v_j = T.axis.spatial(T.int64(1000), i_0_j_0_fused * T.int64(40) + i_2_j_2_fused + j_3 + j_4)
                                v_k = T.axis.reduce(T.int64(512), k_0 * T.int64(64) + k_1 * T.int64(32) + k_2)
                                T.reads(T_matmul_NT_local[v_i, v_j], p0_shared[v_i, v_k], p1_shared[v_j, v_k])
                                T.writes(T_matmul_NT_local[v_i, v_j])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                T_matmul_NT_local[v_i, v_j] = T_matmul_NT_local[v_i, v_j] + p0_shared[v_i, v_k] * p1_shared[v_j, v_k]
                    for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(1000), i_0_j_0_fused * T.int64(40) + i_2_j_2_fused + ax1)
                            T.reads(T_matmul_NT_local[v0, v1], p2[v0, v1])
                            T.writes(T_add[v0, v1])
                            T_add[v0, v1] = T_matmul_NT_local[v0, v1] + p2[v0, v1]
b0 = sch.get_block(name="T_matmul_NT", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l3, l4, l5 = sch.get_loops(block=b0)
v6, v7, v8, v9, v10 = sch.sample_perfect_tile(loop=l3, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l11, l12, l13, l14, l15 = sch.split(loop=l3, factors=[v6, v7, v8, v9, v10], preserve_unit_iters=True)
v16, v17, v18, v19, v20 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[25, 1, 40, 1, 1])
l21, l22, l23, l24, l25 = sch.split(loop=l4, factors=[v16, v17, v18, v19, v20], preserve_unit_iters=True)
v26, v27, v28 = sch.sample_perfect_tile(loop=l5, n=3, max_innermost_factor=64, decision=[8, 2, 32])
l29, l30, l31 = sch.split(loop=l5, factors=[v26, v27, v28], preserve_unit_iters=True)
sch.reorder(l11, l21, l12, l22, l13, l23, l29, l30, l14, l24, l31, l15, l25)
l32 = sch.fuse(l11, l21, preserve_unit_iters=True)
sch.bind(loop=l32, thread_axis="blockIdx.x")
l33 = sch.fuse(l12, l22, preserve_unit_iters=True)
sch.bind(loop=l33, thread_axis="vthread.x")
l34 = sch.fuse(l13, l23, preserve_unit_iters=True)
sch.bind(loop=l34, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b35 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b35, loop=l34, preserve_unit_loops=True, index=-1)
b36 = sch.cache_read(block=b0, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b36, loop=l29, preserve_unit_loops=True, index=-1)
l37, l38, l39, l40, l41, l42 = sch.get_loops(block=b36)
l43 = sch.fuse(l41, l42, preserve_unit_iters=True)
v44 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b36, ann_key="meta_schedule.cooperative_fetch", ann_val=v44)
b45 = sch.cache_read(block=b0, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b45, loop=l29, preserve_unit_loops=True, index=-1)
l46, l47, l48, l49, l50, l51 = sch.get_loops(block=b45)
l52 = sch.fuse(l50, l51, preserve_unit_iters=True)
v53 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b45, ann_key="meta_schedule.cooperative_fetch", ann_val=v53)
sch.reverse_compute_inline(block=b1)
v54 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v54)
sch.enter_postproc()
sch.unannotate(block_or_loop=b36, ann_key="meta_schedule.cooperative_fetch")
l55, l56, l57, l58, l59 = sch.get_loops(block=b36)
l60, l61 = sch.split(loop=l59, factors=[None, 40], preserve_unit_iters=True)
sch.bind(loop=l61, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b45, ann_key="meta_schedule.cooperative_fetch")
l62, l63, l64, l65, l66 = sch.get_loops(block=b45)
l67, l68, l69 = sch.split(loop=l66, factors=[None, 40, 4], preserve_unit_iters=True)
sch.vectorize(loop=l69)
sch.bind(loop=l68, thread_axis="threadIdx.x")
b70 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b70, ann_key="meta_schedule.unroll_explicit")
b71, b72, b73, b74 = sch.get_child_blocks(b70)
l75, l76, l77, l78, l79, l80 = sch.get_loops(block=b71)
l81, l82, l83, l84, l85, l86, l87 = sch.get_loops(block=b72)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b73)
sch.annotate(block_or_loop=l88, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l88, ann_key="pragma_unroll_explicit", ann_val=1)
l98, l99, l100, l101, l102 = sch.get_loops(block=b74)
b103 = sch.get_block(name="T_matmul_NT", func_name="main")
l104, l105, l106, l107, l108, l109, l110, l111, l112, l113 = sch.get_loops(block=b103)
b114 = sch.decompose_reduction(block=b103, loop=l107)
2023-11-11 12:01:18 [INFO] [task_scheduler.cc:121] [Task #28: fused_nn_dense_add] Trial #864: Error in running:
LocalRunner: An exception occurred
Subprocess terminated
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(512)), "float32"), p1: T.Buffer((T.int64(1000), T.int64(512)), "float32"), p2: T.Buffer((T.int64(1), T.int64(1000)), "float32"), T_add: T.Buffer((T.int64(1), T.int64(1000)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        T_matmul_NT_local = T.alloc_buffer((T.int64(1), T.int64(1000)), scope="local")
        p0_shared = T.alloc_buffer((T.int64(1), T.int64(512)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(1000), T.int64(512)), scope="shared")
        for i_0_j_0_fused in T.thread_binding(T.int64(25), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 1024, "pragma_unroll_explicit": 1}):
            for i_1_j_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for i_2_j_2_fused in T.thread_binding(T.int64(40), thread="threadIdx.x"):
                    for i_3_init, j_3_init, i_4_init, j_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_init"):
                            v_i = T.axis.spatial(T.int64(1), i_3_init + i_4_init)
                            v_j = T.axis.spatial(T.int64(1000), i_0_j_0_fused * T.int64(40) + i_2_j_2_fused + j_3_init + j_4_init)
                            T.reads()
                            T.writes(T_matmul_NT_local[v_i, v_j])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            T_matmul_NT_local[v_i, v_j] = T.float32(0)
                    for k_0_fused in T.serial(T.int64(8), annotations={"software_pipeline_async_stages": [0], "software_pipeline_order": [0, 1, 2], "software_pipeline_stage": [0, 0, 2]}):
                        for ax0_ax1_fused_0 in range(T.int64(2)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(40), thread="threadIdx.x"):
                                with T.block("p0_shared"):
                                    v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                    v1 = T.axis.spatial(T.int64(512), k_0_fused * T.int64(64) + (ax0_ax1_fused_0 * T.int64(40) + ax0_ax1_fused_1))
                                    T.where(ax0_ax1_fused_0 * T.int64(40) + ax0_ax1_fused_1 < T.int64(64))
                                    T.reads(p0[v0, v1])
                                    T.writes(p0_shared[v0, v1])
                                    p0_shared[v0, v1] = p0[v0, v1]
                        for ax0_ax1_fused_0 in range(T.int64(16)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(40), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("p1_shared"):
                                        v0 = T.axis.spatial(T.int64(1000), i_0_j_0_fused * T.int64(40) + (ax0_ax1_fused_0 * T.int64(160) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2) // T.int64(64))
                                        v1 = T.axis.spatial(T.int64(512), k_0_fused * T.int64(64) + (ax0_ax1_fused_0 * T.int64(160) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2) % T.int64(64))
                                        T.reads(p1[v0, v1])
                                        T.writes(p1_shared[v0, v1])
                                        p1_shared[v0, v1] = p1[v0, v1]
                        for k_1, i_3, j_3, k_2, i_4, j_4 in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(64), T.int64(1), T.int64(1)):
                            with T.block("T_matmul_NT_update"):
                                v_i = T.axis.spatial(T.int64(1), i_3 + i_4)
                                v_j = T.axis.spatial(T.int64(1000), i_0_j_0_fused * T.int64(40) + i_2_j_2_fused + j_3 + j_4)
                                v_k = T.axis.reduce(T.int64(512), k_0_fused * T.int64(64) + k_1 * T.int64(64) + k_2)
                                T.reads(T_matmul_NT_local[v_i, v_j], p0_shared[v_i, v_k], p1_shared[v_j, v_k])
                                T.writes(T_matmul_NT_local[v_i, v_j])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                T_matmul_NT_local[v_i, v_j] = T_matmul_NT_local[v_i, v_j] + p0_shared[v_i, v_k] * p1_shared[v_j, v_k]
                    for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(1000), i_0_j_0_fused * T.int64(40) + i_2_j_2_fused + ax1)
                            T.reads(T_matmul_NT_local[v0, v1], p2[v0, v1])
                            T.writes(T_add[v0, v1])
                            T_add[v0, v1] = T_matmul_NT_local[v0, v1] + p2[v0, v1]
b0 = sch.get_block(name="T_matmul_NT", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l3, l4, l5 = sch.get_loops(block=b0)
v6, v7, v8, v9, v10 = sch.sample_perfect_tile(loop=l3, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l11, l12, l13, l14, l15 = sch.split(loop=l3, factors=[v6, v7, v8, v9, v10], preserve_unit_iters=True)
v16, v17, v18, v19, v20 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[25, 1, 40, 1, 1])
l21, l22, l23, l24, l25 = sch.split(loop=l4, factors=[v16, v17, v18, v19, v20], preserve_unit_iters=True)
v26, v27, v28 = sch.sample_perfect_tile(loop=l5, n=3, max_innermost_factor=64, decision=[8, 1, 64])
l29, l30, l31 = sch.split(loop=l5, factors=[v26, v27, v28], preserve_unit_iters=True)
sch.reorder(l11, l21, l12, l22, l13, l23, l29, l30, l14, l24, l31, l15, l25)
l32 = sch.fuse(l11, l21, preserve_unit_iters=True)
sch.bind(loop=l32, thread_axis="blockIdx.x")
l33 = sch.fuse(l12, l22, preserve_unit_iters=True)
sch.bind(loop=l33, thread_axis="vthread.x")
l34 = sch.fuse(l13, l23, preserve_unit_iters=True)
sch.bind(loop=l34, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b35 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b35, loop=l34, preserve_unit_loops=True, index=-1)
b36 = sch.cache_read(block=b0, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b36, loop=l29, preserve_unit_loops=True, index=-1)
l37, l38, l39, l40, l41, l42 = sch.get_loops(block=b36)
l43 = sch.fuse(l41, l42, preserve_unit_iters=True)
v44 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b36, ann_key="meta_schedule.cooperative_fetch", ann_val=v44)
b45 = sch.cache_read(block=b0, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b45, loop=l29, preserve_unit_loops=True, index=-1)
l46, l47, l48, l49, l50, l51 = sch.get_loops(block=b45)
l52 = sch.fuse(l50, l51, preserve_unit_iters=True)
v53 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b45, ann_key="meta_schedule.cooperative_fetch", ann_val=v53)
l54 = sch.fuse(l29, preserve_unit_iters=True)
sch.annotate(block_or_loop=l54, ann_key="software_pipeline_stage", ann_val=[0, 0, 2])
sch.annotate(block_or_loop=l54, ann_key="software_pipeline_order", ann_val=[0, 1, 2])
sch.annotate(block_or_loop=l54, ann_key="software_pipeline_async_stages", ann_val=[0])
sch.reverse_compute_inline(block=b1)
v55 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v55)
sch.enter_postproc()
sch.unannotate(block_or_loop=b36, ann_key="meta_schedule.cooperative_fetch")
l56, l57, l58, l59, l60 = sch.get_loops(block=b36)
l61, l62 = sch.split(loop=l60, factors=[None, 40], preserve_unit_iters=True)
sch.bind(loop=l62, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b45, ann_key="meta_schedule.cooperative_fetch")
l63, l64, l65, l66, l67 = sch.get_loops(block=b45)
l68, l69, l70 = sch.split(loop=l67, factors=[None, 40, 4], preserve_unit_iters=True)
sch.vectorize(loop=l70)
sch.bind(loop=l69, thread_axis="threadIdx.x")
b71 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b71, ann_key="meta_schedule.unroll_explicit")
b72, b73, b74, b75 = sch.get_child_blocks(b71)
l76, l77, l78, l79, l80, l81 = sch.get_loops(block=b72)
l82, l83, l84, l85, l86, l87, l88 = sch.get_loops(block=b73)
l89, l90, l91, l92, l93, l94, l95, l96, l97, l98 = sch.get_loops(block=b74)
sch.annotate(block_or_loop=l89, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l89, ann_key="pragma_unroll_explicit", ann_val=1)
l99, l100, l101, l102, l103 = sch.get_loops(block=b75)
b104 = sch.get_block(name="T_matmul_NT", func_name="main")
l105, l106, l107, l108, l109, l110, l111, l112, l113, l114 = sch.get_loops(block=b104)
b115 = sch.decompose_reduction(block=b104, loop=l108)
2023-11-11 12:01:18 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #865: GFLOPs: 67.4129. Time: 15.2048 us. Best GFLOPs: 97.5888
2023-11-11 12:01:18 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #866: GFLOPs: 68.5874. Time: 14.9444 us. Best GFLOPs: 97.5888
2023-11-11 12:01:18 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #867: GFLOPs: 88.6349. Time: 11.5643 us. Best GFLOPs: 97.5888
2023-11-11 12:01:18 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #868: GFLOPs: 79.5754. Time: 12.8809 us. Best GFLOPs: 97.5888
2023-11-11 12:01:18 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #869: GFLOPs: 64.8742. Time: 15.7998 us. Best GFLOPs: 97.5888
2023-11-11 12:01:18 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #870: GFLOPs: 88.5840. Time: 11.5709 us. Best GFLOPs: 97.5888
2023-11-11 12:01:18 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #871: GFLOPs: 88.6271. Time: 11.5653 us. Best GFLOPs: 97.5888
2023-11-11 12:01:18 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #872: GFLOPs: 92.7054. Time: 11.0565 us. Best GFLOPs: 97.5888
2023-11-11 12:01:18 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #873: GFLOPs: 67.4125. Time: 15.2049 us. Best GFLOPs: 97.5888
2023-11-11 12:01:18 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #874: GFLOPs: 54.9086. Time: 18.6674 us. Best GFLOPs: 97.5888
2023-11-11 12:01:18 [INFO] [task_scheduler.cc:121] [Task #28: fused_nn_dense_add] Trial #875: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/home2/xiachunwei/Software/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/home2/xiachunwei/Software/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 159, in _worker_func
    costs: List[float] = f_run_evaluator(
  File "/home2/xiachunwei/Software/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 387, in default_run_evaluator
    return run_evaluator_common(rt_mod, device, evaluator_config, repeated_args)
  File "/home2/xiachunwei/Software/tvm/python/tvm/meta_schedule/runner/utils.py", line 117, in run_evaluator_common
    profile_result = evaluator(*args)
  File "/home2/xiachunwei/Software/tvm/python/tvm/runtime/module.py", line 397, in evaluator
    blob = feval(*args)
  File "/home2/xiachunwei/Software/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/home2/xiachunwei/Software/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  1: tvm::runtime::profiling::WrapTimeEvaluator(tvm::runtime::PackedFunc, DLDevice, int, int, int, int, int, int, int, tvm::runtime::PackedFunc)::{lambda(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*) [clone .isra.779]
  0: tvm::runtime::CUDADeviceAPI::StreamSync(DLDevice, void*)
  File "/home2/xiachunwei/Software/tvm/src/runtime/cuda/cuda_device_api.cc", line 212
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(512)), "float32"), p1: T.Buffer((T.int64(1000), T.int64(512)), "float32"), p2: T.Buffer((T.int64(1), T.int64(1000)), "float32"), T_add: T.Buffer((T.int64(1), T.int64(1000)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        T_matmul_NT_local = T.alloc_buffer((T.int64(1), T.int64(1000)), scope="local")
        p0_shared = T.alloc_buffer((T.int64(1), T.int64(512)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(1000), T.int64(512)), scope="shared")
        for i_0_j_0_fused in T.thread_binding(T.int64(1), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 64, "pragma_unroll_explicit": 1}):
            for i_1_j_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for i_2_j_2_fused in T.thread_binding(T.int64(1000), thread="threadIdx.x"):
                    for i_3_init, j_3_init, i_4_init, j_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_init"):
                            v_i = T.axis.spatial(T.int64(1), i_3_init + i_4_init)
                            v_j = T.axis.spatial(T.int64(1000), i_2_j_2_fused + j_3_init + j_4_init)
                            T.reads()
                            T.writes(T_matmul_NT_local[v_i, v_j])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            T_matmul_NT_local[v_i, v_j] = T.float32(0)
                    for k_0 in range(T.int64(256)):
                        for ax0_ax1_fused_0 in range(T.int64(1)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(1000), thread="threadIdx.x"):
                                with T.block("p0_shared"):
                                    v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                    v1 = T.axis.spatial(T.int64(512), k_0 * T.int64(2) + (ax0_ax1_fused_0 * T.int64(1000) + ax0_ax1_fused_1))
                                    T.where(ax0_ax1_fused_0 * T.int64(1000) + ax0_ax1_fused_1 < T.int64(2))
                                    T.reads(p0[v0, v1])
                                    T.writes(p0_shared[v0, v1])
                                    p0_shared[v0, v1] = p0[v0, v1]
                        for ax0_ax1_fused_0 in range(T.int64(1)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(1000), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("p1_shared"):
                                        v0 = T.axis.spatial(T.int64(1000), (ax0_ax1_fused_0 * T.int64(4000) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2) // T.int64(2))
                                        v1 = T.axis.spatial(T.int64(512), k_0 * T.int64(2) + (ax0_ax1_fused_0 * T.int64(4000) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2) % T.int64(2))
                                        T.where((ax0_ax1_fused_0 * T.int64(1000) + ax0_ax1_fused_1) * T.int64(4) + ax0_ax1_fused_2 < T.int64(2000))
                                        T.reads(p1[v0, v1])
                                        T.writes(p1_shared[v0, v1])
                                        p1_shared[v0, v1] = p1[v0, v1]
                        for k_1, i_3, j_3, k_2, i_4, j_4 in T.grid(T.int64(2), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                            with T.block("T_matmul_NT_update"):
                                v_i = T.axis.spatial(T.int64(1), i_3 + i_4)
                                v_j = T.axis.spatial(T.int64(1000), i_2_j_2_fused + j_3 + j_4)
                                v_k = T.axis.reduce(T.int64(512), k_0 * T.int64(2) + k_1 + k_2)
                                T.reads(T_matmul_NT_local[v_i, v_j], p0_shared[v_i, v_k], p1_shared[v_j, v_k])
                                T.writes(T_matmul_NT_local[v_i, v_j])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                T_matmul_NT_local[v_i, v_j] = T_matmul_NT_local[v_i, v_j] + p0_shared[v_i, v_k] * p1_shared[v_j, v_k]
                    for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(1000), i_2_j_2_fused + ax1)
                            T.reads(T_matmul_NT_local[v0, v1], p2[v0, v1])
                            T.writes(T_add[v0, v1])
                            T_add[v0, v1] = T_matmul_NT_local[v0, v1] + p2[v0, v1]
b0 = sch.get_block(name="T_matmul_NT", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l3, l4, l5 = sch.get_loops(block=b0)
v6, v7, v8, v9, v10 = sch.sample_perfect_tile(loop=l3, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l11, l12, l13, l14, l15 = sch.split(loop=l3, factors=[v6, v7, v8, v9, v10], preserve_unit_iters=True)
v16, v17, v18, v19, v20 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[1, 1, 1000, 1, 1])
l21, l22, l23, l24, l25 = sch.split(loop=l4, factors=[v16, v17, v18, v19, v20], preserve_unit_iters=True)
v26, v27, v28 = sch.sample_perfect_tile(loop=l5, n=3, max_innermost_factor=64, decision=[256, 2, 1])
l29, l30, l31 = sch.split(loop=l5, factors=[v26, v27, v28], preserve_unit_iters=True)
sch.reorder(l11, l21, l12, l22, l13, l23, l29, l30, l14, l24, l31, l15, l25)
l32 = sch.fuse(l11, l21, preserve_unit_iters=True)
sch.bind(loop=l32, thread_axis="blockIdx.x")
l33 = sch.fuse(l12, l22, preserve_unit_iters=True)
sch.bind(loop=l33, thread_axis="vthread.x")
l34 = sch.fuse(l13, l23, preserve_unit_iters=True)
sch.bind(loop=l34, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b35 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b35, loop=l34, preserve_unit_loops=True, index=-1)
b36 = sch.cache_read(block=b0, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b36, loop=l29, preserve_unit_loops=True, index=-1)
l37, l38, l39, l40, l41, l42 = sch.get_loops(block=b36)
l43 = sch.fuse(l41, l42, preserve_unit_iters=True)
v44 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b36, ann_key="meta_schedule.cooperative_fetch", ann_val=v44)
b45 = sch.cache_read(block=b0, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b45, loop=l29, preserve_unit_loops=True, index=-1)
l46, l47, l48, l49, l50, l51 = sch.get_loops(block=b45)
l52 = sch.fuse(l50, l51, preserve_unit_iters=True)
v53 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b45, ann_key="meta_schedule.cooperative_fetch", ann_val=v53)
sch.reverse_compute_inline(block=b1)
v54 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=2)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v54)
sch.enter_postproc()
sch.unannotate(block_or_loop=b36, ann_key="meta_schedule.cooperative_fetch")
l55, l56, l57, l58, l59 = sch.get_loops(block=b36)
l60, l61 = sch.split(loop=l59, factors=[None, 1000], preserve_unit_iters=True)
sch.bind(loop=l61, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b45, ann_key="meta_schedule.cooperative_fetch")
l62, l63, l64, l65, l66 = sch.get_loops(block=b45)
l67, l68, l69 = sch.split(loop=l66, factors=[None, 1000, 4], preserve_unit_iters=True)
sch.vectorize(loop=l69)
sch.bind(loop=l68, thread_axis="threadIdx.x")
b70 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b70, ann_key="meta_schedule.unroll_explicit")
b71, b72, b73, b74 = sch.get_child_blocks(b70)
l75, l76, l77, l78, l79, l80 = sch.get_loops(block=b71)
l81, l82, l83, l84, l85, l86, l87 = sch.get_loops(block=b72)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b73)
sch.annotate(block_or_loop=l88, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l88, ann_key="pragma_unroll_explicit", ann_val=1)
l98, l99, l100, l101, l102 = sch.get_loops(block=b74)
b103 = sch.get_block(name="T_matmul_NT", func_name="main")
l104, l105, l106, l107, l108, l109, l110, l111, l112, l113 = sch.get_loops(block=b103)
b114 = sch.decompose_reduction(block=b103, loop=l107)
2023-11-11 12:01:18 [INFO] [task_scheduler.cc:121] [Task #28: fused_nn_dense_add] Trial #876: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/home2/xiachunwei/Software/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/home2/xiachunwei/Software/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/home2/xiachunwei/Software/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/home2/xiachunwei/Software/tvm/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/home2/xiachunwei/Software/tvm/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/home2/xiachunwei/Software/tvm/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/home2/xiachunwei/Software/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/home2/xiachunwei/Software/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  4: _ZN3tvm7runtime13PackedFun
  3: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  2: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  0: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  File "/home2/xiachunwei/Software/tvm/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(512)), "float32"), p1: T.Buffer((T.int64(1000), T.int64(512)), "float32"), p2: T.Buffer((T.int64(1), T.int64(1000)), "float32"), T_add: T.Buffer((T.int64(1), T.int64(1000)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        T_matmul_NT_local = T.alloc_buffer((T.int64(1), T.int64(1000)), scope="local")
        p0_shared = T.alloc_buffer((T.int64(1), T.int64(512)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(1000), T.int64(512)), scope="shared")
        for i_0_j_0_fused in T.thread_binding(T.int64(25), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 1024, "pragma_unroll_explicit": 1}):
            for i_1_j_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for i_2_j_2_fused in T.thread_binding(T.int64(40), thread="threadIdx.x"):
                    for i_3_init, j_3_init, i_4_init, j_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_init"):
                            v_i = T.axis.spatial(T.int64(1), i_3_init + i_4_init)
                            v_j = T.axis.spatial(T.int64(1000), i_0_j_0_fused * T.int64(40) + i_2_j_2_fused + j_3_init + j_4_init)
                            T.reads()
                            T.writes(T_matmul_NT_local[v_i, v_j])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            T_matmul_NT_local[v_i, v_j] = T.float32(0)
                    for k_0_fused in T.serial(T.int64(8), annotations={"software_pipeline_async_stages": [0], "software_pipeline_order": [0, 1, 2], "software_pipeline_stage": [0, 0, 2]}):
                        for ax0_ax1_fused_0 in range(T.int64(1)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(40), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("p0_shared"):
                                        v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v1 = T.axis.spatial(T.int64(512), k_0_fused * T.int64(64) + (ax0_ax1_fused_0 * T.int64(80) + ax0_ax1_fused_1 * T.int64(2) + ax0_ax1_fused_2))
                                        T.where((ax0_ax1_fused_0 * T.int64(40) + ax0_ax1_fused_1) * T.int64(2) + ax0_ax1_fused_2 < T.int64(64))
                                        T.reads(p0[v0, v1])
                                        T.writes(p0_shared[v0, v1])
                                        p0_shared[v0, v1] = p0[v0, v1]
                        for ax0_ax1_fused_0 in range(T.int64(32)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(40), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("p1_shared"):
                                        v0 = T.axis.spatial(T.int64(1000), i_0_j_0_fused * T.int64(40) + (ax0_ax1_fused_0 * T.int64(80) + ax0_ax1_fused_1 * T.int64(2) + ax0_ax1_fused_2) // T.int64(64))
                                        v1 = T.axis.spatial(T.int64(512), k_0_fused * T.int64(64) + (ax0_ax1_fused_0 * T.int64(80) + ax0_ax1_fused_1 * T.int64(2) + ax0_ax1_fused_2) % T.int64(64))
                                        T.reads(p1[v0, v1])
                                        T.writes(p1_shared[v0, v1])
                                        p1_shared[v0, v1] = p1[v0, v1]
                        for k_1, i_3, j_3, k_2, i_4, j_4 in T.grid(T.int64(8), T.int64(1), T.int64(1), T.int64(8), T.int64(1), T.int64(1)):
                            with T.block("T_matmul_NT_update"):
                                v_i = T.axis.spatial(T.int64(1), i_3 + i_4)
                                v_j = T.axis.spatial(T.int64(1000), i_0_j_0_fused * T.int64(40) + i_2_j_2_fused + j_3 + j_4)
                                v_k = T.axis.reduce(T.int64(512), k_0_fused * T.int64(64) + k_1 * T.int64(8) + k_2)
                                T.reads(T_matmul_NT_local[v_i, v_j], p0_shared[v_i, v_k], p1_shared[v_j, v_k])
                                T.writes(T_matmul_NT_local[v_i, v_j])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                T_matmul_NT_local[v_i, v_j] = T_matmul_NT_local[v_i, v_j] + p0_shared[v_i, v_k] * p1_shared[v_j, v_k]
                    for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(1000), i_0_j_0_fused * T.int64(40) + i_2_j_2_fused + ax1)
                            T.reads(T_matmul_NT_local[v0, v1], p2[v0, v1])
                            T.writes(T_add[v0, v1])
                            T_add[v0, v1] = T_matmul_NT_local[v0, v1] + p2[v0, v1]
b0 = sch.get_block(name="T_matmul_NT", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l3, l4, l5 = sch.get_loops(block=b0)
v6, v7, v8, v9, v10 = sch.sample_perfect_tile(loop=l3, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l11, l12, l13, l14, l15 = sch.split(loop=l3, factors=[v6, v7, v8, v9, v10], preserve_unit_iters=True)
v16, v17, v18, v19, v20 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[25, 1, 40, 1, 1])
l21, l22, l23, l24, l25 = sch.split(loop=l4, factors=[v16, v17, v18, v19, v20], preserve_unit_iters=True)
v26, v27, v28 = sch.sample_perfect_tile(loop=l5, n=3, max_innermost_factor=64, decision=[8, 8, 8])
l29, l30, l31 = sch.split(loop=l5, factors=[v26, v27, v28], preserve_unit_iters=True)
sch.reorder(l11, l21, l12, l22, l13, l23, l29, l30, l14, l24, l31, l15, l25)
l32 = sch.fuse(l11, l21, preserve_unit_iters=True)
sch.bind(loop=l32, thread_axis="blockIdx.x")
l33 = sch.fuse(l12, l22, preserve_unit_iters=True)
sch.bind(loop=l33, thread_axis="vthread.x")
l34 = sch.fuse(l13, l23, preserve_unit_iters=True)
sch.bind(loop=l34, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b35 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b35, loop=l34, preserve_unit_loops=True, index=-1)
b36 = sch.cache_read(block=b0, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b36, loop=l29, preserve_unit_loops=True, index=-1)
l37, l38, l39, l40, l41, l42 = sch.get_loops(block=b36)
l43 = sch.fuse(l41, l42, preserve_unit_iters=True)
v44 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b36, ann_key="meta_schedule.cooperative_fetch", ann_val=v44)
b45 = sch.cache_read(block=b0, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b45, loop=l29, preserve_unit_loops=True, index=-1)
l46, l47, l48, l49, l50, l51 = sch.get_loops(block=b45)
l52 = sch.fuse(l50, l51, preserve_unit_iters=True)
v53 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b45, ann_key="meta_schedule.cooperative_fetch", ann_val=v53)
l54 = sch.fuse(l29, preserve_unit_iters=True)
sch.annotate(block_or_loop=l54, ann_key="software_pipeline_stage", ann_val=[0, 0, 2])
sch.annotate(block_or_loop=l54, ann_key="software_pipeline_order", ann_val=[0, 1, 2])
sch.annotate(block_or_loop=l54, ann_key="software_pipeline_async_stages", ann_val=[0])
sch.reverse_compute_inline(block=b1)
v55 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v55)
sch.enter_postproc()
sch.unannotate(block_or_loop=b36, ann_key="meta_schedule.cooperative_fetch")
l56, l57, l58, l59, l60 = sch.get_loops(block=b36)
l61, l62, l63 = sch.split(loop=l60, factors=[None, 40, 2], preserve_unit_iters=True)
sch.vectorize(loop=l63)
sch.bind(loop=l62, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b45, ann_key="meta_schedule.cooperative_fetch")
l64, l65, l66, l67, l68 = sch.get_loops(block=b45)
l69, l70, l71 = sch.split(loop=l68, factors=[None, 40, 2], preserve_unit_iters=True)
sch.vectorize(loop=l71)
sch.bind(loop=l70, thread_axis="threadIdx.x")
b72 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b72, ann_key="meta_schedule.unroll_explicit")
b73, b74, b75, b76 = sch.get_child_blocks(b72)
l77, l78, l79, l80, l81, l82, l83 = sch.get_loops(block=b73)
l84, l85, l86, l87, l88, l89, l90 = sch.get_loops(block=b74)
l91, l92, l93, l94, l95, l96, l97, l98, l99, l100 = sch.get_loops(block=b75)
sch.annotate(block_or_loop=l91, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l91, ann_key="pragma_unroll_explicit", ann_val=1)
l101, l102, l103, l104, l105 = sch.get_loops(block=b76)
b106 = sch.get_block(name="T_matmul_NT", func_name="main")
l107, l108, l109, l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b106)
b117 = sch.decompose_reduction(block=b106, loop=l110)
2023-11-11 12:01:18 [INFO] [task_scheduler.cc:121] [Task #28: fused_nn_dense_add] Trial #877: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/home2/xiachunwei/Software/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/home2/xiachunwei/Software/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/home2/xiachunwei/Software/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/home2/xiachunwei/Software/tvm/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/home2/xiachunwei/Software/tvm/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/home2/xiachunwei/Software/tvm/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/home2/xiachunwei/Software/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/home2/xiachunwei/Software/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  4: _ZN3tvm7runtime13PackedFun
  3: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  2: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  0: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  File "/home2/xiachunwei/Software/tvm/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(512)), "float32"), p1: T.Buffer((T.int64(1000), T.int64(512)), "float32"), p2: T.Buffer((T.int64(1), T.int64(1000)), "float32"), T_add: T.Buffer((T.int64(1), T.int64(1000)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        T_matmul_NT_local = T.alloc_buffer((T.int64(1), T.int64(1000)), scope="local")
        p0_shared = T.alloc_buffer((T.int64(1), T.int64(512)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(1000), T.int64(512)), scope="shared")
        for i_0_j_0_fused in T.thread_binding(T.int64(25), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 1024, "pragma_unroll_explicit": 1}):
            for i_1_j_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for i_2_j_2_fused in T.thread_binding(T.int64(40), thread="threadIdx.x"):
                    for i_3_init, j_3_init, i_4_init, j_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_init"):
                            v_i = T.axis.spatial(T.int64(1), i_3_init + i_4_init)
                            v_j = T.axis.spatial(T.int64(1000), i_0_j_0_fused * T.int64(40) + i_2_j_2_fused + j_3_init + j_4_init)
                            T.reads()
                            T.writes(T_matmul_NT_local[v_i, v_j])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            T_matmul_NT_local[v_i, v_j] = T.float32(0)
                    for k_0_fused in T.serial(T.int64(8), annotations={"software_pipeline_async_stages": [0], "software_pipeline_order": [0, 1, 2], "software_pipeline_stage": [0, 0, 2]}):
                        for ax0_ax1_fused_0 in range(T.int64(1)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(40), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("p0_shared"):
                                        v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v1 = T.axis.spatial(T.int64(512), k_0_fused * T.int64(64) + (ax0_ax1_fused_0 * T.int64(80) + ax0_ax1_fused_1 * T.int64(2) + ax0_ax1_fused_2))
                                        T.where((ax0_ax1_fused_0 * T.int64(40) + ax0_ax1_fused_1) * T.int64(2) + ax0_ax1_fused_2 < T.int64(64))
                                        T.reads(p0[v0, v1])
                                        T.writes(p0_shared[v0, v1])
                                        p0_shared[v0, v1] = p0[v0, v1]
                        for ax0_ax1_fused_0 in range(T.int64(32)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(40), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("p1_shared"):
                                        v0 = T.axis.spatial(T.int64(1000), i_0_j_0_fused * T.int64(40) + (ax0_ax1_fused_0 * T.int64(80) + ax0_ax1_fused_1 * T.int64(2) + ax0_ax1_fused_2) // T.int64(64))
                                        v1 = T.axis.spatial(T.int64(512), k_0_fused * T.int64(64) + (ax0_ax1_fused_0 * T.int64(80) + ax0_ax1_fused_1 * T.int64(2) + ax0_ax1_fused_2) % T.int64(64))
                                        T.reads(p1[v0, v1])
                                        T.writes(p1_shared[v0, v1])
                                        p1_shared[v0, v1] = p1[v0, v1]
                        for k_1, i_3, j_3, k_2, i_4, j_4 in T.grid(T.int64(4), T.int64(1), T.int64(1), T.int64(16), T.int64(1), T.int64(1)):
                            with T.block("T_matmul_NT_update"):
                                v_i = T.axis.spatial(T.int64(1), i_3 + i_4)
                                v_j = T.axis.spatial(T.int64(1000), i_0_j_0_fused * T.int64(40) + i_2_j_2_fused + j_3 + j_4)
                                v_k = T.axis.reduce(T.int64(512), k_0_fused * T.int64(64) + k_1 * T.int64(16) + k_2)
                                T.reads(T_matmul_NT_local[v_i, v_j], p0_shared[v_i, v_k], p1_shared[v_j, v_k])
                                T.writes(T_matmul_NT_local[v_i, v_j])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                T_matmul_NT_local[v_i, v_j] = T_matmul_NT_local[v_i, v_j] + p0_shared[v_i, v_k] * p1_shared[v_j, v_k]
                    for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(1000), i_0_j_0_fused * T.int64(40) + i_2_j_2_fused + ax1)
                            T.reads(T_matmul_NT_local[v0, v1], p2[v0, v1])
                            T.writes(T_add[v0, v1])
                            T_add[v0, v1] = T_matmul_NT_local[v0, v1] + p2[v0, v1]
b0 = sch.get_block(name="T_matmul_NT", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l3, l4, l5 = sch.get_loops(block=b0)
v6, v7, v8, v9, v10 = sch.sample_perfect_tile(loop=l3, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l11, l12, l13, l14, l15 = sch.split(loop=l3, factors=[v6, v7, v8, v9, v10], preserve_unit_iters=True)
v16, v17, v18, v19, v20 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[25, 1, 40, 1, 1])
l21, l22, l23, l24, l25 = sch.split(loop=l4, factors=[v16, v17, v18, v19, v20], preserve_unit_iters=True)
v26, v27, v28 = sch.sample_perfect_tile(loop=l5, n=3, max_innermost_factor=64, decision=[8, 4, 16])
l29, l30, l31 = sch.split(loop=l5, factors=[v26, v27, v28], preserve_unit_iters=True)
sch.reorder(l11, l21, l12, l22, l13, l23, l29, l30, l14, l24, l31, l15, l25)
l32 = sch.fuse(l11, l21, preserve_unit_iters=True)
sch.bind(loop=l32, thread_axis="blockIdx.x")
l33 = sch.fuse(l12, l22, preserve_unit_iters=True)
sch.bind(loop=l33, thread_axis="vthread.x")
l34 = sch.fuse(l13, l23, preserve_unit_iters=True)
sch.bind(loop=l34, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b35 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b35, loop=l34, preserve_unit_loops=True, index=-1)
b36 = sch.cache_read(block=b0, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b36, loop=l29, preserve_unit_loops=True, index=-1)
l37, l38, l39, l40, l41, l42 = sch.get_loops(block=b36)
l43 = sch.fuse(l41, l42, preserve_unit_iters=True)
v44 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b36, ann_key="meta_schedule.cooperative_fetch", ann_val=v44)
b45 = sch.cache_read(block=b0, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b45, loop=l29, preserve_unit_loops=True, index=-1)
l46, l47, l48, l49, l50, l51 = sch.get_loops(block=b45)
l52 = sch.fuse(l50, l51, preserve_unit_iters=True)
v53 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b45, ann_key="meta_schedule.cooperative_fetch", ann_val=v53)
l54 = sch.fuse(l29, preserve_unit_iters=True)
sch.annotate(block_or_loop=l54, ann_key="software_pipeline_stage", ann_val=[0, 0, 2])
sch.annotate(block_or_loop=l54, ann_key="software_pipeline_order", ann_val=[0, 1, 2])
sch.annotate(block_or_loop=l54, ann_key="software_pipeline_async_stages", ann_val=[0])
sch.reverse_compute_inline(block=b1)
v55 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v55)
sch.enter_postproc()
sch.unannotate(block_or_loop=b36, ann_key="meta_schedule.cooperative_fetch")
l56, l57, l58, l59, l60 = sch.get_loops(block=b36)
l61, l62, l63 = sch.split(loop=l60, factors=[None, 40, 2], preserve_unit_iters=True)
sch.vectorize(loop=l63)
sch.bind(loop=l62, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b45, ann_key="meta_schedule.cooperative_fetch")
l64, l65, l66, l67, l68 = sch.get_loops(block=b45)
l69, l70, l71 = sch.split(loop=l68, factors=[None, 40, 2], preserve_unit_iters=True)
sch.vectorize(loop=l71)
sch.bind(loop=l70, thread_axis="threadIdx.x")
b72 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b72, ann_key="meta_schedule.unroll_explicit")
b73, b74, b75, b76 = sch.get_child_blocks(b72)
l77, l78, l79, l80, l81, l82, l83 = sch.get_loops(block=b73)
l84, l85, l86, l87, l88, l89, l90 = sch.get_loops(block=b74)
l91, l92, l93, l94, l95, l96, l97, l98, l99, l100 = sch.get_loops(block=b75)
sch.annotate(block_or_loop=l91, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l91, ann_key="pragma_unroll_explicit", ann_val=1)
l101, l102, l103, l104, l105 = sch.get_loops(block=b76)
b106 = sch.get_block(name="T_matmul_NT", func_name="main")
l107, l108, l109, l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b106)
b117 = sch.decompose_reduction(block=b106, loop=l110)
2023-11-11 12:01:18 [INFO] [task_scheduler.cc:121] [Task #28: fused_nn_dense_add] Trial #878: Error in running:
LocalRunner: An exception occurred
Subprocess terminated
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(512)), "float32"), p1: T.Buffer((T.int64(1000), T.int64(512)), "float32"), p2: T.Buffer((T.int64(1), T.int64(1000)), "float32"), T_add: T.Buffer((T.int64(1), T.int64(1000)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        T_matmul_NT_local = T.alloc_buffer((T.int64(1), T.int64(1000)), scope="local")
        p0_shared = T.alloc_buffer((T.int64(1), T.int64(512)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(1000), T.int64(512)), scope="shared")
        for i_0_j_0_fused in T.thread_binding(T.int64(25), thread="blockIdx.x"):
            for i_1_j_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for i_2_j_2_fused in T.thread_binding(T.int64(40), thread="threadIdx.x"):
                    for i_3_init, j_3_init, i_4_init, j_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_init"):
                            v_i = T.axis.spatial(T.int64(1), i_3_init + i_4_init)
                            v_j = T.axis.spatial(T.int64(1000), i_0_j_0_fused * T.int64(40) + i_2_j_2_fused + j_3_init + j_4_init)
                            T.reads()
                            T.writes(T_matmul_NT_local[v_i, v_j])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            T_matmul_NT_local[v_i, v_j] = T.float32(0)
                    for k_0_fused in T.serial(T.int64(16), annotations={"software_pipeline_async_stages": [0], "software_pipeline_order": [0, 1, 2], "software_pipeline_stage": [0, 0, 2]}):
                        for ax0_ax1_fused_0 in range(T.int64(1)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(40), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("p0_shared"):
                                        v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v1 = T.axis.spatial(T.int64(512), k_0_fused * T.int64(32) + (ax0_ax1_fused_0 * T.int64(80) + ax0_ax1_fused_1 * T.int64(2) + ax0_ax1_fused_2))
                                        T.where((ax0_ax1_fused_0 * T.int64(40) + ax0_ax1_fused_1) * T.int64(2) + ax0_ax1_fused_2 < T.int64(32))
                                        T.reads(p0[v0, v1])
                                        T.writes(p0_shared[v0, v1])
                                        p0_shared[v0, v1] = p0[v0, v1]
                        for ax0_ax1_fused_0 in range(T.int64(8)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(40), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("p1_shared"):
                                        v0 = T.axis.spatial(T.int64(1000), i_0_j_0_fused * T.int64(40) + (ax0_ax1_fused_0 * T.int64(160) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2) // T.int64(32))
                                        v1 = T.axis.spatial(T.int64(512), k_0_fused * T.int64(32) + (ax0_ax1_fused_0 * T.int64(160) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2) % T.int64(32))
                                        T.reads(p1[v0, v1])
                                        T.writes(p1_shared[v0, v1])
                                        p1_shared[v0, v1] = p1[v0, v1]
                        for k_1, i_3, j_3, k_2, i_4, j_4 in T.grid(T.int64(32), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                            with T.block("T_matmul_NT_update"):
                                v_i = T.axis.spatial(T.int64(1), i_3 + i_4)
                                v_j = T.axis.spatial(T.int64(1000), i_0_j_0_fused * T.int64(40) + i_2_j_2_fused + j_3 + j_4)
                                v_k = T.axis.reduce(T.int64(512), k_0_fused * T.int64(32) + k_1 + k_2)
                                T.reads(T_matmul_NT_local[v_i, v_j], p0_shared[v_i, v_k], p1_shared[v_j, v_k])
                                T.writes(T_matmul_NT_local[v_i, v_j])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                T_matmul_NT_local[v_i, v_j] = T_matmul_NT_local[v_i, v_j] + p0_shared[v_i, v_k] * p1_shared[v_j, v_k]
                    for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(1000), i_0_j_0_fused * T.int64(40) + i_2_j_2_fused + ax1)
                            T.reads(T_matmul_NT_local[v0, v1], p2[v0, v1])
                            T.writes(T_add[v0, v1])
                            T_add[v0, v1] = T_matmul_NT_local[v0, v1] + p2[v0, v1]
b0 = sch.get_block(name="T_matmul_NT", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l3, l4, l5 = sch.get_loops(block=b0)
v6, v7, v8, v9, v10 = sch.sample_perfect_tile(loop=l3, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l11, l12, l13, l14, l15 = sch.split(loop=l3, factors=[v6, v7, v8, v9, v10], preserve_unit_iters=True)
v16, v17, v18, v19, v20 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[25, 1, 40, 1, 1])
l21, l22, l23, l24, l25 = sch.split(loop=l4, factors=[v16, v17, v18, v19, v20], preserve_unit_iters=True)
v26, v27, v28 = sch.sample_perfect_tile(loop=l5, n=3, max_innermost_factor=64, decision=[16, 32, 1])
l29, l30, l31 = sch.split(loop=l5, factors=[v26, v27, v28], preserve_unit_iters=True)
sch.reorder(l11, l21, l12, l22, l13, l23, l29, l30, l14, l24, l31, l15, l25)
l32 = sch.fuse(l11, l21, preserve_unit_iters=True)
sch.bind(loop=l32, thread_axis="blockIdx.x")
l33 = sch.fuse(l12, l22, preserve_unit_iters=True)
sch.bind(loop=l33, thread_axis="vthread.x")
l34 = sch.fuse(l13, l23, preserve_unit_iters=True)
sch.bind(loop=l34, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b35 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b35, loop=l34, preserve_unit_loops=True, index=-1)
b36 = sch.cache_read(block=b0, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b36, loop=l29, preserve_unit_loops=True, index=-1)
l37, l38, l39, l40, l41, l42 = sch.get_loops(block=b36)
l43 = sch.fuse(l41, l42, preserve_unit_iters=True)
v44 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b36, ann_key="meta_schedule.cooperative_fetch", ann_val=v44)
b45 = sch.cache_read(block=b0, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b45, loop=l29, preserve_unit_loops=True, index=-1)
l46, l47, l48, l49, l50, l51 = sch.get_loops(block=b45)
l52 = sch.fuse(l50, l51, preserve_unit_iters=True)
v53 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b45, ann_key="meta_schedule.cooperative_fetch", ann_val=v53)
l54 = sch.fuse(l29, preserve_unit_iters=True)
sch.annotate(block_or_loop=l54, ann_key="software_pipeline_stage", ann_val=[0, 0, 2])
sch.annotate(block_or_loop=l54, ann_key="software_pipeline_order", ann_val=[0, 1, 2])
sch.annotate(block_or_loop=l54, ann_key="software_pipeline_async_stages", ann_val=[0])
sch.reverse_compute_inline(block=b1)
v55 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=0)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v55)
sch.enter_postproc()
sch.unannotate(block_or_loop=b36, ann_key="meta_schedule.cooperative_fetch")
l56, l57, l58, l59, l60 = sch.get_loops(block=b36)
l61, l62, l63 = sch.split(loop=l60, factors=[None, 40, 2], preserve_unit_iters=True)
sch.vectorize(loop=l63)
sch.bind(loop=l62, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b45, ann_key="meta_schedule.cooperative_fetch")
l64, l65, l66, l67, l68 = sch.get_loops(block=b45)
l69, l70, l71 = sch.split(loop=l68, factors=[None, 40, 4], preserve_unit_iters=True)
sch.vectorize(loop=l71)
sch.bind(loop=l70, thread_axis="threadIdx.x")
b72 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b72, ann_key="meta_schedule.unroll_explicit")
b73, b74, b75, b76 = sch.get_child_blocks(b72)
l77, l78, l79, l80, l81, l82, l83 = sch.get_loops(block=b73)
l84, l85, l86, l87, l88, l89, l90 = sch.get_loops(block=b74)
l91, l92, l93, l94, l95, l96, l97, l98, l99, l100 = sch.get_loops(block=b75)
l101, l102, l103, l104, l105 = sch.get_loops(block=b76)
b106 = sch.get_block(name="T_matmul_NT", func_name="main")
l107, l108, l109, l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b106)
b117 = sch.decompose_reduction(block=b106, loop=l110)
2023-11-11 12:01:18 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #879: GFLOPs: 71.5266. Time: 14.3303 us. Best GFLOPs: 97.5888
2023-11-11 12:01:18 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #880: GFLOPs: 89.8069. Time: 11.4134 us. Best GFLOPs: 97.5888
2023-11-11 12:01:18 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #881: GFLOPs: 64.6498. Time: 15.8546 us. Best GFLOPs: 97.5888
2023-11-11 12:01:18 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #882: GFLOPs: 55.3488. Time: 18.5189 us. Best GFLOPs: 97.5888
2023-11-11 12:01:18 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #883: GFLOPs: 55.3622. Time: 18.5144 us. Best GFLOPs: 97.5888
2023-11-11 12:01:18 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #884: GFLOPs: 55.1625. Time: 18.5815 us. Best GFLOPs: 97.5888
2023-11-11 12:01:18 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #885: GFLOPs: 49.9423. Time: 20.5237 us. Best GFLOPs: 97.5888
2023-11-11 12:01:18 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #886: GFLOPs: 57.4023. Time: 17.8564 us. Best GFLOPs: 97.5888
2023-11-11 12:01:18 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #887: GFLOPs: 55.3617. Time: 18.5146 us. Best GFLOPs: 97.5888
2023-11-11 12:01:18 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #888: GFLOPs: 55.3480. Time: 18.5192 us. Best GFLOPs: 97.5888
2023-11-11 12:01:18 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #889: GFLOPs: 65.5457. Time: 15.6379 us. Best GFLOPs: 97.5888
2023-11-11 12:01:18 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #890: GFLOPs: 69.4895. Time: 14.7504 us. Best GFLOPs: 97.5888
2023-11-11 12:01:18 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #891: GFLOPs: 89.7799. Time: 11.4168 us. Best GFLOPs: 97.5888
2023-11-11 12:01:18 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #892: GFLOPs: 13.5188. Time: 75.8204 us. Best GFLOPs: 97.5888
2023-11-11 12:01:18 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #893: GFLOPs: 6.1136. Time: 167.6589 us. Best GFLOPs: 97.5888
2023-11-11 12:01:18 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #894: GFLOPs: 4.4740. Time: 229.1036 us. Best GFLOPs: 97.5888
2023-11-11 12:04:26 [INFO] [evolutionary_search.cc:713] Generating candidates......
2023-11-11 12:04:26 [INFO] [evolutionary_search.cc:715] Picked top 102 candidate(s) from database
2023-11-11 12:04:27 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310a405238)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310e76ec68)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310dc33cd8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310be916f8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56310f803f18)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310eeb7a68)]: 402 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310e771ba8)]: 0 failure(s)
2023-11-11 12:04:28 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310a405238)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310e76ec68)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310dc33cd8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310be916f8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56310f803f18)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310eeb7a68)]: 804 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310e771ba8)]: 0 failure(s)
2023-11-11 12:04:28 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310a405238)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310e76ec68)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310dc33cd8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310be916f8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56310f803f18)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310eeb7a68)]: 1207 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310e771ba8)]: 0 failure(s)
2023-11-11 12:04:29 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310a405238)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310e76ec68)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310dc33cd8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310be916f8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56310f803f18)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310eeb7a68)]: 1608 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310e771ba8)]: 0 failure(s)
2023-11-11 12:04:30 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310a405238)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310e76ec68)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310dc33cd8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310be916f8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56310f803f18)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310eeb7a68)]: 2010 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310e771ba8)]: 0 failure(s)
2023-11-11 12:04:30 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310a405238)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310e76ec68)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310dc33cd8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310be916f8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56310f803f18)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310eeb7a68)]: 2411 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310e771ba8)]: 0 failure(s)
2023-11-11 12:04:31 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310a405238)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310e76ec68)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310dc33cd8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310be916f8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56310f803f18)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310eeb7a68)]: 2815 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310e771ba8)]: 0 failure(s)
2023-11-11 12:04:31 [INFO] [evolutionary_search.cc:723] Sampled 55 candidate(s)
2023-11-11 12:04:33 [INFO] [evolutionary_search.cc:621] Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310a405238)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310e76ec68)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310dc33cd8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310be916f8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56310f803f18)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310eeb7a68)]: 151 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310e771ba8)]: 0 failure(s)
2023-11-11 12:04:36 [INFO] [evolutionary_search.cc:621] Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310a405238)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310e76ec68)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310dc33cd8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310be916f8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56310f803f18)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310eeb7a68)]: 181 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310e771ba8)]: 0 failure(s)
2023-11-11 12:04:39 [INFO] [evolutionary_search.cc:621] Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310a405238)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310e76ec68)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310dc33cd8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310be916f8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56310f803f18)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310eeb7a68)]: 191 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310e771ba8)]: 0 failure(s)
2023-11-11 12:04:42 [INFO] [evolutionary_search.cc:621] Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310a405238)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310e76ec68)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310dc33cd8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310be916f8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56310f803f18)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310eeb7a68)]: 171 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310e771ba8)]: 0 failure(s)
2023-11-11 12:04:43 [INFO] [evolutionary_search.cc:649] Scores of the best 64 candidates:
[1 : 16]:	0.9334  0.8989  0.8922  0.8828  0.8773  0.8612  0.8553  0.8448  0.8437  0.8346  0.8069  0.8037  0.8014  0.7933  0.7920  0.7899
[17 : 32]:	0.7809  0.7675  0.7667  0.7663  0.7532  0.7521  0.7467  0.7465  0.7465  0.7465  0.7303  0.7299  0.7290  0.7259  0.7235  0.7204
[33 : 48]:	0.7164  0.7150  0.7120  0.7118  0.7109  0.7109  0.7101  0.7085  0.7071  0.7053  0.7049  0.7044  0.7030  0.7011  0.7001  0.6999
[49 : 64]:	0.6994  0.6974  0.6972  0.6965  0.6946  0.6937  0.6882  0.6859  0.6857  0.6857  0.6850  0.6844  0.6844  0.6806  0.6787  0.6787
2023-11-11 12:04:43 [INFO] [evolutionary_search.cc:727] Got 64 candidate(s) with evolutionary search
2023-11-11 12:04:43 [INFO] [evolutionary_search.cc:730] Sending 64 candidates(s) for measurement
2023-11-11 12:05:10 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #895: GFLOPs: 74.0028. Time: 13.8508 us. Best GFLOPs: 97.5888
2023-11-11 12:05:10 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #896: GFLOPs: 89.1552. Time: 11.4968 us. Best GFLOPs: 97.5888
2023-11-11 12:05:10 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #897: GFLOPs: 59.6331. Time: 17.1884 us. Best GFLOPs: 97.5888
2023-11-11 12:05:10 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #898: GFLOPs: 89.2335. Time: 11.4867 us. Best GFLOPs: 97.5888
2023-11-11 12:05:10 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #899: GFLOPs: 89.7770. Time: 11.4172 us. Best GFLOPs: 97.5888
2023-11-11 12:05:10 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #900: GFLOPs: 93.4786. Time: 10.9651 us. Best GFLOPs: 97.5888
2023-11-11 12:05:10 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #901: GFLOPs: 85.1305. Time: 12.0403 us. Best GFLOPs: 97.5888
2023-11-11 12:05:10 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #902: GFLOPs: 89.2303. Time: 11.4871 us. Best GFLOPs: 97.5888
2023-11-11 12:05:10 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #903: GFLOPs: 94.3694. Time: 10.8616 us. Best GFLOPs: 97.5888
2023-11-11 12:05:10 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #904: GFLOPs: 81.6210. Time: 12.5580 us. Best GFLOPs: 97.5888
2023-11-11 12:05:10 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #905: GFLOPs: 83.2853. Time: 12.3071 us. Best GFLOPs: 97.5888
2023-11-11 12:05:10 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #906: GFLOPs: 89.5579. Time: 11.4451 us. Best GFLOPs: 97.5888
2023-11-11 12:05:10 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #907: GFLOPs: 85.1174. Time: 12.0422 us. Best GFLOPs: 97.5888
2023-11-11 12:05:10 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #908: GFLOPs: 73.9758. Time: 13.8559 us. Best GFLOPs: 97.5888
2023-11-11 12:05:10 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #909: GFLOPs: 55.3301. Time: 18.5252 us. Best GFLOPs: 97.5888
2023-11-11 12:05:10 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #910: GFLOPs: 27.2276. Time: 37.6457 us. Best GFLOPs: 97.5888
2023-11-11 12:05:10 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #911: GFLOPs: 51.7269. Time: 19.8156 us. Best GFLOPs: 97.5888
2023-11-11 12:05:10 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #912: GFLOPs: 41.7196. Time: 24.5688 us. Best GFLOPs: 97.5888
2023-11-11 12:05:10 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #913: GFLOPs: 11.2538. Time: 91.0801 us. Best GFLOPs: 97.5888
2023-11-11 12:05:10 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #914: GFLOPs: 69.7656. Time: 14.6921 us. Best GFLOPs: 97.5888
2023-11-11 12:05:10 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #915: GFLOPs: 83.3491. Time: 12.2977 us. Best GFLOPs: 97.5888
2023-11-11 12:05:10 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #916: GFLOPs: 93.4263. Time: 10.9712 us. Best GFLOPs: 97.5888
2023-11-11 12:05:10 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #917: GFLOPs: 93.4283. Time: 10.9710 us. Best GFLOPs: 97.5888
2023-11-11 12:05:10 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #918: GFLOPs: 66.5955. Time: 15.3914 us. Best GFLOPs: 97.5888
2023-11-11 12:05:10 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #919: GFLOPs: 66.6001. Time: 15.3904 us. Best GFLOPs: 97.5888
2023-11-11 12:05:10 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #920: GFLOPs: 89.7648. Time: 11.4187 us. Best GFLOPs: 97.5888
2023-11-11 12:05:10 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #921: GFLOPs: 55.3544. Time: 18.5171 us. Best GFLOPs: 97.5888
2023-11-11 12:05:10 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #922: GFLOPs: 55.2919. Time: 18.5380 us. Best GFLOPs: 97.5888
2023-11-11 12:05:10 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #923: GFLOPs: 55.3551. Time: 18.5168 us. Best GFLOPs: 97.5888
2023-11-11 12:05:10 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #924: GFLOPs: 69.6770. Time: 14.7107 us. Best GFLOPs: 97.5888
2023-11-11 12:05:10 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #925: GFLOPs: 89.7987. Time: 11.4144 us. Best GFLOPs: 97.5888
2023-11-11 12:05:10 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #926: GFLOPs: 65.5228. Time: 15.6434 us. Best GFLOPs: 97.5888
2023-11-11 12:05:10 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #927: GFLOPs: 55.2932. Time: 18.5375 us. Best GFLOPs: 97.5888
2023-11-11 12:05:10 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #928: GFLOPs: 89.8047. Time: 11.4137 us. Best GFLOPs: 97.5888
2023-11-11 12:05:10 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #929: GFLOPs: 69.6861. Time: 14.7088 us. Best GFLOPs: 97.5888
2023-11-11 12:05:10 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #930: GFLOPs: 55.2829. Time: 18.5410 us. Best GFLOPs: 97.5888
2023-11-11 12:05:10 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #931: GFLOPs: 55.3555. Time: 18.5167 us. Best GFLOPs: 97.5888
2023-11-11 12:05:10 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #932: GFLOPs: 55.3559. Time: 18.5166 us. Best GFLOPs: 97.5888
2023-11-11 12:05:10 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #933: GFLOPs: 69.4929. Time: 14.7497 us. Best GFLOPs: 97.5888
2023-11-11 12:05:10 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #934: GFLOPs: 69.5579. Time: 14.7359 us. Best GFLOPs: 97.5888
2023-11-11 12:05:10 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #935: GFLOPs: 57.4007. Time: 17.8569 us. Best GFLOPs: 97.5888
2023-11-11 12:05:10 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #936: GFLOPs: 64.6462. Time: 15.8555 us. Best GFLOPs: 97.5888
2023-11-11 12:05:10 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #937: GFLOPs: 80.4187. Time: 12.7458 us. Best GFLOPs: 97.5888
2023-11-11 12:05:10 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #938: GFLOPs: 57.4022. Time: 17.8565 us. Best GFLOPs: 97.5888
2023-11-11 12:05:10 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #939: GFLOPs: 55.1596. Time: 18.5825 us. Best GFLOPs: 97.5888
2023-11-11 12:05:10 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #940: GFLOPs: 69.4563. Time: 14.7575 us. Best GFLOPs: 97.5888
2023-11-11 12:05:10 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #941: GFLOPs: 69.4919. Time: 14.7499 us. Best GFLOPs: 97.5888
2023-11-11 12:05:10 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #942: GFLOPs: 89.8018. Time: 11.4140 us. Best GFLOPs: 97.5888
2023-11-11 12:05:10 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #943: GFLOPs: 46.4175. Time: 22.0822 us. Best GFLOPs: 97.5888
2023-11-11 12:05:10 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #944: GFLOPs: 69.4914. Time: 14.7500 us. Best GFLOPs: 97.5888
2023-11-11 12:05:10 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #945: GFLOPs: 55.1696. Time: 18.5791 us. Best GFLOPs: 97.5888
2023-11-11 12:05:10 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #946: GFLOPs: 32.3943. Time: 31.6413 us. Best GFLOPs: 97.5888
2023-11-11 12:05:10 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #947: GFLOPs: 69.4914. Time: 14.7500 us. Best GFLOPs: 97.5888
2023-11-11 12:05:10 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #948: GFLOPs: 55.1737. Time: 18.5777 us. Best GFLOPs: 97.5888
2023-11-11 12:05:10 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #949: GFLOPs: 55.1730. Time: 18.5779 us. Best GFLOPs: 97.5888
2023-11-11 12:05:10 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #950: GFLOPs: 43.0457. Time: 23.8119 us. Best GFLOPs: 97.5888
2023-11-11 12:05:10 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #951: GFLOPs: 73.9722. Time: 13.8566 us. Best GFLOPs: 97.5888
2023-11-11 12:05:10 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #952: GFLOPs: 43.0452. Time: 23.8122 us. Best GFLOPs: 97.5888
2023-11-11 12:05:10 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #953: GFLOPs: 43.0457. Time: 23.8119 us. Best GFLOPs: 97.5888
2023-11-11 12:05:10 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #954: GFLOPs: 55.1715. Time: 18.5784 us. Best GFLOPs: 97.5888
2023-11-11 12:05:10 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #955: GFLOPs: 55.1674. Time: 18.5798 us. Best GFLOPs: 97.5888
2023-11-11 12:05:10 [INFO] [task_scheduler.cc:121] [Task #28: fused_nn_dense_add] Trial #956: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/home2/xiachunwei/Software/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/home2/xiachunwei/Software/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 159, in _worker_func
    costs: List[float] = f_run_evaluator(
  File "/home2/xiachunwei/Software/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 387, in default_run_evaluator
    return run_evaluator_common(rt_mod, device, evaluator_config, repeated_args)
  File "/home2/xiachunwei/Software/tvm/python/tvm/meta_schedule/runner/utils.py", line 117, in run_evaluator_common
    profile_result = evaluator(*args)
  File "/home2/xiachunwei/Software/tvm/python/tvm/runtime/module.py", line 397, in evaluator
    blob = feval(*args)
  File "/home2/xiachunwei/Software/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/home2/xiachunwei/Software/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  1: tvm::runtime::profiling::WrapTimeEvaluator(tvm::runtime::PackedFunc, DLDevice, int, int, int, int, int, int, int, tvm::runtime::PackedFunc)::{lambda(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*) [clone .isra.779]
  0: tvm::runtime::CUDADeviceAPI::StreamSync(DLDevice, void*)
  File "/home2/xiachunwei/Software/tvm/src/runtime/cuda/cuda_device_api.cc", line 212
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(512)), "float32"), p1: T.Buffer((T.int64(1000), T.int64(512)), "float32"), p2: T.Buffer((T.int64(1), T.int64(1000)), "float32"), T_add: T.Buffer((T.int64(1), T.int64(1000)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        T_matmul_NT_local = T.alloc_buffer((T.int64(1), T.int64(1000)), scope="local")
        p0_shared = T.alloc_buffer((T.int64(1), T.int64(512)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(1000), T.int64(512)), scope="shared")
        for i_0_j_0_fused in T.thread_binding(T.int64(2), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 64, "pragma_unroll_explicit": 1}):
            for i_1_j_1_fused in T.thread_binding(T.int64(10), thread="vthread.x"):
                for i_2_j_2_fused in T.thread_binding(T.int64(50), thread="threadIdx.x"):
                    for i_3_init, j_3_init, i_4_init, j_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_init"):
                            v_i = T.axis.spatial(T.int64(1), i_3_init + i_4_init)
                            v_j = T.axis.spatial(T.int64(1000), i_0_j_0_fused * T.int64(500) + i_1_j_1_fused * T.int64(50) + i_2_j_2_fused + j_3_init + j_4_init)
                            T.reads()
                            T.writes(T_matmul_NT_local[v_i, v_j])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            T_matmul_NT_local[v_i, v_j] = T.float32(0)
                    for k_0 in range(T.int64(256)):
                        for ax0_ax1_fused_0 in range(T.int64(1)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(50), thread="threadIdx.x"):
                                with T.block("p0_shared"):
                                    v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                    v1 = T.axis.spatial(T.int64(512), k_0 * T.int64(2) + (ax0_ax1_fused_0 * T.int64(50) + ax0_ax1_fused_1))
                                    T.where(ax0_ax1_fused_0 * T.int64(50) + ax0_ax1_fused_1 < T.int64(2))
                                    T.reads(p0[v0, v1])
                                    T.writes(p0_shared[v0, v1])
                                    p0_shared[v0, v1] = p0[v0, v1]
                        for ax0_ax1_fused_0 in range(T.int64(5)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(50), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("p1_shared"):
                                        v0 = T.axis.spatial(T.int64(1000), i_0_j_0_fused * T.int64(500) + (ax0_ax1_fused_0 * T.int64(200) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2) // T.int64(2))
                                        v1 = T.axis.spatial(T.int64(512), k_0 * T.int64(2) + (ax0_ax1_fused_0 * T.int64(200) + ax0_ax1_fused_1 * T.int64(4) + ax0_ax1_fused_2) % T.int64(2))
                                        T.reads(p1[v0, v1])
                                        T.writes(p1_shared[v0, v1])
                                        p1_shared[v0, v1] = p1[v0, v1]
                        for k_1, i_3, j_3, k_2, i_4, j_4 in T.grid(T.int64(2), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                            with T.block("T_matmul_NT_update"):
                                v_i = T.axis.spatial(T.int64(1), i_3 + i_4)
                                v_j = T.axis.spatial(T.int64(1000), i_0_j_0_fused * T.int64(500) + i_1_j_1_fused * T.int64(50) + i_2_j_2_fused + j_3 + j_4)
                                v_k = T.axis.reduce(T.int64(512), k_0 * T.int64(2) + k_1 + k_2)
                                T.reads(T_matmul_NT_local[v_i, v_j], p0_shared[v_i, v_k], p1_shared[v_j, v_k])
                                T.writes(T_matmul_NT_local[v_i, v_j])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                T_matmul_NT_local[v_i, v_j] = T_matmul_NT_local[v_i, v_j] + p0_shared[v_i, v_k] * p1_shared[v_j, v_k]
                    for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(1000), i_0_j_0_fused * T.int64(500) + i_1_j_1_fused * T.int64(50) + i_2_j_2_fused + ax1)
                            T.reads(T_matmul_NT_local[v0, v1], p2[v0, v1])
                            T.writes(T_add[v0, v1])
                            T_add[v0, v1] = T_matmul_NT_local[v0, v1] + p2[v0, v1]
b0 = sch.get_block(name="T_matmul_NT", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l3, l4, l5 = sch.get_loops(block=b0)
v6, v7, v8, v9, v10 = sch.sample_perfect_tile(loop=l3, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l11, l12, l13, l14, l15 = sch.split(loop=l3, factors=[v6, v7, v8, v9, v10], preserve_unit_iters=True)
v16, v17, v18, v19, v20 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[2, 10, 50, 1, 1])
l21, l22, l23, l24, l25 = sch.split(loop=l4, factors=[v16, v17, v18, v19, v20], preserve_unit_iters=True)
v26, v27, v28 = sch.sample_perfect_tile(loop=l5, n=3, max_innermost_factor=64, decision=[256, 2, 1])
l29, l30, l31 = sch.split(loop=l5, factors=[v26, v27, v28], preserve_unit_iters=True)
sch.reorder(l11, l21, l12, l22, l13, l23, l29, l30, l14, l24, l31, l15, l25)
l32 = sch.fuse(l11, l21, preserve_unit_iters=True)
sch.bind(loop=l32, thread_axis="blockIdx.x")
l33 = sch.fuse(l12, l22, preserve_unit_iters=True)
sch.bind(loop=l33, thread_axis="vthread.x")
l34 = sch.fuse(l13, l23, preserve_unit_iters=True)
sch.bind(loop=l34, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b35 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b35, loop=l34, preserve_unit_loops=True, index=-1)
b36 = sch.cache_read(block=b0, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b36, loop=l29, preserve_unit_loops=True, index=-1)
l37, l38, l39, l40, l41, l42 = sch.get_loops(block=b36)
l43 = sch.fuse(l41, l42, preserve_unit_iters=True)
v44 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b36, ann_key="meta_schedule.cooperative_fetch", ann_val=v44)
b45 = sch.cache_read(block=b0, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b45, loop=l29, preserve_unit_loops=True, index=-1)
l46, l47, l48, l49, l50, l51 = sch.get_loops(block=b45)
l52 = sch.fuse(l50, l51, preserve_unit_iters=True)
v53 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b45, ann_key="meta_schedule.cooperative_fetch", ann_val=v53)
sch.reverse_compute_inline(block=b1)
v54 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=2)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v54)
sch.enter_postproc()
sch.unannotate(block_or_loop=b36, ann_key="meta_schedule.cooperative_fetch")
l55, l56, l57, l58, l59 = sch.get_loops(block=b36)
l60, l61 = sch.split(loop=l59, factors=[None, 50], preserve_unit_iters=True)
sch.bind(loop=l61, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b45, ann_key="meta_schedule.cooperative_fetch")
l62, l63, l64, l65, l66 = sch.get_loops(block=b45)
l67, l68, l69 = sch.split(loop=l66, factors=[None, 50, 4], preserve_unit_iters=True)
sch.vectorize(loop=l69)
sch.bind(loop=l68, thread_axis="threadIdx.x")
b70 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b70, ann_key="meta_schedule.unroll_explicit")
b71, b72, b73, b74 = sch.get_child_blocks(b70)
l75, l76, l77, l78, l79, l80 = sch.get_loops(block=b71)
l81, l82, l83, l84, l85, l86, l87 = sch.get_loops(block=b72)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b73)
sch.annotate(block_or_loop=l88, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l88, ann_key="pragma_unroll_explicit", ann_val=1)
l98, l99, l100, l101, l102 = sch.get_loops(block=b74)
b103 = sch.get_block(name="T_matmul_NT", func_name="main")
l104, l105, l106, l107, l108, l109, l110, l111, l112, l113 = sch.get_loops(block=b103)
b114 = sch.decompose_reduction(block=b103, loop=l107)
2023-11-11 12:05:10 [INFO] [task_scheduler.cc:121] [Task #28: fused_nn_dense_add] Trial #957: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/home2/xiachunwei/Software/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/home2/xiachunwei/Software/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/home2/xiachunwei/Software/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/home2/xiachunwei/Software/tvm/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/home2/xiachunwei/Software/tvm/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/home2/xiachunwei/Software/tvm/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/home2/xiachunwei/Software/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/home2/xiachunwei/Software/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  4: _ZN3tvm7runtime13PackedFun
  3: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  2: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  0: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  File "/home2/xiachunwei/Software/tvm/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(512)), "float32"), p1: T.Buffer((T.int64(1000), T.int64(512)), "float32"), p2: T.Buffer((T.int64(1), T.int64(1000)), "float32"), T_add: T.Buffer((T.int64(1), T.int64(1000)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        T_matmul_NT_local = T.alloc_buffer((T.int64(1), T.int64(1000)), scope="local")
        p0_shared = T.alloc_buffer((T.int64(1), T.int64(512)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(1000), T.int64(512)), scope="shared")
        for i_0_j_0_fused in T.thread_binding(T.int64(1), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 16, "pragma_unroll_explicit": 1}):
            for i_1_j_1_fused in T.thread_binding(T.int64(5), thread="vthread.x"):
                for i_2_j_2_fused in T.thread_binding(T.int64(40), thread="threadIdx.x"):
                    for i_3_init, j_3_init, i_4_init, j_4_init in T.grid(T.int64(1), T.int64(5), T.int64(1), T.int64(1)):
                        with T.block("T_matmul_NT_init"):
                            v_i = T.axis.spatial(T.int64(1), i_3_init + i_4_init)
                            v_j = T.axis.spatial(T.int64(1000), i_1_j_1_fused * T.int64(200) + i_2_j_2_fused * T.int64(5) + j_3_init + j_4_init)
                            T.reads()
                            T.writes(T_matmul_NT_local[v_i, v_j])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            T_matmul_NT_local[v_i, v_j] = T.float32(0)
                    for k_0 in range(T.int64(256)):
                        for ax0_ax1_fused_0 in range(T.int64(1)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(40), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("p0_shared"):
                                        v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v1 = T.axis.spatial(T.int64(512), k_0 * T.int64(2) + (ax0_ax1_fused_0 * T.int64(80) + ax0_ax1_fused_1 * T.int64(2) + ax0_ax1_fused_2))
                                        T.where((ax0_ax1_fused_0 * T.int64(40) + ax0_ax1_fused_1) * T.int64(2) + ax0_ax1_fused_2 < T.int64(2))
                                        T.reads(p0[v0, v1])
                                        T.writes(p0_shared[v0, v1])
                                        p0_shared[v0, v1] = p0[v0, v1]
                        for ax0_ax1_fused_0 in range(T.int64(50)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(40), thread="threadIdx.x"):
                                with T.block("p1_shared"):
                                    v0 = T.axis.spatial(T.int64(1000), (ax0_ax1_fused_0 * T.int64(40) + ax0_ax1_fused_1) // T.int64(2))
                                    v1 = T.axis.spatial(T.int64(512), k_0 * T.int64(2) + (ax0_ax1_fused_0 * T.int64(40) + ax0_ax1_fused_1) % T.int64(2))
                                    T.reads(p1[v0, v1])
                                    T.writes(p1_shared[v0, v1])
                                    p1_shared[v0, v1] = p1[v0, v1]
                        for k_1, i_3, j_3, k_2, i_4, j_4 in T.grid(T.int64(1), T.int64(1), T.int64(5), T.int64(2), T.int64(1), T.int64(1)):
                            with T.block("T_matmul_NT_update"):
                                v_i = T.axis.spatial(T.int64(1), i_3 + i_4)
                                v_j = T.axis.spatial(T.int64(1000), i_1_j_1_fused * T.int64(200) + i_2_j_2_fused * T.int64(5) + j_3 + j_4)
                                v_k = T.axis.reduce(T.int64(512), k_0 * T.int64(2) + k_1 * T.int64(2) + k_2)
                                T.reads(T_matmul_NT_local[v_i, v_j], p0_shared[v_i, v_k], p1_shared[v_j, v_k])
                                T.writes(T_matmul_NT_local[v_i, v_j])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                T_matmul_NT_local[v_i, v_j] = T_matmul_NT_local[v_i, v_j] + p0_shared[v_i, v_k] * p1_shared[v_j, v_k]
                    for ax0, ax1 in T.grid(T.int64(1), T.int64(5)):
                        with T.block("T_matmul_NT_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(1000), i_1_j_1_fused * T.int64(200) + i_2_j_2_fused * T.int64(5) + ax1)
                            T.reads(T_matmul_NT_local[v0, v1], p2[v0, v1])
                            T.writes(T_add[v0, v1])
                            T_add[v0, v1] = T_matmul_NT_local[v0, v1] + p2[v0, v1]
b0 = sch.get_block(name="T_matmul_NT", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l3, l4, l5 = sch.get_loops(block=b0)
v6, v7, v8, v9, v10 = sch.sample_perfect_tile(loop=l3, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l11, l12, l13, l14, l15 = sch.split(loop=l3, factors=[v6, v7, v8, v9, v10], preserve_unit_iters=True)
v16, v17, v18, v19, v20 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[1, 5, 40, 5, 1])
l21, l22, l23, l24, l25 = sch.split(loop=l4, factors=[v16, v17, v18, v19, v20], preserve_unit_iters=True)
v26, v27, v28 = sch.sample_perfect_tile(loop=l5, n=3, max_innermost_factor=64, decision=[256, 1, 2])
l29, l30, l31 = sch.split(loop=l5, factors=[v26, v27, v28], preserve_unit_iters=True)
sch.reorder(l11, l21, l12, l22, l13, l23, l29, l30, l14, l24, l31, l15, l25)
l32 = sch.fuse(l11, l21, preserve_unit_iters=True)
sch.bind(loop=l32, thread_axis="blockIdx.x")
l33 = sch.fuse(l12, l22, preserve_unit_iters=True)
sch.bind(loop=l33, thread_axis="vthread.x")
l34 = sch.fuse(l13, l23, preserve_unit_iters=True)
sch.bind(loop=l34, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b35 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b35, loop=l34, preserve_unit_loops=True, index=-1)
b36 = sch.cache_read(block=b0, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b36, loop=l29, preserve_unit_loops=True, index=-1)
l37, l38, l39, l40, l41, l42 = sch.get_loops(block=b36)
l43 = sch.fuse(l41, l42, preserve_unit_iters=True)
v44 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b36, ann_key="meta_schedule.cooperative_fetch", ann_val=v44)
b45 = sch.cache_read(block=b0, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b45, loop=l29, preserve_unit_loops=True, index=-1)
l46, l47, l48, l49, l50, l51 = sch.get_loops(block=b45)
l52 = sch.fuse(l50, l51, preserve_unit_iters=True)
v53 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b45, ann_key="meta_schedule.cooperative_fetch", ann_val=v53)
sch.reverse_compute_inline(block=b1)
v54 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=1)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v54)
sch.enter_postproc()
sch.unannotate(block_or_loop=b36, ann_key="meta_schedule.cooperative_fetch")
l55, l56, l57, l58, l59 = sch.get_loops(block=b36)
l60, l61, l62 = sch.split(loop=l59, factors=[None, 40, 2], preserve_unit_iters=True)
sch.vectorize(loop=l62)
sch.bind(loop=l61, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b45, ann_key="meta_schedule.cooperative_fetch")
l63, l64, l65, l66, l67 = sch.get_loops(block=b45)
l68, l69 = sch.split(loop=l67, factors=[None, 40], preserve_unit_iters=True)
sch.bind(loop=l69, thread_axis="threadIdx.x")
b70 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b70, ann_key="meta_schedule.unroll_explicit")
b71, b72, b73, b74 = sch.get_child_blocks(b70)
l75, l76, l77, l78, l79, l80, l81 = sch.get_loops(block=b71)
l82, l83, l84, l85, l86, l87 = sch.get_loops(block=b72)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b73)
sch.annotate(block_or_loop=l88, ann_key="pragma_auto_unroll_max_step", ann_val=16)
sch.annotate(block_or_loop=l88, ann_key="pragma_unroll_explicit", ann_val=1)
l98, l99, l100, l101, l102 = sch.get_loops(block=b74)
b103 = sch.get_block(name="T_matmul_NT", func_name="main")
l104, l105, l106, l107, l108, l109, l110, l111, l112, l113 = sch.get_loops(block=b103)
b114 = sch.decompose_reduction(block=b103, loop=l107)
2023-11-11 12:05:10 [INFO] [task_scheduler.cc:121] [Task #28: fused_nn_dense_add] Trial #958: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/home2/xiachunwei/Software/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/home2/xiachunwei/Software/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/home2/xiachunwei/Software/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/home2/xiachunwei/Software/tvm/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/home2/xiachunwei/Software/tvm/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/home2/xiachunwei/Software/tvm/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/home2/xiachunwei/Software/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/home2/xiachunwei/Software/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  4: _ZN3tvm7runtime13PackedFun
  3: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  2: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  0: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  File "/home2/xiachunwei/Software/tvm/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(512)), "float32"), p1: T.Buffer((T.int64(1000), T.int64(512)), "float32"), p2: T.Buffer((T.int64(1), T.int64(1000)), "float32"), T_add: T.Buffer((T.int64(1), T.int64(1000)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        T_matmul_NT_local = T.alloc_buffer((T.int64(1), T.int64(1000)), scope="local")
        p0_shared = T.alloc_buffer((T.int64(1), T.int64(512)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(1000), T.int64(512)), scope="shared")
        for i_0_j_0_fused in T.thread_binding(T.int64(2), thread="blockIdx.x"):
            for i_1_j_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for i_2_j_2_fused in T.thread_binding(T.int64(50), thread="threadIdx.x"):
                    for i_3_init, j_3_init, i_4_init, j_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(10)):
                        with T.block("T_matmul_NT_init"):
                            v_i = T.axis.spatial(T.int64(1), i_3_init + i_4_init)
                            v_j = T.axis.spatial(T.int64(1000), i_0_j_0_fused * T.int64(500) + i_2_j_2_fused * T.int64(10) + j_3_init * T.int64(10) + j_4_init)
                            T.reads()
                            T.writes(T_matmul_NT_local[v_i, v_j])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            T_matmul_NT_local[v_i, v_j] = T.float32(0)
                    for k_0_fused in T.serial(T.int64(256), annotations={"software_pipeline_async_stages": [0], "software_pipeline_order": [0, 1, 2], "software_pipeline_stage": [0, 0, 3]}):
                        for ax0_ax1_fused_0 in range(T.int64(1)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(50), thread="threadIdx.x"):
                                with T.block("p0_shared"):
                                    v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                    v1 = T.axis.spatial(T.int64(512), k_0_fused * T.int64(2) + (ax0_ax1_fused_0 * T.int64(50) + ax0_ax1_fused_1))
                                    T.where(ax0_ax1_fused_0 * T.int64(50) + ax0_ax1_fused_1 < T.int64(2))
                                    T.reads(p0[v0, v1])
                                    T.writes(p0_shared[v0, v1])
                                    p0_shared[v0, v1] = p0[v0, v1]
                        for ax0_ax1_fused_0 in range(T.int64(10)):
                            for ax0_ax1_fused_1 in T.thread_binding(T.int64(50), thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("p1_shared"):
                                        v0 = T.axis.spatial(T.int64(1000), i_0_j_0_fused * T.int64(500) + (ax0_ax1_fused_0 * T.int64(100) + ax0_ax1_fused_1 * T.int64(2) + ax0_ax1_fused_2) // T.int64(2))
                                        v1 = T.axis.spatial(T.int64(512), k_0_fused * T.int64(2) + (ax0_ax1_fused_0 * T.int64(100) + ax0_ax1_fused_1 * T.int64(2) + ax0_ax1_fused_2) % T.int64(2))
                                        T.reads(p1[v0, v1])
                                        T.writes(p1_shared[v0, v1])
                                        p1_shared[v0, v1] = p1[v0, v1]
                        for k_1, i_3, j_3, k_2, i_4, j_4 in T.grid(T.int64(2), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(10)):
                            with T.block("T_matmul_NT_update"):
                                v_i = T.axis.spatial(T.int64(1), i_3 + i_4)
                                v_j = T.axis.spatial(T.int64(1000), i_0_j_0_fused * T.int64(500) + i_2_j_2_fused * T.int64(10) + j_3 * T.int64(10) + j_4)
                                v_k = T.axis.reduce(T.int64(512), k_0_fused * T.int64(2) + k_1 + k_2)
                                T.reads(T_matmul_NT_local[v_i, v_j], p0_shared[v_i, v_k], p1_shared[v_j, v_k])
                                T.writes(T_matmul_NT_local[v_i, v_j])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                T_matmul_NT_local[v_i, v_j] = T_matmul_NT_local[v_i, v_j] + p0_shared[v_i, v_k] * p1_shared[v_j, v_k]
                    for ax0, ax1 in T.grid(T.int64(1), T.int64(10)):
                        with T.block("T_matmul_NT_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(1000), i_0_j_0_fused * T.int64(500) + i_2_j_2_fused * T.int64(10) + ax1)
                            T.reads(T_matmul_NT_local[v0, v1], p2[v0, v1])
                            T.writes(T_add[v0, v1])
                            T_add[v0, v1] = T_matmul_NT_local[v0, v1] + p2[v0, v1]
b0 = sch.get_block(name="T_matmul_NT", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l3, l4, l5 = sch.get_loops(block=b0)
v6, v7, v8, v9, v10 = sch.sample_perfect_tile(loop=l3, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l11, l12, l13, l14, l15 = sch.split(loop=l3, factors=[v6, v7, v8, v9, v10], preserve_unit_iters=True)
v16, v17, v18, v19, v20 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[2, 1, 50, 1, 10])
l21, l22, l23, l24, l25 = sch.split(loop=l4, factors=[v16, v17, v18, v19, v20], preserve_unit_iters=True)
v26, v27, v28 = sch.sample_perfect_tile(loop=l5, n=3, max_innermost_factor=64, decision=[256, 2, 1])
l29, l30, l31 = sch.split(loop=l5, factors=[v26, v27, v28], preserve_unit_iters=True)
sch.reorder(l11, l21, l12, l22, l13, l23, l29, l30, l14, l24, l31, l15, l25)
l32 = sch.fuse(l11, l21, preserve_unit_iters=True)
sch.bind(loop=l32, thread_axis="blockIdx.x")
l33 = sch.fuse(l12, l22, preserve_unit_iters=True)
sch.bind(loop=l33, thread_axis="vthread.x")
l34 = sch.fuse(l13, l23, preserve_unit_iters=True)
sch.bind(loop=l34, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b35 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b35, loop=l34, preserve_unit_loops=True, index=-1)
b36 = sch.cache_read(block=b0, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b36, loop=l29, preserve_unit_loops=True, index=-1)
l37, l38, l39, l40, l41, l42 = sch.get_loops(block=b36)
l43 = sch.fuse(l41, l42, preserve_unit_iters=True)
v44 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b36, ann_key="meta_schedule.cooperative_fetch", ann_val=v44)
b45 = sch.cache_read(block=b0, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b0])
sch.compute_at(block=b45, loop=l29, preserve_unit_loops=True, index=-1)
l46, l47, l48, l49, l50, l51 = sch.get_loops(block=b45)
l52 = sch.fuse(l50, l51, preserve_unit_iters=True)
v53 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b45, ann_key="meta_schedule.cooperative_fetch", ann_val=v53)
l54 = sch.fuse(l29, preserve_unit_iters=True)
sch.annotate(block_or_loop=l54, ann_key="software_pipeline_stage", ann_val=[0, 0, 3])
sch.annotate(block_or_loop=l54, ann_key="software_pipeline_order", ann_val=[0, 1, 2])
sch.annotate(block_or_loop=l54, ann_key="software_pipeline_async_stages", ann_val=[0])
sch.reverse_compute_inline(block=b1)
v55 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=0)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v55)
sch.enter_postproc()
sch.unannotate(block_or_loop=b36, ann_key="meta_schedule.cooperative_fetch")
l56, l57, l58, l59, l60 = sch.get_loops(block=b36)
l61, l62 = sch.split(loop=l60, factors=[None, 50], preserve_unit_iters=True)
sch.bind(loop=l62, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b45, ann_key="meta_schedule.cooperative_fetch")
l63, l64, l65, l66, l67 = sch.get_loops(block=b45)
l68, l69, l70 = sch.split(loop=l67, factors=[None, 50, 2], preserve_unit_iters=True)
sch.vectorize(loop=l70)
sch.bind(loop=l69, thread_axis="threadIdx.x")
b71 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b71, ann_key="meta_schedule.unroll_explicit")
b72, b73, b74, b75 = sch.get_child_blocks(b71)
l76, l77, l78, l79, l80, l81 = sch.get_loops(block=b72)
l82, l83, l84, l85, l86, l87, l88 = sch.get_loops(block=b73)
l89, l90, l91, l92, l93, l94, l95, l96, l97, l98 = sch.get_loops(block=b74)
l99, l100, l101, l102, l103 = sch.get_loops(block=b75)
b104 = sch.get_block(name="T_matmul_NT", func_name="main")
l105, l106, l107, l108, l109, l110, l111, l112, l113, l114 = sch.get_loops(block=b104)
b115 = sch.decompose_reduction(block=b104, loop=l108)
2023-11-11 12:07:55 [INFO] [evolutionary_search.cc:713] Generating candidates......
2023-11-11 12:07:55 [INFO] [evolutionary_search.cc:715] Picked top 102 candidate(s) from database
2023-11-11 12:07:56 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310a405238)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310e76ec68)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310dc33cd8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310be916f8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56310f803f18)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310eeb7a68)]: 396 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310e771ba8)]: 0 failure(s)
2023-11-11 12:07:56 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310a405238)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310e76ec68)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310dc33cd8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310be916f8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56310f803f18)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310eeb7a68)]: 797 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310e771ba8)]: 0 failure(s)
2023-11-11 12:07:57 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310a405238)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310e76ec68)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310dc33cd8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310be916f8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56310f803f18)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310eeb7a68)]: 1197 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310e771ba8)]: 0 failure(s)
2023-11-11 12:07:58 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310a405238)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310e76ec68)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310dc33cd8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310be916f8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56310f803f18)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310eeb7a68)]: 1596 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310e771ba8)]: 0 failure(s)
2023-11-11 12:07:58 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310a405238)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310e76ec68)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310dc33cd8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310be916f8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56310f803f18)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310eeb7a68)]: 1998 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310e771ba8)]: 0 failure(s)
2023-11-11 12:07:58 [INFO] [evolutionary_search.cc:723] Sampled 52 candidate(s)
2023-11-11 12:08:01 [INFO] [evolutionary_search.cc:621] Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310a405238)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310e76ec68)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310dc33cd8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310be916f8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56310f803f18)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310eeb7a68)]: 184 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310e771ba8)]: 0 failure(s)
2023-11-11 12:08:03 [INFO] [evolutionary_search.cc:621] Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310a405238)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310e76ec68)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310dc33cd8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310be916f8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56310f803f18)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310eeb7a68)]: 157 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310e771ba8)]: 0 failure(s)
2023-11-11 12:08:06 [INFO] [evolutionary_search.cc:621] Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310a405238)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310e76ec68)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310dc33cd8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310be916f8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56310f803f18)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310eeb7a68)]: 164 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310e771ba8)]: 0 failure(s)
2023-11-11 12:08:09 [INFO] [evolutionary_search.cc:621] Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310a405238)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310e76ec68)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310dc33cd8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310be916f8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56310f803f18)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310eeb7a68)]: 175 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310e771ba8)]: 0 failure(s)
2023-11-11 12:08:10 [INFO] [evolutionary_search.cc:649] Scores of the best 42 candidates:
[1 : 16]:	0.9550  0.8828  0.8818  0.8791  0.8772  0.8660  0.8660  0.8632  0.8632  0.8621  0.8613  0.8599  0.8572  0.8383  0.8383  0.8365
[17 : 32]:	0.8247  0.7997  0.7903  0.7872  0.7718  0.7668  0.7659  0.7564  0.7561  0.7534  0.7521  0.7476  0.7473  0.7465  0.7342  0.7342
[33 : 42]:	0.7310  0.7304  0.7227  0.7192  0.7156  0.7150  0.7142  0.7142  0.7140  0.7137
2023-11-11 12:08:10 [INFO] [evolutionary_search.cc:727] Got 42 candidate(s) with evolutionary search
2023-11-11 12:08:10 [INFO] [evolutionary_search.cc:730] Sending 42 candidates(s) for measurement
2023-11-11 12:08:30 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #959: GFLOPs: 69.6256. Time: 14.7216 us. Best GFLOPs: 97.5888
2023-11-11 12:08:30 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #960: GFLOPs: 59.1824. Time: 17.3193 us. Best GFLOPs: 97.5888
2023-11-11 12:08:30 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #961: GFLOPs: 3.1710. Time: 323.2454 us. Best GFLOPs: 97.5888
2023-11-11 12:08:30 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #962: GFLOPs: 91.5808. Time: 11.1923 us. Best GFLOPs: 97.5888
2023-11-11 12:08:30 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #963: GFLOPs: 91.6154. Time: 11.1881 us. Best GFLOPs: 97.5888
2023-11-11 12:08:30 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #964: GFLOPs: 3.0887. Time: 331.8583 us. Best GFLOPs: 97.5888
2023-11-11 12:08:30 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #965: GFLOPs: 3.0888. Time: 331.8475 us. Best GFLOPs: 97.5888
2023-11-11 12:08:30 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #966: GFLOPs: 92.9717. Time: 11.0249 us. Best GFLOPs: 97.5888
2023-11-11 12:08:30 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #967: GFLOPs: 92.9484. Time: 11.0276 us. Best GFLOPs: 97.5888
2023-11-11 12:08:30 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #968: GFLOPs: 93.0116. Time: 11.0201 us. Best GFLOPs: 97.5888
2023-11-11 12:08:30 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #969: GFLOPs: 92.9440. Time: 11.0281 us. Best GFLOPs: 97.5888
2023-11-11 12:08:30 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #970: GFLOPs: 84.9389. Time: 12.0675 us. Best GFLOPs: 97.5888
2023-11-11 12:08:30 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #971: GFLOPs: 92.9906. Time: 11.0226 us. Best GFLOPs: 97.5888
2023-11-11 12:08:30 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #972: GFLOPs: 89.8674. Time: 11.4057 us. Best GFLOPs: 97.5888
2023-11-11 12:08:30 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #973: GFLOPs: 89.8500. Time: 11.4079 us. Best GFLOPs: 97.5888
2023-11-11 12:08:30 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #974: GFLOPs: 89.8616. Time: 11.4064 us. Best GFLOPs: 97.5888
2023-11-11 12:08:30 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #975: GFLOPs: 89.5253. Time: 11.4493 us. Best GFLOPs: 97.5888
2023-11-11 12:08:30 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #976: GFLOPs: 66.8446. Time: 15.3341 us. Best GFLOPs: 97.5888
2023-11-11 12:08:30 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #977: GFLOPs: 80.6428. Time: 12.7104 us. Best GFLOPs: 97.5888
2023-11-11 12:08:30 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #978: GFLOPs: 93.0727. Time: 11.0129 us. Best GFLOPs: 97.5888
2023-11-11 12:08:30 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #979: GFLOPs: 69.6232. Time: 14.7221 us. Best GFLOPs: 97.5888
2023-11-11 12:08:30 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #980: GFLOPs: 42.0813. Time: 24.3576 us. Best GFLOPs: 97.5888
2023-11-11 12:08:30 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #981: GFLOPs: 87.8307. Time: 11.6702 us. Best GFLOPs: 97.5888
2023-11-11 12:08:30 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #982: GFLOPs: 69.6184. Time: 14.7231 us. Best GFLOPs: 97.5888
2023-11-11 12:08:30 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #983: GFLOPs: 69.6226. Time: 14.7222 us. Best GFLOPs: 97.5888
2023-11-11 12:08:30 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #984: GFLOPs: 80.6732. Time: 12.7056 us. Best GFLOPs: 97.5888
2023-11-11 12:08:30 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #985: GFLOPs: 93.2488. Time: 10.9921 us. Best GFLOPs: 97.5888
2023-11-11 12:08:30 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #986: GFLOPs: 51.3222. Time: 19.9719 us. Best GFLOPs: 97.5888
2023-11-11 12:08:30 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #987: GFLOPs: 55.5806. Time: 18.4417 us. Best GFLOPs: 97.5888
2023-11-11 12:08:30 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #988: GFLOPs: 89.5465. Time: 11.4466 us. Best GFLOPs: 97.5888
2023-11-11 12:08:30 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #989: GFLOPs: 69.6236. Time: 14.7220 us. Best GFLOPs: 97.5888
2023-11-11 12:08:30 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #990: GFLOPs: 27.3549. Time: 37.4704 us. Best GFLOPs: 97.5888
2023-11-11 12:08:30 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #991: GFLOPs: 69.6241. Time: 14.7219 us. Best GFLOPs: 97.5888
2023-11-11 12:08:30 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #992: GFLOPs: 69.6232. Time: 14.7221 us. Best GFLOPs: 97.5888
2023-11-11 12:08:30 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #993: GFLOPs: 69.6030. Time: 14.7264 us. Best GFLOPs: 97.5888
2023-11-11 12:08:30 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #994: GFLOPs: 70.6720. Time: 14.5036 us. Best GFLOPs: 97.5888
2023-11-11 12:08:30 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #995: GFLOPs: 55.5834. Time: 18.4408 us. Best GFLOPs: 97.5888
2023-11-11 12:08:30 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #996: GFLOPs: 65.9515. Time: 15.5417 us. Best GFLOPs: 97.5888
2023-11-11 12:08:30 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #997: GFLOPs: 54.2464. Time: 18.8953 us. Best GFLOPs: 97.5888
2023-11-11 12:08:30 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #998: GFLOPs: 55.5803. Time: 18.4418 us. Best GFLOPs: 97.5888
2023-11-11 12:08:30 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #999: GFLOPs: 7.0364. Time: 145.6705 us. Best GFLOPs: 97.5888
2023-11-11 12:08:30 [INFO] [task_scheduler.cc:131] [Task #28: fused_nn_dense_add] Trial #1000: GFLOPs: 15.4371. Time: 66.3984 us. Best GFLOPs: 97.5888
