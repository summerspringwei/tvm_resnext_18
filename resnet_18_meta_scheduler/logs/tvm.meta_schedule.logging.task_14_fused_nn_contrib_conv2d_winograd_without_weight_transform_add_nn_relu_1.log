2023-11-10 23:34:12 [INFO] [task_scheduler.cc:160] Initializing Task #14: "fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1"
2023-11-10 23:34:12 [INFO] [task_scheduler.cc:35] 
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(128), T.int64(28), T.int64(28)), "float32"), p1: T.Buffer((T.int64(4), T.int64(4), T.int64(128), T.int64(128)), "float32"), p2: T.Buffer((T.int64(1), T.int64(128), T.int64(1), T.int64(1)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(128), T.int64(28), T.int64(28)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        data_pad = T.alloc_buffer((T.int64(1), T.int64(128), T.int64(30), T.int64(30)))
        input_tile = T.alloc_buffer((T.int64(128), T.int64(196), T.int64(4), T.int64(4)))
        B = T.alloc_buffer((T.int64(4), T.int64(4)))
        data_pack = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(128), T.int64(196)))
        bgemm = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(128), T.int64(196)))
        A = T.alloc_buffer((T.int64(4), T.int64(2)))
        inverse = T.alloc_buffer((T.int64(128), T.int64(196), T.int64(2), T.int64(2)))
        conv2d_winograd = T.alloc_buffer((T.int64(1), T.int64(128), T.int64(28), T.int64(28)))
        T_add = T.alloc_buffer((T.int64(1), T.int64(128), T.int64(28), T.int64(28)))
        for i0, i1, i2, i3 in T.grid(T.int64(1), T.int64(128), T.int64(30), T.int64(30)):
            with T.block("data_pad"):
                v_i0, v_i1, v_i2, v_i3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(p0[v_i0, v_i1, v_i2 - T.int64(1), v_i3 - T.int64(1)])
                T.writes(data_pad[v_i0, v_i1, v_i2, v_i3])
                data_pad[v_i0, v_i1, v_i2, v_i3] = T.if_then_else(T.int64(1) <= v_i2 and v_i2 < T.int64(29) and T.int64(1) <= v_i3 and v_i3 < T.int64(29), p0[v_i0, v_i1, v_i2 - T.int64(1), v_i3 - T.int64(1)], T.float32(0))
        for ci, p, eps, nu in T.grid(T.int64(128), T.int64(196), T.int64(4), T.int64(4)):
            with T.block("input_tile"):
                v_ci, v_p, v_eps, v_nu = T.axis.remap("SSSS", [ci, p, eps, nu])
                T.reads(data_pad[v_p // T.int64(196), v_ci, v_p % T.int64(196) // T.int64(14) * T.int64(2) + v_eps, v_p % T.int64(14) * T.int64(2) + v_nu])
                T.writes(input_tile[v_ci, v_p, v_eps, v_nu])
                T.block_attr({"schedule_rule": "None"})
                input_tile[v_ci, v_p, v_eps, v_nu] = data_pad[v_p // T.int64(196), v_ci, v_p % T.int64(196) // T.int64(14) * T.int64(2) + v_eps, v_p % T.int64(14) * T.int64(2) + v_nu]
        for i, j in T.grid(T.int64(4), T.int64(4)):
            with T.block("B"):
                v_i, v_j = T.axis.remap("SS", [i, j])
                T.reads()
                T.writes(B[v_i, v_j])
                T.block_attr({"schedule_rule": "None"})
                B[v_i, v_j] = T.Select(v_i % T.int64(4) == T.int64(3) and v_j % T.int64(4) == T.int64(3), T.float32(1), T.Select(v_i % T.int64(4) == T.int64(3) and v_j % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_i % T.int64(4) == T.int64(3) and v_j % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_i % T.int64(4) == T.int64(3) and v_j % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_i % T.int64(4) == T.int64(2) and v_j % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_i % T.int64(4) == T.int64(2) and v_j % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_i % T.int64(4) == T.int64(2) and v_j % T.int64(4) == T.int64(1), T.float32(1), T.Select(v_i % T.int64(4) == T.int64(2) and v_j % T.int64(4) == T.int64(0), T.float32(-1), T.Select(v_i % T.int64(4) == T.int64(1) and v_j % T.int64(4) == T.int64(3), T.float32(-1), T.Select(v_i % T.int64(4) == T.int64(1) and v_j % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_i % T.int64(4) == T.int64(1) and v_j % T.int64(4) == T.int64(1), T.float32(-1), T.Select(v_i % T.int64(4) == T.int64(1) and v_j % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_i % T.int64(4) == T.int64(0) and v_j % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_i % T.int64(4) == T.int64(0) and v_j % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_i % T.int64(4) == T.int64(0) and v_j % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_i % T.int64(4) == T.int64(0) and v_j % T.int64(4) == T.int64(0), T.float32(1), T.float32(0)))))))))))))))))
        for eps, nu, ci, p, r_a, r_b in T.grid(T.int64(4), T.int64(4), T.int64(128), T.int64(196), T.int64(4), T.int64(4)):
            with T.block("data_pack"):
                v_eps, v_nu, v_ci, v_p, v_r_a, v_r_b = T.axis.remap("SSSSRR", [eps, nu, ci, p, r_a, r_b])
                T.reads(input_tile[v_ci, v_p, v_r_a, v_r_b], B[T.min(v_r_a, v_r_b):T.min(v_r_a, v_r_b) + (T.max(v_r_a, v_r_b) + T.int64(1) - T.min(v_r_a, v_r_b)), T.min(v_eps, v_nu):T.min(v_eps, v_nu) + (T.max(v_eps, v_nu) + T.int64(1) - T.min(v_eps, v_nu))])
                T.writes(data_pack[v_eps, v_nu, v_ci, v_p])
                T.block_attr({"schedule_rule": "conv2d_nchw_winograd_data_pack"})
                with T.init():
                    data_pack[v_eps, v_nu, v_ci, v_p] = T.float32(0)
                data_pack[v_eps, v_nu, v_ci, v_p] = data_pack[v_eps, v_nu, v_ci, v_p] + input_tile[v_ci, v_p, v_r_a, v_r_b] * B[v_r_a, v_eps] * B[v_r_b, v_nu]
        for eps, nu, co, p, ci in T.grid(T.int64(4), T.int64(4), T.int64(128), T.int64(196), T.int64(128)):
            with T.block("bgemm"):
                v_eps, v_nu, v_co, v_p, v_ci = T.axis.remap("SSSSR", [eps, nu, co, p, ci])
                T.reads(data_pack[v_eps, v_nu, v_ci, v_p], p1[v_eps, v_nu, v_ci, v_co])
                T.writes(bgemm[v_eps, v_nu, v_co, v_p])
                with T.init():
                    bgemm[v_eps, v_nu, v_co, v_p] = T.float32(0)
                bgemm[v_eps, v_nu, v_co, v_p] = bgemm[v_eps, v_nu, v_co, v_p] + data_pack[v_eps, v_nu, v_ci, v_p] * p1[v_eps, v_nu, v_ci, v_co]
        for i, j in T.grid(T.int64(4), T.int64(2)):
            with T.block("A"):
                v_i, v_j = T.axis.remap("SS", [i, j])
                T.reads()
                T.writes(A[v_i, v_j])
                T.block_attr({"schedule_rule": "None"})
                A[v_i, v_j] = T.Select(v_i % T.int64(4) == T.int64(3) and v_j % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_i % T.int64(4) == T.int64(3) and v_j % T.int64(2) == T.int64(0), T.float32(0), T.Select(v_i % T.int64(4) == T.int64(2) and v_j % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_i % T.int64(4) == T.int64(2) and v_j % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_i % T.int64(4) == T.int64(1) and v_j % T.int64(2) == T.int64(1), T.float32(-1), T.Select(v_i % T.int64(4) == T.int64(1) and v_j % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_i % T.int64(4) == T.int64(0) and v_j % T.int64(2) == T.int64(1), T.float32(0), T.Select(v_i % T.int64(4) == T.int64(0) and v_j % T.int64(2) == T.int64(0), T.float32(1), T.float32(0)))))))))
        for co, p, vh, vw, r_a, r_b in T.grid(T.int64(128), T.int64(196), T.int64(2), T.int64(2), T.int64(4), T.int64(4)):
            with T.block("inverse"):
                v_co, v_p, v_vh, v_vw, v_r_a, v_r_b = T.axis.remap("SSSSRR", [co, p, vh, vw, r_a, r_b])
                T.reads(bgemm[v_r_a, v_r_b, v_co, v_p], A[T.min(v_r_a, v_r_b):T.min(v_r_a, v_r_b) + (T.max(v_r_a, v_r_b) + T.int64(1) - T.min(v_r_a, v_r_b)), T.min(v_vh, v_vw):T.min(v_vh, v_vw) + (T.max(v_vh, v_vw) + T.int64(1) - T.min(v_vh, v_vw))])
                T.writes(inverse[v_co, v_p, v_vh, v_vw])
                T.block_attr({"schedule_rule": "conv2d_nchw_winograd_inverse"})
                with T.init():
                    inverse[v_co, v_p, v_vh, v_vw] = T.float32(0)
                inverse[v_co, v_p, v_vh, v_vw] = inverse[v_co, v_p, v_vh, v_vw] + bgemm[v_r_a, v_r_b, v_co, v_p] * A[v_r_a, v_vh] * A[v_r_b, v_vw]
        for n, co, h, w in T.grid(T.int64(1), T.int64(128), T.int64(28), T.int64(28)):
            with T.block("conv2d_winograd"):
                v_n, v_co, v_h, v_w = T.axis.remap("SSSS", [n, co, h, w])
                T.reads(inverse[v_co, v_n * T.int64(196) + v_h // T.int64(2) * T.int64(14) + v_w // T.int64(2), v_h % T.int64(2), v_w % T.int64(2)])
                T.writes(conv2d_winograd[v_n, v_co, v_h, v_w])
                conv2d_winograd[v_n, v_co, v_h, v_w] = inverse[v_co, v_n * T.int64(196) + v_h // T.int64(2) * T.int64(14) + v_w // T.int64(2), v_h % T.int64(2), v_w % T.int64(2)]
        for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(128), T.int64(28), T.int64(28)):
            with T.block("T_add"):
                v_ax0, v_ax1, v_ax2, v_ax3 = T.axis.remap("SSSS", [ax0, ax1, ax2, ax3])
                T.reads(conv2d_winograd[v_ax0, v_ax1, v_ax2, v_ax3], p2[v_ax0, v_ax1, T.int64(0), T.int64(0)])
                T.writes(T_add[v_ax0, v_ax1, v_ax2, v_ax3])
                T_add[v_ax0, v_ax1, v_ax2, v_ax3] = conv2d_winograd[v_ax0, v_ax1, v_ax2, v_ax3] + p2[v_ax0, v_ax1, T.int64(0), T.int64(0)]
        for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(128), T.int64(28), T.int64(28)):
            with T.block("T_relu"):
                v_ax0, v_ax1, v_ax2, v_ax3 = T.axis.remap("SSSS", [ax0, ax1, ax2, ax3])
                T.reads(T_add[v_ax0, v_ax1, v_ax2, v_ax3])
                T.writes(T_relu[v_ax0, v_ax1, v_ax2, v_ax3])
                T_relu[v_ax0, v_ax1, v_ax2, v_ax3] = T.max(T_add[v_ax0, v_ax1, v_ax2, v_ax3], T.float32(0))
2023-11-10 23:34:12 [INFO] [task_scheduler.cc:164] Total 3 design space(s) generated
2023-11-10 23:34:12 [INFO] [task_scheduler.cc:170] Design space #0:
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(128), T.int64(28), T.int64(28)), "float32"), p1: T.Buffer((T.int64(4), T.int64(4), T.int64(128), T.int64(128)), "float32"), p2: T.Buffer((T.int64(1), T.int64(128), T.int64(1), T.int64(1)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(128), T.int64(28), T.int64(28)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit": 512})
            input_tile_local = T.alloc_buffer((T.int64(128), T.int64(196), T.int64(4), T.int64(4)), scope="local")
            data_pack = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(128), T.int64(196)))
            bgemm = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(128), T.int64(196)))
            inverse_local = T.alloc_buffer((T.int64(128), T.int64(196), T.int64(2), T.int64(2)), scope="local")
            data_pack_local = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(128), T.int64(196)), scope="local")
            bgemm_local = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(128), T.int64(196)), scope="local")
            data_pack_shared = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(128), T.int64(196)), scope="shared")
            p1_shared = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(128), T.int64(128)), scope="shared")
            for ci_p_fused_0 in T.thread_binding(T.int64(784), thread="blockIdx.x"):
                for ci_p_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(1), T.int64(4), T.int64(4)):
                        with T.block("input_tile"):
                            v_ci = T.axis.spatial(T.int64(128), (ci_p_fused_0 * T.int64(32) + ci_p_fused_1) // T.int64(196) + ax0)
                            v_p = T.axis.spatial(T.int64(196), (ci_p_fused_0 * T.int64(32) + ci_p_fused_1) % T.int64(196) + ax1)
                            v_eps, v_nu = T.axis.remap("SS", [ax2, ax3])
                            T.reads(p0[v_p // T.int64(196), v_ci, v_p % T.int64(196) // T.int64(14) * T.int64(2) + v_eps - T.int64(1), v_p % T.int64(14) * T.int64(2) + v_nu - T.int64(1)])
                            T.writes(input_tile_local[v_ci, v_p, v_eps, v_nu])
                            T.block_attr({"schedule_rule": "None"})
                            input_tile_local[v_ci, v_p, v_eps, v_nu] = T.if_then_else(T.int64(1) <= v_p % T.int64(196) // T.int64(14) * T.int64(2) + v_eps and v_p % T.int64(196) // T.int64(14) * T.int64(2) + v_eps < T.int64(29) and T.int64(1) <= v_p % T.int64(14) * T.int64(2) + v_nu and v_p % T.int64(14) * T.int64(2) + v_nu < T.int64(29), p0[v_p // T.int64(196), v_ci, v_p % T.int64(196) // T.int64(14) * T.int64(2) + v_eps - T.int64(1), v_p % T.int64(14) * T.int64(2) + v_nu - T.int64(1)], T.float32(0))
                    for eps in T.unroll(T.int64(4)):
                        for nu in T.unroll(T.int64(4)):
                            for r_a in T.unroll(T.int64(4)):
                                for r_b in T.unroll(T.int64(4)):
                                    with T.block("data_pack"):
                                        v_eps, v_nu = T.axis.remap("SS", [eps, nu])
                                        v_ci = T.axis.spatial(T.int64(128), (ci_p_fused_0 * T.int64(32) + ci_p_fused_1) // T.int64(196))
                                        v_p = T.axis.spatial(T.int64(196), (ci_p_fused_0 * T.int64(32) + ci_p_fused_1) % T.int64(196))
                                        v_r_a, v_r_b = T.axis.remap("RR", [r_a, r_b])
                                        T.reads(input_tile_local[v_ci, v_p, v_r_a, v_r_b])
                                        T.writes(data_pack_local[v_eps, v_nu, v_ci, v_p])
                                        T.block_attr({"schedule_rule": "conv2d_nchw_winograd_data_pack"})
                                        with T.init():
                                            data_pack_local[v_eps, v_nu, v_ci, v_p] = T.float32(0)
                                        data_pack_local[v_eps, v_nu, v_ci, v_p] = data_pack_local[v_eps, v_nu, v_ci, v_p] + input_tile_local[v_ci, v_p, v_r_a, v_r_b] * T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(3), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(1), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(0), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(3), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(1), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(0), T.float32(1), T.float32(0))))))))))))))))) * T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(3), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(1), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(0), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(3), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(1), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(0), T.float32(1), T.float32(0)))))))))))))))))
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(4), T.int64(4), T.int64(1), T.int64(1)):
                        with T.block("data_pack_local"):
                            v0, v1 = T.axis.remap("SS", [ax0, ax1])
                            v2 = T.axis.spatial(T.int64(128), (ci_p_fused_0 * T.int64(32) + ci_p_fused_1) // T.int64(196) + ax2)
                            v3 = T.axis.spatial(T.int64(196), (ci_p_fused_0 * T.int64(32) + ci_p_fused_1) % T.int64(196) + ax3)
                            T.reads(data_pack_local[v0, v1, v2, v3])
                            T.writes(data_pack[v0, v1, v2, v3])
                            data_pack[v0, v1, v2, v3] = data_pack_local[v0, v1, v2, v3]
            for eps_0_nu_0_co_0_p_0_fused in T.thread_binding(T.int64(4), thread="blockIdx.x"):
                for eps_1_nu_1_co_1_p_1_fused in T.thread_binding(T.int64(56), thread="vthread.x"):
                    for eps_2_nu_2_co_2_p_2_fused in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                        for ci_0 in range(T.int64(32)):
                            for ax0_ax1_ax2_ax3_fused in range(T.int64(3136)):
                                with T.block("data_pack_shared"):
                                    v0 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(2) * T.int64(2) + ax0_ax1_ax2_ax3_fused // T.int64(1568))
                                    v1 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused % T.int64(2) * T.int64(2) + ax0_ax1_ax2_ax3_fused % T.int64(1568) // T.int64(784))
                                    v2 = T.axis.spatial(T.int64(128), ci_0 * T.int64(4) + ax0_ax1_ax2_ax3_fused % T.int64(784) // T.int64(196))
                                    v3 = T.axis.spatial(T.int64(196), ax0_ax1_ax2_ax3_fused % T.int64(196))
                                    T.reads(data_pack[v0, v1, v2, v3])
                                    T.writes(data_pack_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch": 4})
                                    data_pack_shared[v0, v1, v2, v3] = data_pack[v0, v1, v2, v3]
                            for ax0_ax1_ax2_ax3_fused in range(T.int64(2048)):
                                with T.block("p1_shared"):
                                    v0 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(2) * T.int64(2) + ax0_ax1_ax2_ax3_fused // T.int64(1024))
                                    v1 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused % T.int64(2) * T.int64(2) + ax0_ax1_ax2_ax3_fused % T.int64(1024) // T.int64(512))
                                    v2 = T.axis.spatial(T.int64(128), ci_0 * T.int64(4) + ax0_ax1_ax2_ax3_fused % T.int64(512) // T.int64(128))
                                    v3 = T.axis.spatial(T.int64(128), ax0_ax1_ax2_ax3_fused % T.int64(128))
                                    T.reads(p1[v0, v1, v2, v3])
                                    T.writes(p1_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch": 4})
                                    p1_shared[v0, v1, v2, v3] = p1[v0, v1, v2, v3]
                            for ci_1, eps_3, nu_3, co_3, p_3, ci_2, eps_4, nu_4, co_4, p_4 in T.grid(T.int64(2), T.int64(2), T.int64(1), T.int64(1), T.int64(14), T.int64(2), T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                                with T.block("bgemm"):
                                    v_eps = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(2) * T.int64(2) + eps_3 + eps_4)
                                    v_nu = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused % T.int64(2) * T.int64(2) + eps_2_nu_2_co_2_p_2_fused // T.int64(32) + nu_3 + nu_4)
                                    v_co = T.axis.spatial(T.int64(128), eps_1_nu_1_co_1_p_1_fused // T.int64(7) * T.int64(16) + eps_2_nu_2_co_2_p_2_fused % T.int64(32) // T.int64(2) + co_3 + co_4)
                                    v_p = T.axis.spatial(T.int64(196), eps_1_nu_1_co_1_p_1_fused % T.int64(7) * T.int64(28) + eps_2_nu_2_co_2_p_2_fused % T.int64(2) * T.int64(14) + p_3 + p_4)
                                    v_ci = T.axis.reduce(T.int64(128), ci_0 * T.int64(4) + ci_1 * T.int64(2) + ci_2)
                                    T.reads(data_pack_shared[v_eps, v_nu, v_ci, v_p], p1_shared[v_eps, v_nu, v_ci, v_co])
                                    T.writes(bgemm_local[v_eps, v_nu, v_co, v_p])
                                    T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                    with T.init():
                                        bgemm_local[v_eps, v_nu, v_co, v_p] = T.float32(0)
                                    bgemm_local[v_eps, v_nu, v_co, v_p] = bgemm_local[v_eps, v_nu, v_co, v_p] + data_pack_shared[v_eps, v_nu, v_ci, v_p] * p1_shared[v_eps, v_nu, v_ci, v_co]
                        for ax0, ax1, ax2, ax3 in T.grid(T.int64(2), T.int64(1), T.int64(1), T.int64(14)):
                            with T.block("bgemm_local"):
                                v0 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(2) * T.int64(2) + ax0)
                                v1 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused % T.int64(2) * T.int64(2) + eps_2_nu_2_co_2_p_2_fused // T.int64(32) + ax1)
                                v2 = T.axis.spatial(T.int64(128), eps_1_nu_1_co_1_p_1_fused // T.int64(7) * T.int64(16) + eps_2_nu_2_co_2_p_2_fused % T.int64(32) // T.int64(2) + ax2)
                                v3 = T.axis.spatial(T.int64(196), eps_1_nu_1_co_1_p_1_fused % T.int64(7) * T.int64(28) + eps_2_nu_2_co_2_p_2_fused % T.int64(2) * T.int64(14) + ax3)
                                T.reads(bgemm_local[v0, v1, v2, v3])
                                T.writes(bgemm[v0, v1, v2, v3])
                                bgemm[v0, v1, v2, v3] = bgemm_local[v0, v1, v2, v3]
            for n_co_h_0_w_0_fused_0 in T.thread_binding(T.int64(392), thread="blockIdx.x"):
                for n_co_h_0_w_0_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                    for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                        for ax2 in T.unroll(T.int64(2)):
                            for ax3 in T.unroll(T.int64(2)):
                                for ax4 in T.unroll(T.int64(4)):
                                    for ax5 in T.unroll(T.int64(4)):
                                        with T.block("inverse"):
                                            v_co = T.axis.spatial(T.int64(128), (n_co_h_0_w_0_fused_0 * T.int64(64) + n_co_h_0_w_0_fused_1) // T.int64(196) + ax0)
                                            v_p = T.axis.spatial(T.int64(196), (n_co_h_0_w_0_fused_0 * T.int64(64) + n_co_h_0_w_0_fused_1) % T.int64(196) + ax1)
                                            v_vh, v_vw, v_r_a, v_r_b = T.axis.remap("SSRR", [ax2, ax3, ax4, ax5])
                                            T.reads(bgemm[v_r_a, v_r_b, v_co, v_p])
                                            T.writes(inverse_local[v_co, v_p, v_vh, v_vw])
                                            T.block_attr({"schedule_rule": "conv2d_nchw_winograd_inverse"})
                                            with T.init():
                                                inverse_local[v_co, v_p, v_vh, v_vw] = T.float32(0)
                                            inverse_local[v_co, v_p, v_vh, v_vw] = inverse_local[v_co, v_p, v_vh, v_vw] + bgemm[v_r_a, v_r_b, v_co, v_p] * T.Select(v_r_a % T.int64(4) == T.int64(3) and v_vh % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_vh % T.int64(2) == T.int64(0), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_vh % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_vh % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_vh % T.int64(2) == T.int64(1), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_vh % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_vh % T.int64(2) == T.int64(1), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_vh % T.int64(2) == T.int64(0), T.float32(1), T.float32(0))))))))) * T.Select(v_r_b % T.int64(4) == T.int64(3) and v_vw % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_vw % T.int64(2) == T.int64(0), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_vw % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_vw % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_vw % T.int64(2) == T.int64(1), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_vw % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_vw % T.int64(2) == T.int64(1), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_vw % T.int64(2) == T.int64(0), T.float32(1), T.float32(0)))))))))
                    for h_1, w_1 in T.grid(T.int64(2), T.int64(2)):
                        with T.block("conv2d_winograd"):
                            v_n = T.axis.spatial(T.int64(1), T.int64(0))
                            v_co = T.axis.spatial(T.int64(128), (n_co_h_0_w_0_fused_0 * T.int64(64) + n_co_h_0_w_0_fused_1) // T.int64(196))
                            v_h = T.axis.spatial(T.int64(28), (n_co_h_0_w_0_fused_0 * T.int64(64) + n_co_h_0_w_0_fused_1) % T.int64(196) // T.int64(14) * T.int64(2) + h_1)
                            v_w = T.axis.spatial(T.int64(28), (n_co_h_0_w_0_fused_0 * T.int64(64) + n_co_h_0_w_0_fused_1) % T.int64(14) * T.int64(2) + w_1)
                            T.reads(inverse_local[v_co, v_n * T.int64(196) + v_h // T.int64(2) * T.int64(14) + v_w // T.int64(2), v_h % T.int64(2), v_w % T.int64(2)], p2[v_n, v_co, T.int64(0), T.int64(0)])
                            T.writes(T_relu[v_n, v_co, v_h, v_w])
                            T_relu[v_n, v_co, v_h, v_w] = T.max(inverse_local[v_co, v_n * T.int64(196) + v_h // T.int64(2) * T.int64(14) + v_w // T.int64(2), v_h % T.int64(2), v_w % T.int64(2)] + p2[v_n, v_co, T.int64(0), T.int64(0)], T.float32(0))
b0 = sch.get_block(name="data_pack", func_name="main")
b1 = sch.get_block(name="bgemm", func_name="main")
b2 = sch.get_block(name="inverse", func_name="main")
b3 = sch.get_block(name="conv2d_winograd", func_name="main")
b4 = sch.get_block(name="T_add", func_name="main")
b5 = sch.get_block(name="T_relu", func_name="main")
b6 = sch.get_block(name="root", func_name="main")
b7, b8 = sch.get_producers(block=b2)
sch.compute_inline(block=b8)
b9, = sch.get_consumers(block=b2)
l10, l11, l12, l13 = sch.get_loops(block=b9)
l14, l15 = sch.split(loop=l12, factors=[None, 2], preserve_unit_iters=True)
l16, l17 = sch.split(loop=l13, factors=[None, 2], preserve_unit_iters=True)
sch.reorder(l14, l16, l15, l17)
sch.compute_at(block=b2, loop=l16, preserve_unit_loops=True, index=-1)
sch.set_scope(block=b2, buffer_index=0, storage_scope="local")
l18, l19, l20, l21, l22, l23, l24, l25, l26, l27 = sch.get_loops(block=b2)
sch.unroll(loop=l24)
sch.unroll(loop=l25)
sch.unroll(loop=l26)
sch.unroll(loop=l27)
b28, b29 = sch.get_producers(block=b0)
sch.compute_inline(block=b29)
b30, = sch.get_producers(block=b28)
l31, l32, l33, l34, l35, l36 = sch.get_loops(block=b0)
sch.reorder(l33, l34, l31, l32, l35, l36)
sch.unroll(loop=l31)
sch.unroll(loop=l32)
sch.unroll(loop=l35)
sch.unroll(loop=l36)
l37 = sch.fuse(l33, l34, preserve_unit_iters=True)
v38 = sch.sample_categorical(candidates=[32, 64, 128, 256, 512, 1024], probs=[0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666], decision=0)
l39, l40 = sch.split(loop=l37, factors=[None, v38], preserve_unit_iters=True)
sch.bind(loop=l39, thread_axis="blockIdx.x")
sch.bind(loop=l40, thread_axis="threadIdx.x")
b41 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b41, loop=l40, preserve_unit_loops=True, index=-1)
sch.compute_at(block=b28, loop=l40, preserve_unit_loops=True, index=-1)
sch.set_scope(block=b28, buffer_index=0, storage_scope="local")
sch.compute_inline(block=b30)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l42, l43, l44, l45, l46 = sch.get_loops(block=b1)
v47, v48, v49, v50, v51 = sch.sample_perfect_tile(loop=l42, n=5, max_innermost_factor=64, decision=[2, 1, 1, 2, 1])
l52, l53, l54, l55, l56 = sch.split(loop=l42, factors=[v47, v48, v49, v50, v51], preserve_unit_iters=True)
v57, v58, v59, v60, v61 = sch.sample_perfect_tile(loop=l43, n=5, max_innermost_factor=64, decision=[2, 1, 2, 1, 1])
l62, l63, l64, l65, l66 = sch.split(loop=l43, factors=[v57, v58, v59, v60, v61], preserve_unit_iters=True)
v67, v68, v69, v70, v71 = sch.sample_perfect_tile(loop=l44, n=5, max_innermost_factor=64, decision=[1, 8, 16, 1, 1])
l72, l73, l74, l75, l76 = sch.split(loop=l44, factors=[v67, v68, v69, v70, v71], preserve_unit_iters=True)
v77, v78, v79, v80, v81 = sch.sample_perfect_tile(loop=l45, n=5, max_innermost_factor=64, decision=[1, 7, 2, 14, 1])
l82, l83, l84, l85, l86 = sch.split(loop=l45, factors=[v77, v78, v79, v80, v81], preserve_unit_iters=True)
v87, v88, v89 = sch.sample_perfect_tile(loop=l46, n=3, max_innermost_factor=64, decision=[32, 2, 2])
l90, l91, l92 = sch.split(loop=l46, factors=[v87, v88, v89], preserve_unit_iters=True)
sch.reorder(l52, l62, l72, l82, l53, l63, l73, l83, l54, l64, l74, l84, l90, l91, l55, l65, l75, l85, l92, l56, l66, l76, l86)
l93 = sch.fuse(l52, l62, l72, l82, preserve_unit_iters=True)
sch.bind(loop=l93, thread_axis="blockIdx.x")
l94 = sch.fuse(l53, l63, l73, l83, preserve_unit_iters=True)
sch.bind(loop=l94, thread_axis="vthread.x")
l95 = sch.fuse(l54, l64, l74, l84, preserve_unit_iters=True)
sch.bind(loop=l95, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b96 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b96, loop=l95, preserve_unit_loops=True, index=-1)
b97 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b97, loop=l90, preserve_unit_loops=True, index=-1)
l98, l99, l100, l101, l102, l103, l104, l105 = sch.get_loops(block=b97)
l106 = sch.fuse(l102, l103, l104, l105, preserve_unit_iters=True)
v107 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b97, ann_key="meta_schedule.cooperative_fetch", ann_val=v107)
b108 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b108, loop=l90, preserve_unit_loops=True, index=-1)
l109, l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b108)
l117 = sch.fuse(l113, l114, l115, l116, preserve_unit_iters=True)
v118 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b108, ann_key="meta_schedule.cooperative_fetch", ann_val=v118)
sch.reverse_compute_inline(block=b5)
sch.reverse_compute_inline(block=b4)
v119 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b6, ann_key="meta_schedule.unroll_explicit", ann_val=v119)
l120, l121, l122, l123, l124, l125 = sch.get_loops(block=b3)
l126 = sch.fuse(l120, l121, l122, l123, preserve_unit_iters=True)
v127 = sch.sample_categorical(candidates=[32, 64, 128, 256, 512, 1024], probs=[0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666], decision=1)
l128, l129 = sch.split(loop=l126, factors=[None, v127], preserve_unit_iters=True)
sch.bind(loop=l128, thread_axis="blockIdx.x")
sch.bind(loop=l129, thread_axis="threadIdx.x")
2023-11-10 23:34:12 [INFO] [task_scheduler.cc:170] Design space #1:
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(128), T.int64(28), T.int64(28)), "float32"), p1: T.Buffer((T.int64(4), T.int64(4), T.int64(128), T.int64(128)), "float32"), p2: T.Buffer((T.int64(1), T.int64(128), T.int64(1), T.int64(1)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(128), T.int64(28), T.int64(28)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit": 16})
            input_tile_local = T.alloc_buffer((T.int64(128), T.int64(196), T.int64(4), T.int64(4)), scope="local")
            data_pack = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(128), T.int64(196)))
            bgemm = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(128), T.int64(196)))
            inverse_local = T.alloc_buffer((T.int64(128), T.int64(196), T.int64(2), T.int64(2)), scope="local")
            data_pack_local = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(128), T.int64(196)), scope="local")
            bgemm_local = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(128), T.int64(196)), scope="local")
            data_pack_shared = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(128), T.int64(196)), scope="shared")
            p1_shared = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(128), T.int64(128)), scope="shared")
            for ci_p_fused_0 in T.thread_binding(T.int64(784), thread="blockIdx.x"):
                for ci_p_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(1), T.int64(4), T.int64(4)):
                        with T.block("input_tile"):
                            v_ci = T.axis.spatial(T.int64(128), (ci_p_fused_0 * T.int64(32) + ci_p_fused_1) // T.int64(196) + ax0)
                            v_p = T.axis.spatial(T.int64(196), (ci_p_fused_0 * T.int64(32) + ci_p_fused_1) % T.int64(196) + ax1)
                            v_eps, v_nu = T.axis.remap("SS", [ax2, ax3])
                            T.reads(p0[v_p // T.int64(196), v_ci, v_p % T.int64(196) // T.int64(14) * T.int64(2) + v_eps - T.int64(1), v_p % T.int64(14) * T.int64(2) + v_nu - T.int64(1)])
                            T.writes(input_tile_local[v_ci, v_p, v_eps, v_nu])
                            T.block_attr({"schedule_rule": "None"})
                            input_tile_local[v_ci, v_p, v_eps, v_nu] = T.if_then_else(T.int64(1) <= v_p % T.int64(196) // T.int64(14) * T.int64(2) + v_eps and v_p % T.int64(196) // T.int64(14) * T.int64(2) + v_eps < T.int64(29) and T.int64(1) <= v_p % T.int64(14) * T.int64(2) + v_nu and v_p % T.int64(14) * T.int64(2) + v_nu < T.int64(29), p0[v_p // T.int64(196), v_ci, v_p % T.int64(196) // T.int64(14) * T.int64(2) + v_eps - T.int64(1), v_p % T.int64(14) * T.int64(2) + v_nu - T.int64(1)], T.float32(0))
                    for eps in T.unroll(T.int64(4)):
                        for nu in T.unroll(T.int64(4)):
                            for r_a in T.unroll(T.int64(4)):
                                for r_b in T.unroll(T.int64(4)):
                                    with T.block("data_pack"):
                                        v_eps, v_nu = T.axis.remap("SS", [eps, nu])
                                        v_ci = T.axis.spatial(T.int64(128), (ci_p_fused_0 * T.int64(32) + ci_p_fused_1) // T.int64(196))
                                        v_p = T.axis.spatial(T.int64(196), (ci_p_fused_0 * T.int64(32) + ci_p_fused_1) % T.int64(196))
                                        v_r_a, v_r_b = T.axis.remap("RR", [r_a, r_b])
                                        T.reads(input_tile_local[v_ci, v_p, v_r_a, v_r_b])
                                        T.writes(data_pack_local[v_eps, v_nu, v_ci, v_p])
                                        T.block_attr({"schedule_rule": "conv2d_nchw_winograd_data_pack"})
                                        with T.init():
                                            data_pack_local[v_eps, v_nu, v_ci, v_p] = T.float32(0)
                                        data_pack_local[v_eps, v_nu, v_ci, v_p] = data_pack_local[v_eps, v_nu, v_ci, v_p] + input_tile_local[v_ci, v_p, v_r_a, v_r_b] * T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(3), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(1), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(0), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(3), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(1), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(0), T.float32(1), T.float32(0))))))))))))))))) * T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(3), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(1), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(0), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(3), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(1), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(0), T.float32(1), T.float32(0)))))))))))))))))
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(4), T.int64(4), T.int64(1), T.int64(1)):
                        with T.block("data_pack_local"):
                            v0, v1 = T.axis.remap("SS", [ax0, ax1])
                            v2 = T.axis.spatial(T.int64(128), (ci_p_fused_0 * T.int64(32) + ci_p_fused_1) // T.int64(196) + ax2)
                            v3 = T.axis.spatial(T.int64(196), (ci_p_fused_0 * T.int64(32) + ci_p_fused_1) % T.int64(196) + ax3)
                            T.reads(data_pack_local[v0, v1, v2, v3])
                            T.writes(data_pack[v0, v1, v2, v3])
                            data_pack[v0, v1, v2, v3] = data_pack_local[v0, v1, v2, v3]
            for eps_0_nu_0_co_0_p_0_fused in T.thread_binding(T.int64(4), thread="blockIdx.x"):
                for eps_1_nu_1_co_1_p_1_fused in T.thread_binding(T.int64(56), thread="vthread.x"):
                    for eps_2_nu_2_co_2_p_2_fused in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                        for ci_0_fused in T.serial(T.int64(32), annotations={"software_pipeline_async_stages": [0], "software_pipeline_order": [0, 1, 2], "software_pipeline_stage": [0, 0, 2]}):
                            for ax0_ax1_ax2_ax3_fused in range(T.int64(3136)):
                                with T.block("data_pack_shared"):
                                    v0 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(2) * T.int64(2) + ax0_ax1_ax2_ax3_fused // T.int64(1568))
                                    v1 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused % T.int64(2) * T.int64(2) + ax0_ax1_ax2_ax3_fused % T.int64(1568) // T.int64(784))
                                    v2 = T.axis.spatial(T.int64(128), ci_0_fused * T.int64(4) + ax0_ax1_ax2_ax3_fused % T.int64(784) // T.int64(196))
                                    v3 = T.axis.spatial(T.int64(196), ax0_ax1_ax2_ax3_fused % T.int64(196))
                                    T.reads(data_pack[v0, v1, v2, v3])
                                    T.writes(data_pack_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch": 4})
                                    data_pack_shared[v0, v1, v2, v3] = data_pack[v0, v1, v2, v3]
                            for ax0_ax1_ax2_ax3_fused in range(T.int64(2048)):
                                with T.block("p1_shared"):
                                    v0 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(2) * T.int64(2) + ax0_ax1_ax2_ax3_fused // T.int64(1024))
                                    v1 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused % T.int64(2) * T.int64(2) + ax0_ax1_ax2_ax3_fused % T.int64(1024) // T.int64(512))
                                    v2 = T.axis.spatial(T.int64(128), ci_0_fused * T.int64(4) + ax0_ax1_ax2_ax3_fused % T.int64(512) // T.int64(128))
                                    v3 = T.axis.spatial(T.int64(128), ax0_ax1_ax2_ax3_fused % T.int64(128))
                                    T.reads(p1[v0, v1, v2, v3])
                                    T.writes(p1_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch": 4})
                                    p1_shared[v0, v1, v2, v3] = p1[v0, v1, v2, v3]
                            for ci_1, eps_3, nu_3, co_3, p_3, ci_2, eps_4, nu_4, co_4, p_4 in T.grid(T.int64(2), T.int64(2), T.int64(1), T.int64(1), T.int64(14), T.int64(2), T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                                with T.block("bgemm"):
                                    v_eps = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(2) * T.int64(2) + eps_3 + eps_4)
                                    v_nu = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused % T.int64(2) * T.int64(2) + eps_2_nu_2_co_2_p_2_fused // T.int64(32) + nu_3 + nu_4)
                                    v_co = T.axis.spatial(T.int64(128), eps_1_nu_1_co_1_p_1_fused // T.int64(7) * T.int64(16) + eps_2_nu_2_co_2_p_2_fused % T.int64(32) // T.int64(2) + co_3 + co_4)
                                    v_p = T.axis.spatial(T.int64(196), eps_1_nu_1_co_1_p_1_fused % T.int64(7) * T.int64(28) + eps_2_nu_2_co_2_p_2_fused % T.int64(2) * T.int64(14) + p_3 + p_4)
                                    v_ci = T.axis.reduce(T.int64(128), ci_0_fused * T.int64(4) + ci_1 * T.int64(2) + ci_2)
                                    T.reads(data_pack_shared[v_eps, v_nu, v_ci, v_p], p1_shared[v_eps, v_nu, v_ci, v_co])
                                    T.writes(bgemm_local[v_eps, v_nu, v_co, v_p])
                                    T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                    with T.init():
                                        bgemm_local[v_eps, v_nu, v_co, v_p] = T.float32(0)
                                    bgemm_local[v_eps, v_nu, v_co, v_p] = bgemm_local[v_eps, v_nu, v_co, v_p] + data_pack_shared[v_eps, v_nu, v_ci, v_p] * p1_shared[v_eps, v_nu, v_ci, v_co]
                        for ax0, ax1, ax2, ax3 in T.grid(T.int64(2), T.int64(1), T.int64(1), T.int64(14)):
                            with T.block("bgemm_local"):
                                v0 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(2) * T.int64(2) + ax0)
                                v1 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused % T.int64(2) * T.int64(2) + eps_2_nu_2_co_2_p_2_fused // T.int64(32) + ax1)
                                v2 = T.axis.spatial(T.int64(128), eps_1_nu_1_co_1_p_1_fused // T.int64(7) * T.int64(16) + eps_2_nu_2_co_2_p_2_fused % T.int64(32) // T.int64(2) + ax2)
                                v3 = T.axis.spatial(T.int64(196), eps_1_nu_1_co_1_p_1_fused % T.int64(7) * T.int64(28) + eps_2_nu_2_co_2_p_2_fused % T.int64(2) * T.int64(14) + ax3)
                                T.reads(bgemm_local[v0, v1, v2, v3])
                                T.writes(bgemm[v0, v1, v2, v3])
                                bgemm[v0, v1, v2, v3] = bgemm_local[v0, v1, v2, v3]
            for n_co_h_0_w_0_fused_0 in T.thread_binding(T.int64(784), thread="blockIdx.x"):
                for n_co_h_0_w_0_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                    for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                        for ax2 in T.unroll(T.int64(2)):
                            for ax3 in T.unroll(T.int64(2)):
                                for ax4 in T.unroll(T.int64(4)):
                                    for ax5 in T.unroll(T.int64(4)):
                                        with T.block("inverse"):
                                            v_co = T.axis.spatial(T.int64(128), (n_co_h_0_w_0_fused_0 * T.int64(32) + n_co_h_0_w_0_fused_1) // T.int64(196) + ax0)
                                            v_p = T.axis.spatial(T.int64(196), (n_co_h_0_w_0_fused_0 * T.int64(32) + n_co_h_0_w_0_fused_1) % T.int64(196) + ax1)
                                            v_vh, v_vw, v_r_a, v_r_b = T.axis.remap("SSRR", [ax2, ax3, ax4, ax5])
                                            T.reads(bgemm[v_r_a, v_r_b, v_co, v_p])
                                            T.writes(inverse_local[v_co, v_p, v_vh, v_vw])
                                            T.block_attr({"schedule_rule": "conv2d_nchw_winograd_inverse"})
                                            with T.init():
                                                inverse_local[v_co, v_p, v_vh, v_vw] = T.float32(0)
                                            inverse_local[v_co, v_p, v_vh, v_vw] = inverse_local[v_co, v_p, v_vh, v_vw] + bgemm[v_r_a, v_r_b, v_co, v_p] * T.Select(v_r_a % T.int64(4) == T.int64(3) and v_vh % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_vh % T.int64(2) == T.int64(0), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_vh % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_vh % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_vh % T.int64(2) == T.int64(1), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_vh % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_vh % T.int64(2) == T.int64(1), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_vh % T.int64(2) == T.int64(0), T.float32(1), T.float32(0))))))))) * T.Select(v_r_b % T.int64(4) == T.int64(3) and v_vw % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_vw % T.int64(2) == T.int64(0), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_vw % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_vw % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_vw % T.int64(2) == T.int64(1), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_vw % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_vw % T.int64(2) == T.int64(1), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_vw % T.int64(2) == T.int64(0), T.float32(1), T.float32(0)))))))))
                    for h_1, w_1 in T.grid(T.int64(2), T.int64(2)):
                        with T.block("conv2d_winograd"):
                            v_n = T.axis.spatial(T.int64(1), T.int64(0))
                            v_co = T.axis.spatial(T.int64(128), (n_co_h_0_w_0_fused_0 * T.int64(32) + n_co_h_0_w_0_fused_1) // T.int64(196))
                            v_h = T.axis.spatial(T.int64(28), (n_co_h_0_w_0_fused_0 * T.int64(32) + n_co_h_0_w_0_fused_1) % T.int64(196) // T.int64(14) * T.int64(2) + h_1)
                            v_w = T.axis.spatial(T.int64(28), (n_co_h_0_w_0_fused_0 * T.int64(32) + n_co_h_0_w_0_fused_1) % T.int64(14) * T.int64(2) + w_1)
                            T.reads(inverse_local[v_co, v_n * T.int64(196) + v_h // T.int64(2) * T.int64(14) + v_w // T.int64(2), v_h % T.int64(2), v_w % T.int64(2)], p2[v_n, v_co, T.int64(0), T.int64(0)])
                            T.writes(T_relu[v_n, v_co, v_h, v_w])
                            T_relu[v_n, v_co, v_h, v_w] = T.max(inverse_local[v_co, v_n * T.int64(196) + v_h // T.int64(2) * T.int64(14) + v_w // T.int64(2), v_h % T.int64(2), v_w % T.int64(2)] + p2[v_n, v_co, T.int64(0), T.int64(0)], T.float32(0))
b0 = sch.get_block(name="data_pack", func_name="main")
b1 = sch.get_block(name="bgemm", func_name="main")
b2 = sch.get_block(name="inverse", func_name="main")
b3 = sch.get_block(name="conv2d_winograd", func_name="main")
b4 = sch.get_block(name="T_add", func_name="main")
b5 = sch.get_block(name="T_relu", func_name="main")
b6 = sch.get_block(name="root", func_name="main")
b7, b8 = sch.get_producers(block=b2)
sch.compute_inline(block=b8)
b9, = sch.get_consumers(block=b2)
l10, l11, l12, l13 = sch.get_loops(block=b9)
l14, l15 = sch.split(loop=l12, factors=[None, 2], preserve_unit_iters=True)
l16, l17 = sch.split(loop=l13, factors=[None, 2], preserve_unit_iters=True)
sch.reorder(l14, l16, l15, l17)
sch.compute_at(block=b2, loop=l16, preserve_unit_loops=True, index=-1)
sch.set_scope(block=b2, buffer_index=0, storage_scope="local")
l18, l19, l20, l21, l22, l23, l24, l25, l26, l27 = sch.get_loops(block=b2)
sch.unroll(loop=l24)
sch.unroll(loop=l25)
sch.unroll(loop=l26)
sch.unroll(loop=l27)
b28, b29 = sch.get_producers(block=b0)
sch.compute_inline(block=b29)
b30, = sch.get_producers(block=b28)
l31, l32, l33, l34, l35, l36 = sch.get_loops(block=b0)
sch.reorder(l33, l34, l31, l32, l35, l36)
sch.unroll(loop=l31)
sch.unroll(loop=l32)
sch.unroll(loop=l35)
sch.unroll(loop=l36)
l37 = sch.fuse(l33, l34, preserve_unit_iters=True)
v38 = sch.sample_categorical(candidates=[32, 64, 128, 256, 512, 1024], probs=[0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666], decision=0)
l39, l40 = sch.split(loop=l37, factors=[None, v38], preserve_unit_iters=True)
sch.bind(loop=l39, thread_axis="blockIdx.x")
sch.bind(loop=l40, thread_axis="threadIdx.x")
b41 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b41, loop=l40, preserve_unit_loops=True, index=-1)
sch.compute_at(block=b28, loop=l40, preserve_unit_loops=True, index=-1)
sch.set_scope(block=b28, buffer_index=0, storage_scope="local")
sch.compute_inline(block=b30)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l42, l43, l44, l45, l46 = sch.get_loops(block=b1)
v47, v48, v49, v50, v51 = sch.sample_perfect_tile(loop=l42, n=5, max_innermost_factor=64, decision=[2, 1, 1, 2, 1])
l52, l53, l54, l55, l56 = sch.split(loop=l42, factors=[v47, v48, v49, v50, v51], preserve_unit_iters=True)
v57, v58, v59, v60, v61 = sch.sample_perfect_tile(loop=l43, n=5, max_innermost_factor=64, decision=[2, 1, 2, 1, 1])
l62, l63, l64, l65, l66 = sch.split(loop=l43, factors=[v57, v58, v59, v60, v61], preserve_unit_iters=True)
v67, v68, v69, v70, v71 = sch.sample_perfect_tile(loop=l44, n=5, max_innermost_factor=64, decision=[1, 8, 16, 1, 1])
l72, l73, l74, l75, l76 = sch.split(loop=l44, factors=[v67, v68, v69, v70, v71], preserve_unit_iters=True)
v77, v78, v79, v80, v81 = sch.sample_perfect_tile(loop=l45, n=5, max_innermost_factor=64, decision=[1, 7, 2, 14, 1])
l82, l83, l84, l85, l86 = sch.split(loop=l45, factors=[v77, v78, v79, v80, v81], preserve_unit_iters=True)
v87, v88, v89 = sch.sample_perfect_tile(loop=l46, n=3, max_innermost_factor=64, decision=[32, 2, 2])
l90, l91, l92 = sch.split(loop=l46, factors=[v87, v88, v89], preserve_unit_iters=True)
sch.reorder(l52, l62, l72, l82, l53, l63, l73, l83, l54, l64, l74, l84, l90, l91, l55, l65, l75, l85, l92, l56, l66, l76, l86)
l93 = sch.fuse(l52, l62, l72, l82, preserve_unit_iters=True)
sch.bind(loop=l93, thread_axis="blockIdx.x")
l94 = sch.fuse(l53, l63, l73, l83, preserve_unit_iters=True)
sch.bind(loop=l94, thread_axis="vthread.x")
l95 = sch.fuse(l54, l64, l74, l84, preserve_unit_iters=True)
sch.bind(loop=l95, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b96 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b96, loop=l95, preserve_unit_loops=True, index=-1)
b97 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b97, loop=l90, preserve_unit_loops=True, index=-1)
l98, l99, l100, l101, l102, l103, l104, l105 = sch.get_loops(block=b97)
l106 = sch.fuse(l102, l103, l104, l105, preserve_unit_iters=True)
v107 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b97, ann_key="meta_schedule.cooperative_fetch", ann_val=v107)
b108 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b108, loop=l90, preserve_unit_loops=True, index=-1)
l109, l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b108)
l117 = sch.fuse(l113, l114, l115, l116, preserve_unit_iters=True)
v118 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b108, ann_key="meta_schedule.cooperative_fetch", ann_val=v118)
l119 = sch.fuse(l90, preserve_unit_iters=True)
sch.annotate(block_or_loop=l119, ann_key="software_pipeline_stage", ann_val=[0, 0, 2])
sch.annotate(block_or_loop=l119, ann_key="software_pipeline_order", ann_val=[0, 1, 2])
sch.annotate(block_or_loop=l119, ann_key="software_pipeline_async_stages", ann_val=[0])
sch.reverse_compute_inline(block=b5)
sch.reverse_compute_inline(block=b4)
v120 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=1)
sch.annotate(block_or_loop=b6, ann_key="meta_schedule.unroll_explicit", ann_val=v120)
l121, l122, l123, l124, l125, l126 = sch.get_loops(block=b3)
l127 = sch.fuse(l121, l122, l123, l124, preserve_unit_iters=True)
v128 = sch.sample_categorical(candidates=[32, 64, 128, 256, 512, 1024], probs=[0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666], decision=0)
l129, l130 = sch.split(loop=l127, factors=[None, v128], preserve_unit_iters=True)
sch.bind(loop=l129, thread_axis="blockIdx.x")
sch.bind(loop=l130, thread_axis="threadIdx.x")
2023-11-10 23:34:12 [INFO] [task_scheduler.cc:170] Design space #2:
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(128), T.int64(28), T.int64(28)), "float32"), p1: T.Buffer((T.int64(4), T.int64(4), T.int64(128), T.int64(128)), "float32"), p2: T.Buffer((T.int64(1), T.int64(128), T.int64(1), T.int64(1)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(128), T.int64(28), T.int64(28)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit": 512})
            input_tile_local = T.alloc_buffer((T.int64(128), T.int64(196), T.int64(4), T.int64(4)), scope="local")
            data_pack = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(128), T.int64(196)))
            bgemm = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(128), T.int64(196)))
            inverse_local = T.alloc_buffer((T.int64(128), T.int64(196), T.int64(2), T.int64(2)), scope="local")
            data_pack_local = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(128), T.int64(196)), scope="local")
            bgemm_local = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(128), T.int64(196)), scope="local")
            data_pack_shared = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(128), T.int64(196)), scope="shared")
            p1_shared = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(128), T.int64(128)), scope="shared")
            for ci_p_fused_0 in T.thread_binding(T.int64(784), thread="blockIdx.x"):
                for ci_p_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(1), T.int64(4), T.int64(4)):
                        with T.block("input_tile"):
                            v_ci = T.axis.spatial(T.int64(128), (ci_p_fused_0 * T.int64(32) + ci_p_fused_1) // T.int64(196) + ax0)
                            v_p = T.axis.spatial(T.int64(196), (ci_p_fused_0 * T.int64(32) + ci_p_fused_1) % T.int64(196) + ax1)
                            v_eps, v_nu = T.axis.remap("SS", [ax2, ax3])
                            T.reads(p0[v_p // T.int64(196), v_ci, v_p % T.int64(196) // T.int64(14) * T.int64(2) + v_eps - T.int64(1), v_p % T.int64(14) * T.int64(2) + v_nu - T.int64(1)])
                            T.writes(input_tile_local[v_ci, v_p, v_eps, v_nu])
                            T.block_attr({"schedule_rule": "None"})
                            input_tile_local[v_ci, v_p, v_eps, v_nu] = T.if_then_else(T.int64(1) <= v_p % T.int64(196) // T.int64(14) * T.int64(2) + v_eps and v_p % T.int64(196) // T.int64(14) * T.int64(2) + v_eps < T.int64(29) and T.int64(1) <= v_p % T.int64(14) * T.int64(2) + v_nu and v_p % T.int64(14) * T.int64(2) + v_nu < T.int64(29), p0[v_p // T.int64(196), v_ci, v_p % T.int64(196) // T.int64(14) * T.int64(2) + v_eps - T.int64(1), v_p % T.int64(14) * T.int64(2) + v_nu - T.int64(1)], T.float32(0))
                    for eps in T.unroll(T.int64(4)):
                        for nu in T.unroll(T.int64(4)):
                            for r_a in T.unroll(T.int64(4)):
                                for r_b in T.unroll(T.int64(4)):
                                    with T.block("data_pack"):
                                        v_eps, v_nu = T.axis.remap("SS", [eps, nu])
                                        v_ci = T.axis.spatial(T.int64(128), (ci_p_fused_0 * T.int64(32) + ci_p_fused_1) // T.int64(196))
                                        v_p = T.axis.spatial(T.int64(196), (ci_p_fused_0 * T.int64(32) + ci_p_fused_1) % T.int64(196))
                                        v_r_a, v_r_b = T.axis.remap("RR", [r_a, r_b])
                                        T.reads(input_tile_local[v_ci, v_p, v_r_a, v_r_b])
                                        T.writes(data_pack_local[v_eps, v_nu, v_ci, v_p])
                                        T.block_attr({"schedule_rule": "conv2d_nchw_winograd_data_pack"})
                                        with T.init():
                                            data_pack_local[v_eps, v_nu, v_ci, v_p] = T.float32(0)
                                        data_pack_local[v_eps, v_nu, v_ci, v_p] = data_pack_local[v_eps, v_nu, v_ci, v_p] + input_tile_local[v_ci, v_p, v_r_a, v_r_b] * T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(3), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(1), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(0), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(3), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(1), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(0), T.float32(1), T.float32(0))))))))))))))))) * T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(3), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(1), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(0), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(3), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(1), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(0), T.float32(1), T.float32(0)))))))))))))))))
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(4), T.int64(4), T.int64(1), T.int64(1)):
                        with T.block("data_pack_local"):
                            v0, v1 = T.axis.remap("SS", [ax0, ax1])
                            v2 = T.axis.spatial(T.int64(128), (ci_p_fused_0 * T.int64(32) + ci_p_fused_1) // T.int64(196) + ax2)
                            v3 = T.axis.spatial(T.int64(196), (ci_p_fused_0 * T.int64(32) + ci_p_fused_1) % T.int64(196) + ax3)
                            T.reads(data_pack_local[v0, v1, v2, v3])
                            T.writes(data_pack[v0, v1, v2, v3])
                            data_pack[v0, v1, v2, v3] = data_pack_local[v0, v1, v2, v3]
            for eps_0_nu_0_co_0_p_0_fused in T.thread_binding(T.int64(4), thread="blockIdx.x"):
                for eps_1_nu_1_co_1_p_1_fused in T.thread_binding(T.int64(56), thread="vthread.x"):
                    for eps_2_nu_2_co_2_p_2_fused in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                        for ci_0_fused in T.serial(T.int64(32), annotations={"software_pipeline_async_stages": [0], "software_pipeline_order": [0, 1, 2], "software_pipeline_stage": [0, 0, 3]}):
                            for ax0_ax1_ax2_ax3_fused in range(T.int64(3136)):
                                with T.block("data_pack_shared"):
                                    v0 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(2) * T.int64(2) + ax0_ax1_ax2_ax3_fused // T.int64(1568))
                                    v1 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused % T.int64(2) * T.int64(2) + ax0_ax1_ax2_ax3_fused % T.int64(1568) // T.int64(784))
                                    v2 = T.axis.spatial(T.int64(128), ci_0_fused * T.int64(4) + ax0_ax1_ax2_ax3_fused % T.int64(784) // T.int64(196))
                                    v3 = T.axis.spatial(T.int64(196), ax0_ax1_ax2_ax3_fused % T.int64(196))
                                    T.reads(data_pack[v0, v1, v2, v3])
                                    T.writes(data_pack_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch": 4})
                                    data_pack_shared[v0, v1, v2, v3] = data_pack[v0, v1, v2, v3]
                            for ax0_ax1_ax2_ax3_fused in range(T.int64(2048)):
                                with T.block("p1_shared"):
                                    v0 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(2) * T.int64(2) + ax0_ax1_ax2_ax3_fused // T.int64(1024))
                                    v1 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused % T.int64(2) * T.int64(2) + ax0_ax1_ax2_ax3_fused % T.int64(1024) // T.int64(512))
                                    v2 = T.axis.spatial(T.int64(128), ci_0_fused * T.int64(4) + ax0_ax1_ax2_ax3_fused % T.int64(512) // T.int64(128))
                                    v3 = T.axis.spatial(T.int64(128), ax0_ax1_ax2_ax3_fused % T.int64(128))
                                    T.reads(p1[v0, v1, v2, v3])
                                    T.writes(p1_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch": 4})
                                    p1_shared[v0, v1, v2, v3] = p1[v0, v1, v2, v3]
                            for ci_1, eps_3, nu_3, co_3, p_3, ci_2, eps_4, nu_4, co_4, p_4 in T.grid(T.int64(2), T.int64(2), T.int64(1), T.int64(1), T.int64(14), T.int64(2), T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                                with T.block("bgemm"):
                                    v_eps = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(2) * T.int64(2) + eps_3 + eps_4)
                                    v_nu = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused % T.int64(2) * T.int64(2) + eps_2_nu_2_co_2_p_2_fused // T.int64(32) + nu_3 + nu_4)
                                    v_co = T.axis.spatial(T.int64(128), eps_1_nu_1_co_1_p_1_fused // T.int64(7) * T.int64(16) + eps_2_nu_2_co_2_p_2_fused % T.int64(32) // T.int64(2) + co_3 + co_4)
                                    v_p = T.axis.spatial(T.int64(196), eps_1_nu_1_co_1_p_1_fused % T.int64(7) * T.int64(28) + eps_2_nu_2_co_2_p_2_fused % T.int64(2) * T.int64(14) + p_3 + p_4)
                                    v_ci = T.axis.reduce(T.int64(128), ci_0_fused * T.int64(4) + ci_1 * T.int64(2) + ci_2)
                                    T.reads(data_pack_shared[v_eps, v_nu, v_ci, v_p], p1_shared[v_eps, v_nu, v_ci, v_co])
                                    T.writes(bgemm_local[v_eps, v_nu, v_co, v_p])
                                    T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                    with T.init():
                                        bgemm_local[v_eps, v_nu, v_co, v_p] = T.float32(0)
                                    bgemm_local[v_eps, v_nu, v_co, v_p] = bgemm_local[v_eps, v_nu, v_co, v_p] + data_pack_shared[v_eps, v_nu, v_ci, v_p] * p1_shared[v_eps, v_nu, v_ci, v_co]
                        for ax0, ax1, ax2, ax3 in T.grid(T.int64(2), T.int64(1), T.int64(1), T.int64(14)):
                            with T.block("bgemm_local"):
                                v0 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(2) * T.int64(2) + ax0)
                                v1 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused % T.int64(2) * T.int64(2) + eps_2_nu_2_co_2_p_2_fused // T.int64(32) + ax1)
                                v2 = T.axis.spatial(T.int64(128), eps_1_nu_1_co_1_p_1_fused // T.int64(7) * T.int64(16) + eps_2_nu_2_co_2_p_2_fused % T.int64(32) // T.int64(2) + ax2)
                                v3 = T.axis.spatial(T.int64(196), eps_1_nu_1_co_1_p_1_fused % T.int64(7) * T.int64(28) + eps_2_nu_2_co_2_p_2_fused % T.int64(2) * T.int64(14) + ax3)
                                T.reads(bgemm_local[v0, v1, v2, v3])
                                T.writes(bgemm[v0, v1, v2, v3])
                                bgemm[v0, v1, v2, v3] = bgemm_local[v0, v1, v2, v3]
            for n_co_h_0_w_0_fused_0 in T.thread_binding(T.int64(49), thread="blockIdx.x"):
                for n_co_h_0_w_0_fused_1 in T.thread_binding(T.int64(512), thread="threadIdx.x"):
                    for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                        for ax2 in T.unroll(T.int64(2)):
                            for ax3 in T.unroll(T.int64(2)):
                                for ax4 in T.unroll(T.int64(4)):
                                    for ax5 in T.unroll(T.int64(4)):
                                        with T.block("inverse"):
                                            v_co = T.axis.spatial(T.int64(128), (n_co_h_0_w_0_fused_0 * T.int64(512) + n_co_h_0_w_0_fused_1) // T.int64(196) + ax0)
                                            v_p = T.axis.spatial(T.int64(196), (n_co_h_0_w_0_fused_0 * T.int64(512) + n_co_h_0_w_0_fused_1) % T.int64(196) + ax1)
                                            v_vh, v_vw, v_r_a, v_r_b = T.axis.remap("SSRR", [ax2, ax3, ax4, ax5])
                                            T.reads(bgemm[v_r_a, v_r_b, v_co, v_p])
                                            T.writes(inverse_local[v_co, v_p, v_vh, v_vw])
                                            T.block_attr({"schedule_rule": "conv2d_nchw_winograd_inverse"})
                                            with T.init():
                                                inverse_local[v_co, v_p, v_vh, v_vw] = T.float32(0)
                                            inverse_local[v_co, v_p, v_vh, v_vw] = inverse_local[v_co, v_p, v_vh, v_vw] + bgemm[v_r_a, v_r_b, v_co, v_p] * T.Select(v_r_a % T.int64(4) == T.int64(3) and v_vh % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_vh % T.int64(2) == T.int64(0), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_vh % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_vh % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_vh % T.int64(2) == T.int64(1), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_vh % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_vh % T.int64(2) == T.int64(1), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_vh % T.int64(2) == T.int64(0), T.float32(1), T.float32(0))))))))) * T.Select(v_r_b % T.int64(4) == T.int64(3) and v_vw % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_vw % T.int64(2) == T.int64(0), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_vw % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_vw % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_vw % T.int64(2) == T.int64(1), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_vw % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_vw % T.int64(2) == T.int64(1), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_vw % T.int64(2) == T.int64(0), T.float32(1), T.float32(0)))))))))
                    for h_1, w_1 in T.grid(T.int64(2), T.int64(2)):
                        with T.block("conv2d_winograd"):
                            v_n = T.axis.spatial(T.int64(1), T.int64(0))
                            v_co = T.axis.spatial(T.int64(128), (n_co_h_0_w_0_fused_0 * T.int64(512) + n_co_h_0_w_0_fused_1) // T.int64(196))
                            v_h = T.axis.spatial(T.int64(28), (n_co_h_0_w_0_fused_0 * T.int64(512) + n_co_h_0_w_0_fused_1) % T.int64(196) // T.int64(14) * T.int64(2) + h_1)
                            v_w = T.axis.spatial(T.int64(28), (n_co_h_0_w_0_fused_0 * T.int64(512) + n_co_h_0_w_0_fused_1) % T.int64(14) * T.int64(2) + w_1)
                            T.reads(inverse_local[v_co, v_n * T.int64(196) + v_h // T.int64(2) * T.int64(14) + v_w // T.int64(2), v_h % T.int64(2), v_w % T.int64(2)], p2[v_n, v_co, T.int64(0), T.int64(0)])
                            T.writes(T_relu[v_n, v_co, v_h, v_w])
                            T_relu[v_n, v_co, v_h, v_w] = T.max(inverse_local[v_co, v_n * T.int64(196) + v_h // T.int64(2) * T.int64(14) + v_w // T.int64(2), v_h % T.int64(2), v_w % T.int64(2)] + p2[v_n, v_co, T.int64(0), T.int64(0)], T.float32(0))
b0 = sch.get_block(name="data_pack", func_name="main")
b1 = sch.get_block(name="bgemm", func_name="main")
b2 = sch.get_block(name="inverse", func_name="main")
b3 = sch.get_block(name="conv2d_winograd", func_name="main")
b4 = sch.get_block(name="T_add", func_name="main")
b5 = sch.get_block(name="T_relu", func_name="main")
b6 = sch.get_block(name="root", func_name="main")
b7, b8 = sch.get_producers(block=b2)
sch.compute_inline(block=b8)
b9, = sch.get_consumers(block=b2)
l10, l11, l12, l13 = sch.get_loops(block=b9)
l14, l15 = sch.split(loop=l12, factors=[None, 2], preserve_unit_iters=True)
l16, l17 = sch.split(loop=l13, factors=[None, 2], preserve_unit_iters=True)
sch.reorder(l14, l16, l15, l17)
sch.compute_at(block=b2, loop=l16, preserve_unit_loops=True, index=-1)
sch.set_scope(block=b2, buffer_index=0, storage_scope="local")
l18, l19, l20, l21, l22, l23, l24, l25, l26, l27 = sch.get_loops(block=b2)
sch.unroll(loop=l24)
sch.unroll(loop=l25)
sch.unroll(loop=l26)
sch.unroll(loop=l27)
b28, b29 = sch.get_producers(block=b0)
sch.compute_inline(block=b29)
b30, = sch.get_producers(block=b28)
l31, l32, l33, l34, l35, l36 = sch.get_loops(block=b0)
sch.reorder(l33, l34, l31, l32, l35, l36)
sch.unroll(loop=l31)
sch.unroll(loop=l32)
sch.unroll(loop=l35)
sch.unroll(loop=l36)
l37 = sch.fuse(l33, l34, preserve_unit_iters=True)
v38 = sch.sample_categorical(candidates=[32, 64, 128, 256, 512, 1024], probs=[0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666], decision=0)
l39, l40 = sch.split(loop=l37, factors=[None, v38], preserve_unit_iters=True)
sch.bind(loop=l39, thread_axis="blockIdx.x")
sch.bind(loop=l40, thread_axis="threadIdx.x")
b41 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b41, loop=l40, preserve_unit_loops=True, index=-1)
sch.compute_at(block=b28, loop=l40, preserve_unit_loops=True, index=-1)
sch.set_scope(block=b28, buffer_index=0, storage_scope="local")
sch.compute_inline(block=b30)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l42, l43, l44, l45, l46 = sch.get_loops(block=b1)
v47, v48, v49, v50, v51 = sch.sample_perfect_tile(loop=l42, n=5, max_innermost_factor=64, decision=[2, 1, 1, 2, 1])
l52, l53, l54, l55, l56 = sch.split(loop=l42, factors=[v47, v48, v49, v50, v51], preserve_unit_iters=True)
v57, v58, v59, v60, v61 = sch.sample_perfect_tile(loop=l43, n=5, max_innermost_factor=64, decision=[2, 1, 2, 1, 1])
l62, l63, l64, l65, l66 = sch.split(loop=l43, factors=[v57, v58, v59, v60, v61], preserve_unit_iters=True)
v67, v68, v69, v70, v71 = sch.sample_perfect_tile(loop=l44, n=5, max_innermost_factor=64, decision=[1, 8, 16, 1, 1])
l72, l73, l74, l75, l76 = sch.split(loop=l44, factors=[v67, v68, v69, v70, v71], preserve_unit_iters=True)
v77, v78, v79, v80, v81 = sch.sample_perfect_tile(loop=l45, n=5, max_innermost_factor=64, decision=[1, 7, 2, 14, 1])
l82, l83, l84, l85, l86 = sch.split(loop=l45, factors=[v77, v78, v79, v80, v81], preserve_unit_iters=True)
v87, v88, v89 = sch.sample_perfect_tile(loop=l46, n=3, max_innermost_factor=64, decision=[32, 2, 2])
l90, l91, l92 = sch.split(loop=l46, factors=[v87, v88, v89], preserve_unit_iters=True)
sch.reorder(l52, l62, l72, l82, l53, l63, l73, l83, l54, l64, l74, l84, l90, l91, l55, l65, l75, l85, l92, l56, l66, l76, l86)
l93 = sch.fuse(l52, l62, l72, l82, preserve_unit_iters=True)
sch.bind(loop=l93, thread_axis="blockIdx.x")
l94 = sch.fuse(l53, l63, l73, l83, preserve_unit_iters=True)
sch.bind(loop=l94, thread_axis="vthread.x")
l95 = sch.fuse(l54, l64, l74, l84, preserve_unit_iters=True)
sch.bind(loop=l95, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b96 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b96, loop=l95, preserve_unit_loops=True, index=-1)
b97 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b97, loop=l90, preserve_unit_loops=True, index=-1)
l98, l99, l100, l101, l102, l103, l104, l105 = sch.get_loops(block=b97)
l106 = sch.fuse(l102, l103, l104, l105, preserve_unit_iters=True)
v107 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b97, ann_key="meta_schedule.cooperative_fetch", ann_val=v107)
b108 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b108, loop=l90, preserve_unit_loops=True, index=-1)
l109, l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b108)
l117 = sch.fuse(l113, l114, l115, l116, preserve_unit_iters=True)
v118 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b108, ann_key="meta_schedule.cooperative_fetch", ann_val=v118)
l119 = sch.fuse(l90, preserve_unit_iters=True)
sch.annotate(block_or_loop=l119, ann_key="software_pipeline_stage", ann_val=[0, 0, 3])
sch.annotate(block_or_loop=l119, ann_key="software_pipeline_order", ann_val=[0, 1, 2])
sch.annotate(block_or_loop=l119, ann_key="software_pipeline_async_stages", ann_val=[0])
sch.reverse_compute_inline(block=b5)
sch.reverse_compute_inline(block=b4)
v120 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b6, ann_key="meta_schedule.unroll_explicit", ann_val=v120)
l121, l122, l123, l124, l125, l126 = sch.get_loops(block=b3)
l127 = sch.fuse(l121, l122, l123, l124, preserve_unit_iters=True)
v128 = sch.sample_categorical(candidates=[32, 64, 128, 256, 512, 1024], probs=[0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666], decision=4)
l129, l130 = sch.split(loop=l127, factors=[None, v128], preserve_unit_iters=True)
sch.bind(loop=l129, thread_axis="blockIdx.x")
sch.bind(loop=l130, thread_axis="threadIdx.x")
2023-11-10 23:52:37 [INFO] [evolutionary_search.cc:713] Generating candidates......
2023-11-10 23:52:37 [INFO] [evolutionary_search.cc:715] Picked top 0 candidate(s) from database
2023-11-10 23:52:44 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310a03fb48)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310e893e78)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310e893578)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310ec9ace8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56310e986408)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310eb1cfb8)]: 491 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310dc6f7b8)]: 0 failure(s)
2023-11-10 23:52:53 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310a03fb48)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310e893e78)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310e893578)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310ec9ace8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56310e986408)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310eb1cfb8)]: 985 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310dc6f7b8)]: 0 failure(s)
2023-11-10 23:53:02 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310a03fb48)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310e893e78)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310e893578)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310ec9ace8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56310e986408)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310eb1cfb8)]: 1474 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310dc6f7b8)]: 0 failure(s)
2023-11-10 23:53:02 [INFO] [evolutionary_search.cc:723] Sampled 62 candidate(s)
2023-11-10 23:53:20 [INFO] [evolutionary_search.cc:621] Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310a03fb48)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310e893e78)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310e893578)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310ec9ace8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56310e986408)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310eb1cfb8)]: 131 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310dc6f7b8)]: 0 failure(s)
2023-11-10 23:53:37 [INFO] [evolutionary_search.cc:621] Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310a03fb48)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310e893e78)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310e893578)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310ec9ace8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56310e986408)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310eb1cfb8)]: 96 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310dc6f7b8)]: 0 failure(s)
2023-11-10 23:53:55 [INFO] [evolutionary_search.cc:621] Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310a03fb48)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310e893e78)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310e893578)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310ec9ace8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56310e986408)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310eb1cfb8)]: 100 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310dc6f7b8)]: 0 failure(s)
2023-11-10 23:54:12 [INFO] [evolutionary_search.cc:621] Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310a03fb48)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310e893e78)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310e893578)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310ec9ace8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56310e986408)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310eb1cfb8)]: 121 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310dc6f7b8)]: 0 failure(s)
2023-11-10 23:54:13 [INFO] [evolutionary_search.cc:649] Scores of the best 64 candidates:
[1 : 16]:	0.9995  0.9970  0.9964  0.9963  0.9962  0.9956  0.9955  0.9952  0.9942  0.9938  0.9925  0.9911  0.9908  0.9903  0.9885  0.9884
[17 : 32]:	0.9874  0.9870  0.9869  0.9863  0.9848  0.9846  0.9839  0.9838  0.9834  0.9807  0.9792  0.9779  0.9777  0.9767  0.9764  0.9763
[33 : 48]:	0.9762  0.9757  0.9757  0.9755  0.9751  0.9749  0.9742  0.9676  0.9674  0.9652  0.9645  0.9636  0.9625  0.9623  0.9614  0.9592
[49 : 64]:	0.9591  0.9588  0.9584  0.9575  0.9573  0.9563  0.9560  0.9545  0.9544  0.9542  0.9539  0.9535  0.9534  0.9528  0.9523  0.9522
2023-11-10 23:54:14 [INFO] [evolutionary_search.cc:727] Got 64 candidate(s) with evolutionary search
2023-11-10 23:54:14 [INFO] [evolutionary_search.cc:730] Sending 64 candidates(s) for measurement
2023-11-11 00:25:34 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #1: GFLOPs: 1455.3220. Time: 87.2973 us. Best GFLOPs: 1455.3220
2023-11-11 00:25:34 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #2: GFLOPs: 3031.7599. Time: 41.9049 us. Best GFLOPs: 3031.7599
2023-11-11 00:25:34 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #3: GFLOPs: 1154.3692. Time: 110.0563 us. Best GFLOPs: 3031.7599
2023-11-11 00:25:34 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #4: GFLOPs: 422.8007. Time: 300.4858 us. Best GFLOPs: 3031.7599
2023-11-11 00:25:34 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #5: GFLOPs: 2861.9166. Time: 44.3918 us. Best GFLOPs: 3031.7599
2023-11-11 00:25:34 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #6: GFLOPs: 2399.8412. Time: 52.9392 us. Best GFLOPs: 3031.7599
2023-11-11 00:25:34 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #7: GFLOPs: 1059.9085. Time: 119.8647 us. Best GFLOPs: 3031.7599
2023-11-11 00:25:34 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #8: GFLOPs: 1707.1516. Time: 74.4197 us. Best GFLOPs: 3031.7599
2023-11-11 00:25:34 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #9: GFLOPs: 247.7632. Time: 512.7704 us. Best GFLOPs: 3031.7599
2023-11-11 00:25:34 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #10: GFLOPs: 885.2878. Time: 143.5077 us. Best GFLOPs: 3031.7599
2023-11-11 00:25:34 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #11: GFLOPs: 93.2577. Time: 1362.3075 us. Best GFLOPs: 3031.7599
2023-11-11 00:25:34 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #12: GFLOPs: 1856.6132. Time: 68.4287 us. Best GFLOPs: 3031.7599
2023-11-11 00:25:34 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #13: GFLOPs: 2570.5790. Time: 49.4230 us. Best GFLOPs: 3031.7599
2023-11-11 00:25:34 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #14: GFLOPs: 327.5895. Time: 387.8196 us. Best GFLOPs: 3031.7599
2023-11-11 00:25:34 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #15: GFLOPs: 419.2924. Time: 303.0001 us. Best GFLOPs: 3031.7599
2023-11-11 00:25:34 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #16: GFLOPs: 827.0511. Time: 153.6128 us. Best GFLOPs: 3031.7599
2023-11-11 00:25:34 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #17: GFLOPs: 44.7284. Time: 2840.3769 us. Best GFLOPs: 3031.7599
2023-11-11 00:25:34 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #18: GFLOPs: 77.3444. Time: 1642.5968 us. Best GFLOPs: 3031.7599
2023-11-11 00:25:34 [INFO] [task_scheduler.cc:121] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #19: Error in building:
LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(128), T.int64(28), T.int64(28)), "float32"), p1: T.Buffer((T.int64(4), T.int64(4), T.int64(128), T.int64(128)), "float32"), p2: T.Buffer((T.int64(1), T.int64(128), T.int64(1), T.int64(1)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(128), T.int64(28), T.int64(28)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        input_tile_local = T.alloc_buffer((T.int64(128), T.int64(196), T.int64(4), T.int64(4)), scope="local")
        data_pack = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(128), T.int64(196)))
        bgemm = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(128), T.int64(196)))
        inverse_local = T.alloc_buffer((T.int64(128), T.int64(196), T.int64(2), T.int64(2)), scope="local")
        data_pack_local = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(128), T.int64(196)), scope="local")
        bgemm_local = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(128), T.int64(196)), scope="local")
        data_pack_shared = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(128), T.int64(196)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(128), T.int64(128)), scope="shared")
        for ci_p_fused_0 in T.thread_binding(T.int64(98), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 1024, "pragma_unroll_explicit": 1}):
            for ci_p_fused_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(1), T.int64(4), T.int64(4)):
                    with T.block("input_tile"):
                        v_ci = T.axis.spatial(T.int64(128), (ci_p_fused_0 * T.int64(256) + ci_p_fused_1) // T.int64(196) + ax0)
                        v_p = T.axis.spatial(T.int64(196), (ci_p_fused_0 * T.int64(60) + ci_p_fused_1) % T.int64(196) + ax1)
                        v_eps, v_nu = T.axis.remap("SS", [ax2, ax3])
                        T.reads(p0[v_p // T.int64(196), v_ci, v_p % T.int64(196) // T.int64(14) * T.int64(2) + v_eps - T.int64(1), v_p % T.int64(14) * T.int64(2) + v_nu - T.int64(1)])
                        T.writes(input_tile_local[v_ci, v_p, v_eps, v_nu])
                        T.block_attr({"schedule_rule": "None"})
                        input_tile_local[v_ci, v_p, v_eps, v_nu] = T.if_then_else(T.int64(1) <= v_p % T.int64(196) // T.int64(14) * T.int64(2) + v_eps and v_p % T.int64(196) // T.int64(14) * T.int64(2) + v_eps < T.int64(29) and T.int64(1) <= v_p % T.int64(14) * T.int64(2) + v_nu and v_p % T.int64(14) * T.int64(2) + v_nu < T.int64(29), p0[v_p // T.int64(196), v_ci, v_p % T.int64(196) // T.int64(14) * T.int64(2) + v_eps - T.int64(1), v_p % T.int64(14) * T.int64(2) + v_nu - T.int64(1)], T.float32(0))
                for eps in T.unroll(T.int64(4)):
                    for nu in T.unroll(T.int64(4)):
                        with T.block("data_pack_init"):
                            v_eps, v_nu = T.axis.remap("SS", [eps, nu])
                            v_ci = T.axis.spatial(T.int64(128), (ci_p_fused_0 * T.int64(256) + ci_p_fused_1) // T.int64(196))
                            v_p = T.axis.spatial(T.int64(196), (ci_p_fused_0 * T.int64(256) + ci_p_fused_1) % T.int64(196))
                            T.reads()
                            T.writes(data_pack_local[v_eps, v_nu, v_ci, v_p])
                            T.block_attr({"schedule_rule": "conv2d_nchw_winograd_data_pack"})
                            data_pack_local[v_eps, v_nu, v_ci, v_p] = T.float32(0)
                        for r_a in T.unroll(T.int64(4)):
                            for r_b in T.unroll(T.int64(4)):
                                with T.block("data_pack_update"):
                                    v_eps, v_nu = T.axis.remap("SS", [eps, nu])
                                    v_ci = T.axis.spatial(T.int64(128), (ci_p_fused_0 * T.int64(256) + ci_p_fused_1) // T.int64(196))
                                    v_p = T.axis.spatial(T.int64(196), (ci_p_fused_0 * T.int64(256) + ci_p_fused_1) % T.int64(196))
                                    v_r_a, v_r_b = T.axis.remap("RR", [r_a, r_b])
                                    T.reads(data_pack_local[v_eps, v_nu, v_ci, v_p], input_tile_local[v_ci, v_p, v_r_a, v_r_b])
                                    T.writes(data_pack_local[v_eps, v_nu, v_ci, v_p])
                                    T.block_attr({"schedule_rule": "conv2d_nchw_winograd_data_pack"})
                                    data_pack_local[v_eps, v_nu, v_ci, v_p] = data_pack_local[v_eps, v_nu, v_ci, v_p] + input_tile_local[v_ci, v_p, v_r_a, v_r_b] * T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(3), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(1), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(0), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(3), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(1), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(0), T.float32(1), T.float32(0))))))))))))))))) * T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(3), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(1), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(0), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(3), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(1), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(0), T.float32(1), T.float32(0)))))))))))))))))
                for ax0, ax1, ax2, ax3 in T.grid(T.int64(4), T.int64(4), T.int64(1), T.int64(1)):
                    with T.block("data_pack_local"):
                        v0, v1 = T.axis.remap("SS", [ax0, ax1])
                        v2 = T.axis.spatial(T.int64(128), (ci_p_fused_0 * T.int64(256) + ci_p_fused_1) // T.int64(196) + ax2)
                        v3 = T.axis.spatial(T.int64(196), (ci_p_fused_0 * T.int64(60) + ci_p_fused_1) % T.int64(196) + ax3)
                        T.reads(data_pack_local[v0, v1, v2, v3])
                        T.writes(data_pack[v0, v1, v2, v3])
                        data_pack[v0, v1, v2, v3] = data_pack_local[v0, v1, v2, v3]
        for eps_0_nu_0_co_0_p_0_fused in T.thread_binding(T.int64(2), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 1024, "pragma_unroll_explicit": 1}):
            for eps_1_nu_1_co_1_p_1_fused in T.thread_binding(T.int64(28), thread="vthread.x"):
                for eps_2_nu_2_co_2_p_2_fused in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                    for eps_3_init, nu_3_init, co_3_init, p_3_init, eps_4_init, nu_4_init, co_4_init, p_4_init in T.grid(T.int64(1), T.int64(1), T.int64(2), T.int64(2), T.int64(2), T.int64(2), T.int64(2), T.int64(7)):
                        with T.block("bgemm_init"):
                            v_eps = T.axis.spatial(T.int64(4), eps_2_nu_2_co_2_p_2_fused // T.int64(16) * T.int64(2) + eps_3_init * T.int64(2) + eps_4_init)
                            v_nu = T.axis.spatial(T.int64(4), eps_2_nu_2_co_2_p_2_fused % T.int64(16) // T.int64(8) * T.int64(2) + nu_3_init * T.int64(2) + nu_4_init)
                            v_co = T.axis.spatial(T.int64(128), eps_0_nu_0_co_0_p_0_fused * T.int64(64) + eps_1_nu_1_co_1_p_1_fused // T.int64(14) * T.int64(32) + eps_2_nu_2_co_2_p_2_fused % T.int64(8) * T.int64(4) + co_3_init * T.int64(2) + co_4_init)
                            v_p = T.axis.spatial(T.int64(196), eps_1_nu_1_co_1_p_1_fused % T.int64(14) * T.int64(14) + p_3_init * T.int64(7) + p_4_init)
                            T.reads()
                            T.writes(bgemm_local[v_eps, v_nu, v_co, v_p])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            bgemm_local[v_eps, v_nu, v_co, v_p] = T.float32(0)
                    for ci_0 in range(T.int64(64)):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(49)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("data_pack_shared"):
                                        v0 = T.axis.spatial(T.int64(4), (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) // T.int64(1568))
                                        v1 = T.axis.spatial(T.int64(4), (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) % T.int64(1568) // T.int64(392))
                                        v2 = T.axis.spatial(T.int64(128), ci_0 * T.int64(2) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) % T.int64(392) // T.int64(196))
                                        v3 = T.axis.spatial(T.int64(196), (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) % T.int64(196))
                                        T.reads(data_pack[v0, v1, v2, v3])
                                        T.writes(data_pack_shared[v0, v1, v2, v3])
                                        data_pack_shared[v0, v1, v2, v3] = data_pack[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(64)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                                with T.block("p1_shared"):
                                    v0 = T.axis.spatial(T.int64(4), (ax0_ax1_ax2_ax3_fused_0 * T.int64(32) + ax0_ax1_ax2_ax3_fused_1) // T.int64(512))
                                    v1 = T.axis.spatial(T.int64(4), (ax0_ax1_ax2_ax3_fused_0 * T.int64(32) + ax0_ax1_ax2_ax3_fused_1) % T.int64(512) // T.int64(128))
                                    v2 = T.axis.spatial(T.int64(128), ci_0 * T.int64(2) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(32) + ax0_ax1_ax2_ax3_fused_1) % T.int64(128) // T.int64(64))
                                    v3 = T.axis.spatial(T.int64(128), eps_0_nu_0_co_0_p_0_fused * T.int64(64) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(32) + ax0_ax1_ax2_ax3_fused_1) % T.int64(64))
                                    T.reads(p1[v0, v1, v2, v3])
                                    T.writes(p1_shared[v0, v1, v2, v3])
                                    p1_shared[v0, v1, v2, v3] = p1[v0, v1, v2, v3]
                        for ci_1, eps_3, nu_3, co_3, p_3, ci_2, eps_4, nu_4, co_4, p_4 in T.grid(T.int64(2), T.int64(1), T.int64(1), T.int64(2), T.int64(2), T.int64(1), T.int64(2), T.int64(2), T.int64(2), T.int64(7)):
                            with T.block("bgemm_update"):
                                v_eps = T.axis.spatial(T.int64(4), eps_2_nu_2_co_2_p_2_fused // T.int64(16) * T.int64(2) + eps_3 * T.int64(2) + eps_4)
                                v_nu = T.axis.spatial(T.int64(4), eps_2_nu_2_co_2_p_2_fused % T.int64(16) // T.int64(8) * T.int64(2) + nu_3 * T.int64(2) + nu_4)
                                v_co = T.axis.spatial(T.int64(128), eps_0_nu_0_co_0_p_0_fused * T.int64(64) + eps_1_nu_1_co_1_p_1_fused // T.int64(14) * T.int64(32) + eps_2_nu_2_co_2_p_2_fused % T.int64(8) * T.int64(4) + co_3 * T.int64(2) + co_4)
                                v_p = T.axis.spatial(T.int64(196), eps_1_nu_1_co_1_p_1_fused % T.int64(14) * T.int64(14) + p_3 * T.int64(7) + p_4)
                                v_ci = T.axis.reduce(T.int64(128), ci_0 * T.int64(2) + ci_1 + ci_2)
                                T.reads(bgemm_local[v_eps, v_nu, v_co, v_p], data_pack_shared[v_eps, v_nu, v_ci, v_p], p1_shared[v_eps, v_nu, v_ci, v_co])
                                T.writes(bgemm_local[v_eps, v_nu, v_co, v_p])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                bgemm_local[v_eps, v_nu, v_co, v_p] = bgemm_local[v_eps, v_nu, v_co, v_p] + data_pack_shared[v_eps, v_nu, v_ci, v_p] * p1_shared[v_eps, v_nu, v_ci, v_co]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(2), T.int64(2), T.int64(4), T.int64(14)):
                        with T.block("bgemm_local"):
                            v0 = T.axis.spatial(T.int64(4), eps_2_nu_2_co_2_p_2_fused // T.int64(16) * T.int64(2) + ax0)
                            v1 = T.axis.spatial(T.int64(4), eps_2_nu_2_co_2_p_2_fused % T.int64(16) // T.int64(8) * T.int64(2) + ax1)
                            v2 = T.axis.spatial(T.int64(128), eps_0_nu_0_co_0_p_0_fused * T.int64(64) + eps_1_nu_1_co_1_p_1_fused // T.int64(14) * T.int64(32) + eps_2_nu_2_co_2_p_2_fused % T.int64(8) * T.int64(4) + ax2)
                            v3 = T.axis.spatial(T.int64(196), eps_1_nu_1_co_1_p_1_fused % T.int64(14) * T.int64(14) + ax3)
                            T.reads(bgemm_local[v0, v1, v2, v3])
                            T.writes(bgemm[v0, v1, v2, v3])
                            bgemm[v0, v1, v2, v3] = bgemm_local[v0, v1, v2, v3]
        for n_co_h_0_w_0_fused_0 in T.thread_binding(T.int64(392), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 1024, "pragma_unroll_explicit": 1}):
            for n_co_h_0_w_0_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                    for ax2 in T.unroll(T.int64(2)):
                        for ax3 in T.unroll(T.int64(2)):
                            with T.block("inverse_init"):
                                v_co = T.axis.spatial(T.int64(128), (n_co_h_0_w_0_fused_0 * T.int64(64) + n_co_h_0_w_0_fused_1) // T.int64(196) + ax0)
                                v_p = T.axis.spatial(T.int64(196), (n_co_h_0_w_0_fused_0 * T.int64(64) + n_co_h_0_w_0_fused_1) % T.int64(196) + ax1)
                                v_vh, v_vw = T.axis.remap("SS", [ax2, ax3])
                                T.reads()
                                T.writes(inverse_local[v_co, v_p, v_vh, v_vw])
                                T.block_attr({"schedule_rule": "conv2d_nchw_winograd_inverse"})
                                inverse_local[v_co, v_p, v_vh, v_vw] = T.float32(0)
                            for ax4 in T.unroll(T.int64(4)):
                                for ax5 in T.unroll(T.int64(4)):
                                    with T.block("inverse_update"):
                                        v_co = T.axis.spatial(T.int64(128), (n_co_h_0_w_0_fused_0 * T.int64(64) + n_co_h_0_w_0_fused_1) // T.int64(196) + ax0)
                                        v_p = T.axis.spatial(T.int64(196), (n_co_h_0_w_0_fused_0 * T.int64(64) + n_co_h_0_w_0_fused_1) % T.int64(196) + ax1)
                                        v_vh, v_vw, v_r_a, v_r_b = T.axis.remap("SSRR", [ax2, ax3, ax4, ax5])
                                        T.reads(inverse_local[v_co, v_p, v_vh, v_vw], bgemm[v_r_a, v_r_b, v_co, v_p])
                                        T.writes(inverse_local[v_co, v_p, v_vh, v_vw])
                                        T.block_attr({"schedule_rule": "conv2d_nchw_winograd_inverse"})
                                        inverse_local[v_co, v_p, v_vh, v_vw] = inverse_local[v_co, v_p, v_vh, v_vw] + bgemm[v_r_a, v_r_b, v_co, v_p] * T.Select(v_r_a % T.int64(4) == T.int64(3) and v_vh % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_vh % T.int64(2) == T.int64(0), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_vh % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_vh % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_vh % T.int64(2) == T.int64(1), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_vh % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_vh % T.int64(2) == T.int64(1), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_vh % T.int64(2) == T.int64(0), T.float32(1), T.float32(0))))))))) * T.Select(v_r_b % T.int64(4) == T.int64(3) and v_vw % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_vw % T.int64(2) == T.int64(0), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_vw % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_vw % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_vw % T.int64(2) == T.int64(1), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_vw % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_vw % T.int64(2) == T.int64(1), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_vw % T.int64(2) == T.int64(0), T.float32(1), T.float32(0)))))))))
                for h_1, w_1 in T.grid(T.int64(2), T.int64(2)):
                    with T.block("conv2d_winograd"):
                        v_n = T.axis.spatial(T.int64(1), T.int64(0))
                        v_co = T.axis.spatial(T.int64(128), (n_co_h_0_w_0_fused_0 * T.int64(64) + n_co_h_0_w_0_fused_1) // T.int64(196))
                        v_h = T.axis.spatial(T.int64(28), (n_co_h_0_w_0_fused_0 * T.int64(64) + n_co_h_0_w_0_fused_1) % T.int64(196) // T.int64(14) * T.int64(2) + h_1)
                        v_w = T.axis.spatial(T.int64(28), (n_co_h_0_w_0_fused_0 * T.int64(64) + n_co_h_0_w_0_fused_1) % T.int64(14) * T.int64(2) + w_1)
                        T.reads(inverse_local[v_co, v_n * T.int64(196) + v_h // T.int64(2) * T.int64(14) + v_w // T.int64(2), v_h % T.int64(2), v_w % T.int64(2)], p2[v_n, v_co, T.int64(0), T.int64(0)])
                        T.writes(T_relu[v_n, v_co, v_h, v_w])
                        T_relu[v_n, v_co, v_h, v_w] = T.max(inverse_local[v_co, v_n * T.int64(196) + v_h // T.int64(2) * T.int64(14) + v_w // T.int64(2), v_h % T.int64(2), v_w % T.int64(2)] + p2[v_n, v_co, T.int64(0), T.int64(0)], T.float32(0))
b0 = sch.get_block(name="data_pack", func_name="main")
b1 = sch.get_block(name="bgemm", func_name="main")
b2 = sch.get_block(name="inverse", func_name="main")
b3 = sch.get_block(name="conv2d_winograd", func_name="main")
b4 = sch.get_block(name="T_add", func_name="main")
b5 = sch.get_block(name="T_relu", func_name="main")
b6 = sch.get_block(name="root", func_name="main")
b7, b8 = sch.get_producers(block=b2)
sch.compute_inline(block=b8)
b9, = sch.get_consumers(block=b2)
l10, l11, l12, l13 = sch.get_loops(block=b9)
l14, l15 = sch.split(loop=l12, factors=[None, 2], preserve_unit_iters=True)
l16, l17 = sch.split(loop=l13, factors=[None, 2], preserve_unit_iters=True)
sch.reorder(l14, l16, l15, l17)
sch.compute_at(block=b2, loop=l16, preserve_unit_loops=True, index=-1)
sch.set_scope(block=b2, buffer_index=0, storage_scope="local")
l18, l19, l20, l21, l22, l23, l24, l25, l26, l27 = sch.get_loops(block=b2)
sch.unroll(loop=l24)
sch.unroll(loop=l25)
sch.unroll(loop=l26)
sch.unroll(loop=l27)
b28, b29 = sch.get_producers(block=b0)
sch.compute_inline(block=b29)
b30, = sch.get_producers(block=b28)
l31, l32, l33, l34, l35, l36 = sch.get_loops(block=b0)
sch.reorder(l33, l34, l31, l32, l35, l36)
sch.unroll(loop=l31)
sch.unroll(loop=l32)
sch.unroll(loop=l35)
sch.unroll(loop=l36)
l37 = sch.fuse(l33, l34, preserve_unit_iters=True)
v38 = sch.sample_categorical(candidates=[32, 64, 128, 256, 512, 1024], probs=[0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666], decision=3)
l39, l40 = sch.split(loop=l37, factors=[None, v38], preserve_unit_iters=True)
sch.bind(loop=l39, thread_axis="blockIdx.x")
sch.bind(loop=l40, thread_axis="threadIdx.x")
b41 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b41, loop=l40, preserve_unit_loops=True, index=-1)
sch.compute_at(block=b28, loop=l40, preserve_unit_loops=True, index=-1)
sch.set_scope(block=b28, buffer_index=0, storage_scope="local")
sch.compute_inline(block=b30)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l42, l43, l44, l45, l46 = sch.get_loops(block=b1)
v47, v48, v49, v50, v51 = sch.sample_perfect_tile(loop=l42, n=5, max_innermost_factor=64, decision=[1, 1, 2, 1, 2])
l52, l53, l54, l55, l56 = sch.split(loop=l42, factors=[v47, v48, v49, v50, v51], preserve_unit_iters=True)
v57, v58, v59, v60, v61 = sch.sample_perfect_tile(loop=l43, n=5, max_innermost_factor=64, decision=[1, 1, 2, 1, 2])
l62, l63, l64, l65, l66 = sch.split(loop=l43, factors=[v57, v58, v59, v60, v61], preserve_unit_iters=True)
v67, v68, v69, v70, v71 = sch.sample_perfect_tile(loop=l44, n=5, max_innermost_factor=64, decision=[2, 2, 8, 2, 2])
l72, l73, l74, l75, l76 = sch.split(loop=l44, factors=[v67, v68, v69, v70, v71], preserve_unit_iters=True)
v77, v78, v79, v80, v81 = sch.sample_perfect_tile(loop=l45, n=5, max_innermost_factor=64, decision=[1, 14, 1, 2, 7])
l82, l83, l84, l85, l86 = sch.split(loop=l45, factors=[v77, v78, v79, v80, v81], preserve_unit_iters=True)
v87, v88, v89 = sch.sample_perfect_tile(loop=l46, n=3, max_innermost_factor=64, decision=[64, 2, 1])
l90, l91, l92 = sch.split(loop=l46, factors=[v87, v88, v89], preserve_unit_iters=True)
sch.reorder(l52, l62, l72, l82, l53, l63, l73, l83, l54, l64, l74, l84, l90, l91, l55, l65, l75, l85, l92, l56, l66, l76, l86)
l93 = sch.fuse(l52, l62, l72, l82, preserve_unit_iters=True)
sch.bind(loop=l93, thread_axis="blockIdx.x")
l94 = sch.fuse(l53, l63, l73, l83, preserve_unit_iters=True)
sch.bind(loop=l94, thread_axis="vthread.x")
l95 = sch.fuse(l54, l64, l74, l84, preserve_unit_iters=True)
sch.bind(loop=l95, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b96 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b96, loop=l95, preserve_unit_loops=True, index=-1)
b97 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b97, loop=l90, preserve_unit_loops=True, index=-1)
l98, l99, l100, l101, l102, l103, l104, l105 = sch.get_loops(block=b97)
l106 = sch.fuse(l102, l103, l104, l105, preserve_unit_iters=True)
v107 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b97, ann_key="meta_schedule.cooperative_fetch", ann_val=v107)
b108 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b108, loop=l90, preserve_unit_loops=True, index=-1)
l109, l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b108)
l117 = sch.fuse(l113, l114, l115, l116, preserve_unit_iters=True)
v118 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b108, ann_key="meta_schedule.cooperative_fetch", ann_val=v118)
sch.reverse_compute_inline(block=b5)
sch.reverse_compute_inline(block=b4)
v119 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b6, ann_key="meta_schedule.unroll_explicit", ann_val=v119)
l120, l121, l122, l123, l124, l125 = sch.get_loops(block=b3)
l126 = sch.fuse(l120, l121, l122, l123, preserve_unit_iters=True)
v127 = sch.sample_categorical(candidates=[32, 64, 128, 256, 512, 1024], probs=[0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666], decision=1)
l128, l129 = sch.split(loop=l126, factors=[None, v127], preserve_unit_iters=True)
sch.bind(loop=l128, thread_axis="blockIdx.x")
sch.bind(loop=l129, thread_axis="threadIdx.x")
sch.enter_postproc()
sch.unannotate(block_or_loop=b97, ann_key="meta_schedule.cooperative_fetch")
l130, l131, l132, l133, l134 = sch.get_loops(block=b97)
l135, l136, l137 = sch.split(loop=l134, factors=[None, 32, 4], preserve_unit_iters=True)
sch.vectorize(loop=l137)
sch.bind(loop=l136, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b108, ann_key="meta_schedule.cooperative_fetch")
l138, l139, l140, l141, l142 = sch.get_loops(block=b108)
l143, l144 = sch.split(loop=l142, factors=[None, 32], preserve_unit_iters=True)
sch.bind(loop=l144, thread_axis="threadIdx.x")
b145 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b145, ann_key="meta_schedule.unroll_explicit")
b146, b147, b148, b149, b150, b151, b152, b153, b154 = sch.get_child_blocks(b145)
l155, l156, l157, l158, l159, l160 = sch.get_loops(block=b146)
l161, l162, l163, l164, l165, l166 = sch.get_loops(block=b147)
sch.annotate(block_or_loop=l161, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l161, ann_key="pragma_unroll_explicit", ann_val=1)
l167, l168, l169, l170, l171, l172 = sch.get_loops(block=b148)
l173, l174, l175, l176, l177, l178, l179 = sch.get_loops(block=b149)
l180, l181, l182, l183, l184, l185 = sch.get_loops(block=b150)
l186, l187, l188, l189, l190, l191, l192, l193, l194, l195, l196, l197, l198, l199 = sch.get_loops(block=b151)
sch.annotate(block_or_loop=l186, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l186, ann_key="pragma_unroll_explicit", ann_val=1)
l200, l201, l202, l203, l204, l205, l206 = sch.get_loops(block=b152)
l207, l208, l209, l210, l211, l212, l213, l214 = sch.get_loops(block=b153)
sch.annotate(block_or_loop=l207, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l207, ann_key="pragma_unroll_explicit", ann_val=1)
l215, l216, l217, l218 = sch.get_loops(block=b154)
b219 = sch.get_block(name="data_pack", func_name="main")
l220, l221, l222, l223, l224, l225 = sch.get_loops(block=b219)
b226 = sch.decompose_reduction(block=b219, loop=l224)
b227 = sch.get_block(name="bgemm", func_name="main")
l228, l229, l230, l231, l232, l233, l234, l235, l236, l237, l238, l239, l240, l241 = sch.get_loops(block=b227)
b242 = sch.decompose_reduction(block=b227, loop=l231)
b243 = sch.get_block(name="inverse", func_name="main")
l244, l245, l246, l247, l248, l249, l250, l251 = sch.get_loops(block=b243)
b252 = sch.decompose_reduction(block=b243, loop=l250)
2023-11-11 00:25:34 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #20: GFLOPs: 401.7956. Time: 316.1947 us. Best GFLOPs: 3031.7599
2023-11-11 00:25:34 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #21: GFLOPs: 602.0612. Time: 211.0178 us. Best GFLOPs: 3031.7599
2023-11-11 00:25:34 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #22: GFLOPs: 456.5730. Time: 278.2592 us. Best GFLOPs: 3031.7599
2023-11-11 00:25:34 [INFO] [task_scheduler.cc:121] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #23: Error in building:
LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(128), T.int64(28), T.int64(28)), "float32"), p1: T.Buffer((T.int64(4), T.int64(4), T.int64(128), T.int64(128)), "float32"), p2: T.Buffer((T.int64(1), T.int64(128), T.int64(1), T.int64(1)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(128), T.int64(28), T.int64(28)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        input_tile_local = T.alloc_buffer((T.int64(128), T.int64(196), T.int64(4), T.int64(4)), scope="local")
        data_pack = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(128), T.int64(196)))
        bgemm = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(128), T.int64(196)))
        inverse_local = T.alloc_buffer((T.int64(128), T.int64(196), T.int64(2), T.int64(2)), scope="local")
        data_pack_local = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(128), T.int64(196)), scope="local")
        bgemm_local = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(128), T.int64(196)), scope="local")
        data_pack_shared = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(128), T.int64(196)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(128), T.int64(128)), scope="shared")
        for ci_p_fused_0 in T.thread_binding(T.int64(98), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 1024, "pragma_unroll_explicit": 1}):
            for ci_p_fused_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(1), T.int64(4), T.int64(4)):
                    with T.block("input_tile"):
                        v_ci = T.axis.spatial(T.int64(128), (ci_p_fused_0 * T.int64(256) + ci_p_fused_1) // T.int64(196) + ax0)
                        v_p = T.axis.spatial(T.int64(196), (ci_p_fused_0 * T.int64(60) + ci_p_fused_1) % T.int64(196) + ax1)
                        v_eps, v_nu = T.axis.remap("SS", [ax2, ax3])
                        T.reads(p0[v_p // T.int64(196), v_ci, v_p % T.int64(196) // T.int64(14) * T.int64(2) + v_eps - T.int64(1), v_p % T.int64(14) * T.int64(2) + v_nu - T.int64(1)])
                        T.writes(input_tile_local[v_ci, v_p, v_eps, v_nu])
                        T.block_attr({"schedule_rule": "None"})
                        input_tile_local[v_ci, v_p, v_eps, v_nu] = T.if_then_else(T.int64(1) <= v_p % T.int64(196) // T.int64(14) * T.int64(2) + v_eps and v_p % T.int64(196) // T.int64(14) * T.int64(2) + v_eps < T.int64(29) and T.int64(1) <= v_p % T.int64(14) * T.int64(2) + v_nu and v_p % T.int64(14) * T.int64(2) + v_nu < T.int64(29), p0[v_p // T.int64(196), v_ci, v_p % T.int64(196) // T.int64(14) * T.int64(2) + v_eps - T.int64(1), v_p % T.int64(14) * T.int64(2) + v_nu - T.int64(1)], T.float32(0))
                for eps in T.unroll(T.int64(4)):
                    for nu in T.unroll(T.int64(4)):
                        with T.block("data_pack_init"):
                            v_eps, v_nu = T.axis.remap("SS", [eps, nu])
                            v_ci = T.axis.spatial(T.int64(128), (ci_p_fused_0 * T.int64(256) + ci_p_fused_1) // T.int64(196))
                            v_p = T.axis.spatial(T.int64(196), (ci_p_fused_0 * T.int64(256) + ci_p_fused_1) % T.int64(196))
                            T.reads()
                            T.writes(data_pack_local[v_eps, v_nu, v_ci, v_p])
                            T.block_attr({"schedule_rule": "conv2d_nchw_winograd_data_pack"})
                            data_pack_local[v_eps, v_nu, v_ci, v_p] = T.float32(0)
                        for r_a in T.unroll(T.int64(4)):
                            for r_b in T.unroll(T.int64(4)):
                                with T.block("data_pack_update"):
                                    v_eps, v_nu = T.axis.remap("SS", [eps, nu])
                                    v_ci = T.axis.spatial(T.int64(128), (ci_p_fused_0 * T.int64(256) + ci_p_fused_1) // T.int64(196))
                                    v_p = T.axis.spatial(T.int64(196), (ci_p_fused_0 * T.int64(256) + ci_p_fused_1) % T.int64(196))
                                    v_r_a, v_r_b = T.axis.remap("RR", [r_a, r_b])
                                    T.reads(data_pack_local[v_eps, v_nu, v_ci, v_p], input_tile_local[v_ci, v_p, v_r_a, v_r_b])
                                    T.writes(data_pack_local[v_eps, v_nu, v_ci, v_p])
                                    T.block_attr({"schedule_rule": "conv2d_nchw_winograd_data_pack"})
                                    data_pack_local[v_eps, v_nu, v_ci, v_p] = data_pack_local[v_eps, v_nu, v_ci, v_p] + input_tile_local[v_ci, v_p, v_r_a, v_r_b] * T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(3), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(1), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(0), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(3), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(1), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(0), T.float32(1), T.float32(0))))))))))))))))) * T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(3), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(1), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(0), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(3), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(1), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(0), T.float32(1), T.float32(0)))))))))))))))))
                for ax0, ax1, ax2, ax3 in T.grid(T.int64(4), T.int64(4), T.int64(1), T.int64(1)):
                    with T.block("data_pack_local"):
                        v0, v1 = T.axis.remap("SS", [ax0, ax1])
                        v2 = T.axis.spatial(T.int64(128), (ci_p_fused_0 * T.int64(256) + ci_p_fused_1) // T.int64(196) + ax2)
                        v3 = T.axis.spatial(T.int64(196), (ci_p_fused_0 * T.int64(60) + ci_p_fused_1) % T.int64(196) + ax3)
                        T.reads(data_pack_local[v0, v1, v2, v3])
                        T.writes(data_pack[v0, v1, v2, v3])
                        data_pack[v0, v1, v2, v3] = data_pack_local[v0, v1, v2, v3]
        for eps_0_nu_0_co_0_p_0_fused in T.thread_binding(T.int64(2), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 1024, "pragma_unroll_explicit": 1}):
            for eps_1_nu_1_co_1_p_1_fused in T.thread_binding(T.int64(28), thread="vthread.x"):
                for eps_2_nu_2_co_2_p_2_fused in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                    for eps_3_init, nu_3_init, co_3_init, p_3_init, eps_4_init, nu_4_init, co_4_init, p_4_init in T.grid(T.int64(1), T.int64(1), T.int64(2), T.int64(2), T.int64(2), T.int64(2), T.int64(2), T.int64(7)):
                        with T.block("bgemm_init"):
                            v_eps = T.axis.spatial(T.int64(4), eps_2_nu_2_co_2_p_2_fused // T.int64(16) * T.int64(2) + eps_3_init * T.int64(2) + eps_4_init)
                            v_nu = T.axis.spatial(T.int64(4), eps_2_nu_2_co_2_p_2_fused % T.int64(16) // T.int64(8) * T.int64(2) + nu_3_init * T.int64(2) + nu_4_init)
                            v_co = T.axis.spatial(T.int64(128), eps_0_nu_0_co_0_p_0_fused * T.int64(64) + eps_1_nu_1_co_1_p_1_fused // T.int64(14) * T.int64(32) + eps_2_nu_2_co_2_p_2_fused % T.int64(8) * T.int64(4) + co_3_init * T.int64(2) + co_4_init)
                            v_p = T.axis.spatial(T.int64(196), eps_1_nu_1_co_1_p_1_fused % T.int64(14) * T.int64(14) + p_3_init * T.int64(7) + p_4_init)
                            T.reads()
                            T.writes(bgemm_local[v_eps, v_nu, v_co, v_p])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            bgemm_local[v_eps, v_nu, v_co, v_p] = T.float32(0)
                    for ci_0 in range(T.int64(64)):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(98)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("data_pack_shared"):
                                        v0 = T.axis.spatial(T.int64(4), (ax0_ax1_ax2_ax3_fused_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) // T.int64(1568))
                                        v1 = T.axis.spatial(T.int64(4), (ax0_ax1_ax2_ax3_fused_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) % T.int64(1568) // T.int64(392))
                                        v2 = T.axis.spatial(T.int64(128), ci_0 * T.int64(2) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) % T.int64(392) // T.int64(196))
                                        v3 = T.axis.spatial(T.int64(196), (ax0_ax1_ax2_ax3_fused_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) % T.int64(196))
                                        T.reads(data_pack[v0, v1, v2, v3])
                                        T.writes(data_pack_shared[v0, v1, v2, v3])
                                        data_pack_shared[v0, v1, v2, v3] = data_pack[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(16)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("p1_shared"):
                                        v0 = T.axis.spatial(T.int64(4), (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) // T.int64(512))
                                        v1 = T.axis.spatial(T.int64(4), (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) % T.int64(512) // T.int64(128))
                                        v2 = T.axis.spatial(T.int64(128), ci_0 * T.int64(2) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) % T.int64(128) // T.int64(64))
                                        v3 = T.axis.spatial(T.int64(128), eps_0_nu_0_co_0_p_0_fused * T.int64(64) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) % T.int64(64))
                                        T.reads(p1[v0, v1, v2, v3])
                                        T.writes(p1_shared[v0, v1, v2, v3])
                                        p1_shared[v0, v1, v2, v3] = p1[v0, v1, v2, v3]
                        for ci_1, eps_3, nu_3, co_3, p_3, ci_2, eps_4, nu_4, co_4, p_4 in T.grid(T.int64(2), T.int64(1), T.int64(1), T.int64(2), T.int64(2), T.int64(1), T.int64(2), T.int64(2), T.int64(2), T.int64(7)):
                            with T.block("bgemm_update"):
                                v_eps = T.axis.spatial(T.int64(4), eps_2_nu_2_co_2_p_2_fused // T.int64(16) * T.int64(2) + eps_3 * T.int64(2) + eps_4)
                                v_nu = T.axis.spatial(T.int64(4), eps_2_nu_2_co_2_p_2_fused % T.int64(16) // T.int64(8) * T.int64(2) + nu_3 * T.int64(2) + nu_4)
                                v_co = T.axis.spatial(T.int64(128), eps_0_nu_0_co_0_p_0_fused * T.int64(64) + eps_1_nu_1_co_1_p_1_fused // T.int64(14) * T.int64(32) + eps_2_nu_2_co_2_p_2_fused % T.int64(8) * T.int64(4) + co_3 * T.int64(2) + co_4)
                                v_p = T.axis.spatial(T.int64(196), eps_1_nu_1_co_1_p_1_fused % T.int64(14) * T.int64(14) + p_3 * T.int64(7) + p_4)
                                v_ci = T.axis.reduce(T.int64(128), ci_0 * T.int64(2) + ci_1 + ci_2)
                                T.reads(bgemm_local[v_eps, v_nu, v_co, v_p], data_pack_shared[v_eps, v_nu, v_ci, v_p], p1_shared[v_eps, v_nu, v_ci, v_co])
                                T.writes(bgemm_local[v_eps, v_nu, v_co, v_p])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                bgemm_local[v_eps, v_nu, v_co, v_p] = bgemm_local[v_eps, v_nu, v_co, v_p] + data_pack_shared[v_eps, v_nu, v_ci, v_p] * p1_shared[v_eps, v_nu, v_ci, v_co]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(2), T.int64(2), T.int64(4), T.int64(14)):
                        with T.block("bgemm_local"):
                            v0 = T.axis.spatial(T.int64(4), eps_2_nu_2_co_2_p_2_fused // T.int64(16) * T.int64(2) + ax0)
                            v1 = T.axis.spatial(T.int64(4), eps_2_nu_2_co_2_p_2_fused % T.int64(16) // T.int64(8) * T.int64(2) + ax1)
                            v2 = T.axis.spatial(T.int64(128), eps_0_nu_0_co_0_p_0_fused * T.int64(64) + eps_1_nu_1_co_1_p_1_fused // T.int64(14) * T.int64(32) + eps_2_nu_2_co_2_p_2_fused % T.int64(8) * T.int64(4) + ax2)
                            v3 = T.axis.spatial(T.int64(196), eps_1_nu_1_co_1_p_1_fused % T.int64(14) * T.int64(14) + ax3)
                            T.reads(bgemm_local[v0, v1, v2, v3])
                            T.writes(bgemm[v0, v1, v2, v3])
                            bgemm[v0, v1, v2, v3] = bgemm_local[v0, v1, v2, v3]
        for n_co_h_0_w_0_fused_0 in T.thread_binding(T.int64(392), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 1024, "pragma_unroll_explicit": 1}):
            for n_co_h_0_w_0_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                    for ax2 in T.unroll(T.int64(2)):
                        for ax3 in T.unroll(T.int64(2)):
                            with T.block("inverse_init"):
                                v_co = T.axis.spatial(T.int64(128), (n_co_h_0_w_0_fused_0 * T.int64(64) + n_co_h_0_w_0_fused_1) // T.int64(196) + ax0)
                                v_p = T.axis.spatial(T.int64(196), (n_co_h_0_w_0_fused_0 * T.int64(64) + n_co_h_0_w_0_fused_1) % T.int64(196) + ax1)
                                v_vh, v_vw = T.axis.remap("SS", [ax2, ax3])
                                T.reads()
                                T.writes(inverse_local[v_co, v_p, v_vh, v_vw])
                                T.block_attr({"schedule_rule": "conv2d_nchw_winograd_inverse"})
                                inverse_local[v_co, v_p, v_vh, v_vw] = T.float32(0)
                            for ax4 in T.unroll(T.int64(4)):
                                for ax5 in T.unroll(T.int64(4)):
                                    with T.block("inverse_update"):
                                        v_co = T.axis.spatial(T.int64(128), (n_co_h_0_w_0_fused_0 * T.int64(64) + n_co_h_0_w_0_fused_1) // T.int64(196) + ax0)
                                        v_p = T.axis.spatial(T.int64(196), (n_co_h_0_w_0_fused_0 * T.int64(64) + n_co_h_0_w_0_fused_1) % T.int64(196) + ax1)
                                        v_vh, v_vw, v_r_a, v_r_b = T.axis.remap("SSRR", [ax2, ax3, ax4, ax5])
                                        T.reads(inverse_local[v_co, v_p, v_vh, v_vw], bgemm[v_r_a, v_r_b, v_co, v_p])
                                        T.writes(inverse_local[v_co, v_p, v_vh, v_vw])
                                        T.block_attr({"schedule_rule": "conv2d_nchw_winograd_inverse"})
                                        inverse_local[v_co, v_p, v_vh, v_vw] = inverse_local[v_co, v_p, v_vh, v_vw] + bgemm[v_r_a, v_r_b, v_co, v_p] * T.Select(v_r_a % T.int64(4) == T.int64(3) and v_vh % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_vh % T.int64(2) == T.int64(0), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_vh % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_vh % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_vh % T.int64(2) == T.int64(1), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_vh % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_vh % T.int64(2) == T.int64(1), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_vh % T.int64(2) == T.int64(0), T.float32(1), T.float32(0))))))))) * T.Select(v_r_b % T.int64(4) == T.int64(3) and v_vw % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_vw % T.int64(2) == T.int64(0), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_vw % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_vw % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_vw % T.int64(2) == T.int64(1), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_vw % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_vw % T.int64(2) == T.int64(1), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_vw % T.int64(2) == T.int64(0), T.float32(1), T.float32(0)))))))))
                for h_1, w_1 in T.grid(T.int64(2), T.int64(2)):
                    with T.block("conv2d_winograd"):
                        v_n = T.axis.spatial(T.int64(1), T.int64(0))
                        v_co = T.axis.spatial(T.int64(128), (n_co_h_0_w_0_fused_0 * T.int64(64) + n_co_h_0_w_0_fused_1) // T.int64(196))
                        v_h = T.axis.spatial(T.int64(28), (n_co_h_0_w_0_fused_0 * T.int64(64) + n_co_h_0_w_0_fused_1) % T.int64(196) // T.int64(14) * T.int64(2) + h_1)
                        v_w = T.axis.spatial(T.int64(28), (n_co_h_0_w_0_fused_0 * T.int64(64) + n_co_h_0_w_0_fused_1) % T.int64(14) * T.int64(2) + w_1)
                        T.reads(inverse_local[v_co, v_n * T.int64(196) + v_h // T.int64(2) * T.int64(14) + v_w // T.int64(2), v_h % T.int64(2), v_w % T.int64(2)], p2[v_n, v_co, T.int64(0), T.int64(0)])
                        T.writes(T_relu[v_n, v_co, v_h, v_w])
                        T_relu[v_n, v_co, v_h, v_w] = T.max(inverse_local[v_co, v_n * T.int64(196) + v_h // T.int64(2) * T.int64(14) + v_w // T.int64(2), v_h % T.int64(2), v_w % T.int64(2)] + p2[v_n, v_co, T.int64(0), T.int64(0)], T.float32(0))
b0 = sch.get_block(name="data_pack", func_name="main")
b1 = sch.get_block(name="bgemm", func_name="main")
b2 = sch.get_block(name="inverse", func_name="main")
b3 = sch.get_block(name="conv2d_winograd", func_name="main")
b4 = sch.get_block(name="T_add", func_name="main")
b5 = sch.get_block(name="T_relu", func_name="main")
b6 = sch.get_block(name="root", func_name="main")
b7, b8 = sch.get_producers(block=b2)
sch.compute_inline(block=b8)
b9, = sch.get_consumers(block=b2)
l10, l11, l12, l13 = sch.get_loops(block=b9)
l14, l15 = sch.split(loop=l12, factors=[None, 2], preserve_unit_iters=True)
l16, l17 = sch.split(loop=l13, factors=[None, 2], preserve_unit_iters=True)
sch.reorder(l14, l16, l15, l17)
sch.compute_at(block=b2, loop=l16, preserve_unit_loops=True, index=-1)
sch.set_scope(block=b2, buffer_index=0, storage_scope="local")
l18, l19, l20, l21, l22, l23, l24, l25, l26, l27 = sch.get_loops(block=b2)
sch.unroll(loop=l24)
sch.unroll(loop=l25)
sch.unroll(loop=l26)
sch.unroll(loop=l27)
b28, b29 = sch.get_producers(block=b0)
sch.compute_inline(block=b29)
b30, = sch.get_producers(block=b28)
l31, l32, l33, l34, l35, l36 = sch.get_loops(block=b0)
sch.reorder(l33, l34, l31, l32, l35, l36)
sch.unroll(loop=l31)
sch.unroll(loop=l32)
sch.unroll(loop=l35)
sch.unroll(loop=l36)
l37 = sch.fuse(l33, l34, preserve_unit_iters=True)
v38 = sch.sample_categorical(candidates=[32, 64, 128, 256, 512, 1024], probs=[0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666], decision=3)
l39, l40 = sch.split(loop=l37, factors=[None, v38], preserve_unit_iters=True)
sch.bind(loop=l39, thread_axis="blockIdx.x")
sch.bind(loop=l40, thread_axis="threadIdx.x")
b41 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b41, loop=l40, preserve_unit_loops=True, index=-1)
sch.compute_at(block=b28, loop=l40, preserve_unit_loops=True, index=-1)
sch.set_scope(block=b28, buffer_index=0, storage_scope="local")
sch.compute_inline(block=b30)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l42, l43, l44, l45, l46 = sch.get_loops(block=b1)
v47, v48, v49, v50, v51 = sch.sample_perfect_tile(loop=l42, n=5, max_innermost_factor=64, decision=[1, 1, 2, 1, 2])
l52, l53, l54, l55, l56 = sch.split(loop=l42, factors=[v47, v48, v49, v50, v51], preserve_unit_iters=True)
v57, v58, v59, v60, v61 = sch.sample_perfect_tile(loop=l43, n=5, max_innermost_factor=64, decision=[1, 1, 2, 1, 2])
l62, l63, l64, l65, l66 = sch.split(loop=l43, factors=[v57, v58, v59, v60, v61], preserve_unit_iters=True)
v67, v68, v69, v70, v71 = sch.sample_perfect_tile(loop=l44, n=5, max_innermost_factor=64, decision=[2, 2, 8, 2, 2])
l72, l73, l74, l75, l76 = sch.split(loop=l44, factors=[v67, v68, v69, v70, v71], preserve_unit_iters=True)
v77, v78, v79, v80, v81 = sch.sample_perfect_tile(loop=l45, n=5, max_innermost_factor=64, decision=[1, 14, 1, 2, 7])
l82, l83, l84, l85, l86 = sch.split(loop=l45, factors=[v77, v78, v79, v80, v81], preserve_unit_iters=True)
v87, v88, v89 = sch.sample_perfect_tile(loop=l46, n=3, max_innermost_factor=64, decision=[64, 2, 1])
l90, l91, l92 = sch.split(loop=l46, factors=[v87, v88, v89], preserve_unit_iters=True)
sch.reorder(l52, l62, l72, l82, l53, l63, l73, l83, l54, l64, l74, l84, l90, l91, l55, l65, l75, l85, l92, l56, l66, l76, l86)
l93 = sch.fuse(l52, l62, l72, l82, preserve_unit_iters=True)
sch.bind(loop=l93, thread_axis="blockIdx.x")
l94 = sch.fuse(l53, l63, l73, l83, preserve_unit_iters=True)
sch.bind(loop=l94, thread_axis="vthread.x")
l95 = sch.fuse(l54, l64, l74, l84, preserve_unit_iters=True)
sch.bind(loop=l95, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b96 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b96, loop=l95, preserve_unit_loops=True, index=-1)
b97 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b97, loop=l90, preserve_unit_loops=True, index=-1)
l98, l99, l100, l101, l102, l103, l104, l105 = sch.get_loops(block=b97)
l106 = sch.fuse(l102, l103, l104, l105, preserve_unit_iters=True)
v107 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b97, ann_key="meta_schedule.cooperative_fetch", ann_val=v107)
b108 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b108, loop=l90, preserve_unit_loops=True, index=-1)
l109, l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b108)
l117 = sch.fuse(l113, l114, l115, l116, preserve_unit_iters=True)
v118 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b108, ann_key="meta_schedule.cooperative_fetch", ann_val=v118)
sch.reverse_compute_inline(block=b5)
sch.reverse_compute_inline(block=b4)
v119 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b6, ann_key="meta_schedule.unroll_explicit", ann_val=v119)
l120, l121, l122, l123, l124, l125 = sch.get_loops(block=b3)
l126 = sch.fuse(l120, l121, l122, l123, preserve_unit_iters=True)
v127 = sch.sample_categorical(candidates=[32, 64, 128, 256, 512, 1024], probs=[0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666], decision=1)
l128, l129 = sch.split(loop=l126, factors=[None, v127], preserve_unit_iters=True)
sch.bind(loop=l128, thread_axis="blockIdx.x")
sch.bind(loop=l129, thread_axis="threadIdx.x")
sch.enter_postproc()
sch.unannotate(block_or_loop=b97, ann_key="meta_schedule.cooperative_fetch")
l130, l131, l132, l133, l134 = sch.get_loops(block=b97)
l135, l136, l137 = sch.split(loop=l134, factors=[None, 32, 2], preserve_unit_iters=True)
sch.vectorize(loop=l137)
sch.bind(loop=l136, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b108, ann_key="meta_schedule.cooperative_fetch")
l138, l139, l140, l141, l142 = sch.get_loops(block=b108)
l143, l144, l145 = sch.split(loop=l142, factors=[None, 32, 4], preserve_unit_iters=True)
sch.vectorize(loop=l145)
sch.bind(loop=l144, thread_axis="threadIdx.x")
b146 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b146, ann_key="meta_schedule.unroll_explicit")
b147, b148, b149, b150, b151, b152, b153, b154, b155 = sch.get_child_blocks(b146)
l156, l157, l158, l159, l160, l161 = sch.get_loops(block=b147)
l162, l163, l164, l165, l166, l167 = sch.get_loops(block=b148)
sch.annotate(block_or_loop=l162, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l162, ann_key="pragma_unroll_explicit", ann_val=1)
l168, l169, l170, l171, l172, l173 = sch.get_loops(block=b149)
l174, l175, l176, l177, l178, l179, l180 = sch.get_loops(block=b150)
l181, l182, l183, l184, l185, l186, l187 = sch.get_loops(block=b151)
l188, l189, l190, l191, l192, l193, l194, l195, l196, l197, l198, l199, l200, l201 = sch.get_loops(block=b152)
sch.annotate(block_or_loop=l188, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l188, ann_key="pragma_unroll_explicit", ann_val=1)
l202, l203, l204, l205, l206, l207, l208 = sch.get_loops(block=b153)
l209, l210, l211, l212, l213, l214, l215, l216 = sch.get_loops(block=b154)
sch.annotate(block_or_loop=l209, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l209, ann_key="pragma_unroll_explicit", ann_val=1)
l217, l218, l219, l220 = sch.get_loops(block=b155)
b221 = sch.get_block(name="data_pack", func_name="main")
l222, l223, l224, l225, l226, l227 = sch.get_loops(block=b221)
b228 = sch.decompose_reduction(block=b221, loop=l226)
b229 = sch.get_block(name="bgemm", func_name="main")
l230, l231, l232, l233, l234, l235, l236, l237, l238, l239, l240, l241, l242, l243 = sch.get_loops(block=b229)
b244 = sch.decompose_reduction(block=b229, loop=l233)
b245 = sch.get_block(name="inverse", func_name="main")
l246, l247, l248, l249, l250, l251, l252, l253 = sch.get_loops(block=b245)
b254 = sch.decompose_reduction(block=b245, loop=l252)
2023-11-11 00:25:34 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #24: GFLOPs: 35.8232. Time: 3546.4651 us. Best GFLOPs: 3031.7599
2023-11-11 00:25:34 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #25: GFLOPs: 1042.3881. Time: 121.8794 us. Best GFLOPs: 3031.7599
2023-11-11 00:25:34 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #26: GFLOPs: 2263.4551. Time: 56.1291 us. Best GFLOPs: 3031.7599
2023-11-11 00:25:34 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #27: GFLOPs: 149.6078. Time: 849.1911 us. Best GFLOPs: 3031.7599
2023-11-11 00:25:34 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #28: GFLOPs: 1065.0126. Time: 119.2903 us. Best GFLOPs: 3031.7599
2023-11-11 00:25:34 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #29: GFLOPs: 926.9823. Time: 137.0529 us. Best GFLOPs: 3031.7599
2023-11-11 00:25:34 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #30: GFLOPs: 2480.7798. Time: 51.2120 us. Best GFLOPs: 3031.7599
2023-11-11 00:25:34 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #31: GFLOPs: 100.6086. Time: 1262.7712 us. Best GFLOPs: 3031.7599
2023-11-11 00:25:34 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #32: GFLOPs: 78.6855. Time: 1614.6003 us. Best GFLOPs: 3031.7599
2023-11-11 00:25:34 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #33: GFLOPs: 36.7432. Time: 3457.6595 us. Best GFLOPs: 3031.7599
2023-11-11 00:25:34 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #34: GFLOPs: 1796.4728. Time: 70.7195 us. Best GFLOPs: 3031.7599
2023-11-11 00:25:34 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #35: GFLOPs: 12.6728. Time: 10025.0625 us. Best GFLOPs: 3031.7599
2023-11-11 00:25:34 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #36: GFLOPs: 1705.2642. Time: 74.5020 us. Best GFLOPs: 3031.7599
2023-11-11 00:25:34 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #37: GFLOPs: 255.1588. Time: 497.9081 us. Best GFLOPs: 3031.7599
2023-11-11 00:25:34 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #38: GFLOPs: 1978.1291. Time: 64.2251 us. Best GFLOPs: 3031.7599
2023-11-11 00:25:34 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #39: GFLOPs: 5.3410. Time: 23786.7004 us. Best GFLOPs: 3031.7599
2023-11-11 00:25:34 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #40: GFLOPs: 1969.5891. Time: 64.5036 us. Best GFLOPs: 3031.7599
2023-11-11 00:25:34 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #41: GFLOPs: 238.5655. Time: 532.5399 us. Best GFLOPs: 3031.7599
2023-11-11 00:25:34 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #42: GFLOPs: 158.0951. Time: 803.6024 us. Best GFLOPs: 3031.7599
2023-11-11 00:25:34 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #43: GFLOPs: 23.4851. Time: 5409.6302 us. Best GFLOPs: 3031.7599
2023-11-11 00:25:34 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #44: GFLOPs: 1776.8586. Time: 71.5001 us. Best GFLOPs: 3031.7599
2023-11-11 00:25:34 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #45: GFLOPs: 55.1291. Time: 2304.5120 us. Best GFLOPs: 3031.7599
2023-11-11 00:25:34 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #46: GFLOPs: 1152.0572. Time: 110.2772 us. Best GFLOPs: 3031.7599
2023-11-11 00:25:34 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #47: GFLOPs: 22.9635. Time: 5532.5104 us. Best GFLOPs: 3031.7599
2023-11-11 00:25:34 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #48: GFLOPs: 83.9695. Time: 1512.9982 us. Best GFLOPs: 3031.7599
2023-11-11 00:25:34 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #49: GFLOPs: 510.8928. Time: 248.6737 us. Best GFLOPs: 3031.7599
2023-11-11 00:25:34 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #50: GFLOPs: 1072.1171. Time: 118.4998 us. Best GFLOPs: 3031.7599
2023-11-11 00:25:34 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #51: GFLOPs: 1937.4232. Time: 65.5745 us. Best GFLOPs: 3031.7599
2023-11-11 00:25:34 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #52: GFLOPs: 1847.8263. Time: 68.7541 us. Best GFLOPs: 3031.7599
2023-11-11 00:25:34 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #53: GFLOPs: 1269.6658. Time: 100.0623 us. Best GFLOPs: 3031.7599
2023-11-11 00:25:34 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #54: GFLOPs: 2403.8028. Time: 52.8519 us. Best GFLOPs: 3031.7599
2023-11-11 00:25:34 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #55: GFLOPs: 47.9188. Time: 2651.2708 us. Best GFLOPs: 3031.7599
2023-11-11 00:25:34 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #56: GFLOPs: 1306.4716. Time: 97.2433 us. Best GFLOPs: 3031.7599
2023-11-11 00:25:34 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #57: GFLOPs: 1459.2016. Time: 87.0652 us. Best GFLOPs: 3031.7599
2023-11-11 00:25:34 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #58: GFLOPs: 394.3564. Time: 322.1594 us. Best GFLOPs: 3031.7599
2023-11-11 00:25:34 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #59: GFLOPs: 2.7064. Time: 46942.2097 us. Best GFLOPs: 3031.7599
2023-11-11 00:25:34 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #60: GFLOPs: 1022.8602. Time: 124.2063 us. Best GFLOPs: 3031.7599
2023-11-11 00:25:34 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #61: GFLOPs: 1944.7706. Time: 65.3268 us. Best GFLOPs: 3031.7599
2023-11-11 00:25:34 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #62: GFLOPs: 2560.6503. Time: 49.6146 us. Best GFLOPs: 3031.7599
2023-11-11 00:25:34 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #63: GFLOPs: 1701.6998. Time: 74.6581 us. Best GFLOPs: 3031.7599
2023-11-11 00:25:34 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #64: GFLOPs: 248.0636. Time: 512.1495 us. Best GFLOPs: 3031.7599
2023-11-11 00:56:20 [INFO] [evolutionary_search.cc:713] Generating candidates......
2023-11-11 00:56:22 [INFO] [evolutionary_search.cc:715] Picked top 62 candidate(s) from database
2023-11-11 00:56:29 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310a03fb48)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310e893e78)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310e893578)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310ec9ace8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56310e986408)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310eb1cfb8)]: 433 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310dc6f7b8)]: 0 failure(s)
2023-11-11 00:56:37 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310a03fb48)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310e893e78)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310e893578)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310ec9ace8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56310e986408)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310eb1cfb8)]: 868 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310dc6f7b8)]: 0 failure(s)
2023-11-11 00:56:45 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310a03fb48)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310e893e78)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310e893578)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310ec9ace8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56310e986408)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310eb1cfb8)]: 1295 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310dc6f7b8)]: 0 failure(s)
2023-11-11 00:56:45 [INFO] [evolutionary_search.cc:723] Sampled 55 candidate(s)
2023-11-11 00:57:04 [INFO] [evolutionary_search.cc:621] Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310a03fb48)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310e893e78)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310e893578)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310ec9ace8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56310e986408)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310eb1cfb8)]: 101 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310dc6f7b8)]: 0 failure(s)
2023-11-11 00:57:28 [INFO] [evolutionary_search.cc:621] Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310a03fb48)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310e893e78)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310e893578)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310ec9ace8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56310e986408)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310eb1cfb8)]: 89 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310dc6f7b8)]: 0 failure(s)
2023-11-11 00:57:51 [INFO] [evolutionary_search.cc:621] Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310a03fb48)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310e893e78)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310e893578)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310ec9ace8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56310e986408)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310eb1cfb8)]: 99 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310dc6f7b8)]: 0 failure(s)
2023-11-11 00:58:14 [INFO] [evolutionary_search.cc:621] Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310a03fb48)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310e893e78)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310e893578)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310ec9ace8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56310e986408)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310eb1cfb8)]: 69 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310dc6f7b8)]: 0 failure(s)
2023-11-11 00:58:21 [INFO] [evolutionary_search.cc:649] Scores of the best 64 candidates:
[1 : 16]:	1.4468  1.4328  1.3850  1.3208  1.3184  1.3091  1.2982  1.2760  1.2700  1.2688  1.2653  1.2613  1.2502  1.2496  1.2453  1.2413
[17 : 32]:	1.2378  1.2376  1.2346  1.2304  1.2262  1.2257  1.2218  1.2204  1.2085  1.2066  1.2044  1.2009  1.1920  1.1920  1.1915  1.1879
[33 : 48]:	1.1878  1.1847  1.1784  1.1723  1.1649  1.1617  1.1587  1.1563  1.1547  1.1532  1.1448  1.1408  1.1393  1.1315  1.1267  1.1227
[49 : 64]:	1.1215  1.1214  1.1181  1.1136  1.1115  1.1076  1.1009  1.0987  1.0967  1.0958  1.0925  1.0911  1.0883  1.0870  1.0782  1.0772
2023-11-11 00:58:21 [INFO] [evolutionary_search.cc:727] Got 64 candidate(s) with evolutionary search
2023-11-11 00:58:21 [INFO] [evolutionary_search.cc:730] Sending 64 candidates(s) for measurement
2023-11-11 00:58:55 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #65: GFLOPs: 2033.9938. Time: 62.4612 us. Best GFLOPs: 3031.7599
2023-11-11 00:58:55 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #66: GFLOPs: 1525.2628. Time: 83.2943 us. Best GFLOPs: 3031.7599
2023-11-11 00:58:55 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #67: GFLOPs: 2034.7175. Time: 62.4390 us. Best GFLOPs: 3031.7599
2023-11-11 00:58:55 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #68: GFLOPs: 526.7019. Time: 241.2098 us. Best GFLOPs: 3031.7599
2023-11-11 00:58:55 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #69: GFLOPs: 1603.5707. Time: 79.2267 us. Best GFLOPs: 3031.7599
2023-11-11 00:58:55 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #70: GFLOPs: 2574.6024. Time: 49.3457 us. Best GFLOPs: 3031.7599
2023-11-11 00:58:55 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #71: GFLOPs: 1603.6016. Time: 79.2252 us. Best GFLOPs: 3031.7599
2023-11-11 00:58:55 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #72: GFLOPs: 2356.2517. Time: 53.9185 us. Best GFLOPs: 3031.7599
2023-11-11 00:58:55 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #73: GFLOPs: 1574.5341. Time: 80.6878 us. Best GFLOPs: 3031.7599
2023-11-11 00:58:55 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #74: GFLOPs: 2289.0895. Time: 55.5005 us. Best GFLOPs: 3031.7599
2023-11-11 00:58:55 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #75: GFLOPs: 2285.0006. Time: 55.5998 us. Best GFLOPs: 3031.7599
2023-11-11 00:58:55 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #76: GFLOPs: 2289.6397. Time: 55.4872 us. Best GFLOPs: 3031.7599
2023-11-11 00:58:55 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #77: GFLOPs: 1067.7886. Time: 118.9801 us. Best GFLOPs: 3031.7599
2023-11-11 00:58:55 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #78: GFLOPs: 920.6582. Time: 137.9943 us. Best GFLOPs: 3031.7599
2023-11-11 00:58:55 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #79: GFLOPs: 1525.7728. Time: 83.2664 us. Best GFLOPs: 3031.7599
2023-11-11 00:58:55 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #80: GFLOPs: 2417.3379. Time: 52.5560 us. Best GFLOPs: 3031.7599
2023-11-11 00:58:55 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #81: GFLOPs: 2248.4674. Time: 56.5032 us. Best GFLOPs: 3031.7599
2023-11-11 00:58:55 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #82: GFLOPs: 2210.3429. Time: 57.4778 us. Best GFLOPs: 3031.7599
2023-11-11 00:58:55 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #83: GFLOPs: 2275.3439. Time: 55.8358 us. Best GFLOPs: 3031.7599
2023-11-11 00:58:55 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #84: GFLOPs: 2406.6497. Time: 52.7894 us. Best GFLOPs: 3031.7599
2023-11-11 00:58:55 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #85: GFLOPs: 2123.7715. Time: 59.8208 us. Best GFLOPs: 3031.7599
2023-11-11 00:58:55 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #86: GFLOPs: 2335.6619. Time: 54.3938 us. Best GFLOPs: 3031.7599
2023-11-11 00:58:55 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #87: GFLOPs: 2338.1133. Time: 54.3368 us. Best GFLOPs: 3031.7599
2023-11-11 00:58:55 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #88: GFLOPs: 809.6954. Time: 156.9055 us. Best GFLOPs: 3031.7599
2023-11-11 00:58:55 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #89: GFLOPs: 1751.6740. Time: 72.5281 us. Best GFLOPs: 3031.7599
2023-11-11 00:58:55 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #90: GFLOPs: 934.9050. Time: 135.8915 us. Best GFLOPs: 3031.7599
2023-11-11 00:58:55 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #91: GFLOPs: 1629.8287. Time: 77.9503 us. Best GFLOPs: 3031.7599
2023-11-11 00:58:55 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #92: GFLOPs: 2262.9432. Time: 56.1418 us. Best GFLOPs: 3031.7599
2023-11-11 00:58:55 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #93: GFLOPs: 896.3924. Time: 141.7299 us. Best GFLOPs: 3031.7599
2023-11-11 00:58:55 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #94: GFLOPs: 912.4393. Time: 139.2373 us. Best GFLOPs: 3031.7599
2023-11-11 00:58:55 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #95: GFLOPs: 2111.3659. Time: 60.1722 us. Best GFLOPs: 3031.7599
2023-11-11 00:58:55 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #96: GFLOPs: 2366.0626. Time: 53.6950 us. Best GFLOPs: 3031.7599
2023-11-11 00:58:55 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #97: GFLOPs: 2381.7198. Time: 53.3420 us. Best GFLOPs: 3031.7599
2023-11-11 00:58:55 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #98: GFLOPs: 1829.1681. Time: 69.4554 us. Best GFLOPs: 3031.7599
2023-11-11 00:58:55 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #99: GFLOPs: 2303.4351. Time: 55.1549 us. Best GFLOPs: 3031.7599
2023-11-11 00:58:55 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #100: GFLOPs: 2089.3028. Time: 60.8077 us. Best GFLOPs: 3031.7599
2023-11-11 00:58:55 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #101: GFLOPs: 2378.6513. Time: 53.4108 us. Best GFLOPs: 3031.7599
2023-11-11 00:58:55 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #102: GFLOPs: 2861.4714. Time: 44.3987 us. Best GFLOPs: 3031.7599
2023-11-11 00:58:55 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #103: GFLOPs: 741.6937. Time: 171.2912 us. Best GFLOPs: 3031.7599
2023-11-11 00:58:55 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #104: GFLOPs: 2044.1997. Time: 62.1493 us. Best GFLOPs: 3031.7599
2023-11-11 00:58:55 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #105: GFLOPs: 2065.6225. Time: 61.5048 us. Best GFLOPs: 3031.7599
2023-11-11 00:58:55 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #106: GFLOPs: 2396.0414. Time: 53.0231 us. Best GFLOPs: 3031.7599
2023-11-11 00:58:55 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #107: GFLOPs: 2934.9682. Time: 43.2869 us. Best GFLOPs: 3031.7599
2023-11-11 00:58:55 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #108: GFLOPs: 1904.8401. Time: 66.6962 us. Best GFLOPs: 3031.7599
2023-11-11 00:58:55 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #109: GFLOPs: 1575.8238. Time: 80.6217 us. Best GFLOPs: 3031.7599
2023-11-11 00:58:55 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #110: GFLOPs: 1693.8050. Time: 75.0061 us. Best GFLOPs: 3031.7599
2023-11-11 00:58:55 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #111: GFLOPs: 1015.3266. Time: 125.1278 us. Best GFLOPs: 3031.7599
2023-11-11 00:58:55 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #112: GFLOPs: 2919.8307. Time: 43.5113 us. Best GFLOPs: 3031.7599
2023-11-11 00:58:55 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #113: GFLOPs: 810.8553. Time: 156.6810 us. Best GFLOPs: 3031.7599
2023-11-11 00:58:55 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #114: GFLOPs: 1609.7639. Time: 78.9219 us. Best GFLOPs: 3031.7599
2023-11-11 00:58:55 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #115: GFLOPs: 2355.8957. Time: 53.9267 us. Best GFLOPs: 3031.7599
2023-11-11 00:58:55 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #116: GFLOPs: 742.6146. Time: 171.0788 us. Best GFLOPs: 3031.7599
2023-11-11 00:58:55 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #117: GFLOPs: 2903.2959. Time: 43.7591 us. Best GFLOPs: 3031.7599
2023-11-11 00:58:55 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #118: GFLOPs: 1981.8451. Time: 64.1047 us. Best GFLOPs: 3031.7599
2023-11-11 00:58:55 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #119: GFLOPs: 2890.2626. Time: 43.9564 us. Best GFLOPs: 3031.7599
2023-11-11 00:58:55 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #120: GFLOPs: 3489.3456. Time: 36.4096 us. Best GFLOPs: 3489.3456
2023-11-11 00:58:55 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #121: GFLOPs: 1537.2254. Time: 82.6461 us. Best GFLOPs: 3489.3456
2023-11-11 00:58:55 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #122: GFLOPs: 2053.4907. Time: 61.8681 us. Best GFLOPs: 3489.3456
2023-11-11 00:58:55 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #123: GFLOPs: 2818.3203. Time: 45.0785 us. Best GFLOPs: 3489.3456
2023-11-11 00:58:55 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #124: GFLOPs: 2328.8527. Time: 54.5529 us. Best GFLOPs: 3489.3456
2023-11-11 00:58:55 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #125: GFLOPs: 2816.6931. Time: 45.1045 us. Best GFLOPs: 3489.3456
2023-11-11 00:58:55 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #126: GFLOPs: 2706.5636. Time: 46.9398 us. Best GFLOPs: 3489.3456
2023-11-11 00:58:55 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #127: GFLOPs: 1739.9741. Time: 73.0158 us. Best GFLOPs: 3489.3456
2023-11-11 00:58:55 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #128: GFLOPs: 2110.3036. Time: 60.2025 us. Best GFLOPs: 3489.3456
2023-11-11 01:40:20 [INFO] [evolutionary_search.cc:713] Generating candidates......
2023-11-11 01:40:23 [INFO] [evolutionary_search.cc:715] Picked top 102 candidate(s) from database
2023-11-11 01:40:29 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310a03fb48)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310e893e78)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310e893578)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310ec9ace8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56310e986408)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310eb1cfb8)]: 396 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310dc6f7b8)]: 0 failure(s)
2023-11-11 01:40:36 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310a03fb48)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310e893e78)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310e893578)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310ec9ace8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56310e986408)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310eb1cfb8)]: 792 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310dc6f7b8)]: 0 failure(s)
2023-11-11 01:40:42 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310a03fb48)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310e893e78)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310e893578)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310ec9ace8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56310e986408)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310eb1cfb8)]: 1193 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310dc6f7b8)]: 0 failure(s)
2023-11-11 01:40:49 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310a03fb48)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310e893e78)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310e893578)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310ec9ace8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56310e986408)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310eb1cfb8)]: 1591 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310dc6f7b8)]: 0 failure(s)
2023-11-11 01:40:55 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310a03fb48)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310e893e78)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310e893578)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310ec9ace8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56310e986408)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310eb1cfb8)]: 1991 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310dc6f7b8)]: 0 failure(s)
2023-11-11 01:40:55 [INFO] [evolutionary_search.cc:723] Sampled 59 candidate(s)
2023-11-11 01:41:13 [INFO] [evolutionary_search.cc:621] Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310a03fb48)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310e893e78)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310e893578)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310ec9ace8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56310e986408)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310eb1cfb8)]: 78 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310dc6f7b8)]: 0 failure(s)
2023-11-11 01:41:35 [INFO] [evolutionary_search.cc:621] Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310a03fb48)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310e893e78)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310e893578)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310ec9ace8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56310e986408)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310eb1cfb8)]: 94 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310dc6f7b8)]: 0 failure(s)
2023-11-11 01:41:57 [INFO] [evolutionary_search.cc:621] Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310a03fb48)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310e893e78)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310e893578)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310ec9ace8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56310e986408)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310eb1cfb8)]: 85 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310dc6f7b8)]: 0 failure(s)
2023-11-11 01:42:18 [INFO] [evolutionary_search.cc:621] Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310a03fb48)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310e893e78)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310e893578)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310ec9ace8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56310e986408)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310eb1cfb8)]: 70 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310dc6f7b8)]: 0 failure(s)
2023-11-11 01:42:25 [INFO] [evolutionary_search.cc:649] Scores of the best 64 candidates:
[1 : 16]:	0.9945  0.9766  0.9727  0.9713  0.9692  0.9600  0.9593  0.9560  0.9497  0.9491  0.9455  0.9455  0.9361  0.9307  0.9292  0.9278
[17 : 32]:	0.9265  0.9246  0.9246  0.9232  0.9232  0.9232  0.9228  0.9195  0.9195  0.9161  0.9142  0.9132  0.9132  0.9062  0.9052  0.9019
[33 : 48]:	0.9016  0.9009  0.9006  0.9006  0.9005  0.9000  0.8936  0.8917  0.8890  0.8854  0.8822  0.8779  0.8777  0.8731  0.8672  0.8641
[49 : 64]:	0.8635  0.8629  0.8617  0.8608  0.8607  0.8600  0.8592  0.8591  0.8574  0.8571  0.8568  0.8556  0.8552  0.8547  0.8524  0.8512
2023-11-11 01:42:25 [INFO] [evolutionary_search.cc:727] Got 64 candidate(s) with evolutionary search
2023-11-11 01:42:25 [INFO] [evolutionary_search.cc:730] Sending 64 candidates(s) for measurement
2023-11-11 01:43:00 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #129: GFLOPs: 3377.4456. Time: 37.6159 us. Best GFLOPs: 3489.3456
2023-11-11 01:43:00 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #130: GFLOPs: 3455.1046. Time: 36.7704 us. Best GFLOPs: 3489.3456
2023-11-11 01:43:00 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #131: GFLOPs: 3451.9297. Time: 36.8042 us. Best GFLOPs: 3489.3456
2023-11-11 01:43:00 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #132: GFLOPs: 3180.7272. Time: 39.9423 us. Best GFLOPs: 3489.3456
2023-11-11 01:43:00 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #133: GFLOPs: 2979.0258. Time: 42.6467 us. Best GFLOPs: 3489.3456
2023-11-11 01:43:00 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #134: GFLOPs: 3610.2382. Time: 35.1904 us. Best GFLOPs: 3610.2382
2023-11-11 01:43:00 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #135: GFLOPs: 3432.2089. Time: 37.0157 us. Best GFLOPs: 3610.2382
2023-11-11 01:43:00 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #136: GFLOPs: 3509.3920. Time: 36.2016 us. Best GFLOPs: 3610.2382
2023-11-11 01:43:00 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #137: GFLOPs: 3358.9200. Time: 37.8234 us. Best GFLOPs: 3610.2382
2023-11-11 01:43:00 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #138: GFLOPs: 3627.4206. Time: 35.0237 us. Best GFLOPs: 3627.4206
2023-11-11 01:43:00 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #139: GFLOPs: 3478.4818. Time: 36.5233 us. Best GFLOPs: 3627.4206
2023-11-11 01:43:00 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #140: GFLOPs: 3453.2467. Time: 36.7902 us. Best GFLOPs: 3627.4206
2023-11-11 01:43:00 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #141: GFLOPs: 4025.7573. Time: 31.5582 us. Best GFLOPs: 4025.7573
2023-11-11 01:43:00 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #142: GFLOPs: 3846.5162. Time: 33.0288 us. Best GFLOPs: 4025.7573
2023-11-11 01:43:00 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #143: GFLOPs: 2672.6390. Time: 47.5357 us. Best GFLOPs: 4025.7573
2023-11-11 01:43:00 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #144: GFLOPs: 2917.6891. Time: 43.5432 us. Best GFLOPs: 4025.7573
2023-11-11 01:43:00 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #145: GFLOPs: 2974.1601. Time: 42.7165 us. Best GFLOPs: 4025.7573
2023-11-11 01:43:00 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #146: GFLOPs: 2785.5116. Time: 45.6094 us. Best GFLOPs: 4025.7573
2023-11-11 01:43:00 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #147: GFLOPs: 2778.5360. Time: 45.7239 us. Best GFLOPs: 4025.7573
2023-11-11 01:43:00 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #148: GFLOPs: 2941.6720. Time: 43.1882 us. Best GFLOPs: 4025.7573
2023-11-11 01:43:00 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #149: GFLOPs: 2408.2932. Time: 52.7534 us. Best GFLOPs: 4025.7573
2023-11-11 01:43:00 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #150: GFLOPs: 3380.4135. Time: 37.5829 us. Best GFLOPs: 4025.7573
2023-11-11 01:43:00 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #151: GFLOPs: 3511.8860. Time: 36.1759 us. Best GFLOPs: 4025.7573
2023-11-11 01:43:00 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #152: GFLOPs: 2588.9262. Time: 49.0727 us. Best GFLOPs: 4025.7573
2023-11-11 01:43:00 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #153: GFLOPs: 2682.7219. Time: 47.3570 us. Best GFLOPs: 4025.7573
2023-11-11 01:43:00 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #154: GFLOPs: 3527.2812. Time: 36.0180 us. Best GFLOPs: 4025.7573
2023-11-11 01:43:00 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #155: GFLOPs: 3206.4051. Time: 39.6225 us. Best GFLOPs: 4025.7573
2023-11-11 01:43:00 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #156: GFLOPs: 2785.4941. Time: 45.6097 us. Best GFLOPs: 4025.7573
2023-11-11 01:43:00 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #157: GFLOPs: 2779.0629. Time: 45.7153 us. Best GFLOPs: 4025.7573
2023-11-11 01:43:00 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #158: GFLOPs: 3393.3943. Time: 37.4391 us. Best GFLOPs: 4025.7573
2023-11-11 01:43:00 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #159: GFLOPs: 2998.6151. Time: 42.3681 us. Best GFLOPs: 4025.7573
2023-11-11 01:43:00 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #160: GFLOPs: 3534.1168. Time: 35.9483 us. Best GFLOPs: 4025.7573
2023-11-11 01:43:00 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #161: GFLOPs: 2279.7112. Time: 55.7288 us. Best GFLOPs: 4025.7573
2023-11-11 01:43:00 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #162: GFLOPs: 2220.7485. Time: 57.2085 us. Best GFLOPs: 4025.7573
2023-11-11 01:43:00 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #163: GFLOPs: 2438.4307. Time: 52.1014 us. Best GFLOPs: 4025.7573
2023-11-11 01:43:00 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #164: GFLOPs: 2437.4376. Time: 52.1226 us. Best GFLOPs: 4025.7573
2023-11-11 01:43:00 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #165: GFLOPs: 3427.2252. Time: 37.0695 us. Best GFLOPs: 4025.7573
2023-11-11 01:43:00 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #166: GFLOPs: 2139.2003. Time: 59.3893 us. Best GFLOPs: 4025.7573
2023-11-11 01:43:00 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #167: GFLOPs: 3486.1253. Time: 36.4432 us. Best GFLOPs: 4025.7573
2023-11-11 01:43:00 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #168: GFLOPs: 20.4393. Time: 6215.7404 us. Best GFLOPs: 4025.7573
2023-11-11 01:43:00 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #169: GFLOPs: 4350.6551. Time: 29.2015 us. Best GFLOPs: 4350.6551
2023-11-11 01:43:00 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #170: GFLOPs: 2431.9908. Time: 52.2394 us. Best GFLOPs: 4350.6551
2023-11-11 01:43:00 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #171: GFLOPs: 3363.7979. Time: 37.7685 us. Best GFLOPs: 4350.6551
2023-11-11 01:43:00 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #172: GFLOPs: 2705.2409. Time: 46.9628 us. Best GFLOPs: 4350.6551
2023-11-11 01:43:00 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #173: GFLOPs: 25.0729. Time: 5067.0593 us. Best GFLOPs: 4350.6551
2023-11-11 01:43:00 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #174: GFLOPs: 3574.6959. Time: 35.5403 us. Best GFLOPs: 4350.6551
2023-11-11 01:43:00 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #175: GFLOPs: 3039.2409. Time: 41.8018 us. Best GFLOPs: 4350.6551
2023-11-11 01:43:00 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #176: GFLOPs: 3011.4501. Time: 42.1875 us. Best GFLOPs: 4350.6551
2023-11-11 01:43:00 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #177: GFLOPs: 2297.9852. Time: 55.2857 us. Best GFLOPs: 4350.6551
2023-11-11 01:43:00 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #178: GFLOPs: 2931.3379. Time: 43.3405 us. Best GFLOPs: 4350.6551
2023-11-11 01:43:00 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #179: GFLOPs: 2951.1864. Time: 43.0490 us. Best GFLOPs: 4350.6551
2023-11-11 01:43:00 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #180: GFLOPs: 27.9392. Time: 4547.2117 us. Best GFLOPs: 4350.6551
2023-11-11 01:43:00 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #181: GFLOPs: 2734.0974. Time: 46.4671 us. Best GFLOPs: 4350.6551
2023-11-11 01:43:00 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #182: GFLOPs: 34.7175. Time: 3659.4102 us. Best GFLOPs: 4350.6551
2023-11-11 01:43:00 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #183: GFLOPs: 3066.7780. Time: 41.4264 us. Best GFLOPs: 4350.6551
2023-11-11 01:43:00 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #184: GFLOPs: 2432.6426. Time: 52.2254 us. Best GFLOPs: 4350.6551
2023-11-11 01:43:00 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #185: GFLOPs: 2930.7082. Time: 43.3498 us. Best GFLOPs: 4350.6551
2023-11-11 01:43:00 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #186: GFLOPs: 2984.3544. Time: 42.5706 us. Best GFLOPs: 4350.6551
2023-11-11 01:43:00 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #187: GFLOPs: 2761.2414. Time: 46.0103 us. Best GFLOPs: 4350.6551
2023-11-11 01:43:00 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #188: GFLOPs: 3125.9735. Time: 40.6419 us. Best GFLOPs: 4350.6551
2023-11-11 01:43:00 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #189: GFLOPs: 2802.6730. Time: 45.3302 us. Best GFLOPs: 4350.6551
2023-11-11 01:43:00 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #190: GFLOPs: 2946.4205. Time: 43.1186 us. Best GFLOPs: 4350.6551
2023-11-11 01:43:00 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #191: GFLOPs: 822.4279. Time: 154.4763 us. Best GFLOPs: 4350.6551
2023-11-11 01:43:00 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #192: GFLOPs: 1476.6187. Time: 86.0382 us. Best GFLOPs: 4350.6551
2023-11-11 02:40:51 [INFO] [evolutionary_search.cc:713] Generating candidates......
2023-11-11 02:40:54 [INFO] [evolutionary_search.cc:715] Picked top 102 candidate(s) from database
2023-11-11 02:41:01 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310a03fb48)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310e893e78)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310e893578)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310ec9ace8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56310e986408)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310eb1cfb8)]: 399 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310dc6f7b8)]: 0 failure(s)
2023-11-11 02:41:08 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310a03fb48)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310e893e78)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310e893578)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310ec9ace8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56310e986408)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310eb1cfb8)]: 790 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310dc6f7b8)]: 0 failure(s)
2023-11-11 02:41:14 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310a03fb48)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310e893e78)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310e893578)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310ec9ace8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56310e986408)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310eb1cfb8)]: 1186 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310dc6f7b8)]: 0 failure(s)
2023-11-11 02:41:20 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310a03fb48)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310e893e78)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310e893578)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310ec9ace8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56310e986408)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310eb1cfb8)]: 1580 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310dc6f7b8)]: 0 failure(s)
2023-11-11 02:41:20 [INFO] [evolutionary_search.cc:723] Sampled 60 candidate(s)
2023-11-11 02:41:39 [INFO] [evolutionary_search.cc:621] Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310a03fb48)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310e893e78)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310e893578)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310ec9ace8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56310e986408)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310eb1cfb8)]: 95 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310dc6f7b8)]: 0 failure(s)
2023-11-11 02:42:01 [INFO] [evolutionary_search.cc:621] Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310a03fb48)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310e893e78)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310e893578)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310ec9ace8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56310e986408)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310eb1cfb8)]: 87 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310dc6f7b8)]: 0 failure(s)
2023-11-11 02:42:24 [INFO] [evolutionary_search.cc:621] Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310a03fb48)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310e893e78)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310e893578)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310ec9ace8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56310e986408)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310eb1cfb8)]: 101 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310dc6f7b8)]: 0 failure(s)
2023-11-11 02:42:46 [INFO] [evolutionary_search.cc:621] Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310a03fb48)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310e893e78)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310e893578)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310ec9ace8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56310e986408)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310eb1cfb8)]: 109 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310dc6f7b8)]: 0 failure(s)
2023-11-11 02:42:52 [INFO] [evolutionary_search.cc:649] Scores of the best 64 candidates:
[1 : 16]:	0.9718  0.9702  0.9702  0.9627  0.9609  0.9580  0.9471  0.9378  0.9353  0.9346  0.9168  0.9130  0.9069  0.9044  0.9032  0.9027
[17 : 32]:	0.9008  0.8994  0.8978  0.8978  0.8957  0.8950  0.8933  0.8879  0.8862  0.8851  0.8842  0.8831  0.8785  0.8734  0.8720  0.8719
[33 : 48]:	0.8681  0.8681  0.8665  0.8664  0.8631  0.8614  0.8590  0.8554  0.8545  0.8533  0.8518  0.8516  0.8507  0.8497  0.8496  0.8488
[49 : 64]:	0.8479  0.8477  0.8477  0.8471  0.8463  0.8461  0.8453  0.8441  0.8430  0.8422  0.8419  0.8413  0.8413  0.8413  0.8413  0.8413
2023-11-11 02:42:53 [INFO] [evolutionary_search.cc:727] Got 64 candidate(s) with evolutionary search
2023-11-11 02:42:53 [INFO] [evolutionary_search.cc:730] Sending 64 candidates(s) for measurement
2023-11-11 02:43:22 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #193: GFLOPs: 4385.6711. Time: 28.9683 us. Best GFLOPs: 4385.6711
2023-11-11 02:43:22 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #194: GFLOPs: 4383.4273. Time: 28.9832 us. Best GFLOPs: 4385.6711
2023-11-11 02:43:22 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #195: GFLOPs: 4381.5594. Time: 28.9955 us. Best GFLOPs: 4385.6711
2023-11-11 02:43:22 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #196: GFLOPs: 4340.3385. Time: 29.2709 us. Best GFLOPs: 4385.6711
2023-11-11 02:43:22 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #197: GFLOPs: 4384.1462. Time: 28.9784 us. Best GFLOPs: 4385.6711
2023-11-11 02:43:22 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #198: GFLOPs: 4359.9845. Time: 29.1390 us. Best GFLOPs: 4385.6711
2023-11-11 02:43:22 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #199: GFLOPs: 4382.3651. Time: 28.9902 us. Best GFLOPs: 4385.6711
2023-11-11 02:43:22 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #200: GFLOPs: 4385.7573. Time: 28.9678 us. Best GFLOPs: 4385.7573
2023-11-11 02:43:22 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #201: GFLOPs: 4120.8370. Time: 30.8301 us. Best GFLOPs: 4385.7573
2023-11-11 02:43:22 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #202: GFLOPs: 3737.8097. Time: 33.9893 us. Best GFLOPs: 4385.7573
2023-11-11 02:43:22 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #203: GFLOPs: 4401.6723. Time: 28.8630 us. Best GFLOPs: 4401.6723
2023-11-11 02:43:22 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #204: GFLOPs: 4580.8904. Time: 27.7338 us. Best GFLOPs: 4580.8904
2023-11-11 02:43:22 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #205: GFLOPs: 4804.5921. Time: 26.4425 us. Best GFLOPs: 4804.5921
2023-11-11 02:43:22 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #206: GFLOPs: 4383.3346. Time: 28.9838 us. Best GFLOPs: 4804.5921
2023-11-11 02:43:22 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #207: GFLOPs: 4890.3955. Time: 25.9786 us. Best GFLOPs: 4890.3955
2023-11-11 02:43:22 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #208: GFLOPs: 4880.2029. Time: 26.0329 us. Best GFLOPs: 4890.3955
2023-11-11 02:43:22 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #209: GFLOPs: 4427.7415. Time: 28.6931 us. Best GFLOPs: 4890.3955
2023-11-11 02:43:22 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #210: GFLOPs: 4545.0865. Time: 27.9523 us. Best GFLOPs: 4890.3955
2023-11-11 02:43:22 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #211: GFLOPs: 3408.5337. Time: 37.2728 us. Best GFLOPs: 4890.3955
2023-11-11 02:43:22 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #212: GFLOPs: 4862.2837. Time: 26.1288 us. Best GFLOPs: 4890.3955
2023-11-11 02:43:22 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #213: GFLOPs: 3275.8494. Time: 38.7825 us. Best GFLOPs: 4890.3955
2023-11-11 02:43:22 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #214: GFLOPs: 3418.0868. Time: 37.1686 us. Best GFLOPs: 4890.3955
2023-11-11 02:43:22 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #215: GFLOPs: 4769.3730. Time: 26.6378 us. Best GFLOPs: 4890.3955
2023-11-11 02:43:22 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #216: GFLOPs: 4298.1391. Time: 29.5583 us. Best GFLOPs: 4890.3955
2023-11-11 02:43:22 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #217: GFLOPs: 4801.1290. Time: 26.4616 us. Best GFLOPs: 4890.3955
2023-11-11 02:43:22 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #218: GFLOPs: 4445.1163. Time: 28.5809 us. Best GFLOPs: 4890.3955
2023-11-11 02:43:22 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #219: GFLOPs: 4828.2862. Time: 26.3128 us. Best GFLOPs: 4890.3955
2023-11-11 02:43:22 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #220: GFLOPs: 4190.7940. Time: 30.3154 us. Best GFLOPs: 4890.3955
2023-11-11 02:43:22 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #221: GFLOPs: 4372.6308. Time: 29.0547 us. Best GFLOPs: 4890.3955
2023-11-11 02:43:22 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #222: GFLOPs: 3722.0073. Time: 34.1336 us. Best GFLOPs: 4890.3955
2023-11-11 02:43:22 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #223: GFLOPs: 4876.5130. Time: 26.0526 us. Best GFLOPs: 4890.3955
2023-11-11 02:43:22 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #224: GFLOPs: 4136.5025. Time: 30.7133 us. Best GFLOPs: 4890.3955
2023-11-11 02:43:22 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #225: GFLOPs: 4041.3898. Time: 31.4361 us. Best GFLOPs: 4890.3955
2023-11-11 02:43:22 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #226: GFLOPs: 3718.4574. Time: 34.1662 us. Best GFLOPs: 4890.3955
2023-11-11 02:43:22 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #227: GFLOPs: 3403.6246. Time: 37.3266 us. Best GFLOPs: 4890.3955
2023-11-11 02:43:22 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #228: GFLOPs: 3316.3023. Time: 38.3094 us. Best GFLOPs: 4890.3955
2023-11-11 02:43:22 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #229: GFLOPs: 3404.0920. Time: 37.3214 us. Best GFLOPs: 4890.3955
2023-11-11 02:43:22 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #230: GFLOPs: 3669.1843. Time: 34.6250 us. Best GFLOPs: 4890.3955
2023-11-11 02:43:22 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #231: GFLOPs: 3632.9502. Time: 34.9704 us. Best GFLOPs: 4890.3955
2023-11-11 02:43:22 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #232: GFLOPs: 3947.6180. Time: 32.1829 us. Best GFLOPs: 4890.3955
2023-11-11 02:43:22 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #233: GFLOPs: 3141.8289. Time: 40.4368 us. Best GFLOPs: 4890.3955
2023-11-11 02:43:22 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #234: GFLOPs: 4606.9035. Time: 27.5772 us. Best GFLOPs: 4890.3955
2023-11-11 02:43:22 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #235: GFLOPs: 3542.6093. Time: 35.8622 us. Best GFLOPs: 4890.3955
2023-11-11 02:43:22 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #236: GFLOPs: 4013.1072. Time: 31.6577 us. Best GFLOPs: 4890.3955
2023-11-11 02:43:22 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #237: GFLOPs: 3134.3118. Time: 40.5338 us. Best GFLOPs: 4890.3955
2023-11-11 02:43:22 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #238: GFLOPs: 4151.4795. Time: 30.6025 us. Best GFLOPs: 4890.3955
2023-11-11 02:43:22 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #239: GFLOPs: 3612.0001. Time: 35.1732 us. Best GFLOPs: 4890.3955
2023-11-11 02:43:22 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #240: GFLOPs: 3633.3105. Time: 34.9669 us. Best GFLOPs: 4890.3955
2023-11-11 02:43:22 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #241: GFLOPs: 3348.7977. Time: 37.9377 us. Best GFLOPs: 4890.3955
2023-11-11 02:43:22 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #242: GFLOPs: 3153.9687. Time: 40.2812 us. Best GFLOPs: 4890.3955
2023-11-11 02:43:22 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #243: GFLOPs: 3600.3469. Time: 35.2871 us. Best GFLOPs: 4890.3955
2023-11-11 02:43:22 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #244: GFLOPs: 4812.1043. Time: 26.4013 us. Best GFLOPs: 4890.3955
2023-11-11 02:43:22 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #245: GFLOPs: 4890.8837. Time: 25.9760 us. Best GFLOPs: 4890.8837
2023-11-11 02:43:22 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #246: GFLOPs: 3848.3983. Time: 33.0126 us. Best GFLOPs: 4890.8837
2023-11-11 02:43:22 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #247: GFLOPs: 3883.3464. Time: 32.7155 us. Best GFLOPs: 4890.8837
2023-11-11 02:43:22 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #248: GFLOPs: 3471.0081. Time: 36.6019 us. Best GFLOPs: 4890.8837
2023-11-11 02:43:22 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #249: GFLOPs: 3564.1045. Time: 35.6459 us. Best GFLOPs: 4890.8837
2023-11-11 02:43:22 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #250: GFLOPs: 3590.1007. Time: 35.3878 us. Best GFLOPs: 4890.8837
2023-11-11 02:43:22 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #251: GFLOPs: 3887.0426. Time: 32.6844 us. Best GFLOPs: 4890.8837
2023-11-11 02:43:22 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #252: GFLOPs: 4227.4280. Time: 30.0527 us. Best GFLOPs: 4890.8837
2023-11-11 02:43:22 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #253: GFLOPs: 4036.6992. Time: 31.4727 us. Best GFLOPs: 4890.8837
2023-11-11 02:43:22 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #254: GFLOPs: 78.9931. Time: 1608.3139 us. Best GFLOPs: 4890.8837
2023-11-11 02:43:22 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #255: GFLOPs: 715.6448. Time: 177.5261 us. Best GFLOPs: 4890.8837
2023-11-11 02:43:22 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #256: GFLOPs: 564.5125. Time: 225.0537 us. Best GFLOPs: 4890.8837
2023-11-11 03:15:23 [INFO] [evolutionary_search.cc:713] Generating candidates......
2023-11-11 03:15:26 [INFO] [evolutionary_search.cc:715] Picked top 102 candidate(s) from database
2023-11-11 03:15:33 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310a03fb48)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310e893e78)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310e893578)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310ec9ace8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56310e986408)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310eb1cfb8)]: 398 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310dc6f7b8)]: 0 failure(s)
2023-11-11 03:15:40 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310a03fb48)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310e893e78)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310e893578)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310ec9ace8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56310e986408)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310eb1cfb8)]: 793 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310dc6f7b8)]: 0 failure(s)
2023-11-11 03:15:46 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310a03fb48)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310e893e78)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310e893578)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310ec9ace8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56310e986408)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310eb1cfb8)]: 1189 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310dc6f7b8)]: 0 failure(s)
2023-11-11 03:15:52 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310a03fb48)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310e893e78)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310e893578)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310ec9ace8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56310e986408)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310eb1cfb8)]: 1586 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310dc6f7b8)]: 0 failure(s)
2023-11-11 03:15:52 [INFO] [evolutionary_search.cc:723] Sampled 54 candidate(s)
2023-11-11 03:16:10 [INFO] [evolutionary_search.cc:621] Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310a03fb48)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310e893e78)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310e893578)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310ec9ace8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56310e986408)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310eb1cfb8)]: 74 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310dc6f7b8)]: 0 failure(s)
2023-11-11 03:16:31 [INFO] [evolutionary_search.cc:621] Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310a03fb48)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310e893e78)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310e893578)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310ec9ace8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56310e986408)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310eb1cfb8)]: 76 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310dc6f7b8)]: 0 failure(s)
2023-11-11 03:16:53 [INFO] [evolutionary_search.cc:621] Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310a03fb48)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310e893e78)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310e893578)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310ec9ace8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56310e986408)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310eb1cfb8)]: 71 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310dc6f7b8)]: 0 failure(s)
2023-11-11 03:17:15 [INFO] [evolutionary_search.cc:621] Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310a03fb48)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310e893e78)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310e893578)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310ec9ace8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56310e986408)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310eb1cfb8)]: 73 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310dc6f7b8)]: 0 failure(s)
2023-11-11 03:17:21 [INFO] [evolutionary_search.cc:649] Scores of the best 64 candidates:
[1 : 16]:	1.0075  1.0027  0.9985  0.9976  0.9942  0.9942  0.9929  0.9928  0.9918  0.9915  0.9914  0.9910  0.9896  0.9895  0.9882  0.9882
[17 : 32]:	0.9877  0.9866  0.9860  0.9820  0.9820  0.9809  0.9801  0.9787  0.9785  0.9781  0.9772  0.9770  0.9769  0.9760  0.9726  0.9721
[33 : 48]:	0.9720  0.9720  0.9718  0.9705  0.9693  0.9673  0.9668  0.9664  0.9626  0.9599  0.9589  0.9583  0.9567  0.9564  0.9546  0.9542
[49 : 64]:	0.9531  0.9516  0.9511  0.9510  0.9502  0.9495  0.9489  0.9489  0.9449  0.9410  0.9403  0.9403  0.9388  0.9385  0.9379  0.9366
2023-11-11 03:17:21 [INFO] [evolutionary_search.cc:727] Got 64 candidate(s) with evolutionary search
2023-11-11 03:17:21 [INFO] [evolutionary_search.cc:730] Sending 64 candidates(s) for measurement
2023-11-11 03:17:54 [INFO] [task_scheduler.cc:121] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #257: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/home2/xiachunwei/Software/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/home2/xiachunwei/Software/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/home2/xiachunwei/Software/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/home2/xiachunwei/Software/tvm/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/home2/xiachunwei/Software/tvm/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/home2/xiachunwei/Software/tvm/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/home2/xiachunwei/Software/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/home2/xiachunwei/Software/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  4: _ZN3tvm7runtime13PackedFun
  3: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  2: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  0: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  File "/home2/xiachunwei/Software/tvm/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(128), T.int64(28), T.int64(28)), "float32"), p1: T.Buffer((T.int64(4), T.int64(4), T.int64(128), T.int64(128)), "float32"), p2: T.Buffer((T.int64(1), T.int64(128), T.int64(1), T.int64(1)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(128), T.int64(28), T.int64(28)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        input_tile_local = T.alloc_buffer((T.int64(128), T.int64(196), T.int64(4), T.int64(4)), scope="local")
        data_pack = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(128), T.int64(196)))
        bgemm = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(128), T.int64(196)))
        inverse_local = T.alloc_buffer((T.int64(128), T.int64(196), T.int64(2), T.int64(2)), scope="local")
        data_pack_local = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(128), T.int64(196)), scope="local")
        bgemm_local = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(128), T.int64(196)), scope="local")
        data_pack_shared = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(128), T.int64(196)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(128), T.int64(128)), scope="shared")
        for ci_p_fused_0 in T.thread_binding(T.int64(392), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 16, "pragma_unroll_explicit": 1}):
            for ci_p_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(1), T.int64(4), T.int64(4)):
                    with T.block("input_tile"):
                        v_ci = T.axis.spatial(T.int64(128), (ci_p_fused_0 * T.int64(64) + ci_p_fused_1) // T.int64(196) + ax0)
                        v_p = T.axis.spatial(T.int64(196), (ci_p_fused_0 * T.int64(64) + ci_p_fused_1) % T.int64(196) + ax1)
                        v_eps, v_nu = T.axis.remap("SS", [ax2, ax3])
                        T.reads(p0[v_p // T.int64(196), v_ci, v_p % T.int64(196) // T.int64(14) * T.int64(2) + v_eps - T.int64(1), v_p % T.int64(14) * T.int64(2) + v_nu - T.int64(1)])
                        T.writes(input_tile_local[v_ci, v_p, v_eps, v_nu])
                        T.block_attr({"schedule_rule": "None"})
                        input_tile_local[v_ci, v_p, v_eps, v_nu] = T.if_then_else(T.int64(1) <= v_p % T.int64(196) // T.int64(14) * T.int64(2) + v_eps and v_p % T.int64(196) // T.int64(14) * T.int64(2) + v_eps < T.int64(29) and T.int64(1) <= v_p % T.int64(14) * T.int64(2) + v_nu and v_p % T.int64(14) * T.int64(2) + v_nu < T.int64(29), p0[v_p // T.int64(196), v_ci, v_p % T.int64(196) // T.int64(14) * T.int64(2) + v_eps - T.int64(1), v_p % T.int64(14) * T.int64(2) + v_nu - T.int64(1)], T.float32(0))
                for eps in T.unroll(T.int64(4)):
                    for nu in T.unroll(T.int64(4)):
                        with T.block("data_pack_init"):
                            v_eps, v_nu = T.axis.remap("SS", [eps, nu])
                            v_ci = T.axis.spatial(T.int64(128), (ci_p_fused_0 * T.int64(64) + ci_p_fused_1) // T.int64(196))
                            v_p = T.axis.spatial(T.int64(196), (ci_p_fused_0 * T.int64(64) + ci_p_fused_1) % T.int64(196))
                            T.reads()
                            T.writes(data_pack_local[v_eps, v_nu, v_ci, v_p])
                            T.block_attr({"schedule_rule": "conv2d_nchw_winograd_data_pack"})
                            data_pack_local[v_eps, v_nu, v_ci, v_p] = T.float32(0)
                        for r_a in T.unroll(T.int64(4)):
                            for r_b in T.unroll(T.int64(4)):
                                with T.block("data_pack_update"):
                                    v_eps, v_nu = T.axis.remap("SS", [eps, nu])
                                    v_ci = T.axis.spatial(T.int64(128), (ci_p_fused_0 * T.int64(64) + ci_p_fused_1) // T.int64(196))
                                    v_p = T.axis.spatial(T.int64(196), (ci_p_fused_0 * T.int64(64) + ci_p_fused_1) % T.int64(196))
                                    v_r_a, v_r_b = T.axis.remap("RR", [r_a, r_b])
                                    T.reads(data_pack_local[v_eps, v_nu, v_ci, v_p], input_tile_local[v_ci, v_p, v_r_a, v_r_b])
                                    T.writes(data_pack_local[v_eps, v_nu, v_ci, v_p])
                                    T.block_attr({"schedule_rule": "conv2d_nchw_winograd_data_pack"})
                                    data_pack_local[v_eps, v_nu, v_ci, v_p] = data_pack_local[v_eps, v_nu, v_ci, v_p] + input_tile_local[v_ci, v_p, v_r_a, v_r_b] * T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(3), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(1), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(0), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(3), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(1), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(0), T.float32(1), T.float32(0))))))))))))))))) * T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(3), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(1), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(0), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(3), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(1), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(0), T.float32(1), T.float32(0)))))))))))))))))
                for ax0, ax1, ax2, ax3 in T.grid(T.int64(4), T.int64(4), T.int64(1), T.int64(1)):
                    with T.block("data_pack_local"):
                        v0, v1 = T.axis.remap("SS", [ax0, ax1])
                        v2 = T.axis.spatial(T.int64(128), (ci_p_fused_0 * T.int64(64) + ci_p_fused_1) // T.int64(196) + ax2)
                        v3 = T.axis.spatial(T.int64(196), (ci_p_fused_0 * T.int64(64) + ci_p_fused_1) % T.int64(196) + ax3)
                        T.reads(data_pack_local[v0, v1, v2, v3])
                        T.writes(data_pack[v0, v1, v2, v3])
                        data_pack[v0, v1, v2, v3] = data_pack_local[v0, v1, v2, v3]
        for eps_0_nu_0_co_0_p_0_fused in T.thread_binding(T.int64(448), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 16, "pragma_unroll_explicit": 1}):
            for eps_1_nu_1_co_1_p_1_fused in T.thread_binding(T.int64(2), thread="vthread.x"):
                for eps_2_nu_2_co_2_p_2_fused in T.thread_binding(T.int64(112), thread="threadIdx.x"):
                    for eps_3_init, nu_3_init, co_3_init, p_3_init, eps_4_init, nu_4_init, co_4_init, p_4_init in T.grid(T.int64(1), T.int64(1), T.int64(4), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("bgemm_init"):
                            v_eps = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(112) + eps_3_init + eps_4_init)
                            v_nu = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused % T.int64(112) // T.int64(28) + nu_3_init + nu_4_init)
                            v_co = T.axis.spatial(T.int64(128), eps_0_nu_0_co_0_p_0_fused % T.int64(28) // T.int64(7) * T.int64(32) + eps_2_nu_2_co_2_p_2_fused // T.int64(14) * T.int64(4) + co_3_init + co_4_init)
                            v_p = T.axis.spatial(T.int64(196), eps_0_nu_0_co_0_p_0_fused % T.int64(7) * T.int64(28) + eps_1_nu_1_co_1_p_1_fused * T.int64(14) + eps_2_nu_2_co_2_p_2_fused % T.int64(14) + p_3_init + p_4_init)
                            T.reads()
                            T.writes(bgemm_local[v_eps, v_nu, v_co, v_p])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            bgemm_local[v_eps, v_nu, v_co, v_p] = T.float32(0)
                    for ci_0_fused in T.serial(T.int64(8), annotations={"software_pipeline_async_stages": [0], "software_pipeline_order": [0, 1, 2], "software_pipeline_stage": [0, 0, 2]}):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(4)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(112), thread="threadIdx.x"):
                                with T.block("data_pack_shared"):
                                    v0 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(112))
                                    v1 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused % T.int64(112) // T.int64(28))
                                    v2 = T.axis.spatial(T.int64(128), ci_0_fused * T.int64(16) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(112) + ax0_ax1_ax2_ax3_fused_1) // T.int64(28))
                                    v3 = T.axis.spatial(T.int64(196), eps_0_nu_0_co_0_p_0_fused % T.int64(7) * T.int64(28) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(112) + ax0_ax1_ax2_ax3_fused_1) % T.int64(28))
                                    T.reads(data_pack[v0, v1, v2, v3])
                                    T.writes(data_pack_shared[v0, v1, v2, v3])
                                    data_pack_shared[v0, v1, v2, v3] = data_pack[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(2)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(112), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("p1_shared"):
                                        v0 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(112))
                                        v1 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused % T.int64(112) // T.int64(28))
                                        v2 = T.axis.spatial(T.int64(128), ci_0_fused * T.int64(16) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(448) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) // T.int64(32))
                                        v3 = T.axis.spatial(T.int64(128), eps_0_nu_0_co_0_p_0_fused % T.int64(28) // T.int64(7) * T.int64(32) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(448) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) % T.int64(32))
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * T.int64(112) + ax0_ax1_ax2_ax3_fused_1) * T.int64(4) + ax0_ax1_ax2_ax3_fused_2 < T.int64(512))
                                        T.reads(p1[v0, v1, v2, v3])
                                        T.writes(p1_shared[v0, v1, v2, v3])
                                        p1_shared[v0, v1, v2, v3] = p1[v0, v1, v2, v3]
                        for ci_1, eps_3, nu_3, co_3, p_3, ci_2, eps_4, nu_4, co_4, p_4 in T.grid(T.int64(4), T.int64(1), T.int64(1), T.int64(4), T.int64(1), T.int64(4), T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                            with T.block("bgemm_update"):
                                v_eps = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(112) + eps_3 + eps_4)
                                v_nu = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused % T.int64(112) // T.int64(28) + nu_3 + nu_4)
                                v_co = T.axis.spatial(T.int64(128), eps_0_nu_0_co_0_p_0_fused % T.int64(28) // T.int64(7) * T.int64(32) + eps_2_nu_2_co_2_p_2_fused // T.int64(14) * T.int64(4) + co_3 + co_4)
                                v_p = T.axis.spatial(T.int64(196), eps_0_nu_0_co_0_p_0_fused % T.int64(7) * T.int64(28) + eps_1_nu_1_co_1_p_1_fused * T.int64(14) + eps_2_nu_2_co_2_p_2_fused % T.int64(14) + p_3 + p_4)
                                v_ci = T.axis.reduce(T.int64(128), ci_0_fused * T.int64(16) + ci_1 * T.int64(4) + ci_2)
                                T.reads(bgemm_local[v_eps, v_nu, v_co, v_p], data_pack_shared[v_eps, v_nu, v_ci, v_p], p1_shared[v_eps, v_nu, v_ci, v_co])
                                T.writes(bgemm_local[v_eps, v_nu, v_co, v_p])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                bgemm_local[v_eps, v_nu, v_co, v_p] = bgemm_local[v_eps, v_nu, v_co, v_p] + data_pack_shared[v_eps, v_nu, v_ci, v_p] * p1_shared[v_eps, v_nu, v_ci, v_co]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(1), T.int64(4), T.int64(1)):
                        with T.block("bgemm_local"):
                            v0 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(112) + ax0)
                            v1 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused % T.int64(112) // T.int64(28) + ax1)
                            v2 = T.axis.spatial(T.int64(128), eps_0_nu_0_co_0_p_0_fused % T.int64(28) // T.int64(7) * T.int64(32) + eps_2_nu_2_co_2_p_2_fused // T.int64(14) * T.int64(4) + ax2)
                            v3 = T.axis.spatial(T.int64(196), eps_0_nu_0_co_0_p_0_fused % T.int64(7) * T.int64(28) + eps_1_nu_1_co_1_p_1_fused * T.int64(14) + eps_2_nu_2_co_2_p_2_fused % T.int64(14) + ax3)
                            T.reads(bgemm_local[v0, v1, v2, v3])
                            T.writes(bgemm[v0, v1, v2, v3])
                            bgemm[v0, v1, v2, v3] = bgemm_local[v0, v1, v2, v3]
        for n_co_h_0_w_0_fused_0 in T.thread_binding(T.int64(784), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 16, "pragma_unroll_explicit": 1}):
            for n_co_h_0_w_0_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                    for ax2 in T.unroll(T.int64(2)):
                        for ax3 in T.unroll(T.int64(2)):
                            with T.block("inverse_init"):
                                v_co = T.axis.spatial(T.int64(128), (n_co_h_0_w_0_fused_0 * T.int64(32) + n_co_h_0_w_0_fused_1) // T.int64(196) + ax0)
                                v_p = T.axis.spatial(T.int64(196), (n_co_h_0_w_0_fused_0 * T.int64(32) + n_co_h_0_w_0_fused_1) % T.int64(196) + ax1)
                                v_vh, v_vw = T.axis.remap("SS", [ax2, ax3])
                                T.reads()
                                T.writes(inverse_local[v_co, v_p, v_vh, v_vw])
                                T.block_attr({"schedule_rule": "conv2d_nchw_winograd_inverse"})
                                inverse_local[v_co, v_p, v_vh, v_vw] = T.float32(0)
                            for ax4 in T.unroll(T.int64(4)):
                                for ax5 in T.unroll(T.int64(4)):
                                    with T.block("inverse_update"):
                                        v_co = T.axis.spatial(T.int64(128), (n_co_h_0_w_0_fused_0 * T.int64(32) + n_co_h_0_w_0_fused_1) // T.int64(196) + ax0)
                                        v_p = T.axis.spatial(T.int64(196), (n_co_h_0_w_0_fused_0 * T.int64(32) + n_co_h_0_w_0_fused_1) % T.int64(196) + ax1)
                                        v_vh, v_vw, v_r_a, v_r_b = T.axis.remap("SSRR", [ax2, ax3, ax4, ax5])
                                        T.reads(inverse_local[v_co, v_p, v_vh, v_vw], bgemm[v_r_a, v_r_b, v_co, v_p])
                                        T.writes(inverse_local[v_co, v_p, v_vh, v_vw])
                                        T.block_attr({"schedule_rule": "conv2d_nchw_winograd_inverse"})
                                        inverse_local[v_co, v_p, v_vh, v_vw] = inverse_local[v_co, v_p, v_vh, v_vw] + bgemm[v_r_a, v_r_b, v_co, v_p] * T.Select(v_r_a % T.int64(4) == T.int64(3) and v_vh % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_vh % T.int64(2) == T.int64(0), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_vh % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_vh % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_vh % T.int64(2) == T.int64(1), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_vh % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_vh % T.int64(2) == T.int64(1), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_vh % T.int64(2) == T.int64(0), T.float32(1), T.float32(0))))))))) * T.Select(v_r_b % T.int64(4) == T.int64(3) and v_vw % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_vw % T.int64(2) == T.int64(0), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_vw % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_vw % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_vw % T.int64(2) == T.int64(1), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_vw % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_vw % T.int64(2) == T.int64(1), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_vw % T.int64(2) == T.int64(0), T.float32(1), T.float32(0)))))))))
                for h_1, w_1 in T.grid(T.int64(2), T.int64(2)):
                    with T.block("conv2d_winograd"):
                        v_n = T.axis.spatial(T.int64(1), T.int64(0))
                        v_co = T.axis.spatial(T.int64(128), (n_co_h_0_w_0_fused_0 * T.int64(32) + n_co_h_0_w_0_fused_1) // T.int64(196))
                        v_h = T.axis.spatial(T.int64(28), (n_co_h_0_w_0_fused_0 * T.int64(32) + n_co_h_0_w_0_fused_1) % T.int64(196) // T.int64(14) * T.int64(2) + h_1)
                        v_w = T.axis.spatial(T.int64(28), (n_co_h_0_w_0_fused_0 * T.int64(32) + n_co_h_0_w_0_fused_1) % T.int64(14) * T.int64(2) + w_1)
                        T.reads(inverse_local[v_co, v_n * T.int64(196) + v_h // T.int64(2) * T.int64(14) + v_w // T.int64(2), v_h % T.int64(2), v_w % T.int64(2)], p2[v_n, v_co, T.int64(0), T.int64(0)])
                        T.writes(T_relu[v_n, v_co, v_h, v_w])
                        T_relu[v_n, v_co, v_h, v_w] = T.max(inverse_local[v_co, v_n * T.int64(196) + v_h // T.int64(2) * T.int64(14) + v_w // T.int64(2), v_h % T.int64(2), v_w % T.int64(2)] + p2[v_n, v_co, T.int64(0), T.int64(0)], T.float32(0))
b0 = sch.get_block(name="data_pack", func_name="main")
b1 = sch.get_block(name="bgemm", func_name="main")
b2 = sch.get_block(name="inverse", func_name="main")
b3 = sch.get_block(name="conv2d_winograd", func_name="main")
b4 = sch.get_block(name="T_add", func_name="main")
b5 = sch.get_block(name="T_relu", func_name="main")
b6 = sch.get_block(name="root", func_name="main")
b7, b8 = sch.get_producers(block=b2)
sch.compute_inline(block=b8)
b9, = sch.get_consumers(block=b2)
l10, l11, l12, l13 = sch.get_loops(block=b9)
l14, l15 = sch.split(loop=l12, factors=[None, 2], preserve_unit_iters=True)
l16, l17 = sch.split(loop=l13, factors=[None, 2], preserve_unit_iters=True)
sch.reorder(l14, l16, l15, l17)
sch.compute_at(block=b2, loop=l16, preserve_unit_loops=True, index=-1)
sch.set_scope(block=b2, buffer_index=0, storage_scope="local")
l18, l19, l20, l21, l22, l23, l24, l25, l26, l27 = sch.get_loops(block=b2)
sch.unroll(loop=l24)
sch.unroll(loop=l25)
sch.unroll(loop=l26)
sch.unroll(loop=l27)
b28, b29 = sch.get_producers(block=b0)
sch.compute_inline(block=b29)
b30, = sch.get_producers(block=b28)
l31, l32, l33, l34, l35, l36 = sch.get_loops(block=b0)
sch.reorder(l33, l34, l31, l32, l35, l36)
sch.unroll(loop=l31)
sch.unroll(loop=l32)
sch.unroll(loop=l35)
sch.unroll(loop=l36)
l37 = sch.fuse(l33, l34, preserve_unit_iters=True)
v38 = sch.sample_categorical(candidates=[32, 64, 128, 256, 512, 1024], probs=[0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666], decision=1)
l39, l40 = sch.split(loop=l37, factors=[None, v38], preserve_unit_iters=True)
sch.bind(loop=l39, thread_axis="blockIdx.x")
sch.bind(loop=l40, thread_axis="threadIdx.x")
b41 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b41, loop=l40, preserve_unit_loops=True, index=-1)
sch.compute_at(block=b28, loop=l40, preserve_unit_loops=True, index=-1)
sch.set_scope(block=b28, buffer_index=0, storage_scope="local")
sch.compute_inline(block=b30)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l42, l43, l44, l45, l46 = sch.get_loops(block=b1)
v47, v48, v49, v50, v51 = sch.sample_perfect_tile(loop=l42, n=5, max_innermost_factor=64, decision=[4, 1, 1, 1, 1])
l52, l53, l54, l55, l56 = sch.split(loop=l42, factors=[v47, v48, v49, v50, v51], preserve_unit_iters=True)
v57, v58, v59, v60, v61 = sch.sample_perfect_tile(loop=l43, n=5, max_innermost_factor=64, decision=[4, 1, 1, 1, 1])
l62, l63, l64, l65, l66 = sch.split(loop=l43, factors=[v57, v58, v59, v60, v61], preserve_unit_iters=True)
v67, v68, v69, v70, v71 = sch.sample_perfect_tile(loop=l44, n=5, max_innermost_factor=64, decision=[4, 1, 8, 4, 1])
l72, l73, l74, l75, l76 = sch.split(loop=l44, factors=[v67, v68, v69, v70, v71], preserve_unit_iters=True)
v77, v78, v79, v80, v81 = sch.sample_perfect_tile(loop=l45, n=5, max_innermost_factor=64, decision=[7, 2, 14, 1, 1])
l82, l83, l84, l85, l86 = sch.split(loop=l45, factors=[v77, v78, v79, v80, v81], preserve_unit_iters=True)
v87, v88, v89 = sch.sample_perfect_tile(loop=l46, n=3, max_innermost_factor=64, decision=[8, 4, 4])
l90, l91, l92 = sch.split(loop=l46, factors=[v87, v88, v89], preserve_unit_iters=True)
sch.reorder(l52, l62, l72, l82, l53, l63, l73, l83, l54, l64, l74, l84, l90, l91, l55, l65, l75, l85, l92, l56, l66, l76, l86)
l93 = sch.fuse(l52, l62, l72, l82, preserve_unit_iters=True)
sch.bind(loop=l93, thread_axis="blockIdx.x")
l94 = sch.fuse(l53, l63, l73, l83, preserve_unit_iters=True)
sch.bind(loop=l94, thread_axis="vthread.x")
l95 = sch.fuse(l54, l64, l74, l84, preserve_unit_iters=True)
sch.bind(loop=l95, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b96 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b96, loop=l95, preserve_unit_loops=True, index=-1)
b97 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b97, loop=l90, preserve_unit_loops=True, index=-1)
l98, l99, l100, l101, l102, l103, l104, l105 = sch.get_loops(block=b97)
l106 = sch.fuse(l102, l103, l104, l105, preserve_unit_iters=True)
v107 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b97, ann_key="meta_schedule.cooperative_fetch", ann_val=v107)
b108 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b108, loop=l90, preserve_unit_loops=True, index=-1)
l109, l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b108)
l117 = sch.fuse(l113, l114, l115, l116, preserve_unit_iters=True)
v118 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b108, ann_key="meta_schedule.cooperative_fetch", ann_val=v118)
l119 = sch.fuse(l90, preserve_unit_iters=True)
sch.annotate(block_or_loop=l119, ann_key="software_pipeline_stage", ann_val=[0, 0, 2])
sch.annotate(block_or_loop=l119, ann_key="software_pipeline_order", ann_val=[0, 1, 2])
sch.annotate(block_or_loop=l119, ann_key="software_pipeline_async_stages", ann_val=[0])
sch.reverse_compute_inline(block=b5)
sch.reverse_compute_inline(block=b4)
v120 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=1)
sch.annotate(block_or_loop=b6, ann_key="meta_schedule.unroll_explicit", ann_val=v120)
l121, l122, l123, l124, l125, l126 = sch.get_loops(block=b3)
l127 = sch.fuse(l121, l122, l123, l124, preserve_unit_iters=True)
v128 = sch.sample_categorical(candidates=[32, 64, 128, 256, 512, 1024], probs=[0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666], decision=0)
l129, l130 = sch.split(loop=l127, factors=[None, v128], preserve_unit_iters=True)
sch.bind(loop=l129, thread_axis="blockIdx.x")
sch.bind(loop=l130, thread_axis="threadIdx.x")
sch.enter_postproc()
sch.unannotate(block_or_loop=b97, ann_key="meta_schedule.cooperative_fetch")
l131, l132, l133, l134, l135 = sch.get_loops(block=b97)
l136, l137 = sch.split(loop=l135, factors=[None, 112], preserve_unit_iters=True)
sch.bind(loop=l137, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b108, ann_key="meta_schedule.cooperative_fetch")
l138, l139, l140, l141, l142 = sch.get_loops(block=b108)
l143, l144, l145 = sch.split(loop=l142, factors=[None, 112, 4], preserve_unit_iters=True)
sch.vectorize(loop=l145)
sch.bind(loop=l144, thread_axis="threadIdx.x")
b146 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b146, ann_key="meta_schedule.unroll_explicit")
b147, b148, b149, b150, b151, b152, b153, b154, b155 = sch.get_child_blocks(b146)
l156, l157, l158, l159, l160, l161 = sch.get_loops(block=b147)
l162, l163, l164, l165, l166, l167 = sch.get_loops(block=b148)
sch.annotate(block_or_loop=l162, ann_key="pragma_auto_unroll_max_step", ann_val=16)
sch.annotate(block_or_loop=l162, ann_key="pragma_unroll_explicit", ann_val=1)
l168, l169, l170, l171, l172, l173 = sch.get_loops(block=b149)
l174, l175, l176, l177, l178, l179 = sch.get_loops(block=b150)
l180, l181, l182, l183, l184, l185, l186 = sch.get_loops(block=b151)
l187, l188, l189, l190, l191, l192, l193, l194, l195, l196, l197, l198, l199, l200 = sch.get_loops(block=b152)
sch.annotate(block_or_loop=l187, ann_key="pragma_auto_unroll_max_step", ann_val=16)
sch.annotate(block_or_loop=l187, ann_key="pragma_unroll_explicit", ann_val=1)
l201, l202, l203, l204, l205, l206, l207 = sch.get_loops(block=b153)
l208, l209, l210, l211, l212, l213, l214, l215 = sch.get_loops(block=b154)
sch.annotate(block_or_loop=l208, ann_key="pragma_auto_unroll_max_step", ann_val=16)
sch.annotate(block_or_loop=l208, ann_key="pragma_unroll_explicit", ann_val=1)
l216, l217, l218, l219 = sch.get_loops(block=b155)
b220 = sch.get_block(name="data_pack", func_name="main")
l221, l222, l223, l224, l225, l226 = sch.get_loops(block=b220)
b227 = sch.decompose_reduction(block=b220, loop=l225)
b228 = sch.get_block(name="bgemm", func_name="main")
l229, l230, l231, l232, l233, l234, l235, l236, l237, l238, l239, l240, l241, l242 = sch.get_loops(block=b228)
b243 = sch.decompose_reduction(block=b228, loop=l232)
b244 = sch.get_block(name="inverse", func_name="main")
l245, l246, l247, l248, l249, l250, l251, l252 = sch.get_loops(block=b244)
b253 = sch.decompose_reduction(block=b244, loop=l251)
2023-11-11 03:17:54 [INFO] [task_scheduler.cc:121] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #258: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/home2/xiachunwei/Software/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/home2/xiachunwei/Software/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/home2/xiachunwei/Software/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/home2/xiachunwei/Software/tvm/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/home2/xiachunwei/Software/tvm/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/home2/xiachunwei/Software/tvm/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/home2/xiachunwei/Software/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/home2/xiachunwei/Software/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  4: _ZN3tvm7runtime13PackedFun
  3: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  2: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  0: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  File "/home2/xiachunwei/Software/tvm/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(128), T.int64(28), T.int64(28)), "float32"), p1: T.Buffer((T.int64(4), T.int64(4), T.int64(128), T.int64(128)), "float32"), p2: T.Buffer((T.int64(1), T.int64(128), T.int64(1), T.int64(1)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(128), T.int64(28), T.int64(28)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        input_tile_local = T.alloc_buffer((T.int64(128), T.int64(196), T.int64(4), T.int64(4)), scope="local")
        data_pack = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(128), T.int64(196)))
        bgemm = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(128), T.int64(196)))
        inverse_local = T.alloc_buffer((T.int64(128), T.int64(196), T.int64(2), T.int64(2)), scope="local")
        data_pack_local = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(128), T.int64(196)), scope="local")
        bgemm_local = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(128), T.int64(196)), scope="local")
        data_pack_shared = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(128), T.int64(196)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(128), T.int64(128)), scope="shared")
        for ci_p_fused_0 in T.thread_binding(T.int64(392), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 16, "pragma_unroll_explicit": 1}):
            for ci_p_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(1), T.int64(4), T.int64(4)):
                    with T.block("input_tile"):
                        v_ci = T.axis.spatial(T.int64(128), (ci_p_fused_0 * T.int64(64) + ci_p_fused_1) // T.int64(196) + ax0)
                        v_p = T.axis.spatial(T.int64(196), (ci_p_fused_0 * T.int64(64) + ci_p_fused_1) % T.int64(196) + ax1)
                        v_eps, v_nu = T.axis.remap("SS", [ax2, ax3])
                        T.reads(p0[v_p // T.int64(196), v_ci, v_p % T.int64(196) // T.int64(14) * T.int64(2) + v_eps - T.int64(1), v_p % T.int64(14) * T.int64(2) + v_nu - T.int64(1)])
                        T.writes(input_tile_local[v_ci, v_p, v_eps, v_nu])
                        T.block_attr({"schedule_rule": "None"})
                        input_tile_local[v_ci, v_p, v_eps, v_nu] = T.if_then_else(T.int64(1) <= v_p % T.int64(196) // T.int64(14) * T.int64(2) + v_eps and v_p % T.int64(196) // T.int64(14) * T.int64(2) + v_eps < T.int64(29) and T.int64(1) <= v_p % T.int64(14) * T.int64(2) + v_nu and v_p % T.int64(14) * T.int64(2) + v_nu < T.int64(29), p0[v_p // T.int64(196), v_ci, v_p % T.int64(196) // T.int64(14) * T.int64(2) + v_eps - T.int64(1), v_p % T.int64(14) * T.int64(2) + v_nu - T.int64(1)], T.float32(0))
                for eps in T.unroll(T.int64(4)):
                    for nu in T.unroll(T.int64(4)):
                        with T.block("data_pack_init"):
                            v_eps, v_nu = T.axis.remap("SS", [eps, nu])
                            v_ci = T.axis.spatial(T.int64(128), (ci_p_fused_0 * T.int64(64) + ci_p_fused_1) // T.int64(196))
                            v_p = T.axis.spatial(T.int64(196), (ci_p_fused_0 * T.int64(64) + ci_p_fused_1) % T.int64(196))
                            T.reads()
                            T.writes(data_pack_local[v_eps, v_nu, v_ci, v_p])
                            T.block_attr({"schedule_rule": "conv2d_nchw_winograd_data_pack"})
                            data_pack_local[v_eps, v_nu, v_ci, v_p] = T.float32(0)
                        for r_a in T.unroll(T.int64(4)):
                            for r_b in T.unroll(T.int64(4)):
                                with T.block("data_pack_update"):
                                    v_eps, v_nu = T.axis.remap("SS", [eps, nu])
                                    v_ci = T.axis.spatial(T.int64(128), (ci_p_fused_0 * T.int64(64) + ci_p_fused_1) // T.int64(196))
                                    v_p = T.axis.spatial(T.int64(196), (ci_p_fused_0 * T.int64(64) + ci_p_fused_1) % T.int64(196))
                                    v_r_a, v_r_b = T.axis.remap("RR", [r_a, r_b])
                                    T.reads(data_pack_local[v_eps, v_nu, v_ci, v_p], input_tile_local[v_ci, v_p, v_r_a, v_r_b])
                                    T.writes(data_pack_local[v_eps, v_nu, v_ci, v_p])
                                    T.block_attr({"schedule_rule": "conv2d_nchw_winograd_data_pack"})
                                    data_pack_local[v_eps, v_nu, v_ci, v_p] = data_pack_local[v_eps, v_nu, v_ci, v_p] + input_tile_local[v_ci, v_p, v_r_a, v_r_b] * T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(3), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(1), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(0), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(3), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(1), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(0), T.float32(1), T.float32(0))))))))))))))))) * T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(3), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(1), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(0), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(3), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(1), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(0), T.float32(1), T.float32(0)))))))))))))))))
                for ax0, ax1, ax2, ax3 in T.grid(T.int64(4), T.int64(4), T.int64(1), T.int64(1)):
                    with T.block("data_pack_local"):
                        v0, v1 = T.axis.remap("SS", [ax0, ax1])
                        v2 = T.axis.spatial(T.int64(128), (ci_p_fused_0 * T.int64(64) + ci_p_fused_1) // T.int64(196) + ax2)
                        v3 = T.axis.spatial(T.int64(196), (ci_p_fused_0 * T.int64(64) + ci_p_fused_1) % T.int64(196) + ax3)
                        T.reads(data_pack_local[v0, v1, v2, v3])
                        T.writes(data_pack[v0, v1, v2, v3])
                        data_pack[v0, v1, v2, v3] = data_pack_local[v0, v1, v2, v3]
        for eps_0_nu_0_co_0_p_0_fused in T.thread_binding(T.int64(448), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 16, "pragma_unroll_explicit": 1}):
            for eps_1_nu_1_co_1_p_1_fused in T.thread_binding(T.int64(2), thread="vthread.x"):
                for eps_2_nu_2_co_2_p_2_fused in T.thread_binding(T.int64(112), thread="threadIdx.x"):
                    for eps_3_init, nu_3_init, co_3_init, p_3_init, eps_4_init, nu_4_init, co_4_init, p_4_init in T.grid(T.int64(1), T.int64(1), T.int64(4), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("bgemm_init"):
                            v_eps = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(112) + eps_3_init + eps_4_init)
                            v_nu = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused % T.int64(112) // T.int64(28) + nu_3_init + nu_4_init)
                            v_co = T.axis.spatial(T.int64(128), eps_0_nu_0_co_0_p_0_fused % T.int64(28) // T.int64(7) * T.int64(32) + eps_2_nu_2_co_2_p_2_fused // T.int64(14) * T.int64(4) + co_3_init + co_4_init)
                            v_p = T.axis.spatial(T.int64(196), eps_0_nu_0_co_0_p_0_fused % T.int64(7) * T.int64(28) + eps_1_nu_1_co_1_p_1_fused * T.int64(14) + eps_2_nu_2_co_2_p_2_fused % T.int64(14) + p_3_init + p_4_init)
                            T.reads()
                            T.writes(bgemm_local[v_eps, v_nu, v_co, v_p])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            bgemm_local[v_eps, v_nu, v_co, v_p] = T.float32(0)
                    for ci_0_fused in T.serial(T.int64(8), annotations={"software_pipeline_async_stages": [0], "software_pipeline_order": [0, 1, 2], "software_pipeline_stage": [0, 0, 2]}):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(1)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(112), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("data_pack_shared"):
                                        v0 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(112))
                                        v1 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused % T.int64(112) // T.int64(28))
                                        v2 = T.axis.spatial(T.int64(128), ci_0_fused * T.int64(16) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(448) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) // T.int64(28))
                                        v3 = T.axis.spatial(T.int64(196), eps_0_nu_0_co_0_p_0_fused % T.int64(7) * T.int64(28) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(448) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) % T.int64(28))
                                        T.reads(data_pack[v0, v1, v2, v3])
                                        T.writes(data_pack_shared[v0, v1, v2, v3])
                                        data_pack_shared[v0, v1, v2, v3] = data_pack[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(2)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(112), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("p1_shared"):
                                        v0 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(112))
                                        v1 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused % T.int64(112) // T.int64(28))
                                        v2 = T.axis.spatial(T.int64(128), ci_0_fused * T.int64(16) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(448) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) // T.int64(32))
                                        v3 = T.axis.spatial(T.int64(128), eps_0_nu_0_co_0_p_0_fused % T.int64(28) // T.int64(7) * T.int64(32) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(448) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) % T.int64(32))
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * T.int64(112) + ax0_ax1_ax2_ax3_fused_1) * T.int64(4) + ax0_ax1_ax2_ax3_fused_2 < T.int64(512))
                                        T.reads(p1[v0, v1, v2, v3])
                                        T.writes(p1_shared[v0, v1, v2, v3])
                                        p1_shared[v0, v1, v2, v3] = p1[v0, v1, v2, v3]
                        for ci_1, eps_3, nu_3, co_3, p_3, ci_2, eps_4, nu_4, co_4, p_4 in T.grid(T.int64(4), T.int64(1), T.int64(1), T.int64(4), T.int64(1), T.int64(4), T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                            with T.block("bgemm_update"):
                                v_eps = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(112) + eps_3 + eps_4)
                                v_nu = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused % T.int64(112) // T.int64(28) + nu_3 + nu_4)
                                v_co = T.axis.spatial(T.int64(128), eps_0_nu_0_co_0_p_0_fused % T.int64(28) // T.int64(7) * T.int64(32) + eps_2_nu_2_co_2_p_2_fused // T.int64(14) * T.int64(4) + co_3 + co_4)
                                v_p = T.axis.spatial(T.int64(196), eps_0_nu_0_co_0_p_0_fused % T.int64(7) * T.int64(28) + eps_1_nu_1_co_1_p_1_fused * T.int64(14) + eps_2_nu_2_co_2_p_2_fused % T.int64(14) + p_3 + p_4)
                                v_ci = T.axis.reduce(T.int64(128), ci_0_fused * T.int64(16) + ci_1 * T.int64(4) + ci_2)
                                T.reads(bgemm_local[v_eps, v_nu, v_co, v_p], data_pack_shared[v_eps, v_nu, v_ci, v_p], p1_shared[v_eps, v_nu, v_ci, v_co])
                                T.writes(bgemm_local[v_eps, v_nu, v_co, v_p])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                bgemm_local[v_eps, v_nu, v_co, v_p] = bgemm_local[v_eps, v_nu, v_co, v_p] + data_pack_shared[v_eps, v_nu, v_ci, v_p] * p1_shared[v_eps, v_nu, v_ci, v_co]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(1), T.int64(4), T.int64(1)):
                        with T.block("bgemm_local"):
                            v0 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(112) + ax0)
                            v1 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused % T.int64(112) // T.int64(28) + ax1)
                            v2 = T.axis.spatial(T.int64(128), eps_0_nu_0_co_0_p_0_fused % T.int64(28) // T.int64(7) * T.int64(32) + eps_2_nu_2_co_2_p_2_fused // T.int64(14) * T.int64(4) + ax2)
                            v3 = T.axis.spatial(T.int64(196), eps_0_nu_0_co_0_p_0_fused % T.int64(7) * T.int64(28) + eps_1_nu_1_co_1_p_1_fused * T.int64(14) + eps_2_nu_2_co_2_p_2_fused % T.int64(14) + ax3)
                            T.reads(bgemm_local[v0, v1, v2, v3])
                            T.writes(bgemm[v0, v1, v2, v3])
                            bgemm[v0, v1, v2, v3] = bgemm_local[v0, v1, v2, v3]
        for n_co_h_0_w_0_fused_0 in T.thread_binding(T.int64(784), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 16, "pragma_unroll_explicit": 1}):
            for n_co_h_0_w_0_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                    for ax2 in T.unroll(T.int64(2)):
                        for ax3 in T.unroll(T.int64(2)):
                            with T.block("inverse_init"):
                                v_co = T.axis.spatial(T.int64(128), (n_co_h_0_w_0_fused_0 * T.int64(32) + n_co_h_0_w_0_fused_1) // T.int64(196) + ax0)
                                v_p = T.axis.spatial(T.int64(196), (n_co_h_0_w_0_fused_0 * T.int64(32) + n_co_h_0_w_0_fused_1) % T.int64(196) + ax1)
                                v_vh, v_vw = T.axis.remap("SS", [ax2, ax3])
                                T.reads()
                                T.writes(inverse_local[v_co, v_p, v_vh, v_vw])
                                T.block_attr({"schedule_rule": "conv2d_nchw_winograd_inverse"})
                                inverse_local[v_co, v_p, v_vh, v_vw] = T.float32(0)
                            for ax4 in T.unroll(T.int64(4)):
                                for ax5 in T.unroll(T.int64(4)):
                                    with T.block("inverse_update"):
                                        v_co = T.axis.spatial(T.int64(128), (n_co_h_0_w_0_fused_0 * T.int64(32) + n_co_h_0_w_0_fused_1) // T.int64(196) + ax0)
                                        v_p = T.axis.spatial(T.int64(196), (n_co_h_0_w_0_fused_0 * T.int64(32) + n_co_h_0_w_0_fused_1) % T.int64(196) + ax1)
                                        v_vh, v_vw, v_r_a, v_r_b = T.axis.remap("SSRR", [ax2, ax3, ax4, ax5])
                                        T.reads(inverse_local[v_co, v_p, v_vh, v_vw], bgemm[v_r_a, v_r_b, v_co, v_p])
                                        T.writes(inverse_local[v_co, v_p, v_vh, v_vw])
                                        T.block_attr({"schedule_rule": "conv2d_nchw_winograd_inverse"})
                                        inverse_local[v_co, v_p, v_vh, v_vw] = inverse_local[v_co, v_p, v_vh, v_vw] + bgemm[v_r_a, v_r_b, v_co, v_p] * T.Select(v_r_a % T.int64(4) == T.int64(3) and v_vh % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_vh % T.int64(2) == T.int64(0), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_vh % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_vh % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_vh % T.int64(2) == T.int64(1), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_vh % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_vh % T.int64(2) == T.int64(1), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_vh % T.int64(2) == T.int64(0), T.float32(1), T.float32(0))))))))) * T.Select(v_r_b % T.int64(4) == T.int64(3) and v_vw % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_vw % T.int64(2) == T.int64(0), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_vw % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_vw % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_vw % T.int64(2) == T.int64(1), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_vw % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_vw % T.int64(2) == T.int64(1), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_vw % T.int64(2) == T.int64(0), T.float32(1), T.float32(0)))))))))
                for h_1, w_1 in T.grid(T.int64(2), T.int64(2)):
                    with T.block("conv2d_winograd"):
                        v_n = T.axis.spatial(T.int64(1), T.int64(0))
                        v_co = T.axis.spatial(T.int64(128), (n_co_h_0_w_0_fused_0 * T.int64(32) + n_co_h_0_w_0_fused_1) // T.int64(196))
                        v_h = T.axis.spatial(T.int64(28), (n_co_h_0_w_0_fused_0 * T.int64(32) + n_co_h_0_w_0_fused_1) % T.int64(196) // T.int64(14) * T.int64(2) + h_1)
                        v_w = T.axis.spatial(T.int64(28), (n_co_h_0_w_0_fused_0 * T.int64(32) + n_co_h_0_w_0_fused_1) % T.int64(14) * T.int64(2) + w_1)
                        T.reads(inverse_local[v_co, v_n * T.int64(196) + v_h // T.int64(2) * T.int64(14) + v_w // T.int64(2), v_h % T.int64(2), v_w % T.int64(2)], p2[v_n, v_co, T.int64(0), T.int64(0)])
                        T.writes(T_relu[v_n, v_co, v_h, v_w])
                        T_relu[v_n, v_co, v_h, v_w] = T.max(inverse_local[v_co, v_n * T.int64(196) + v_h // T.int64(2) * T.int64(14) + v_w // T.int64(2), v_h % T.int64(2), v_w % T.int64(2)] + p2[v_n, v_co, T.int64(0), T.int64(0)], T.float32(0))
b0 = sch.get_block(name="data_pack", func_name="main")
b1 = sch.get_block(name="bgemm", func_name="main")
b2 = sch.get_block(name="inverse", func_name="main")
b3 = sch.get_block(name="conv2d_winograd", func_name="main")
b4 = sch.get_block(name="T_add", func_name="main")
b5 = sch.get_block(name="T_relu", func_name="main")
b6 = sch.get_block(name="root", func_name="main")
b7, b8 = sch.get_producers(block=b2)
sch.compute_inline(block=b8)
b9, = sch.get_consumers(block=b2)
l10, l11, l12, l13 = sch.get_loops(block=b9)
l14, l15 = sch.split(loop=l12, factors=[None, 2], preserve_unit_iters=True)
l16, l17 = sch.split(loop=l13, factors=[None, 2], preserve_unit_iters=True)
sch.reorder(l14, l16, l15, l17)
sch.compute_at(block=b2, loop=l16, preserve_unit_loops=True, index=-1)
sch.set_scope(block=b2, buffer_index=0, storage_scope="local")
l18, l19, l20, l21, l22, l23, l24, l25, l26, l27 = sch.get_loops(block=b2)
sch.unroll(loop=l24)
sch.unroll(loop=l25)
sch.unroll(loop=l26)
sch.unroll(loop=l27)
b28, b29 = sch.get_producers(block=b0)
sch.compute_inline(block=b29)
b30, = sch.get_producers(block=b28)
l31, l32, l33, l34, l35, l36 = sch.get_loops(block=b0)
sch.reorder(l33, l34, l31, l32, l35, l36)
sch.unroll(loop=l31)
sch.unroll(loop=l32)
sch.unroll(loop=l35)
sch.unroll(loop=l36)
l37 = sch.fuse(l33, l34, preserve_unit_iters=True)
v38 = sch.sample_categorical(candidates=[32, 64, 128, 256, 512, 1024], probs=[0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666], decision=1)
l39, l40 = sch.split(loop=l37, factors=[None, v38], preserve_unit_iters=True)
sch.bind(loop=l39, thread_axis="blockIdx.x")
sch.bind(loop=l40, thread_axis="threadIdx.x")
b41 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b41, loop=l40, preserve_unit_loops=True, index=-1)
sch.compute_at(block=b28, loop=l40, preserve_unit_loops=True, index=-1)
sch.set_scope(block=b28, buffer_index=0, storage_scope="local")
sch.compute_inline(block=b30)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l42, l43, l44, l45, l46 = sch.get_loops(block=b1)
v47, v48, v49, v50, v51 = sch.sample_perfect_tile(loop=l42, n=5, max_innermost_factor=64, decision=[4, 1, 1, 1, 1])
l52, l53, l54, l55, l56 = sch.split(loop=l42, factors=[v47, v48, v49, v50, v51], preserve_unit_iters=True)
v57, v58, v59, v60, v61 = sch.sample_perfect_tile(loop=l43, n=5, max_innermost_factor=64, decision=[4, 1, 1, 1, 1])
l62, l63, l64, l65, l66 = sch.split(loop=l43, factors=[v57, v58, v59, v60, v61], preserve_unit_iters=True)
v67, v68, v69, v70, v71 = sch.sample_perfect_tile(loop=l44, n=5, max_innermost_factor=64, decision=[4, 1, 8, 4, 1])
l72, l73, l74, l75, l76 = sch.split(loop=l44, factors=[v67, v68, v69, v70, v71], preserve_unit_iters=True)
v77, v78, v79, v80, v81 = sch.sample_perfect_tile(loop=l45, n=5, max_innermost_factor=64, decision=[7, 2, 14, 1, 1])
l82, l83, l84, l85, l86 = sch.split(loop=l45, factors=[v77, v78, v79, v80, v81], preserve_unit_iters=True)
v87, v88, v89 = sch.sample_perfect_tile(loop=l46, n=3, max_innermost_factor=64, decision=[8, 4, 4])
l90, l91, l92 = sch.split(loop=l46, factors=[v87, v88, v89], preserve_unit_iters=True)
sch.reorder(l52, l62, l72, l82, l53, l63, l73, l83, l54, l64, l74, l84, l90, l91, l55, l65, l75, l85, l92, l56, l66, l76, l86)
l93 = sch.fuse(l52, l62, l72, l82, preserve_unit_iters=True)
sch.bind(loop=l93, thread_axis="blockIdx.x")
l94 = sch.fuse(l53, l63, l73, l83, preserve_unit_iters=True)
sch.bind(loop=l94, thread_axis="vthread.x")
l95 = sch.fuse(l54, l64, l74, l84, preserve_unit_iters=True)
sch.bind(loop=l95, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b96 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b96, loop=l95, preserve_unit_loops=True, index=-1)
b97 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b97, loop=l90, preserve_unit_loops=True, index=-1)
l98, l99, l100, l101, l102, l103, l104, l105 = sch.get_loops(block=b97)
l106 = sch.fuse(l102, l103, l104, l105, preserve_unit_iters=True)
v107 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b97, ann_key="meta_schedule.cooperative_fetch", ann_val=v107)
b108 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b108, loop=l90, preserve_unit_loops=True, index=-1)
l109, l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b108)
l117 = sch.fuse(l113, l114, l115, l116, preserve_unit_iters=True)
v118 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b108, ann_key="meta_schedule.cooperative_fetch", ann_val=v118)
l119 = sch.fuse(l90, preserve_unit_iters=True)
sch.annotate(block_or_loop=l119, ann_key="software_pipeline_stage", ann_val=[0, 0, 2])
sch.annotate(block_or_loop=l119, ann_key="software_pipeline_order", ann_val=[0, 1, 2])
sch.annotate(block_or_loop=l119, ann_key="software_pipeline_async_stages", ann_val=[0])
sch.reverse_compute_inline(block=b5)
sch.reverse_compute_inline(block=b4)
v120 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=1)
sch.annotate(block_or_loop=b6, ann_key="meta_schedule.unroll_explicit", ann_val=v120)
l121, l122, l123, l124, l125, l126 = sch.get_loops(block=b3)
l127 = sch.fuse(l121, l122, l123, l124, preserve_unit_iters=True)
v128 = sch.sample_categorical(candidates=[32, 64, 128, 256, 512, 1024], probs=[0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666], decision=0)
l129, l130 = sch.split(loop=l127, factors=[None, v128], preserve_unit_iters=True)
sch.bind(loop=l129, thread_axis="blockIdx.x")
sch.bind(loop=l130, thread_axis="threadIdx.x")
sch.enter_postproc()
sch.unannotate(block_or_loop=b97, ann_key="meta_schedule.cooperative_fetch")
l131, l132, l133, l134, l135 = sch.get_loops(block=b97)
l136, l137, l138 = sch.split(loop=l135, factors=[None, 112, 4], preserve_unit_iters=True)
sch.vectorize(loop=l138)
sch.bind(loop=l137, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b108, ann_key="meta_schedule.cooperative_fetch")
l139, l140, l141, l142, l143 = sch.get_loops(block=b108)
l144, l145, l146 = sch.split(loop=l143, factors=[None, 112, 4], preserve_unit_iters=True)
sch.vectorize(loop=l146)
sch.bind(loop=l145, thread_axis="threadIdx.x")
b147 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b147, ann_key="meta_schedule.unroll_explicit")
b148, b149, b150, b151, b152, b153, b154, b155, b156 = sch.get_child_blocks(b147)
l157, l158, l159, l160, l161, l162 = sch.get_loops(block=b148)
l163, l164, l165, l166, l167, l168 = sch.get_loops(block=b149)
sch.annotate(block_or_loop=l163, ann_key="pragma_auto_unroll_max_step", ann_val=16)
sch.annotate(block_or_loop=l163, ann_key="pragma_unroll_explicit", ann_val=1)
l169, l170, l171, l172, l173, l174 = sch.get_loops(block=b150)
l175, l176, l177, l178, l179, l180, l181 = sch.get_loops(block=b151)
l182, l183, l184, l185, l186, l187, l188 = sch.get_loops(block=b152)
l189, l190, l191, l192, l193, l194, l195, l196, l197, l198, l199, l200, l201, l202 = sch.get_loops(block=b153)
sch.annotate(block_or_loop=l189, ann_key="pragma_auto_unroll_max_step", ann_val=16)
sch.annotate(block_or_loop=l189, ann_key="pragma_unroll_explicit", ann_val=1)
l203, l204, l205, l206, l207, l208, l209 = sch.get_loops(block=b154)
l210, l211, l212, l213, l214, l215, l216, l217 = sch.get_loops(block=b155)
sch.annotate(block_or_loop=l210, ann_key="pragma_auto_unroll_max_step", ann_val=16)
sch.annotate(block_or_loop=l210, ann_key="pragma_unroll_explicit", ann_val=1)
l218, l219, l220, l221 = sch.get_loops(block=b156)
b222 = sch.get_block(name="data_pack", func_name="main")
l223, l224, l225, l226, l227, l228 = sch.get_loops(block=b222)
b229 = sch.decompose_reduction(block=b222, loop=l227)
b230 = sch.get_block(name="bgemm", func_name="main")
l231, l232, l233, l234, l235, l236, l237, l238, l239, l240, l241, l242, l243, l244 = sch.get_loops(block=b230)
b245 = sch.decompose_reduction(block=b230, loop=l234)
b246 = sch.get_block(name="inverse", func_name="main")
l247, l248, l249, l250, l251, l252, l253, l254 = sch.get_loops(block=b246)
b255 = sch.decompose_reduction(block=b246, loop=l253)
2023-11-11 03:17:54 [INFO] [task_scheduler.cc:121] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #259: Error in running:
LocalRunner: An exception occurred
Subprocess terminated
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(128), T.int64(28), T.int64(28)), "float32"), p1: T.Buffer((T.int64(4), T.int64(4), T.int64(128), T.int64(128)), "float32"), p2: T.Buffer((T.int64(1), T.int64(128), T.int64(1), T.int64(1)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(128), T.int64(28), T.int64(28)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        input_tile_local = T.alloc_buffer((T.int64(128), T.int64(196), T.int64(4), T.int64(4)), scope="local")
        data_pack = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(128), T.int64(196)))
        bgemm = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(128), T.int64(196)))
        inverse_local = T.alloc_buffer((T.int64(128), T.int64(196), T.int64(2), T.int64(2)), scope="local")
        data_pack_local = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(128), T.int64(196)), scope="local")
        bgemm_local = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(128), T.int64(196)), scope="local")
        data_pack_shared = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(128), T.int64(196)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(128), T.int64(128)), scope="shared")
        for ci_p_fused_0 in T.thread_binding(T.int64(392), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 512, "pragma_unroll_explicit": 1}):
            for ci_p_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(1), T.int64(4), T.int64(4)):
                    with T.block("input_tile"):
                        v_ci = T.axis.spatial(T.int64(128), (ci_p_fused_0 * T.int64(64) + ci_p_fused_1) // T.int64(196) + ax0)
                        v_p = T.axis.spatial(T.int64(196), (ci_p_fused_0 * T.int64(64) + ci_p_fused_1) % T.int64(196) + ax1)
                        v_eps, v_nu = T.axis.remap("SS", [ax2, ax3])
                        T.reads(p0[v_p // T.int64(196), v_ci, v_p % T.int64(196) // T.int64(14) * T.int64(2) + v_eps - T.int64(1), v_p % T.int64(14) * T.int64(2) + v_nu - T.int64(1)])
                        T.writes(input_tile_local[v_ci, v_p, v_eps, v_nu])
                        T.block_attr({"schedule_rule": "None"})
                        input_tile_local[v_ci, v_p, v_eps, v_nu] = T.if_then_else(T.int64(1) <= v_p % T.int64(196) // T.int64(14) * T.int64(2) + v_eps and v_p % T.int64(196) // T.int64(14) * T.int64(2) + v_eps < T.int64(29) and T.int64(1) <= v_p % T.int64(14) * T.int64(2) + v_nu and v_p % T.int64(14) * T.int64(2) + v_nu < T.int64(29), p0[v_p // T.int64(196), v_ci, v_p % T.int64(196) // T.int64(14) * T.int64(2) + v_eps - T.int64(1), v_p % T.int64(14) * T.int64(2) + v_nu - T.int64(1)], T.float32(0))
                for eps in T.unroll(T.int64(4)):
                    for nu in T.unroll(T.int64(4)):
                        with T.block("data_pack_init"):
                            v_eps, v_nu = T.axis.remap("SS", [eps, nu])
                            v_ci = T.axis.spatial(T.int64(128), (ci_p_fused_0 * T.int64(64) + ci_p_fused_1) // T.int64(196))
                            v_p = T.axis.spatial(T.int64(196), (ci_p_fused_0 * T.int64(64) + ci_p_fused_1) % T.int64(196))
                            T.reads()
                            T.writes(data_pack_local[v_eps, v_nu, v_ci, v_p])
                            T.block_attr({"schedule_rule": "conv2d_nchw_winograd_data_pack"})
                            data_pack_local[v_eps, v_nu, v_ci, v_p] = T.float32(0)
                        for r_a in T.unroll(T.int64(4)):
                            for r_b in T.unroll(T.int64(4)):
                                with T.block("data_pack_update"):
                                    v_eps, v_nu = T.axis.remap("SS", [eps, nu])
                                    v_ci = T.axis.spatial(T.int64(128), (ci_p_fused_0 * T.int64(64) + ci_p_fused_1) // T.int64(196))
                                    v_p = T.axis.spatial(T.int64(196), (ci_p_fused_0 * T.int64(64) + ci_p_fused_1) % T.int64(196))
                                    v_r_a, v_r_b = T.axis.remap("RR", [r_a, r_b])
                                    T.reads(data_pack_local[v_eps, v_nu, v_ci, v_p], input_tile_local[v_ci, v_p, v_r_a, v_r_b])
                                    T.writes(data_pack_local[v_eps, v_nu, v_ci, v_p])
                                    T.block_attr({"schedule_rule": "conv2d_nchw_winograd_data_pack"})
                                    data_pack_local[v_eps, v_nu, v_ci, v_p] = data_pack_local[v_eps, v_nu, v_ci, v_p] + input_tile_local[v_ci, v_p, v_r_a, v_r_b] * T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(3), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(1), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(0), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(3), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(1), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(0), T.float32(1), T.float32(0))))))))))))))))) * T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(3), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(1), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(0), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(3), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(1), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(0), T.float32(1), T.float32(0)))))))))))))))))
                for ax0, ax1, ax2, ax3 in T.grid(T.int64(4), T.int64(4), T.int64(1), T.int64(1)):
                    with T.block("data_pack_local"):
                        v0, v1 = T.axis.remap("SS", [ax0, ax1])
                        v2 = T.axis.spatial(T.int64(128), (ci_p_fused_0 * T.int64(64) + ci_p_fused_1) // T.int64(196) + ax2)
                        v3 = T.axis.spatial(T.int64(196), (ci_p_fused_0 * T.int64(64) + ci_p_fused_1) % T.int64(196) + ax3)
                        T.reads(data_pack_local[v0, v1, v2, v3])
                        T.writes(data_pack[v0, v1, v2, v3])
                        data_pack[v0, v1, v2, v3] = data_pack_local[v0, v1, v2, v3]
        for eps_0_nu_0_co_0_p_0_fused in T.thread_binding(T.int64(112), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 512, "pragma_unroll_explicit": 1}):
            for eps_1_nu_1_co_1_p_1_fused in T.thread_binding(T.int64(2), thread="vthread.x"):
                for eps_2_nu_2_co_2_p_2_fused in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                    for eps_3_init, nu_3_init, co_3_init, p_3_init, eps_4_init, nu_4_init, co_4_init, p_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(28), T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("bgemm_init"):
                            v_eps = T.axis.spatial(T.int64(4), eps_1_nu_1_co_1_p_1_fused * T.int64(2) + eps_2_nu_2_co_2_p_2_fused // T.int64(32) + eps_3_init + eps_4_init)
                            v_nu = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(56) * T.int64(2) + eps_2_nu_2_co_2_p_2_fused % T.int64(32) // T.int64(16) + nu_3_init + nu_4_init)
                            v_co = T.axis.spatial(T.int64(128), eps_0_nu_0_co_0_p_0_fused % T.int64(56) // T.int64(7) * T.int64(16) + eps_2_nu_2_co_2_p_2_fused % T.int64(16) + co_3_init + co_4_init)
                            v_p = T.axis.spatial(T.int64(196), eps_0_nu_0_co_0_p_0_fused % T.int64(7) * T.int64(28) + p_3_init + p_4_init)
                            T.reads()
                            T.writes(bgemm_local[v_eps, v_nu, v_co, v_p])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            bgemm_local[v_eps, v_nu, v_co, v_p] = T.float32(0)
                    for ci_0_fused in T.serial(T.int64(16), annotations={"software_pipeline_async_stages": [0], "software_pipeline_order": [0, 1, 2], "software_pipeline_stage": [0, 0, 2]}):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(7)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("data_pack_shared"):
                                        v0 = T.axis.spatial(T.int64(4), (ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) // T.int64(448))
                                        v1 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(56) * T.int64(2) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) % T.int64(448) // T.int64(224))
                                        v2 = T.axis.spatial(T.int64(128), ci_0_fused * T.int64(8) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) % T.int64(224) // T.int64(28))
                                        v3 = T.axis.spatial(T.int64(196), eps_0_nu_0_co_0_p_0_fused % T.int64(7) * T.int64(28) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) % T.int64(28))
                                        T.reads(data_pack[v0, v1, v2, v3])
                                        T.writes(data_pack_shared[v0, v1, v2, v3])
                                        data_pack_shared[v0, v1, v2, v3] = data_pack[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(8)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("p1_shared"):
                                        v0 = T.axis.spatial(T.int64(4), (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) // T.int64(256))
                                        v1 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(56) * T.int64(2) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) % T.int64(256) // T.int64(128))
                                        v2 = T.axis.spatial(T.int64(128), ci_0_fused * T.int64(8) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) % T.int64(128) // T.int64(16))
                                        v3 = T.axis.spatial(T.int64(128), eps_0_nu_0_co_0_p_0_fused % T.int64(56) // T.int64(7) * T.int64(16) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) % T.int64(16))
                                        T.reads(p1[v0, v1, v2, v3])
                                        T.writes(p1_shared[v0, v1, v2, v3])
                                        p1_shared[v0, v1, v2, v3] = p1[v0, v1, v2, v3]
                        for ci_1, eps_3, nu_3, co_3, p_3, ci_2, eps_4, nu_4, co_4, p_4 in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(28), T.int64(8), T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                            with T.block("bgemm_update"):
                                v_eps = T.axis.spatial(T.int64(4), eps_1_nu_1_co_1_p_1_fused * T.int64(2) + eps_2_nu_2_co_2_p_2_fused // T.int64(32) + eps_3 + eps_4)
                                v_nu = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(56) * T.int64(2) + eps_2_nu_2_co_2_p_2_fused % T.int64(32) // T.int64(16) + nu_3 + nu_4)
                                v_co = T.axis.spatial(T.int64(128), eps_0_nu_0_co_0_p_0_fused % T.int64(56) // T.int64(7) * T.int64(16) + eps_2_nu_2_co_2_p_2_fused % T.int64(16) + co_3 + co_4)
                                v_p = T.axis.spatial(T.int64(196), eps_0_nu_0_co_0_p_0_fused % T.int64(7) * T.int64(28) + p_3 + p_4)
                                v_ci = T.axis.reduce(T.int64(128), ci_0_fused * T.int64(8) + ci_1 * T.int64(8) + ci_2)
                                T.reads(bgemm_local[v_eps, v_nu, v_co, v_p], data_pack_shared[v_eps, v_nu, v_ci, v_p], p1_shared[v_eps, v_nu, v_ci, v_co])
                                T.writes(bgemm_local[v_eps, v_nu, v_co, v_p])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                bgemm_local[v_eps, v_nu, v_co, v_p] = bgemm_local[v_eps, v_nu, v_co, v_p] + data_pack_shared[v_eps, v_nu, v_ci, v_p] * p1_shared[v_eps, v_nu, v_ci, v_co]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(28)):
                        with T.block("bgemm_local"):
                            v0 = T.axis.spatial(T.int64(4), eps_1_nu_1_co_1_p_1_fused * T.int64(2) + eps_2_nu_2_co_2_p_2_fused // T.int64(32) + ax0)
                            v1 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(56) * T.int64(2) + eps_2_nu_2_co_2_p_2_fused % T.int64(32) // T.int64(16) + ax1)
                            v2 = T.axis.spatial(T.int64(128), eps_0_nu_0_co_0_p_0_fused % T.int64(56) // T.int64(7) * T.int64(16) + eps_2_nu_2_co_2_p_2_fused % T.int64(16) + ax2)
                            v3 = T.axis.spatial(T.int64(196), eps_0_nu_0_co_0_p_0_fused % T.int64(7) * T.int64(28) + ax3)
                            T.reads(bgemm_local[v0, v1, v2, v3])
                            T.writes(bgemm[v0, v1, v2, v3])
                            bgemm[v0, v1, v2, v3] = bgemm_local[v0, v1, v2, v3]
        for n_co_h_0_w_0_fused_0 in T.thread_binding(T.int64(98), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 512, "pragma_unroll_explicit": 1}):
            for n_co_h_0_w_0_fused_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                    for ax2 in T.unroll(T.int64(2)):
                        for ax3 in T.unroll(T.int64(2)):
                            with T.block("inverse_init"):
                                v_co = T.axis.spatial(T.int64(128), (n_co_h_0_w_0_fused_0 * T.int64(256) + n_co_h_0_w_0_fused_1) // T.int64(196) + ax0)
                                v_p = T.axis.spatial(T.int64(196), (n_co_h_0_w_0_fused_0 * T.int64(256) + n_co_h_0_w_0_fused_1) % T.int64(196) + ax1)
                                v_vh, v_vw = T.axis.remap("SS", [ax2, ax3])
                                T.reads()
                                T.writes(inverse_local[v_co, v_p, v_vh, v_vw])
                                T.block_attr({"schedule_rule": "conv2d_nchw_winograd_inverse"})
                                inverse_local[v_co, v_p, v_vh, v_vw] = T.float32(0)
                            for ax4 in T.unroll(T.int64(4)):
                                for ax5 in T.unroll(T.int64(4)):
                                    with T.block("inverse_update"):
                                        v_co = T.axis.spatial(T.int64(128), (n_co_h_0_w_0_fused_0 * T.int64(256) + n_co_h_0_w_0_fused_1) // T.int64(196) + ax0)
                                        v_p = T.axis.spatial(T.int64(196), (n_co_h_0_w_0_fused_0 * T.int64(256) + n_co_h_0_w_0_fused_1) % T.int64(196) + ax1)
                                        v_vh, v_vw, v_r_a, v_r_b = T.axis.remap("SSRR", [ax2, ax3, ax4, ax5])
                                        T.reads(inverse_local[v_co, v_p, v_vh, v_vw], bgemm[v_r_a, v_r_b, v_co, v_p])
                                        T.writes(inverse_local[v_co, v_p, v_vh, v_vw])
                                        T.block_attr({"schedule_rule": "conv2d_nchw_winograd_inverse"})
                                        inverse_local[v_co, v_p, v_vh, v_vw] = inverse_local[v_co, v_p, v_vh, v_vw] + bgemm[v_r_a, v_r_b, v_co, v_p] * T.Select(v_r_a % T.int64(4) == T.int64(3) and v_vh % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_vh % T.int64(2) == T.int64(0), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_vh % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_vh % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_vh % T.int64(2) == T.int64(1), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_vh % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_vh % T.int64(2) == T.int64(1), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_vh % T.int64(2) == T.int64(0), T.float32(1), T.float32(0))))))))) * T.Select(v_r_b % T.int64(4) == T.int64(3) and v_vw % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_vw % T.int64(2) == T.int64(0), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_vw % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_vw % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_vw % T.int64(2) == T.int64(1), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_vw % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_vw % T.int64(2) == T.int64(1), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_vw % T.int64(2) == T.int64(0), T.float32(1), T.float32(0)))))))))
                for h_1, w_1 in T.grid(T.int64(2), T.int64(2)):
                    with T.block("conv2d_winograd"):
                        v_n = T.axis.spatial(T.int64(1), T.int64(0))
                        v_co = T.axis.spatial(T.int64(128), (n_co_h_0_w_0_fused_0 * T.int64(256) + n_co_h_0_w_0_fused_1) // T.int64(196))
                        v_h = T.axis.spatial(T.int64(28), (n_co_h_0_w_0_fused_0 * T.int64(256) + n_co_h_0_w_0_fused_1) % T.int64(196) // T.int64(14) * T.int64(2) + h_1)
                        v_w = T.axis.spatial(T.int64(28), (n_co_h_0_w_0_fused_0 * T.int64(256) + n_co_h_0_w_0_fused_1) % T.int64(14) * T.int64(2) + w_1)
                        T.reads(inverse_local[v_co, v_n * T.int64(196) + v_h // T.int64(2) * T.int64(14) + v_w // T.int64(2), v_h % T.int64(2), v_w % T.int64(2)], p2[v_n, v_co, T.int64(0), T.int64(0)])
                        T.writes(T_relu[v_n, v_co, v_h, v_w])
                        T_relu[v_n, v_co, v_h, v_w] = T.max(inverse_local[v_co, v_n * T.int64(196) + v_h // T.int64(2) * T.int64(14) + v_w // T.int64(2), v_h % T.int64(2), v_w % T.int64(2)] + p2[v_n, v_co, T.int64(0), T.int64(0)], T.float32(0))
b0 = sch.get_block(name="data_pack", func_name="main")
b1 = sch.get_block(name="bgemm", func_name="main")
b2 = sch.get_block(name="inverse", func_name="main")
b3 = sch.get_block(name="conv2d_winograd", func_name="main")
b4 = sch.get_block(name="T_add", func_name="main")
b5 = sch.get_block(name="T_relu", func_name="main")
b6 = sch.get_block(name="root", func_name="main")
b7, b8 = sch.get_producers(block=b2)
sch.compute_inline(block=b8)
b9, = sch.get_consumers(block=b2)
l10, l11, l12, l13 = sch.get_loops(block=b9)
l14, l15 = sch.split(loop=l12, factors=[None, 2], preserve_unit_iters=True)
l16, l17 = sch.split(loop=l13, factors=[None, 2], preserve_unit_iters=True)
sch.reorder(l14, l16, l15, l17)
sch.compute_at(block=b2, loop=l16, preserve_unit_loops=True, index=-1)
sch.set_scope(block=b2, buffer_index=0, storage_scope="local")
l18, l19, l20, l21, l22, l23, l24, l25, l26, l27 = sch.get_loops(block=b2)
sch.unroll(loop=l24)
sch.unroll(loop=l25)
sch.unroll(loop=l26)
sch.unroll(loop=l27)
b28, b29 = sch.get_producers(block=b0)
sch.compute_inline(block=b29)
b30, = sch.get_producers(block=b28)
l31, l32, l33, l34, l35, l36 = sch.get_loops(block=b0)
sch.reorder(l33, l34, l31, l32, l35, l36)
sch.unroll(loop=l31)
sch.unroll(loop=l32)
sch.unroll(loop=l35)
sch.unroll(loop=l36)
l37 = sch.fuse(l33, l34, preserve_unit_iters=True)
v38 = sch.sample_categorical(candidates=[32, 64, 128, 256, 512, 1024], probs=[0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666], decision=1)
l39, l40 = sch.split(loop=l37, factors=[None, v38], preserve_unit_iters=True)
sch.bind(loop=l39, thread_axis="blockIdx.x")
sch.bind(loop=l40, thread_axis="threadIdx.x")
b41 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b41, loop=l40, preserve_unit_loops=True, index=-1)
sch.compute_at(block=b28, loop=l40, preserve_unit_loops=True, index=-1)
sch.set_scope(block=b28, buffer_index=0, storage_scope="local")
sch.compute_inline(block=b30)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l42, l43, l44, l45, l46 = sch.get_loops(block=b1)
v47, v48, v49, v50, v51 = sch.sample_perfect_tile(loop=l42, n=5, max_innermost_factor=64, decision=[1, 2, 2, 1, 1])
l52, l53, l54, l55, l56 = sch.split(loop=l42, factors=[v47, v48, v49, v50, v51], preserve_unit_iters=True)
v57, v58, v59, v60, v61 = sch.sample_perfect_tile(loop=l43, n=5, max_innermost_factor=64, decision=[2, 1, 2, 1, 1])
l62, l63, l64, l65, l66 = sch.split(loop=l43, factors=[v57, v58, v59, v60, v61], preserve_unit_iters=True)
v67, v68, v69, v70, v71 = sch.sample_perfect_tile(loop=l44, n=5, max_innermost_factor=64, decision=[8, 1, 16, 1, 1])
l72, l73, l74, l75, l76 = sch.split(loop=l44, factors=[v67, v68, v69, v70, v71], preserve_unit_iters=True)
v77, v78, v79, v80, v81 = sch.sample_perfect_tile(loop=l45, n=5, max_innermost_factor=64, decision=[7, 1, 1, 28, 1])
l82, l83, l84, l85, l86 = sch.split(loop=l45, factors=[v77, v78, v79, v80, v81], preserve_unit_iters=True)
v87, v88, v89 = sch.sample_perfect_tile(loop=l46, n=3, max_innermost_factor=64, decision=[16, 1, 8])
l90, l91, l92 = sch.split(loop=l46, factors=[v87, v88, v89], preserve_unit_iters=True)
sch.reorder(l52, l62, l72, l82, l53, l63, l73, l83, l54, l64, l74, l84, l90, l91, l55, l65, l75, l85, l92, l56, l66, l76, l86)
l93 = sch.fuse(l52, l62, l72, l82, preserve_unit_iters=True)
sch.bind(loop=l93, thread_axis="blockIdx.x")
l94 = sch.fuse(l53, l63, l73, l83, preserve_unit_iters=True)
sch.bind(loop=l94, thread_axis="vthread.x")
l95 = sch.fuse(l54, l64, l74, l84, preserve_unit_iters=True)
sch.bind(loop=l95, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b96 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b96, loop=l95, preserve_unit_loops=True, index=-1)
b97 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b97, loop=l90, preserve_unit_loops=True, index=-1)
l98, l99, l100, l101, l102, l103, l104, l105 = sch.get_loops(block=b97)
l106 = sch.fuse(l102, l103, l104, l105, preserve_unit_iters=True)
v107 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b97, ann_key="meta_schedule.cooperative_fetch", ann_val=v107)
b108 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b108, loop=l90, preserve_unit_loops=True, index=-1)
l109, l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b108)
l117 = sch.fuse(l113, l114, l115, l116, preserve_unit_iters=True)
v118 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b108, ann_key="meta_schedule.cooperative_fetch", ann_val=v118)
l119 = sch.fuse(l90, preserve_unit_iters=True)
sch.annotate(block_or_loop=l119, ann_key="software_pipeline_stage", ann_val=[0, 0, 2])
sch.annotate(block_or_loop=l119, ann_key="software_pipeline_order", ann_val=[0, 1, 2])
sch.annotate(block_or_loop=l119, ann_key="software_pipeline_async_stages", ann_val=[0])
sch.reverse_compute_inline(block=b5)
sch.reverse_compute_inline(block=b4)
v120 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b6, ann_key="meta_schedule.unroll_explicit", ann_val=v120)
l121, l122, l123, l124, l125, l126 = sch.get_loops(block=b3)
l127 = sch.fuse(l121, l122, l123, l124, preserve_unit_iters=True)
v128 = sch.sample_categorical(candidates=[32, 64, 128, 256, 512, 1024], probs=[0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666], decision=3)
l129, l130 = sch.split(loop=l127, factors=[None, v128], preserve_unit_iters=True)
sch.bind(loop=l129, thread_axis="blockIdx.x")
sch.bind(loop=l130, thread_axis="threadIdx.x")
sch.enter_postproc()
sch.unannotate(block_or_loop=b97, ann_key="meta_schedule.cooperative_fetch")
l131, l132, l133, l134, l135 = sch.get_loops(block=b97)
l136, l137, l138 = sch.split(loop=l135, factors=[None, 64, 4], preserve_unit_iters=True)
sch.vectorize(loop=l138)
sch.bind(loop=l137, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b108, ann_key="meta_schedule.cooperative_fetch")
l139, l140, l141, l142, l143 = sch.get_loops(block=b108)
l144, l145, l146 = sch.split(loop=l143, factors=[None, 64, 2], preserve_unit_iters=True)
sch.vectorize(loop=l146)
sch.bind(loop=l145, thread_axis="threadIdx.x")
b147 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b147, ann_key="meta_schedule.unroll_explicit")
b148, b149, b150, b151, b152, b153, b154, b155, b156 = sch.get_child_blocks(b147)
l157, l158, l159, l160, l161, l162 = sch.get_loops(block=b148)
l163, l164, l165, l166, l167, l168 = sch.get_loops(block=b149)
sch.annotate(block_or_loop=l163, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l163, ann_key="pragma_unroll_explicit", ann_val=1)
l169, l170, l171, l172, l173, l174 = sch.get_loops(block=b150)
l175, l176, l177, l178, l179, l180, l181 = sch.get_loops(block=b151)
l182, l183, l184, l185, l186, l187, l188 = sch.get_loops(block=b152)
l189, l190, l191, l192, l193, l194, l195, l196, l197, l198, l199, l200, l201, l202 = sch.get_loops(block=b153)
sch.annotate(block_or_loop=l189, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l189, ann_key="pragma_unroll_explicit", ann_val=1)
l203, l204, l205, l206, l207, l208, l209 = sch.get_loops(block=b154)
l210, l211, l212, l213, l214, l215, l216, l217 = sch.get_loops(block=b155)
sch.annotate(block_or_loop=l210, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l210, ann_key="pragma_unroll_explicit", ann_val=1)
l218, l219, l220, l221 = sch.get_loops(block=b156)
b222 = sch.get_block(name="data_pack", func_name="main")
l223, l224, l225, l226, l227, l228 = sch.get_loops(block=b222)
b229 = sch.decompose_reduction(block=b222, loop=l227)
b230 = sch.get_block(name="bgemm", func_name="main")
l231, l232, l233, l234, l235, l236, l237, l238, l239, l240, l241, l242, l243, l244 = sch.get_loops(block=b230)
b245 = sch.decompose_reduction(block=b230, loop=l234)
b246 = sch.get_block(name="inverse", func_name="main")
l247, l248, l249, l250, l251, l252, l253, l254 = sch.get_loops(block=b246)
b255 = sch.decompose_reduction(block=b246, loop=l253)
2023-11-11 03:17:54 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #260: GFLOPs: 4927.8915. Time: 25.7809 us. Best GFLOPs: 4927.8915
2023-11-11 03:17:54 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #261: GFLOPs: 4894.2695. Time: 25.9580 us. Best GFLOPs: 4927.8915
2023-11-11 03:17:54 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #262: GFLOPs: 4761.0409. Time: 26.6844 us. Best GFLOPs: 4927.8915
2023-11-11 03:17:54 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #263: GFLOPs: 4716.6075. Time: 26.9358 us. Best GFLOPs: 4927.8915
2023-11-11 03:17:54 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #264: GFLOPs: 4893.6334. Time: 25.9614 us. Best GFLOPs: 4927.8915
2023-11-11 03:17:54 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #265: GFLOPs: 4810.9595. Time: 26.4075 us. Best GFLOPs: 4927.8915
2023-11-11 03:17:54 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #266: GFLOPs: 4848.0957. Time: 26.2053 us. Best GFLOPs: 4927.8915
2023-11-11 03:17:54 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #267: GFLOPs: 4858.7369. Time: 26.1479 us. Best GFLOPs: 4927.8915
2023-11-11 03:17:54 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #268: GFLOPs: 4914.2576. Time: 25.8525 us. Best GFLOPs: 4927.8915
2023-11-11 03:17:54 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #269: GFLOPs: 4903.6894. Time: 25.9082 us. Best GFLOPs: 4927.8915
2023-11-11 03:17:54 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #270: GFLOPs: 4933.8796. Time: 25.7496 us. Best GFLOPs: 4933.8796
2023-11-11 03:17:54 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #271: GFLOPs: 4807.8373. Time: 26.4247 us. Best GFLOPs: 4933.8796
2023-11-11 03:17:54 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #272: GFLOPs: 4925.9127. Time: 25.7913 us. Best GFLOPs: 4933.8796
2023-11-11 03:17:54 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #273: GFLOPs: 3333.1532. Time: 38.1157 us. Best GFLOPs: 4933.8796
2023-11-11 03:17:54 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #274: GFLOPs: 4905.6165. Time: 25.8980 us. Best GFLOPs: 4933.8796
2023-11-11 03:17:54 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #275: GFLOPs: 4890.6142. Time: 25.9774 us. Best GFLOPs: 4933.8796
2023-11-11 03:17:54 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #276: GFLOPs: 4471.8839. Time: 28.4099 us. Best GFLOPs: 4933.8796
2023-11-11 03:17:54 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #277: GFLOPs: 4793.8529. Time: 26.5018 us. Best GFLOPs: 4933.8796
2023-11-11 03:17:54 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #278: GFLOPs: 4874.0557. Time: 26.0657 us. Best GFLOPs: 4933.8796
2023-11-11 03:17:54 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #279: GFLOPs: 4843.3093. Time: 26.2312 us. Best GFLOPs: 4933.8796
2023-11-11 03:17:54 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #280: GFLOPs: 4881.7581. Time: 26.0246 us. Best GFLOPs: 4933.8796
2023-11-11 03:17:54 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #281: GFLOPs: 4665.9139. Time: 27.2285 us. Best GFLOPs: 4933.8796
2023-11-11 03:17:54 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #282: GFLOPs: 4900.4351. Time: 25.9254 us. Best GFLOPs: 4933.8796
2023-11-11 03:17:54 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #283: GFLOPs: 5324.9194. Time: 23.8587 us. Best GFLOPs: 5324.9194
2023-11-11 03:17:54 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #284: GFLOPs: 4837.6614. Time: 26.2618 us. Best GFLOPs: 5324.9194
2023-11-11 03:17:54 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #285: GFLOPs: 4859.3877. Time: 26.1444 us. Best GFLOPs: 5324.9194
2023-11-11 03:17:54 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #286: GFLOPs: 5293.9409. Time: 23.9983 us. Best GFLOPs: 5324.9194
2023-11-11 03:17:54 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #287: GFLOPs: 4758.3916. Time: 26.6993 us. Best GFLOPs: 5324.9194
2023-11-11 03:17:54 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #288: GFLOPs: 4922.4171. Time: 25.8096 us. Best GFLOPs: 5324.9194
2023-11-11 03:17:54 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #289: GFLOPs: 4921.8102. Time: 25.8128 us. Best GFLOPs: 5324.9194
2023-11-11 03:17:54 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #290: GFLOPs: 4918.4328. Time: 25.8305 us. Best GFLOPs: 5324.9194
2023-11-11 03:17:54 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #291: GFLOPs: 4813.9948. Time: 26.3909 us. Best GFLOPs: 5324.9194
2023-11-11 03:17:54 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #292: GFLOPs: 4920.9774. Time: 25.8172 us. Best GFLOPs: 5324.9194
2023-11-11 03:17:54 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #293: GFLOPs: 4912.3529. Time: 25.8625 us. Best GFLOPs: 5324.9194
2023-11-11 03:17:54 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #294: GFLOPs: 4875.2126. Time: 26.0595 us. Best GFLOPs: 5324.9194
2023-11-11 03:17:54 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #295: GFLOPs: 4926.8369. Time: 25.7864 us. Best GFLOPs: 5324.9194
2023-11-11 03:17:54 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #296: GFLOPs: 4917.4540. Time: 25.8357 us. Best GFLOPs: 5324.9194
2023-11-11 03:17:54 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #297: GFLOPs: 4825.8753. Time: 26.3259 us. Best GFLOPs: 5324.9194
2023-11-11 03:17:54 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #298: GFLOPs: 4819.1024. Time: 26.3629 us. Best GFLOPs: 5324.9194
2023-11-11 03:17:54 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #299: GFLOPs: 5293.0415. Time: 24.0024 us. Best GFLOPs: 5324.9194
2023-11-11 03:17:54 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #300: GFLOPs: 4913.1409. Time: 25.8583 us. Best GFLOPs: 5324.9194
2023-11-11 03:17:54 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #301: GFLOPs: 4954.9609. Time: 25.6401 us. Best GFLOPs: 5324.9194
2023-11-11 03:17:54 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #302: GFLOPs: 4648.2681. Time: 27.3318 us. Best GFLOPs: 5324.9194
2023-11-11 03:17:54 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #303: GFLOPs: 4734.5719. Time: 26.8336 us. Best GFLOPs: 5324.9194
2023-11-11 03:17:54 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #304: GFLOPs: 4833.2319. Time: 26.2859 us. Best GFLOPs: 5324.9194
2023-11-11 03:17:54 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #305: GFLOPs: 4821.8000. Time: 26.3482 us. Best GFLOPs: 5324.9194
2023-11-11 03:17:54 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #306: GFLOPs: 4797.1233. Time: 26.4837 us. Best GFLOPs: 5324.9194
2023-11-11 03:17:54 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #307: GFLOPs: 4439.6038. Time: 28.6164 us. Best GFLOPs: 5324.9194
2023-11-11 03:17:54 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #308: GFLOPs: 4895.7361. Time: 25.9503 us. Best GFLOPs: 5324.9194
2023-11-11 03:17:54 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #309: GFLOPs: 4833.7002. Time: 26.2833 us. Best GFLOPs: 5324.9194
2023-11-11 03:17:54 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #310: GFLOPs: 4888.5422. Time: 25.9884 us. Best GFLOPs: 5324.9194
2023-11-11 03:17:54 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #311: GFLOPs: 4795.8962. Time: 26.4905 us. Best GFLOPs: 5324.9194
2023-11-11 03:17:54 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #312: GFLOPs: 4834.6020. Time: 26.2784 us. Best GFLOPs: 5324.9194
2023-11-11 03:17:54 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #313: GFLOPs: 4532.9052. Time: 28.0274 us. Best GFLOPs: 5324.9194
2023-11-11 03:17:54 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #314: GFLOPs: 4538.7755. Time: 27.9912 us. Best GFLOPs: 5324.9194
2023-11-11 03:17:54 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #315: GFLOPs: 5116.6206. Time: 24.8300 us. Best GFLOPs: 5324.9194
2023-11-11 03:17:54 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #316: GFLOPs: 3588.7907. Time: 35.4007 us. Best GFLOPs: 5324.9194
2023-11-11 03:17:54 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #317: GFLOPs: 5390.4879. Time: 23.5685 us. Best GFLOPs: 5390.4879
2023-11-11 03:17:54 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #318: GFLOPs: 106.3646. Time: 1194.4350 us. Best GFLOPs: 5390.4879
2023-11-11 03:17:54 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #319: GFLOPs: 1923.7840. Time: 66.0394 us. Best GFLOPs: 5390.4879
2023-11-11 03:17:54 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #320: GFLOPs: 3410.3361. Time: 37.2531 us. Best GFLOPs: 5390.4879
2023-11-11 04:03:22 [INFO] [evolutionary_search.cc:713] Generating candidates......
2023-11-11 04:03:25 [INFO] [evolutionary_search.cc:715] Picked top 102 candidate(s) from database
2023-11-11 04:03:32 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310a03fb48)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310e893e78)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310e893578)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310ec9ace8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56310e986408)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310eb1cfb8)]: 388 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310dc6f7b8)]: 0 failure(s)
2023-11-11 04:03:39 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310a03fb48)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310e893e78)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310e893578)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310ec9ace8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56310e986408)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310eb1cfb8)]: 784 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310dc6f7b8)]: 0 failure(s)
2023-11-11 04:03:46 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310a03fb48)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310e893e78)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310e893578)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310ec9ace8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56310e986408)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310eb1cfb8)]: 1181 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310dc6f7b8)]: 0 failure(s)
2023-11-11 04:03:52 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310a03fb48)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310e893e78)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310e893578)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310ec9ace8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56310e986408)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310eb1cfb8)]: 1585 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310dc6f7b8)]: 0 failure(s)
2023-11-11 04:03:52 [INFO] [evolutionary_search.cc:723] Sampled 55 candidate(s)
2023-11-11 04:04:10 [INFO] [evolutionary_search.cc:621] Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310a03fb48)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310e893e78)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310e893578)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310ec9ace8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56310e986408)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310eb1cfb8)]: 38 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310dc6f7b8)]: 0 failure(s)
2023-11-11 04:04:32 [INFO] [evolutionary_search.cc:621] Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310a03fb48)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310e893e78)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310e893578)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310ec9ace8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56310e986408)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310eb1cfb8)]: 85 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310dc6f7b8)]: 0 failure(s)
2023-11-11 04:04:55 [INFO] [evolutionary_search.cc:621] Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310a03fb48)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310e893e78)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310e893578)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310ec9ace8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56310e986408)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310eb1cfb8)]: 85 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310dc6f7b8)]: 0 failure(s)
2023-11-11 04:05:18 [INFO] [evolutionary_search.cc:621] Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310a03fb48)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310e893e78)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310e893578)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310ec9ace8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56310e986408)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310eb1cfb8)]: 113 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310dc6f7b8)]: 0 failure(s)
2023-11-11 04:05:24 [INFO] [evolutionary_search.cc:649] Scores of the best 64 candidates:
[1 : 16]:	0.9894  0.9796  0.9753  0.9689  0.9635  0.9628  0.9603  0.9562  0.9553  0.9551  0.9544  0.9533  0.9528  0.9526  0.9514  0.9502
[17 : 32]:	0.9475  0.9465  0.9456  0.9442  0.9432  0.9423  0.9421  0.9417  0.9397  0.9370  0.9355  0.9348  0.9346  0.9341  0.9338  0.9321
[33 : 48]:	0.9294  0.9281  0.9281  0.9279  0.9275  0.9275  0.9270  0.9267  0.9264  0.9263  0.9263  0.9258  0.9253  0.9249  0.9242  0.9233
[49 : 64]:	0.9231  0.9231  0.9228  0.9225  0.9223  0.9218  0.9218  0.9214  0.9212  0.9210  0.9204  0.9202  0.9197  0.9194  0.9191  0.9174
2023-11-11 04:05:24 [INFO] [evolutionary_search.cc:727] Got 64 candidate(s) with evolutionary search
2023-11-11 04:05:24 [INFO] [evolutionary_search.cc:730] Sending 64 candidates(s) for measurement
2023-11-11 04:06:15 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #321: GFLOPs: 5758.9506. Time: 22.0606 us. Best GFLOPs: 5758.9506
2023-11-11 04:06:15 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #322: GFLOPs: 5732.6472. Time: 22.1618 us. Best GFLOPs: 5758.9506
2023-11-11 04:06:15 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #323: GFLOPs: 5820.2945. Time: 21.8280 us. Best GFLOPs: 5820.2945
2023-11-11 04:06:15 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #324: GFLOPs: 5375.2257. Time: 23.6354 us. Best GFLOPs: 5820.2945
2023-11-11 04:06:15 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #325: GFLOPs: 5675.1526. Time: 22.3863 us. Best GFLOPs: 5820.2945
2023-11-11 04:06:15 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #326: GFLOPs: 5732.9225. Time: 22.1607 us. Best GFLOPs: 5820.2945
2023-11-11 04:06:15 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #327: GFLOPs: 5093.8222. Time: 24.9411 us. Best GFLOPs: 5820.2945
2023-11-11 04:06:15 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #328: GFLOPs: 5678.2539. Time: 22.3741 us. Best GFLOPs: 5820.2945
2023-11-11 04:06:15 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #329: GFLOPs: 5368.7405. Time: 23.6640 us. Best GFLOPs: 5820.2945
2023-11-11 04:06:15 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #330: GFLOPs: 5108.4384. Time: 24.8698 us. Best GFLOPs: 5820.2945
2023-11-11 04:06:15 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #331: GFLOPs: 5656.6159. Time: 22.4597 us. Best GFLOPs: 5820.2945
2023-11-11 04:06:15 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #332: GFLOPs: 5151.1373. Time: 24.6636 us. Best GFLOPs: 5820.2945
2023-11-11 04:06:15 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #333: GFLOPs: 5324.3838. Time: 23.8611 us. Best GFLOPs: 5820.2945
2023-11-11 04:06:15 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #334: GFLOPs: 5245.8164. Time: 24.2185 us. Best GFLOPs: 5820.2945
2023-11-11 04:06:15 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #335: GFLOPs: 5268.4766. Time: 24.1143 us. Best GFLOPs: 5820.2945
2023-11-11 04:06:15 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #336: GFLOPs: 5138.2000. Time: 24.7257 us. Best GFLOPs: 5820.2945
2023-11-11 04:06:15 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #337: GFLOPs: 5128.4704. Time: 24.7726 us. Best GFLOPs: 5820.2945
2023-11-11 04:06:15 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #338: GFLOPs: 5141.4513. Time: 24.7101 us. Best GFLOPs: 5820.2945
2023-11-11 04:06:15 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #339: GFLOPs: 5021.9354. Time: 25.2981 us. Best GFLOPs: 5820.2945
2023-11-11 04:06:15 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #340: GFLOPs: 5270.9202. Time: 24.1031 us. Best GFLOPs: 5820.2945
2023-11-11 04:06:15 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #341: GFLOPs: 5265.1165. Time: 24.1297 us. Best GFLOPs: 5820.2945
2023-11-11 04:06:15 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #342: GFLOPs: 5186.2442. Time: 24.4967 us. Best GFLOPs: 5820.2945
2023-11-11 04:06:15 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #343: GFLOPs: 5154.3836. Time: 24.6481 us. Best GFLOPs: 5820.2945
2023-11-11 04:06:15 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #344: GFLOPs: 5250.5986. Time: 24.1964 us. Best GFLOPs: 5820.2945
2023-11-11 04:06:15 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #345: GFLOPs: 4156.0382. Time: 30.5689 us. Best GFLOPs: 5820.2945
2023-11-11 04:06:15 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #346: GFLOPs: 4148.5442. Time: 30.6241 us. Best GFLOPs: 5820.2945
2023-11-11 04:06:15 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #347: GFLOPs: 5050.7042. Time: 25.1540 us. Best GFLOPs: 5820.2945
2023-11-11 04:06:15 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #348: GFLOPs: 5212.3301. Time: 24.3741 us. Best GFLOPs: 5820.2945
2023-11-11 04:06:15 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #349: GFLOPs: 4036.9252. Time: 31.4709 us. Best GFLOPs: 5820.2945
2023-11-11 04:06:15 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #350: GFLOPs: 5086.7998. Time: 24.9756 us. Best GFLOPs: 5820.2945
2023-11-11 04:06:15 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #351: GFLOPs: 4861.4548. Time: 26.1333 us. Best GFLOPs: 5820.2945
2023-11-11 04:06:15 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #352: GFLOPs: 5093.8901. Time: 24.9408 us. Best GFLOPs: 5820.2945
2023-11-11 04:06:15 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #353: GFLOPs: 4902.3041. Time: 25.9155 us. Best GFLOPs: 5820.2945
2023-11-11 04:06:15 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #354: GFLOPs: 4632.5577. Time: 27.4245 us. Best GFLOPs: 5820.2945
2023-11-11 04:06:15 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #355: GFLOPs: 5484.5175. Time: 23.1644 us. Best GFLOPs: 5820.2945
2023-11-11 04:06:15 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #356: GFLOPs: 4148.1853. Time: 30.6268 us. Best GFLOPs: 5820.2945
2023-11-11 04:06:15 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #357: GFLOPs: 5325.8809. Time: 23.8544 us. Best GFLOPs: 5820.2945
2023-11-11 04:06:15 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #358: GFLOPs: 5257.6820. Time: 24.1638 us. Best GFLOPs: 5820.2945
2023-11-11 04:06:15 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #359: GFLOPs: 4734.9142. Time: 26.8317 us. Best GFLOPs: 5820.2945
2023-11-11 04:06:15 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #360: GFLOPs: 4874.3289. Time: 26.0642 us. Best GFLOPs: 5820.2945
2023-11-11 04:06:15 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #361: GFLOPs: 5049.2500. Time: 25.1613 us. Best GFLOPs: 5820.2945
2023-11-11 04:06:15 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #362: GFLOPs: 5310.0395. Time: 23.9256 us. Best GFLOPs: 5820.2945
2023-11-11 04:06:15 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #363: GFLOPs: 5268.2750. Time: 24.1152 us. Best GFLOPs: 5820.2945
2023-11-11 04:06:15 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #364: GFLOPs: 4756.2148. Time: 26.7115 us. Best GFLOPs: 5820.2945
2023-11-11 04:06:15 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #365: GFLOPs: 5280.5239. Time: 24.0593 us. Best GFLOPs: 5820.2945
2023-11-11 04:06:15 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #366: GFLOPs: 4759.0262. Time: 26.6957 us. Best GFLOPs: 5820.2945
2023-11-11 04:06:15 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #367: GFLOPs: 4865.1862. Time: 26.1132 us. Best GFLOPs: 5820.2945
2023-11-11 04:06:15 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #368: GFLOPs: 4928.3595. Time: 25.7785 us. Best GFLOPs: 5820.2945
2023-11-11 04:06:15 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #369: GFLOPs: 4918.9773. Time: 25.8277 us. Best GFLOPs: 5820.2945
2023-11-11 04:06:15 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #370: GFLOPs: 4755.4458. Time: 26.7158 us. Best GFLOPs: 5820.2945
2023-11-11 04:06:15 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #371: GFLOPs: 5035.5606. Time: 25.2297 us. Best GFLOPs: 5820.2945
2023-11-11 04:06:15 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #372: GFLOPs: 5257.7741. Time: 24.1634 us. Best GFLOPs: 5820.2945
2023-11-11 04:06:15 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #373: GFLOPs: 4895.5403. Time: 25.9513 us. Best GFLOPs: 5820.2945
2023-11-11 04:06:15 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #374: GFLOPs: 4893.2599. Time: 25.9634 us. Best GFLOPs: 5820.2945
2023-11-11 04:06:15 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #375: GFLOPs: 4013.1133. Time: 31.6576 us. Best GFLOPs: 5820.2945
2023-11-11 04:06:15 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #376: GFLOPs: 5197.6608. Time: 24.4428 us. Best GFLOPs: 5820.2945
2023-11-11 04:06:15 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #377: GFLOPs: 5654.5981. Time: 22.4677 us. Best GFLOPs: 5820.2945
2023-11-11 04:06:15 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #378: GFLOPs: 5249.5294. Time: 24.2013 us. Best GFLOPs: 5820.2945
2023-11-11 04:06:15 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #379: GFLOPs: 4614.2028. Time: 27.5336 us. Best GFLOPs: 5820.2945
2023-11-11 04:06:15 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #380: GFLOPs: 5268.7952. Time: 24.1128 us. Best GFLOPs: 5820.2945
2023-11-11 04:06:15 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #381: GFLOPs: 4012.9746. Time: 31.6587 us. Best GFLOPs: 5820.2945
2023-11-11 04:06:15 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #382: GFLOPs: 49.3851. Time: 2572.5506 us. Best GFLOPs: 5820.2945
2023-11-11 04:06:15 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #383: GFLOPs: 183.5337. Time: 692.2196 us. Best GFLOPs: 5820.2945
2023-11-11 04:06:15 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #384: GFLOPs: 2485.7034. Time: 51.1105 us. Best GFLOPs: 5820.2945
2023-11-11 05:12:54 [INFO] [evolutionary_search.cc:713] Generating candidates......
2023-11-11 05:12:57 [INFO] [evolutionary_search.cc:715] Picked top 102 candidate(s) from database
2023-11-11 05:13:04 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310a03fb48)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310e893e78)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310e893578)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310ec9ace8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56310e986408)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310eb1cfb8)]: 392 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310dc6f7b8)]: 0 failure(s)
2023-11-11 05:13:10 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310a03fb48)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310e893e78)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310e893578)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310ec9ace8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56310e986408)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310eb1cfb8)]: 794 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310dc6f7b8)]: 0 failure(s)
2023-11-11 05:13:17 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310a03fb48)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310e893e78)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310e893578)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310ec9ace8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56310e986408)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310eb1cfb8)]: 1197 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310dc6f7b8)]: 0 failure(s)
2023-11-11 05:13:23 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310a03fb48)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310e893e78)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310e893578)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310ec9ace8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56310e986408)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310eb1cfb8)]: 1591 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310dc6f7b8)]: 0 failure(s)
2023-11-11 05:13:30 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310a03fb48)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310e893e78)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310e893578)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310ec9ace8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56310e986408)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310eb1cfb8)]: 1985 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310dc6f7b8)]: 0 failure(s)
2023-11-11 05:13:30 [INFO] [evolutionary_search.cc:723] Sampled 65 candidate(s)
2023-11-11 05:13:49 [INFO] [evolutionary_search.cc:621] Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310a03fb48)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310e893e78)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310e893578)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310ec9ace8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56310e986408)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310eb1cfb8)]: 114 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310dc6f7b8)]: 0 failure(s)
2023-11-11 05:14:11 [INFO] [evolutionary_search.cc:621] Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310a03fb48)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310e893e78)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310e893578)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310ec9ace8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56310e986408)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310eb1cfb8)]: 114 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310dc6f7b8)]: 0 failure(s)
2023-11-11 05:14:34 [INFO] [evolutionary_search.cc:621] Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310a03fb48)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310e893e78)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310e893578)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310ec9ace8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56310e986408)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310eb1cfb8)]: 120 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310dc6f7b8)]: 0 failure(s)
2023-11-11 05:14:56 [INFO] [evolutionary_search.cc:621] Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310a03fb48)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310e893e78)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310e893578)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310ec9ace8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56310e986408)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310eb1cfb8)]: 104 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310dc6f7b8)]: 0 failure(s)
2023-11-11 05:15:02 [INFO] [evolutionary_search.cc:649] Scores of the best 64 candidates:
[1 : 16]:	1.0500  0.9833  0.9777  0.9754  0.9683  0.9682  0.9679  0.9607  0.9604  0.9531  0.9531  0.9526  0.9522  0.9521  0.9506  0.9506
[17 : 32]:	0.9503  0.9499  0.9470  0.9463  0.9462  0.9409  0.9404  0.9402  0.9398  0.9370  0.9369  0.9342  0.9315  0.9277  0.9272  0.9251
[33 : 48]:	0.9237  0.9227  0.9219  0.9208  0.9190  0.9184  0.9172  0.9170  0.9169  0.9165  0.9157  0.9151  0.9151  0.9141  0.9136  0.9127
[49 : 64]:	0.9126  0.9123  0.9117  0.9115  0.9108  0.9107  0.9106  0.9088  0.9081  0.9080  0.9063  0.9036  0.9034  0.9033  0.9030  0.9030
2023-11-11 05:15:03 [INFO] [evolutionary_search.cc:727] Got 64 candidate(s) with evolutionary search
2023-11-11 05:15:03 [INFO] [evolutionary_search.cc:730] Sending 64 candidates(s) for measurement
2023-11-11 05:15:37 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #385: GFLOPs: 1465.1662. Time: 86.7107 us. Best GFLOPs: 5820.2945
2023-11-11 05:15:37 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #386: GFLOPs: 5812.1975. Time: 21.8585 us. Best GFLOPs: 5820.2945
2023-11-11 05:15:37 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #387: GFLOPs: 5969.0685. Time: 21.2840 us. Best GFLOPs: 5969.0685
2023-11-11 05:15:37 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #388: GFLOPs: 5922.5496. Time: 21.4512 us. Best GFLOPs: 5969.0685
2023-11-11 05:15:37 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #389: GFLOPs: 5825.9777. Time: 21.8067 us. Best GFLOPs: 5969.0685
2023-11-11 05:15:37 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #390: GFLOPs: 5751.0919. Time: 22.0907 us. Best GFLOPs: 5969.0685
2023-11-11 05:15:37 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #391: GFLOPs: 5852.5887. Time: 21.7076 us. Best GFLOPs: 5969.0685
2023-11-11 05:15:37 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #392: GFLOPs: 5960.2713. Time: 21.3154 us. Best GFLOPs: 5969.0685
2023-11-11 05:15:37 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #393: GFLOPs: 5792.0864. Time: 21.9343 us. Best GFLOPs: 5969.0685
2023-11-11 05:15:37 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #394: GFLOPs: 5764.2827. Time: 22.0401 us. Best GFLOPs: 5969.0685
2023-11-11 05:15:37 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #395: GFLOPs: 5808.6331. Time: 21.8719 us. Best GFLOPs: 5969.0685
2023-11-11 05:15:37 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #396: GFLOPs: 5965.6705. Time: 21.2961 us. Best GFLOPs: 5969.0685
2023-11-11 05:15:37 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #397: GFLOPs: 5844.5049. Time: 21.7376 us. Best GFLOPs: 5969.0685
2023-11-11 05:15:37 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #398: GFLOPs: 5813.3682. Time: 21.8540 us. Best GFLOPs: 5969.0685
2023-11-11 05:15:37 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #399: GFLOPs: 5834.4273. Time: 21.7752 us. Best GFLOPs: 5969.0685
2023-11-11 05:15:37 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #400: GFLOPs: 5948.8400. Time: 21.3564 us. Best GFLOPs: 5969.0685
2023-11-11 05:15:37 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #401: GFLOPs: 5759.2972. Time: 22.0592 us. Best GFLOPs: 5969.0685
2023-11-11 05:15:37 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #402: GFLOPs: 5677.6965. Time: 22.3763 us. Best GFLOPs: 5969.0685
2023-11-11 05:15:37 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #403: GFLOPs: 5838.3659. Time: 21.7605 us. Best GFLOPs: 5969.0685
2023-11-11 05:15:37 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #404: GFLOPs: 5773.8100. Time: 22.0038 us. Best GFLOPs: 5969.0685
2023-11-11 05:15:37 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #405: GFLOPs: 5969.9463. Time: 21.2809 us. Best GFLOPs: 5969.9463
2023-11-11 05:15:37 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #406: GFLOPs: 5836.7499. Time: 21.7665 us. Best GFLOPs: 5969.9463
2023-11-11 05:15:37 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #407: GFLOPs: 5729.6386. Time: 22.1734 us. Best GFLOPs: 5969.9463
2023-11-11 05:15:37 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #408: GFLOPs: 5825.3888. Time: 21.8090 us. Best GFLOPs: 5969.9463
2023-11-11 05:15:37 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #409: GFLOPs: 5778.8598. Time: 21.9846 us. Best GFLOPs: 5969.9463
2023-11-11 05:15:37 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #410: GFLOPs: 5624.1779. Time: 22.5892 us. Best GFLOPs: 5969.9463
2023-11-11 05:15:37 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #411: GFLOPs: 5628.8001. Time: 22.5706 us. Best GFLOPs: 5969.9463
2023-11-11 05:15:37 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #412: GFLOPs: 5839.1940. Time: 21.7574 us. Best GFLOPs: 5969.9463
2023-11-11 05:15:37 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #413: GFLOPs: 5710.4793. Time: 22.2478 us. Best GFLOPs: 5969.9463
2023-11-11 05:15:37 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #414: GFLOPs: 5840.9288. Time: 21.7509 us. Best GFLOPs: 5969.9463
2023-11-11 05:15:37 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #415: GFLOPs: 5902.2003. Time: 21.5251 us. Best GFLOPs: 5969.9463
2023-11-11 05:15:37 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #416: GFLOPs: 5942.0963. Time: 21.3806 us. Best GFLOPs: 5969.9463
2023-11-11 05:15:37 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #417: GFLOPs: 5672.9732. Time: 22.3949 us. Best GFLOPs: 5969.9463
2023-11-11 05:15:37 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #418: GFLOPs: 5752.5339. Time: 22.0852 us. Best GFLOPs: 5969.9463
2023-11-11 05:15:37 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #419: GFLOPs: 5242.8496. Time: 24.2322 us. Best GFLOPs: 5969.9463
2023-11-11 05:15:37 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #420: GFLOPs: 5822.9584. Time: 21.8181 us. Best GFLOPs: 5969.9463
2023-11-11 05:15:37 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #421: GFLOPs: 5656.2816. Time: 22.4610 us. Best GFLOPs: 5969.9463
2023-11-11 05:15:37 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #422: GFLOPs: 5800.5685. Time: 21.9023 us. Best GFLOPs: 5969.9463
2023-11-11 05:15:37 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #423: GFLOPs: 5480.7494. Time: 23.1803 us. Best GFLOPs: 5969.9463
2023-11-11 05:15:37 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #424: GFLOPs: 5333.1345. Time: 23.8219 us. Best GFLOPs: 5969.9463
2023-11-11 05:15:37 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #425: GFLOPs: 5793.1962. Time: 21.9301 us. Best GFLOPs: 5969.9463
2023-11-11 05:15:37 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #426: GFLOPs: 5501.2569. Time: 23.0939 us. Best GFLOPs: 5969.9463
2023-11-11 05:15:37 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #427: GFLOPs: 5452.0438. Time: 23.3024 us. Best GFLOPs: 5969.9463
2023-11-11 05:15:37 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #428: GFLOPs: 5479.0529. Time: 23.1875 us. Best GFLOPs: 5969.9463
2023-11-11 05:15:37 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #429: GFLOPs: 5418.5673. Time: 23.4464 us. Best GFLOPs: 5969.9463
2023-11-11 05:15:37 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #430: GFLOPs: 5390.5798. Time: 23.5681 us. Best GFLOPs: 5969.9463
2023-11-11 05:15:37 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #431: GFLOPs: 5537.0607. Time: 22.9446 us. Best GFLOPs: 5969.9463
2023-11-11 05:15:37 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #432: GFLOPs: 5332.3122. Time: 23.8256 us. Best GFLOPs: 5969.9463
2023-11-11 05:15:37 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #433: GFLOPs: 5430.8391. Time: 23.3934 us. Best GFLOPs: 5969.9463
2023-11-11 05:15:37 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #434: GFLOPs: 5332.3833. Time: 23.8253 us. Best GFLOPs: 5969.9463
2023-11-11 05:15:37 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #435: GFLOPs: 5832.5213. Time: 21.7823 us. Best GFLOPs: 5969.9463
2023-11-11 05:15:37 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #436: GFLOPs: 5777.9200. Time: 21.9881 us. Best GFLOPs: 5969.9463
2023-11-11 05:15:37 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #437: GFLOPs: 5665.5418. Time: 22.4243 us. Best GFLOPs: 5969.9463
2023-11-11 05:15:37 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #438: GFLOPs: 5473.9916. Time: 23.2090 us. Best GFLOPs: 5969.9463
2023-11-11 05:15:37 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #439: GFLOPs: 5802.0689. Time: 21.8966 us. Best GFLOPs: 5969.9463
2023-11-11 05:15:37 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #440: GFLOPs: 5764.2432. Time: 22.0403 us. Best GFLOPs: 5969.9463
2023-11-11 05:15:37 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #441: GFLOPs: 5668.8673. Time: 22.4111 us. Best GFLOPs: 5969.9463
2023-11-11 05:15:37 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #442: GFLOPs: 5457.0993. Time: 23.2808 us. Best GFLOPs: 5969.9463
2023-11-11 05:15:37 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #443: GFLOPs: 5707.7635. Time: 22.2584 us. Best GFLOPs: 5969.9463
2023-11-11 05:15:37 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #444: GFLOPs: 5481.5700. Time: 23.1769 us. Best GFLOPs: 5969.9463
2023-11-11 05:15:37 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #445: GFLOPs: 5588.8016. Time: 22.7322 us. Best GFLOPs: 5969.9463
2023-11-11 05:15:37 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #446: GFLOPs: 28.0694. Time: 4526.1243 us. Best GFLOPs: 5969.9463
2023-11-11 05:15:37 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #447: GFLOPs: 2701.0168. Time: 47.0362 us. Best GFLOPs: 5969.9463
2023-11-11 05:15:37 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #448: GFLOPs: 1400.9575. Time: 90.6849 us. Best GFLOPs: 5969.9463
2023-11-11 06:17:11 [INFO] [evolutionary_search.cc:713] Generating candidates......
2023-11-11 06:17:14 [INFO] [evolutionary_search.cc:715] Picked top 102 candidate(s) from database
2023-11-11 06:17:21 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310a03fb48)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310e893e78)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310e893578)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310ec9ace8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56310e986408)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310eb1cfb8)]: 396 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310dc6f7b8)]: 0 failure(s)
2023-11-11 06:17:28 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310a03fb48)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310e893e78)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310e893578)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310ec9ace8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56310e986408)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310eb1cfb8)]: 795 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310dc6f7b8)]: 0 failure(s)
2023-11-11 06:17:34 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310a03fb48)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310e893e78)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310e893578)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310ec9ace8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56310e986408)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310eb1cfb8)]: 1191 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310dc6f7b8)]: 0 failure(s)
2023-11-11 06:17:41 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310a03fb48)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310e893e78)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310e893578)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310ec9ace8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56310e986408)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310eb1cfb8)]: 1590 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310dc6f7b8)]: 0 failure(s)
2023-11-11 06:17:41 [INFO] [evolutionary_search.cc:723] Sampled 50 candidate(s)
2023-11-11 06:18:03 [INFO] [evolutionary_search.cc:621] Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310a03fb48)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310e893e78)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310e893578)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310ec9ace8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56310e986408)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310eb1cfb8)]: 238 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310dc6f7b8)]: 0 failure(s)
2023-11-11 06:18:28 [INFO] [evolutionary_search.cc:621] Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310a03fb48)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310e893e78)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310e893578)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310ec9ace8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56310e986408)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310eb1cfb8)]: 192 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310dc6f7b8)]: 0 failure(s)
2023-11-11 06:18:52 [INFO] [evolutionary_search.cc:621] Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310a03fb48)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310e893e78)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310e893578)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310ec9ace8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56310e986408)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310eb1cfb8)]: 171 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310dc6f7b8)]: 0 failure(s)
2023-11-11 06:19:18 [INFO] [evolutionary_search.cc:621] Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310a03fb48)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310e893e78)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310e893578)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310ec9ace8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56310e986408)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310eb1cfb8)]: 179 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310dc6f7b8)]: 0 failure(s)
2023-11-11 06:19:24 [INFO] [evolutionary_search.cc:649] Scores of the best 64 candidates:
[1 : 16]:	1.1254  1.0986  1.0156  1.0106  1.0062  1.0048  1.0044  0.9994  0.9967  0.9966  0.9962  0.9936  0.9929  0.9928  0.9928  0.9914
[17 : 32]:	0.9911  0.9904  0.9896  0.9895  0.9894  0.9893  0.9883  0.9877  0.9873  0.9862  0.9854  0.9852  0.9852  0.9845  0.9840  0.9829
[33 : 48]:	0.9826  0.9821  0.9816  0.9815  0.9809  0.9804  0.9798  0.9785  0.9782  0.9776  0.9775  0.9773  0.9766  0.9762  0.9760  0.9756
[49 : 64]:	0.9752  0.9750  0.9748  0.9748  0.9747  0.9743  0.9741  0.9740  0.9738  0.9737  0.9737  0.9733  0.9726  0.9726  0.9722  0.9720
2023-11-11 06:19:24 [INFO] [evolutionary_search.cc:727] Got 64 candidate(s) with evolutionary search
2023-11-11 06:19:25 [INFO] [evolutionary_search.cc:730] Sending 64 candidates(s) for measurement
2023-11-11 06:19:58 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #449: GFLOPs: 3708.7460. Time: 34.2557 us. Best GFLOPs: 5969.9463
2023-11-11 06:19:58 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #450: GFLOPs: 3685.7592. Time: 34.4693 us. Best GFLOPs: 5969.9463
2023-11-11 06:19:58 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #451: GFLOPs: 5900.2613. Time: 21.5322 us. Best GFLOPs: 5969.9463
2023-11-11 06:19:58 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #452: GFLOPs: 5832.4210. Time: 21.7827 us. Best GFLOPs: 5969.9463
2023-11-11 06:19:58 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #453: GFLOPs: 5913.1741. Time: 21.4852 us. Best GFLOPs: 5969.9463
2023-11-11 06:19:58 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #454: GFLOPs: 5848.1281. Time: 21.7242 us. Best GFLOPs: 5969.9463
2023-11-11 06:19:58 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #455: GFLOPs: 5854.3319. Time: 21.7011 us. Best GFLOPs: 5969.9463
2023-11-11 06:19:58 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #456: GFLOPs: 5891.0692. Time: 21.5658 us. Best GFLOPs: 5969.9463
2023-11-11 06:19:58 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #457: GFLOPs: 5908.3582. Time: 21.5027 us. Best GFLOPs: 5969.9463
2023-11-11 06:19:58 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #458: GFLOPs: 5880.7235. Time: 21.6037 us. Best GFLOPs: 5969.9463
2023-11-11 06:19:58 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #459: GFLOPs: 5947.0266. Time: 21.3629 us. Best GFLOPs: 5969.9463
2023-11-11 06:19:58 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #460: GFLOPs: 5951.5023. Time: 21.3468 us. Best GFLOPs: 5969.9463
2023-11-11 06:19:58 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #461: GFLOPs: 5869.9473. Time: 21.6434 us. Best GFLOPs: 5969.9463
2023-11-11 06:19:58 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #462: GFLOPs: 5901.5257. Time: 21.5276 us. Best GFLOPs: 5969.9463
2023-11-11 06:19:58 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #463: GFLOPs: 5871.4533. Time: 21.6379 us. Best GFLOPs: 5969.9463
2023-11-11 06:19:58 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #464: GFLOPs: 5831.4837. Time: 21.7862 us. Best GFLOPs: 5969.9463
2023-11-11 06:19:58 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #465: GFLOPs: 5830.4940. Time: 21.7899 us. Best GFLOPs: 5969.9463
2023-11-11 06:19:58 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #466: GFLOPs: 5838.2479. Time: 21.7609 us. Best GFLOPs: 5969.9463
2023-11-11 06:19:58 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #467: GFLOPs: 5922.1102. Time: 21.4528 us. Best GFLOPs: 5969.9463
2023-11-11 06:19:58 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #468: GFLOPs: 5903.3627. Time: 21.5209 us. Best GFLOPs: 5969.9463
2023-11-11 06:19:58 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #469: GFLOPs: 5828.4438. Time: 21.7975 us. Best GFLOPs: 5969.9463
2023-11-11 06:19:58 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #470: GFLOPs: 5743.1110. Time: 22.1214 us. Best GFLOPs: 5969.9463
2023-11-11 06:19:58 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #471: GFLOPs: 5740.5233. Time: 22.1314 us. Best GFLOPs: 5969.9463
2023-11-11 06:19:58 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #472: GFLOPs: 5879.9014. Time: 21.6068 us. Best GFLOPs: 5969.9463
2023-11-11 06:19:58 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #473: GFLOPs: 5833.9187. Time: 21.7771 us. Best GFLOPs: 5969.9463
2023-11-11 06:19:58 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #474: GFLOPs: 5784.5074. Time: 21.9631 us. Best GFLOPs: 5969.9463
2023-11-11 06:19:58 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #475: GFLOPs: 5919.2746. Time: 21.4630 us. Best GFLOPs: 5969.9463
2023-11-11 06:19:58 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #476: GFLOPs: 5959.0979. Time: 21.3196 us. Best GFLOPs: 5969.9463
2023-11-11 06:19:58 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #477: GFLOPs: 5931.3604. Time: 21.4193 us. Best GFLOPs: 5969.9463
2023-11-11 06:19:58 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #478: GFLOPs: 5790.4803. Time: 21.9404 us. Best GFLOPs: 5969.9463
2023-11-11 06:19:58 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #479: GFLOPs: 5755.1107. Time: 22.0753 us. Best GFLOPs: 5969.9463
2023-11-11 06:19:58 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #480: GFLOPs: 5875.2909. Time: 21.6237 us. Best GFLOPs: 5969.9463
2023-11-11 06:19:58 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #481: GFLOPs: 5932.4661. Time: 21.4153 us. Best GFLOPs: 5969.9463
2023-11-11 06:19:58 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #482: GFLOPs: 5951.0586. Time: 21.3484 us. Best GFLOPs: 5969.9463
2023-11-11 06:19:58 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #483: GFLOPs: 5780.8839. Time: 21.9769 us. Best GFLOPs: 5969.9463
2023-11-11 06:19:58 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #484: GFLOPs: 5849.5015. Time: 21.7191 us. Best GFLOPs: 5969.9463
2023-11-11 06:19:58 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #485: GFLOPs: 5880.0443. Time: 21.6062 us. Best GFLOPs: 5969.9463
2023-11-11 06:19:58 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #486: GFLOPs: 5877.4869. Time: 21.6156 us. Best GFLOPs: 5969.9463
2023-11-11 06:19:58 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #487: GFLOPs: 5784.5858. Time: 21.9628 us. Best GFLOPs: 5969.9463
2023-11-11 06:19:58 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #488: GFLOPs: 5701.8145. Time: 22.2816 us. Best GFLOPs: 5969.9463
2023-11-11 06:19:58 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #489: GFLOPs: 6044.2565. Time: 21.0192 us. Best GFLOPs: 6044.2565
2023-11-11 06:19:58 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #490: GFLOPs: 5784.5101. Time: 21.9631 us. Best GFLOPs: 6044.2565
2023-11-11 06:19:58 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #491: GFLOPs: 5762.0508. Time: 22.0487 us. Best GFLOPs: 6044.2565
2023-11-11 06:19:58 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #492: GFLOPs: 5953.9114. Time: 21.3382 us. Best GFLOPs: 6044.2565
2023-11-11 06:19:58 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #493: GFLOPs: 5846.2985. Time: 21.7310 us. Best GFLOPs: 6044.2565
2023-11-11 06:19:58 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #494: GFLOPs: 5772.6760. Time: 22.0081 us. Best GFLOPs: 6044.2565
2023-11-11 06:19:58 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #495: GFLOPs: 5967.3881. Time: 21.2900 us. Best GFLOPs: 6044.2565
2023-11-11 06:19:58 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #496: GFLOPs: 5706.0964. Time: 22.2649 us. Best GFLOPs: 6044.2565
2023-11-11 06:19:58 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #497: GFLOPs: 5962.6603. Time: 21.3069 us. Best GFLOPs: 6044.2565
2023-11-11 06:19:58 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #498: GFLOPs: 5779.0237. Time: 21.9839 us. Best GFLOPs: 6044.2565
2023-11-11 06:19:58 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #499: GFLOPs: 5855.6402. Time: 21.6963 us. Best GFLOPs: 6044.2565
2023-11-11 06:19:58 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #500: GFLOPs: 5902.1607. Time: 21.5253 us. Best GFLOPs: 6044.2565
2023-11-11 06:19:58 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #501: GFLOPs: 5790.5986. Time: 21.9400 us. Best GFLOPs: 6044.2565
2023-11-11 06:19:58 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #502: GFLOPs: 5891.8610. Time: 21.5629 us. Best GFLOPs: 6044.2565
2023-11-11 06:19:58 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #503: GFLOPs: 5831.8511. Time: 21.7848 us. Best GFLOPs: 6044.2565
2023-11-11 06:19:58 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #504: GFLOPs: 5690.3186. Time: 22.3266 us. Best GFLOPs: 6044.2565
2023-11-11 06:19:58 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #505: GFLOPs: 5768.8534. Time: 22.0227 us. Best GFLOPs: 6044.2565
2023-11-11 06:19:58 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #506: GFLOPs: 5846.4169. Time: 21.7305 us. Best GFLOPs: 6044.2565
2023-11-11 06:19:58 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #507: GFLOPs: 5803.8104. Time: 21.8900 us. Best GFLOPs: 6044.2565
2023-11-11 06:19:58 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #508: GFLOPs: 5849.2244. Time: 21.7201 us. Best GFLOPs: 6044.2565
2023-11-11 06:19:58 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #509: GFLOPs: 5837.3413. Time: 21.7643 us. Best GFLOPs: 6044.2565
2023-11-11 06:19:58 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #510: GFLOPs: 263.6428. Time: 481.8855 us. Best GFLOPs: 6044.2565
2023-11-11 06:19:58 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #511: GFLOPs: 939.6561. Time: 135.2044 us. Best GFLOPs: 6044.2565
2023-11-11 06:19:58 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #512: GFLOPs: 427.3422. Time: 297.2925 us. Best GFLOPs: 6044.2565
2023-11-11 07:27:14 [INFO] [evolutionary_search.cc:713] Generating candidates......
2023-11-11 07:27:17 [INFO] [evolutionary_search.cc:715] Picked top 102 candidate(s) from database
2023-11-11 07:27:24 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310a03fb48)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310e893e78)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310e893578)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310ec9ace8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56310e986408)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310eb1cfb8)]: 392 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310dc6f7b8)]: 0 failure(s)
2023-11-11 07:27:31 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310a03fb48)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310e893e78)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310e893578)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310ec9ace8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56310e986408)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310eb1cfb8)]: 788 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310dc6f7b8)]: 0 failure(s)
2023-11-11 07:27:37 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310a03fb48)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310e893e78)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310e893578)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310ec9ace8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56310e986408)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310eb1cfb8)]: 1182 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310dc6f7b8)]: 0 failure(s)
2023-11-11 07:27:44 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310a03fb48)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310e893e78)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310e893578)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310ec9ace8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56310e986408)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310eb1cfb8)]: 1575 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310dc6f7b8)]: 0 failure(s)
2023-11-11 07:27:44 [INFO] [evolutionary_search.cc:723] Sampled 65 candidate(s)
2023-11-11 07:28:08 [INFO] [evolutionary_search.cc:621] Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310a03fb48)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310e893e78)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310e893578)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310ec9ace8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56310e986408)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310eb1cfb8)]: 261 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310dc6f7b8)]: 0 failure(s)
2023-11-11 07:28:35 [INFO] [evolutionary_search.cc:621] Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310a03fb48)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310e893e78)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310e893578)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310ec9ace8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56310e986408)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310eb1cfb8)]: 220 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310dc6f7b8)]: 0 failure(s)
2023-11-11 07:28:59 [INFO] [evolutionary_search.cc:621] Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310a03fb48)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310e893e78)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310e893578)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310ec9ace8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56310e986408)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310eb1cfb8)]: 194 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310dc6f7b8)]: 0 failure(s)
2023-11-11 07:29:26 [INFO] [evolutionary_search.cc:621] Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310a03fb48)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310e893e78)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310e893578)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310ec9ace8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56310e986408)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310eb1cfb8)]: 238 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310dc6f7b8)]: 0 failure(s)
2023-11-11 07:29:32 [INFO] [evolutionary_search.cc:649] Scores of the best 64 candidates:
[1 : 16]:	1.0061  1.0051  1.0049  1.0043  1.0039  1.0039  1.0004  1.0003  1.0000  0.9997  0.9973  0.9971  0.9971  0.9967  0.9966  0.9966
[17 : 32]:	0.9963  0.9959  0.9943  0.9940  0.9937  0.9932  0.9926  0.9922  0.9917  0.9909  0.9905  0.9901  0.9900  0.9890  0.9885  0.9885
[33 : 48]:	0.9885  0.9883  0.9878  0.9874  0.9873  0.9869  0.9869  0.9866  0.9864  0.9863  0.9858  0.9858  0.9852  0.9849  0.9843  0.9842
[49 : 64]:	0.9842  0.9839  0.9839  0.9828  0.9826  0.9823  0.9819  0.9816  0.9814  0.9804  0.9804  0.9802  0.9800  0.9797  0.9796  0.9791
2023-11-11 07:29:32 [INFO] [evolutionary_search.cc:727] Got 64 candidate(s) with evolutionary search
2023-11-11 07:29:33 [INFO] [evolutionary_search.cc:730] Sending 64 candidates(s) for measurement
2023-11-11 07:30:18 [INFO] [task_scheduler.cc:121] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #513: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/home2/xiachunwei/Software/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/home2/xiachunwei/Software/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/home2/xiachunwei/Software/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/home2/xiachunwei/Software/tvm/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/home2/xiachunwei/Software/tvm/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/home2/xiachunwei/Software/tvm/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/home2/xiachunwei/Software/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/home2/xiachunwei/Software/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  4: _ZN3tvm7runtime13PackedFun
  3: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  2: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  0: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  File "/home2/xiachunwei/Software/tvm/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(128), T.int64(28), T.int64(28)), "float32"), p1: T.Buffer((T.int64(4), T.int64(4), T.int64(128), T.int64(128)), "float32"), p2: T.Buffer((T.int64(1), T.int64(128), T.int64(1), T.int64(1)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(128), T.int64(28), T.int64(28)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        input_tile_local = T.alloc_buffer((T.int64(128), T.int64(196), T.int64(4), T.int64(4)), scope="local")
        data_pack = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(128), T.int64(196)))
        bgemm = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(128), T.int64(196)))
        inverse_local = T.alloc_buffer((T.int64(128), T.int64(196), T.int64(2), T.int64(2)), scope="local")
        data_pack_local = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(128), T.int64(196)), scope="local")
        bgemm_local = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(128), T.int64(196)), scope="local")
        data_pack_shared = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(128), T.int64(196)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(128), T.int64(128)), scope="shared")
        for ci_p_fused_0 in T.thread_binding(T.int64(392), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 1024, "pragma_unroll_explicit": 1}):
            for ci_p_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(1), T.int64(4), T.int64(4)):
                    with T.block("input_tile"):
                        v_ci = T.axis.spatial(T.int64(128), (ci_p_fused_0 * T.int64(64) + ci_p_fused_1) // T.int64(196) + ax0)
                        v_p = T.axis.spatial(T.int64(196), (ci_p_fused_0 * T.int64(64) + ci_p_fused_1) % T.int64(196) + ax1)
                        v_eps, v_nu = T.axis.remap("SS", [ax2, ax3])
                        T.reads(p0[v_p // T.int64(196), v_ci, v_p % T.int64(196) // T.int64(14) * T.int64(2) + v_eps - T.int64(1), v_p % T.int64(14) * T.int64(2) + v_nu - T.int64(1)])
                        T.writes(input_tile_local[v_ci, v_p, v_eps, v_nu])
                        T.block_attr({"schedule_rule": "None"})
                        input_tile_local[v_ci, v_p, v_eps, v_nu] = T.if_then_else(T.int64(1) <= v_p % T.int64(196) // T.int64(14) * T.int64(2) + v_eps and v_p % T.int64(196) // T.int64(14) * T.int64(2) + v_eps < T.int64(29) and T.int64(1) <= v_p % T.int64(14) * T.int64(2) + v_nu and v_p % T.int64(14) * T.int64(2) + v_nu < T.int64(29), p0[v_p // T.int64(196), v_ci, v_p % T.int64(196) // T.int64(14) * T.int64(2) + v_eps - T.int64(1), v_p % T.int64(14) * T.int64(2) + v_nu - T.int64(1)], T.float32(0))
                for eps in T.unroll(T.int64(4)):
                    for nu in T.unroll(T.int64(4)):
                        with T.block("data_pack_init"):
                            v_eps, v_nu = T.axis.remap("SS", [eps, nu])
                            v_ci = T.axis.spatial(T.int64(128), (ci_p_fused_0 * T.int64(64) + ci_p_fused_1) // T.int64(196))
                            v_p = T.axis.spatial(T.int64(196), (ci_p_fused_0 * T.int64(64) + ci_p_fused_1) % T.int64(196))
                            T.reads()
                            T.writes(data_pack_local[v_eps, v_nu, v_ci, v_p])
                            T.block_attr({"schedule_rule": "conv2d_nchw_winograd_data_pack"})
                            data_pack_local[v_eps, v_nu, v_ci, v_p] = T.float32(0)
                        for r_a in T.unroll(T.int64(4)):
                            for r_b in T.unroll(T.int64(4)):
                                with T.block("data_pack_update"):
                                    v_eps, v_nu = T.axis.remap("SS", [eps, nu])
                                    v_ci = T.axis.spatial(T.int64(128), (ci_p_fused_0 * T.int64(64) + ci_p_fused_1) // T.int64(196))
                                    v_p = T.axis.spatial(T.int64(196), (ci_p_fused_0 * T.int64(64) + ci_p_fused_1) % T.int64(196))
                                    v_r_a, v_r_b = T.axis.remap("RR", [r_a, r_b])
                                    T.reads(data_pack_local[v_eps, v_nu, v_ci, v_p], input_tile_local[v_ci, v_p, v_r_a, v_r_b])
                                    T.writes(data_pack_local[v_eps, v_nu, v_ci, v_p])
                                    T.block_attr({"schedule_rule": "conv2d_nchw_winograd_data_pack"})
                                    data_pack_local[v_eps, v_nu, v_ci, v_p] = data_pack_local[v_eps, v_nu, v_ci, v_p] + input_tile_local[v_ci, v_p, v_r_a, v_r_b] * T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(3), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(1), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(0), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(3), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(1), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(0), T.float32(1), T.float32(0))))))))))))))))) * T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(3), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(1), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(0), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(3), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(1), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(0), T.float32(1), T.float32(0)))))))))))))))))
                for ax0, ax1, ax2, ax3 in T.grid(T.int64(4), T.int64(4), T.int64(1), T.int64(1)):
                    with T.block("data_pack_local"):
                        v0, v1 = T.axis.remap("SS", [ax0, ax1])
                        v2 = T.axis.spatial(T.int64(128), (ci_p_fused_0 * T.int64(64) + ci_p_fused_1) // T.int64(196) + ax2)
                        v3 = T.axis.spatial(T.int64(196), (ci_p_fused_0 * T.int64(64) + ci_p_fused_1) % T.int64(196) + ax3)
                        T.reads(data_pack_local[v0, v1, v2, v3])
                        T.writes(data_pack[v0, v1, v2, v3])
                        data_pack[v0, v1, v2, v3] = data_pack_local[v0, v1, v2, v3]
        for eps_0_nu_0_co_0_p_0_fused in T.thread_binding(T.int64(448), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 1024, "pragma_unroll_explicit": 1}):
            for eps_1_nu_1_co_1_p_1_fused in T.thread_binding(T.int64(2), thread="vthread.x"):
                for eps_2_nu_2_co_2_p_2_fused in T.thread_binding(T.int64(112), thread="threadIdx.x"):
                    for eps_3_init, nu_3_init, co_3_init, p_3_init, eps_4_init, nu_4_init, co_4_init, p_4_init in T.grid(T.int64(1), T.int64(1), T.int64(2), T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(1)):
                        with T.block("bgemm_init"):
                            v_eps = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(112) + eps_3_init + eps_4_init)
                            v_nu = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused % T.int64(112) // T.int64(28) + nu_3_init + nu_4_init)
                            v_co = T.axis.spatial(T.int64(128), eps_0_nu_0_co_0_p_0_fused % T.int64(28) // T.int64(7) * T.int64(32) + eps_2_nu_2_co_2_p_2_fused // T.int64(14) * T.int64(4) + co_3_init * T.int64(2) + co_4_init)
                            v_p = T.axis.spatial(T.int64(196), eps_0_nu_0_co_0_p_0_fused % T.int64(7) * T.int64(28) + eps_1_nu_1_co_1_p_1_fused * T.int64(14) + eps_2_nu_2_co_2_p_2_fused % T.int64(14) + p_3_init + p_4_init)
                            T.reads()
                            T.writes(bgemm_local[v_eps, v_nu, v_co, v_p])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            bgemm_local[v_eps, v_nu, v_co, v_p] = T.float32(0)
                    for ci_0_fused in T.serial(T.int64(2), annotations={"software_pipeline_async_stages": [0], "software_pipeline_order": [0, 1, 2], "software_pipeline_stage": [0, 0, 2]}):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(16)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(112), thread="threadIdx.x"):
                                with T.block("data_pack_shared"):
                                    v0 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(112))
                                    v1 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused % T.int64(112) // T.int64(28))
                                    v2 = T.axis.spatial(T.int64(128), ci_0_fused * T.int64(64) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(112) + ax0_ax1_ax2_ax3_fused_1) // T.int64(28))
                                    v3 = T.axis.spatial(T.int64(196), eps_0_nu_0_co_0_p_0_fused % T.int64(7) * T.int64(28) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(112) + ax0_ax1_ax2_ax3_fused_1) % T.int64(28))
                                    T.reads(data_pack[v0, v1, v2, v3])
                                    T.writes(data_pack_shared[v0, v1, v2, v3])
                                    data_pack_shared[v0, v1, v2, v3] = data_pack[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(5)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(112), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("p1_shared"):
                                        v0 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(112))
                                        v1 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused % T.int64(112) // T.int64(28))
                                        v2 = T.axis.spatial(T.int64(128), ci_0_fused * T.int64(64) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(448) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) // T.int64(32))
                                        v3 = T.axis.spatial(T.int64(128), eps_0_nu_0_co_0_p_0_fused % T.int64(28) // T.int64(7) * T.int64(32) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(448) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) % T.int64(32))
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * T.int64(112) + ax0_ax1_ax2_ax3_fused_1) * T.int64(4) + ax0_ax1_ax2_ax3_fused_2 < T.int64(2048))
                                        T.reads(p1[v0, v1, v2, v3])
                                        T.writes(p1_shared[v0, v1, v2, v3])
                                        p1_shared[v0, v1, v2, v3] = p1[v0, v1, v2, v3]
                        for ci_1, eps_3, nu_3, co_3, p_3, ci_2, eps_4, nu_4, co_4, p_4 in T.grid(T.int64(4), T.int64(1), T.int64(1), T.int64(2), T.int64(1), T.int64(16), T.int64(1), T.int64(1), T.int64(2), T.int64(1)):
                            with T.block("bgemm_update"):
                                v_eps = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(112) + eps_3 + eps_4)
                                v_nu = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused % T.int64(112) // T.int64(28) + nu_3 + nu_4)
                                v_co = T.axis.spatial(T.int64(128), eps_0_nu_0_co_0_p_0_fused % T.int64(28) // T.int64(7) * T.int64(32) + eps_2_nu_2_co_2_p_2_fused // T.int64(14) * T.int64(4) + co_3 * T.int64(2) + co_4)
                                v_p = T.axis.spatial(T.int64(196), eps_0_nu_0_co_0_p_0_fused % T.int64(7) * T.int64(28) + eps_1_nu_1_co_1_p_1_fused * T.int64(14) + eps_2_nu_2_co_2_p_2_fused % T.int64(14) + p_3 + p_4)
                                v_ci = T.axis.reduce(T.int64(128), ci_0_fused * T.int64(64) + ci_1 * T.int64(16) + ci_2)
                                T.reads(bgemm_local[v_eps, v_nu, v_co, v_p], data_pack_shared[v_eps, v_nu, v_ci, v_p], p1_shared[v_eps, v_nu, v_ci, v_co])
                                T.writes(bgemm_local[v_eps, v_nu, v_co, v_p])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                bgemm_local[v_eps, v_nu, v_co, v_p] = bgemm_local[v_eps, v_nu, v_co, v_p] + data_pack_shared[v_eps, v_nu, v_ci, v_p] * p1_shared[v_eps, v_nu, v_ci, v_co]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(1), T.int64(4), T.int64(1)):
                        with T.block("bgemm_local"):
                            v0 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(112) + ax0)
                            v1 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused % T.int64(112) // T.int64(28) + ax1)
                            v2 = T.axis.spatial(T.int64(128), eps_0_nu_0_co_0_p_0_fused % T.int64(28) // T.int64(7) * T.int64(32) + eps_2_nu_2_co_2_p_2_fused // T.int64(14) * T.int64(4) + ax2)
                            v3 = T.axis.spatial(T.int64(196), eps_0_nu_0_co_0_p_0_fused % T.int64(7) * T.int64(28) + eps_1_nu_1_co_1_p_1_fused * T.int64(14) + eps_2_nu_2_co_2_p_2_fused % T.int64(14) + ax3)
                            T.reads(bgemm_local[v0, v1, v2, v3])
                            T.writes(bgemm[v0, v1, v2, v3])
                            bgemm[v0, v1, v2, v3] = bgemm_local[v0, v1, v2, v3]
        for n_co_h_0_w_0_fused_0 in T.thread_binding(T.int64(196), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 1024, "pragma_unroll_explicit": 1}):
            for n_co_h_0_w_0_fused_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                    for ax2 in T.unroll(T.int64(2)):
                        for ax3 in T.unroll(T.int64(2)):
                            with T.block("inverse_init"):
                                v_co = T.axis.spatial(T.int64(128), (n_co_h_0_w_0_fused_0 * T.int64(128) + n_co_h_0_w_0_fused_1) // T.int64(196) + ax0)
                                v_p = T.axis.spatial(T.int64(196), (n_co_h_0_w_0_fused_0 * T.int64(128) + n_co_h_0_w_0_fused_1) % T.int64(196) + ax1)
                                v_vh, v_vw = T.axis.remap("SS", [ax2, ax3])
                                T.reads()
                                T.writes(inverse_local[v_co, v_p, v_vh, v_vw])
                                T.block_attr({"schedule_rule": "conv2d_nchw_winograd_inverse"})
                                inverse_local[v_co, v_p, v_vh, v_vw] = T.float32(0)
                            for ax4 in T.unroll(T.int64(4)):
                                for ax5 in T.unroll(T.int64(4)):
                                    with T.block("inverse_update"):
                                        v_co = T.axis.spatial(T.int64(128), (n_co_h_0_w_0_fused_0 * T.int64(128) + n_co_h_0_w_0_fused_1) // T.int64(196) + ax0)
                                        v_p = T.axis.spatial(T.int64(196), (n_co_h_0_w_0_fused_0 * T.int64(128) + n_co_h_0_w_0_fused_1) % T.int64(196) + ax1)
                                        v_vh, v_vw, v_r_a, v_r_b = T.axis.remap("SSRR", [ax2, ax3, ax4, ax5])
                                        T.reads(inverse_local[v_co, v_p, v_vh, v_vw], bgemm[v_r_a, v_r_b, v_co, v_p])
                                        T.writes(inverse_local[v_co, v_p, v_vh, v_vw])
                                        T.block_attr({"schedule_rule": "conv2d_nchw_winograd_inverse"})
                                        inverse_local[v_co, v_p, v_vh, v_vw] = inverse_local[v_co, v_p, v_vh, v_vw] + bgemm[v_r_a, v_r_b, v_co, v_p] * T.Select(v_r_a % T.int64(4) == T.int64(3) and v_vh % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_vh % T.int64(2) == T.int64(0), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_vh % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_vh % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_vh % T.int64(2) == T.int64(1), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_vh % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_vh % T.int64(2) == T.int64(1), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_vh % T.int64(2) == T.int64(0), T.float32(1), T.float32(0))))))))) * T.Select(v_r_b % T.int64(4) == T.int64(3) and v_vw % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_vw % T.int64(2) == T.int64(0), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_vw % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_vw % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_vw % T.int64(2) == T.int64(1), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_vw % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_vw % T.int64(2) == T.int64(1), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_vw % T.int64(2) == T.int64(0), T.float32(1), T.float32(0)))))))))
                for h_1, w_1 in T.grid(T.int64(2), T.int64(2)):
                    with T.block("conv2d_winograd"):
                        v_n = T.axis.spatial(T.int64(1), T.int64(0))
                        v_co = T.axis.spatial(T.int64(128), (n_co_h_0_w_0_fused_0 * T.int64(128) + n_co_h_0_w_0_fused_1) // T.int64(196))
                        v_h = T.axis.spatial(T.int64(28), (n_co_h_0_w_0_fused_0 * T.int64(128) + n_co_h_0_w_0_fused_1) % T.int64(196) // T.int64(14) * T.int64(2) + h_1)
                        v_w = T.axis.spatial(T.int64(28), (n_co_h_0_w_0_fused_0 * T.int64(128) + n_co_h_0_w_0_fused_1) % T.int64(14) * T.int64(2) + w_1)
                        T.reads(inverse_local[v_co, v_n * T.int64(196) + v_h // T.int64(2) * T.int64(14) + v_w // T.int64(2), v_h % T.int64(2), v_w % T.int64(2)], p2[v_n, v_co, T.int64(0), T.int64(0)])
                        T.writes(T_relu[v_n, v_co, v_h, v_w])
                        T_relu[v_n, v_co, v_h, v_w] = T.max(inverse_local[v_co, v_n * T.int64(196) + v_h // T.int64(2) * T.int64(14) + v_w // T.int64(2), v_h % T.int64(2), v_w % T.int64(2)] + p2[v_n, v_co, T.int64(0), T.int64(0)], T.float32(0))
b0 = sch.get_block(name="data_pack", func_name="main")
b1 = sch.get_block(name="bgemm", func_name="main")
b2 = sch.get_block(name="inverse", func_name="main")
b3 = sch.get_block(name="conv2d_winograd", func_name="main")
b4 = sch.get_block(name="T_add", func_name="main")
b5 = sch.get_block(name="T_relu", func_name="main")
b6 = sch.get_block(name="root", func_name="main")
b7, b8 = sch.get_producers(block=b2)
sch.compute_inline(block=b8)
b9, = sch.get_consumers(block=b2)
l10, l11, l12, l13 = sch.get_loops(block=b9)
l14, l15 = sch.split(loop=l12, factors=[None, 2], preserve_unit_iters=True)
l16, l17 = sch.split(loop=l13, factors=[None, 2], preserve_unit_iters=True)
sch.reorder(l14, l16, l15, l17)
sch.compute_at(block=b2, loop=l16, preserve_unit_loops=True, index=-1)
sch.set_scope(block=b2, buffer_index=0, storage_scope="local")
l18, l19, l20, l21, l22, l23, l24, l25, l26, l27 = sch.get_loops(block=b2)
sch.unroll(loop=l24)
sch.unroll(loop=l25)
sch.unroll(loop=l26)
sch.unroll(loop=l27)
b28, b29 = sch.get_producers(block=b0)
sch.compute_inline(block=b29)
b30, = sch.get_producers(block=b28)
l31, l32, l33, l34, l35, l36 = sch.get_loops(block=b0)
sch.reorder(l33, l34, l31, l32, l35, l36)
sch.unroll(loop=l31)
sch.unroll(loop=l32)
sch.unroll(loop=l35)
sch.unroll(loop=l36)
l37 = sch.fuse(l33, l34, preserve_unit_iters=True)
v38 = sch.sample_categorical(candidates=[32, 64, 128, 256, 512, 1024], probs=[0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666], decision=1)
l39, l40 = sch.split(loop=l37, factors=[None, v38], preserve_unit_iters=True)
sch.bind(loop=l39, thread_axis="blockIdx.x")
sch.bind(loop=l40, thread_axis="threadIdx.x")
b41 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b41, loop=l40, preserve_unit_loops=True, index=-1)
sch.compute_at(block=b28, loop=l40, preserve_unit_loops=True, index=-1)
sch.set_scope(block=b28, buffer_index=0, storage_scope="local")
sch.compute_inline(block=b30)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l42, l43, l44, l45, l46 = sch.get_loops(block=b1)
v47, v48, v49, v50, v51 = sch.sample_perfect_tile(loop=l42, n=5, max_innermost_factor=64, decision=[4, 1, 1, 1, 1])
l52, l53, l54, l55, l56 = sch.split(loop=l42, factors=[v47, v48, v49, v50, v51], preserve_unit_iters=True)
v57, v58, v59, v60, v61 = sch.sample_perfect_tile(loop=l43, n=5, max_innermost_factor=64, decision=[4, 1, 1, 1, 1])
l62, l63, l64, l65, l66 = sch.split(loop=l43, factors=[v57, v58, v59, v60, v61], preserve_unit_iters=True)
v67, v68, v69, v70, v71 = sch.sample_perfect_tile(loop=l44, n=5, max_innermost_factor=64, decision=[4, 1, 8, 2, 2])
l72, l73, l74, l75, l76 = sch.split(loop=l44, factors=[v67, v68, v69, v70, v71], preserve_unit_iters=True)
v77, v78, v79, v80, v81 = sch.sample_perfect_tile(loop=l45, n=5, max_innermost_factor=64, decision=[7, 2, 14, 1, 1])
l82, l83, l84, l85, l86 = sch.split(loop=l45, factors=[v77, v78, v79, v80, v81], preserve_unit_iters=True)
v87, v88, v89 = sch.sample_perfect_tile(loop=l46, n=3, max_innermost_factor=64, decision=[2, 4, 16])
l90, l91, l92 = sch.split(loop=l46, factors=[v87, v88, v89], preserve_unit_iters=True)
sch.reorder(l52, l62, l72, l82, l53, l63, l73, l83, l54, l64, l74, l84, l90, l91, l55, l65, l75, l85, l92, l56, l66, l76, l86)
l93 = sch.fuse(l52, l62, l72, l82, preserve_unit_iters=True)
sch.bind(loop=l93, thread_axis="blockIdx.x")
l94 = sch.fuse(l53, l63, l73, l83, preserve_unit_iters=True)
sch.bind(loop=l94, thread_axis="vthread.x")
l95 = sch.fuse(l54, l64, l74, l84, preserve_unit_iters=True)
sch.bind(loop=l95, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b96 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b96, loop=l95, preserve_unit_loops=True, index=-1)
b97 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b97, loop=l90, preserve_unit_loops=True, index=-1)
l98, l99, l100, l101, l102, l103, l104, l105 = sch.get_loops(block=b97)
l106 = sch.fuse(l102, l103, l104, l105, preserve_unit_iters=True)
v107 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b97, ann_key="meta_schedule.cooperative_fetch", ann_val=v107)
b108 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b108, loop=l90, preserve_unit_loops=True, index=-1)
l109, l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b108)
l117 = sch.fuse(l113, l114, l115, l116, preserve_unit_iters=True)
v118 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b108, ann_key="meta_schedule.cooperative_fetch", ann_val=v118)
l119 = sch.fuse(l90, preserve_unit_iters=True)
sch.annotate(block_or_loop=l119, ann_key="software_pipeline_stage", ann_val=[0, 0, 2])
sch.annotate(block_or_loop=l119, ann_key="software_pipeline_order", ann_val=[0, 1, 2])
sch.annotate(block_or_loop=l119, ann_key="software_pipeline_async_stages", ann_val=[0])
sch.reverse_compute_inline(block=b5)
sch.reverse_compute_inline(block=b4)
v120 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b6, ann_key="meta_schedule.unroll_explicit", ann_val=v120)
l121, l122, l123, l124, l125, l126 = sch.get_loops(block=b3)
l127 = sch.fuse(l121, l122, l123, l124, preserve_unit_iters=True)
v128 = sch.sample_categorical(candidates=[32, 64, 128, 256, 512, 1024], probs=[0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666], decision=2)
l129, l130 = sch.split(loop=l127, factors=[None, v128], preserve_unit_iters=True)
sch.bind(loop=l129, thread_axis="blockIdx.x")
sch.bind(loop=l130, thread_axis="threadIdx.x")
sch.enter_postproc()
sch.unannotate(block_or_loop=b97, ann_key="meta_schedule.cooperative_fetch")
l131, l132, l133, l134, l135 = sch.get_loops(block=b97)
l136, l137 = sch.split(loop=l135, factors=[None, 112], preserve_unit_iters=True)
sch.bind(loop=l137, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b108, ann_key="meta_schedule.cooperative_fetch")
l138, l139, l140, l141, l142 = sch.get_loops(block=b108)
l143, l144, l145 = sch.split(loop=l142, factors=[None, 112, 4], preserve_unit_iters=True)
sch.vectorize(loop=l145)
sch.bind(loop=l144, thread_axis="threadIdx.x")
b146 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b146, ann_key="meta_schedule.unroll_explicit")
b147, b148, b149, b150, b151, b152, b153, b154, b155 = sch.get_child_blocks(b146)
l156, l157, l158, l159, l160, l161 = sch.get_loops(block=b147)
l162, l163, l164, l165, l166, l167 = sch.get_loops(block=b148)
sch.annotate(block_or_loop=l162, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l162, ann_key="pragma_unroll_explicit", ann_val=1)
l168, l169, l170, l171, l172, l173 = sch.get_loops(block=b149)
l174, l175, l176, l177, l178, l179 = sch.get_loops(block=b150)
l180, l181, l182, l183, l184, l185, l186 = sch.get_loops(block=b151)
l187, l188, l189, l190, l191, l192, l193, l194, l195, l196, l197, l198, l199, l200 = sch.get_loops(block=b152)
sch.annotate(block_or_loop=l187, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l187, ann_key="pragma_unroll_explicit", ann_val=1)
l201, l202, l203, l204, l205, l206, l207 = sch.get_loops(block=b153)
l208, l209, l210, l211, l212, l213, l214, l215 = sch.get_loops(block=b154)
sch.annotate(block_or_loop=l208, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l208, ann_key="pragma_unroll_explicit", ann_val=1)
l216, l217, l218, l219 = sch.get_loops(block=b155)
b220 = sch.get_block(name="data_pack", func_name="main")
l221, l222, l223, l224, l225, l226 = sch.get_loops(block=b220)
b227 = sch.decompose_reduction(block=b220, loop=l225)
b228 = sch.get_block(name="bgemm", func_name="main")
l229, l230, l231, l232, l233, l234, l235, l236, l237, l238, l239, l240, l241, l242 = sch.get_loops(block=b228)
b243 = sch.decompose_reduction(block=b228, loop=l232)
b244 = sch.get_block(name="inverse", func_name="main")
l245, l246, l247, l248, l249, l250, l251, l252 = sch.get_loops(block=b244)
b253 = sch.decompose_reduction(block=b244, loop=l251)
2023-11-11 07:30:18 [INFO] [task_scheduler.cc:121] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #514: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/home2/xiachunwei/Software/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/home2/xiachunwei/Software/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/home2/xiachunwei/Software/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/home2/xiachunwei/Software/tvm/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/home2/xiachunwei/Software/tvm/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/home2/xiachunwei/Software/tvm/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/home2/xiachunwei/Software/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/home2/xiachunwei/Software/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  4: _ZN3tvm7runtime13PackedFun
  3: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  2: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  0: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  File "/home2/xiachunwei/Software/tvm/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(128), T.int64(28), T.int64(28)), "float32"), p1: T.Buffer((T.int64(4), T.int64(4), T.int64(128), T.int64(128)), "float32"), p2: T.Buffer((T.int64(1), T.int64(128), T.int64(1), T.int64(1)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(128), T.int64(28), T.int64(28)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        input_tile_local = T.alloc_buffer((T.int64(128), T.int64(196), T.int64(4), T.int64(4)), scope="local")
        data_pack = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(128), T.int64(196)))
        bgemm = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(128), T.int64(196)))
        inverse_local = T.alloc_buffer((T.int64(128), T.int64(196), T.int64(2), T.int64(2)), scope="local")
        data_pack_local = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(128), T.int64(196)), scope="local")
        bgemm_local = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(128), T.int64(196)), scope="local")
        data_pack_shared = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(128), T.int64(196)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(128), T.int64(128)), scope="shared")
        for ci_p_fused_0 in T.thread_binding(T.int64(392), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 1024, "pragma_unroll_explicit": 1}):
            for ci_p_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(1), T.int64(4), T.int64(4)):
                    with T.block("input_tile"):
                        v_ci = T.axis.spatial(T.int64(128), (ci_p_fused_0 * T.int64(64) + ci_p_fused_1) // T.int64(196) + ax0)
                        v_p = T.axis.spatial(T.int64(196), (ci_p_fused_0 * T.int64(64) + ci_p_fused_1) % T.int64(196) + ax1)
                        v_eps, v_nu = T.axis.remap("SS", [ax2, ax3])
                        T.reads(p0[v_p // T.int64(196), v_ci, v_p % T.int64(196) // T.int64(14) * T.int64(2) + v_eps - T.int64(1), v_p % T.int64(14) * T.int64(2) + v_nu - T.int64(1)])
                        T.writes(input_tile_local[v_ci, v_p, v_eps, v_nu])
                        T.block_attr({"schedule_rule": "None"})
                        input_tile_local[v_ci, v_p, v_eps, v_nu] = T.if_then_else(T.int64(1) <= v_p % T.int64(196) // T.int64(14) * T.int64(2) + v_eps and v_p % T.int64(196) // T.int64(14) * T.int64(2) + v_eps < T.int64(29) and T.int64(1) <= v_p % T.int64(14) * T.int64(2) + v_nu and v_p % T.int64(14) * T.int64(2) + v_nu < T.int64(29), p0[v_p // T.int64(196), v_ci, v_p % T.int64(196) // T.int64(14) * T.int64(2) + v_eps - T.int64(1), v_p % T.int64(14) * T.int64(2) + v_nu - T.int64(1)], T.float32(0))
                for eps in T.unroll(T.int64(4)):
                    for nu in T.unroll(T.int64(4)):
                        with T.block("data_pack_init"):
                            v_eps, v_nu = T.axis.remap("SS", [eps, nu])
                            v_ci = T.axis.spatial(T.int64(128), (ci_p_fused_0 * T.int64(64) + ci_p_fused_1) // T.int64(196))
                            v_p = T.axis.spatial(T.int64(196), (ci_p_fused_0 * T.int64(64) + ci_p_fused_1) % T.int64(196))
                            T.reads()
                            T.writes(data_pack_local[v_eps, v_nu, v_ci, v_p])
                            T.block_attr({"schedule_rule": "conv2d_nchw_winograd_data_pack"})
                            data_pack_local[v_eps, v_nu, v_ci, v_p] = T.float32(0)
                        for r_a in T.unroll(T.int64(4)):
                            for r_b in T.unroll(T.int64(4)):
                                with T.block("data_pack_update"):
                                    v_eps, v_nu = T.axis.remap("SS", [eps, nu])
                                    v_ci = T.axis.spatial(T.int64(128), (ci_p_fused_0 * T.int64(64) + ci_p_fused_1) // T.int64(196))
                                    v_p = T.axis.spatial(T.int64(196), (ci_p_fused_0 * T.int64(64) + ci_p_fused_1) % T.int64(196))
                                    v_r_a, v_r_b = T.axis.remap("RR", [r_a, r_b])
                                    T.reads(data_pack_local[v_eps, v_nu, v_ci, v_p], input_tile_local[v_ci, v_p, v_r_a, v_r_b])
                                    T.writes(data_pack_local[v_eps, v_nu, v_ci, v_p])
                                    T.block_attr({"schedule_rule": "conv2d_nchw_winograd_data_pack"})
                                    data_pack_local[v_eps, v_nu, v_ci, v_p] = data_pack_local[v_eps, v_nu, v_ci, v_p] + input_tile_local[v_ci, v_p, v_r_a, v_r_b] * T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(3), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(1), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(0), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(3), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(1), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(0), T.float32(1), T.float32(0))))))))))))))))) * T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(3), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(1), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(0), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(3), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(1), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(0), T.float32(1), T.float32(0)))))))))))))))))
                for ax0, ax1, ax2, ax3 in T.grid(T.int64(4), T.int64(4), T.int64(1), T.int64(1)):
                    with T.block("data_pack_local"):
                        v0, v1 = T.axis.remap("SS", [ax0, ax1])
                        v2 = T.axis.spatial(T.int64(128), (ci_p_fused_0 * T.int64(64) + ci_p_fused_1) // T.int64(196) + ax2)
                        v3 = T.axis.spatial(T.int64(196), (ci_p_fused_0 * T.int64(64) + ci_p_fused_1) % T.int64(196) + ax3)
                        T.reads(data_pack_local[v0, v1, v2, v3])
                        T.writes(data_pack[v0, v1, v2, v3])
                        data_pack[v0, v1, v2, v3] = data_pack_local[v0, v1, v2, v3]
        for eps_0_nu_0_co_0_p_0_fused in T.thread_binding(T.int64(448), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 1024, "pragma_unroll_explicit": 1}):
            for eps_1_nu_1_co_1_p_1_fused in T.thread_binding(T.int64(2), thread="vthread.x"):
                for eps_2_nu_2_co_2_p_2_fused in T.thread_binding(T.int64(112), thread="threadIdx.x"):
                    for eps_3_init, nu_3_init, co_3_init, p_3_init, eps_4_init, nu_4_init, co_4_init, p_4_init in T.grid(T.int64(1), T.int64(1), T.int64(4), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("bgemm_init"):
                            v_eps = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(112) + eps_3_init + eps_4_init)
                            v_nu = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused % T.int64(112) // T.int64(28) + nu_3_init + nu_4_init)
                            v_co = T.axis.spatial(T.int64(128), eps_0_nu_0_co_0_p_0_fused % T.int64(28) // T.int64(7) * T.int64(32) + eps_2_nu_2_co_2_p_2_fused // T.int64(14) * T.int64(4) + co_3_init + co_4_init)
                            v_p = T.axis.spatial(T.int64(196), eps_0_nu_0_co_0_p_0_fused % T.int64(7) * T.int64(28) + eps_1_nu_1_co_1_p_1_fused * T.int64(14) + eps_2_nu_2_co_2_p_2_fused % T.int64(14) + p_3_init + p_4_init)
                            T.reads()
                            T.writes(bgemm_local[v_eps, v_nu, v_co, v_p])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            bgemm_local[v_eps, v_nu, v_co, v_p] = T.float32(0)
                    for ci_0_fused in T.serial(T.int64(2), annotations={"software_pipeline_async_stages": [0], "software_pipeline_order": [0, 1, 2], "software_pipeline_stage": [0, 0, 2]}):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(16)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(112), thread="threadIdx.x"):
                                with T.block("data_pack_shared"):
                                    v0 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(112))
                                    v1 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused % T.int64(112) // T.int64(28))
                                    v2 = T.axis.spatial(T.int64(128), ci_0_fused * T.int64(64) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(112) + ax0_ax1_ax2_ax3_fused_1) // T.int64(28))
                                    v3 = T.axis.spatial(T.int64(196), eps_0_nu_0_co_0_p_0_fused % T.int64(7) * T.int64(28) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(112) + ax0_ax1_ax2_ax3_fused_1) % T.int64(28))
                                    T.reads(data_pack[v0, v1, v2, v3])
                                    T.writes(data_pack_shared[v0, v1, v2, v3])
                                    data_pack_shared[v0, v1, v2, v3] = data_pack[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(5)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(112), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("p1_shared"):
                                        v0 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(112))
                                        v1 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused % T.int64(112) // T.int64(28))
                                        v2 = T.axis.spatial(T.int64(128), ci_0_fused * T.int64(64) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(448) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) // T.int64(32))
                                        v3 = T.axis.spatial(T.int64(128), eps_0_nu_0_co_0_p_0_fused % T.int64(28) // T.int64(7) * T.int64(32) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(448) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) % T.int64(32))
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * T.int64(112) + ax0_ax1_ax2_ax3_fused_1) * T.int64(4) + ax0_ax1_ax2_ax3_fused_2 < T.int64(2048))
                                        T.reads(p1[v0, v1, v2, v3])
                                        T.writes(p1_shared[v0, v1, v2, v3])
                                        p1_shared[v0, v1, v2, v3] = p1[v0, v1, v2, v3]
                        for ci_1, eps_3, nu_3, co_3, p_3, ci_2, eps_4, nu_4, co_4, p_4 in T.grid(T.int64(16), T.int64(1), T.int64(1), T.int64(4), T.int64(1), T.int64(4), T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                            with T.block("bgemm_update"):
                                v_eps = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(112) + eps_3 + eps_4)
                                v_nu = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused % T.int64(112) // T.int64(28) + nu_3 + nu_4)
                                v_co = T.axis.spatial(T.int64(128), eps_0_nu_0_co_0_p_0_fused % T.int64(28) // T.int64(7) * T.int64(32) + eps_2_nu_2_co_2_p_2_fused // T.int64(14) * T.int64(4) + co_3 + co_4)
                                v_p = T.axis.spatial(T.int64(196), eps_0_nu_0_co_0_p_0_fused % T.int64(7) * T.int64(28) + eps_1_nu_1_co_1_p_1_fused * T.int64(14) + eps_2_nu_2_co_2_p_2_fused % T.int64(14) + p_3 + p_4)
                                v_ci = T.axis.reduce(T.int64(128), ci_0_fused * T.int64(64) + ci_1 * T.int64(4) + ci_2)
                                T.reads(bgemm_local[v_eps, v_nu, v_co, v_p], data_pack_shared[v_eps, v_nu, v_ci, v_p], p1_shared[v_eps, v_nu, v_ci, v_co])
                                T.writes(bgemm_local[v_eps, v_nu, v_co, v_p])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                bgemm_local[v_eps, v_nu, v_co, v_p] = bgemm_local[v_eps, v_nu, v_co, v_p] + data_pack_shared[v_eps, v_nu, v_ci, v_p] * p1_shared[v_eps, v_nu, v_ci, v_co]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(1), T.int64(4), T.int64(1)):
                        with T.block("bgemm_local"):
                            v0 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(112) + ax0)
                            v1 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused % T.int64(112) // T.int64(28) + ax1)
                            v2 = T.axis.spatial(T.int64(128), eps_0_nu_0_co_0_p_0_fused % T.int64(28) // T.int64(7) * T.int64(32) + eps_2_nu_2_co_2_p_2_fused // T.int64(14) * T.int64(4) + ax2)
                            v3 = T.axis.spatial(T.int64(196), eps_0_nu_0_co_0_p_0_fused % T.int64(7) * T.int64(28) + eps_1_nu_1_co_1_p_1_fused * T.int64(14) + eps_2_nu_2_co_2_p_2_fused % T.int64(14) + ax3)
                            T.reads(bgemm_local[v0, v1, v2, v3])
                            T.writes(bgemm[v0, v1, v2, v3])
                            bgemm[v0, v1, v2, v3] = bgemm_local[v0, v1, v2, v3]
        for n_co_h_0_w_0_fused_0 in T.thread_binding(T.int64(196), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 1024, "pragma_unroll_explicit": 1}):
            for n_co_h_0_w_0_fused_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                    for ax2 in T.unroll(T.int64(2)):
                        for ax3 in T.unroll(T.int64(2)):
                            with T.block("inverse_init"):
                                v_co = T.axis.spatial(T.int64(128), (n_co_h_0_w_0_fused_0 * T.int64(128) + n_co_h_0_w_0_fused_1) // T.int64(196) + ax0)
                                v_p = T.axis.spatial(T.int64(196), (n_co_h_0_w_0_fused_0 * T.int64(128) + n_co_h_0_w_0_fused_1) % T.int64(196) + ax1)
                                v_vh, v_vw = T.axis.remap("SS", [ax2, ax3])
                                T.reads()
                                T.writes(inverse_local[v_co, v_p, v_vh, v_vw])
                                T.block_attr({"schedule_rule": "conv2d_nchw_winograd_inverse"})
                                inverse_local[v_co, v_p, v_vh, v_vw] = T.float32(0)
                            for ax4 in T.unroll(T.int64(4)):
                                for ax5 in T.unroll(T.int64(4)):
                                    with T.block("inverse_update"):
                                        v_co = T.axis.spatial(T.int64(128), (n_co_h_0_w_0_fused_0 * T.int64(128) + n_co_h_0_w_0_fused_1) // T.int64(196) + ax0)
                                        v_p = T.axis.spatial(T.int64(196), (n_co_h_0_w_0_fused_0 * T.int64(128) + n_co_h_0_w_0_fused_1) % T.int64(196) + ax1)
                                        v_vh, v_vw, v_r_a, v_r_b = T.axis.remap("SSRR", [ax2, ax3, ax4, ax5])
                                        T.reads(inverse_local[v_co, v_p, v_vh, v_vw], bgemm[v_r_a, v_r_b, v_co, v_p])
                                        T.writes(inverse_local[v_co, v_p, v_vh, v_vw])
                                        T.block_attr({"schedule_rule": "conv2d_nchw_winograd_inverse"})
                                        inverse_local[v_co, v_p, v_vh, v_vw] = inverse_local[v_co, v_p, v_vh, v_vw] + bgemm[v_r_a, v_r_b, v_co, v_p] * T.Select(v_r_a % T.int64(4) == T.int64(3) and v_vh % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_vh % T.int64(2) == T.int64(0), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_vh % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_vh % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_vh % T.int64(2) == T.int64(1), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_vh % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_vh % T.int64(2) == T.int64(1), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_vh % T.int64(2) == T.int64(0), T.float32(1), T.float32(0))))))))) * T.Select(v_r_b % T.int64(4) == T.int64(3) and v_vw % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_vw % T.int64(2) == T.int64(0), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_vw % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_vw % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_vw % T.int64(2) == T.int64(1), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_vw % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_vw % T.int64(2) == T.int64(1), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_vw % T.int64(2) == T.int64(0), T.float32(1), T.float32(0)))))))))
                for h_1, w_1 in T.grid(T.int64(2), T.int64(2)):
                    with T.block("conv2d_winograd"):
                        v_n = T.axis.spatial(T.int64(1), T.int64(0))
                        v_co = T.axis.spatial(T.int64(128), (n_co_h_0_w_0_fused_0 * T.int64(128) + n_co_h_0_w_0_fused_1) // T.int64(196))
                        v_h = T.axis.spatial(T.int64(28), (n_co_h_0_w_0_fused_0 * T.int64(128) + n_co_h_0_w_0_fused_1) % T.int64(196) // T.int64(14) * T.int64(2) + h_1)
                        v_w = T.axis.spatial(T.int64(28), (n_co_h_0_w_0_fused_0 * T.int64(128) + n_co_h_0_w_0_fused_1) % T.int64(14) * T.int64(2) + w_1)
                        T.reads(inverse_local[v_co, v_n * T.int64(196) + v_h // T.int64(2) * T.int64(14) + v_w // T.int64(2), v_h % T.int64(2), v_w % T.int64(2)], p2[v_n, v_co, T.int64(0), T.int64(0)])
                        T.writes(T_relu[v_n, v_co, v_h, v_w])
                        T_relu[v_n, v_co, v_h, v_w] = T.max(inverse_local[v_co, v_n * T.int64(196) + v_h // T.int64(2) * T.int64(14) + v_w // T.int64(2), v_h % T.int64(2), v_w % T.int64(2)] + p2[v_n, v_co, T.int64(0), T.int64(0)], T.float32(0))
b0 = sch.get_block(name="data_pack", func_name="main")
b1 = sch.get_block(name="bgemm", func_name="main")
b2 = sch.get_block(name="inverse", func_name="main")
b3 = sch.get_block(name="conv2d_winograd", func_name="main")
b4 = sch.get_block(name="T_add", func_name="main")
b5 = sch.get_block(name="T_relu", func_name="main")
b6 = sch.get_block(name="root", func_name="main")
b7, b8 = sch.get_producers(block=b2)
sch.compute_inline(block=b8)
b9, = sch.get_consumers(block=b2)
l10, l11, l12, l13 = sch.get_loops(block=b9)
l14, l15 = sch.split(loop=l12, factors=[None, 2], preserve_unit_iters=True)
l16, l17 = sch.split(loop=l13, factors=[None, 2], preserve_unit_iters=True)
sch.reorder(l14, l16, l15, l17)
sch.compute_at(block=b2, loop=l16, preserve_unit_loops=True, index=-1)
sch.set_scope(block=b2, buffer_index=0, storage_scope="local")
l18, l19, l20, l21, l22, l23, l24, l25, l26, l27 = sch.get_loops(block=b2)
sch.unroll(loop=l24)
sch.unroll(loop=l25)
sch.unroll(loop=l26)
sch.unroll(loop=l27)
b28, b29 = sch.get_producers(block=b0)
sch.compute_inline(block=b29)
b30, = sch.get_producers(block=b28)
l31, l32, l33, l34, l35, l36 = sch.get_loops(block=b0)
sch.reorder(l33, l34, l31, l32, l35, l36)
sch.unroll(loop=l31)
sch.unroll(loop=l32)
sch.unroll(loop=l35)
sch.unroll(loop=l36)
l37 = sch.fuse(l33, l34, preserve_unit_iters=True)
v38 = sch.sample_categorical(candidates=[32, 64, 128, 256, 512, 1024], probs=[0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666], decision=1)
l39, l40 = sch.split(loop=l37, factors=[None, v38], preserve_unit_iters=True)
sch.bind(loop=l39, thread_axis="blockIdx.x")
sch.bind(loop=l40, thread_axis="threadIdx.x")
b41 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b41, loop=l40, preserve_unit_loops=True, index=-1)
sch.compute_at(block=b28, loop=l40, preserve_unit_loops=True, index=-1)
sch.set_scope(block=b28, buffer_index=0, storage_scope="local")
sch.compute_inline(block=b30)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l42, l43, l44, l45, l46 = sch.get_loops(block=b1)
v47, v48, v49, v50, v51 = sch.sample_perfect_tile(loop=l42, n=5, max_innermost_factor=64, decision=[4, 1, 1, 1, 1])
l52, l53, l54, l55, l56 = sch.split(loop=l42, factors=[v47, v48, v49, v50, v51], preserve_unit_iters=True)
v57, v58, v59, v60, v61 = sch.sample_perfect_tile(loop=l43, n=5, max_innermost_factor=64, decision=[4, 1, 1, 1, 1])
l62, l63, l64, l65, l66 = sch.split(loop=l43, factors=[v57, v58, v59, v60, v61], preserve_unit_iters=True)
v67, v68, v69, v70, v71 = sch.sample_perfect_tile(loop=l44, n=5, max_innermost_factor=64, decision=[4, 1, 8, 4, 1])
l72, l73, l74, l75, l76 = sch.split(loop=l44, factors=[v67, v68, v69, v70, v71], preserve_unit_iters=True)
v77, v78, v79, v80, v81 = sch.sample_perfect_tile(loop=l45, n=5, max_innermost_factor=64, decision=[7, 2, 14, 1, 1])
l82, l83, l84, l85, l86 = sch.split(loop=l45, factors=[v77, v78, v79, v80, v81], preserve_unit_iters=True)
v87, v88, v89 = sch.sample_perfect_tile(loop=l46, n=3, max_innermost_factor=64, decision=[2, 16, 4])
l90, l91, l92 = sch.split(loop=l46, factors=[v87, v88, v89], preserve_unit_iters=True)
sch.reorder(l52, l62, l72, l82, l53, l63, l73, l83, l54, l64, l74, l84, l90, l91, l55, l65, l75, l85, l92, l56, l66, l76, l86)
l93 = sch.fuse(l52, l62, l72, l82, preserve_unit_iters=True)
sch.bind(loop=l93, thread_axis="blockIdx.x")
l94 = sch.fuse(l53, l63, l73, l83, preserve_unit_iters=True)
sch.bind(loop=l94, thread_axis="vthread.x")
l95 = sch.fuse(l54, l64, l74, l84, preserve_unit_iters=True)
sch.bind(loop=l95, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b96 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b96, loop=l95, preserve_unit_loops=True, index=-1)
b97 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b97, loop=l90, preserve_unit_loops=True, index=-1)
l98, l99, l100, l101, l102, l103, l104, l105 = sch.get_loops(block=b97)
l106 = sch.fuse(l102, l103, l104, l105, preserve_unit_iters=True)
v107 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b97, ann_key="meta_schedule.cooperative_fetch", ann_val=v107)
b108 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b108, loop=l90, preserve_unit_loops=True, index=-1)
l109, l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b108)
l117 = sch.fuse(l113, l114, l115, l116, preserve_unit_iters=True)
v118 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b108, ann_key="meta_schedule.cooperative_fetch", ann_val=v118)
l119 = sch.fuse(l90, preserve_unit_iters=True)
sch.annotate(block_or_loop=l119, ann_key="software_pipeline_stage", ann_val=[0, 0, 2])
sch.annotate(block_or_loop=l119, ann_key="software_pipeline_order", ann_val=[0, 1, 2])
sch.annotate(block_or_loop=l119, ann_key="software_pipeline_async_stages", ann_val=[0])
sch.reverse_compute_inline(block=b5)
sch.reverse_compute_inline(block=b4)
v120 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b6, ann_key="meta_schedule.unroll_explicit", ann_val=v120)
l121, l122, l123, l124, l125, l126 = sch.get_loops(block=b3)
l127 = sch.fuse(l121, l122, l123, l124, preserve_unit_iters=True)
v128 = sch.sample_categorical(candidates=[32, 64, 128, 256, 512, 1024], probs=[0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666], decision=2)
l129, l130 = sch.split(loop=l127, factors=[None, v128], preserve_unit_iters=True)
sch.bind(loop=l129, thread_axis="blockIdx.x")
sch.bind(loop=l130, thread_axis="threadIdx.x")
sch.enter_postproc()
sch.unannotate(block_or_loop=b97, ann_key="meta_schedule.cooperative_fetch")
l131, l132, l133, l134, l135 = sch.get_loops(block=b97)
l136, l137 = sch.split(loop=l135, factors=[None, 112], preserve_unit_iters=True)
sch.bind(loop=l137, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b108, ann_key="meta_schedule.cooperative_fetch")
l138, l139, l140, l141, l142 = sch.get_loops(block=b108)
l143, l144, l145 = sch.split(loop=l142, factors=[None, 112, 4], preserve_unit_iters=True)
sch.vectorize(loop=l145)
sch.bind(loop=l144, thread_axis="threadIdx.x")
b146 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b146, ann_key="meta_schedule.unroll_explicit")
b147, b148, b149, b150, b151, b152, b153, b154, b155 = sch.get_child_blocks(b146)
l156, l157, l158, l159, l160, l161 = sch.get_loops(block=b147)
l162, l163, l164, l165, l166, l167 = sch.get_loops(block=b148)
sch.annotate(block_or_loop=l162, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l162, ann_key="pragma_unroll_explicit", ann_val=1)
l168, l169, l170, l171, l172, l173 = sch.get_loops(block=b149)
l174, l175, l176, l177, l178, l179 = sch.get_loops(block=b150)
l180, l181, l182, l183, l184, l185, l186 = sch.get_loops(block=b151)
l187, l188, l189, l190, l191, l192, l193, l194, l195, l196, l197, l198, l199, l200 = sch.get_loops(block=b152)
sch.annotate(block_or_loop=l187, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l187, ann_key="pragma_unroll_explicit", ann_val=1)
l201, l202, l203, l204, l205, l206, l207 = sch.get_loops(block=b153)
l208, l209, l210, l211, l212, l213, l214, l215 = sch.get_loops(block=b154)
sch.annotate(block_or_loop=l208, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l208, ann_key="pragma_unroll_explicit", ann_val=1)
l216, l217, l218, l219 = sch.get_loops(block=b155)
b220 = sch.get_block(name="data_pack", func_name="main")
l221, l222, l223, l224, l225, l226 = sch.get_loops(block=b220)
b227 = sch.decompose_reduction(block=b220, loop=l225)
b228 = sch.get_block(name="bgemm", func_name="main")
l229, l230, l231, l232, l233, l234, l235, l236, l237, l238, l239, l240, l241, l242 = sch.get_loops(block=b228)
b243 = sch.decompose_reduction(block=b228, loop=l232)
b244 = sch.get_block(name="inverse", func_name="main")
l245, l246, l247, l248, l249, l250, l251, l252 = sch.get_loops(block=b244)
b253 = sch.decompose_reduction(block=b244, loop=l251)
2023-11-11 07:30:18 [INFO] [task_scheduler.cc:121] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #515: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/home2/xiachunwei/Software/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/home2/xiachunwei/Software/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/home2/xiachunwei/Software/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/home2/xiachunwei/Software/tvm/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/home2/xiachunwei/Software/tvm/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/home2/xiachunwei/Software/tvm/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/home2/xiachunwei/Software/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/home2/xiachunwei/Software/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  4: _ZN3tvm7runtime13PackedFun
  3: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  2: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  0: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  File "/home2/xiachunwei/Software/tvm/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(128), T.int64(28), T.int64(28)), "float32"), p1: T.Buffer((T.int64(4), T.int64(4), T.int64(128), T.int64(128)), "float32"), p2: T.Buffer((T.int64(1), T.int64(128), T.int64(1), T.int64(1)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(128), T.int64(28), T.int64(28)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        input_tile_local = T.alloc_buffer((T.int64(128), T.int64(196), T.int64(4), T.int64(4)), scope="local")
        data_pack = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(128), T.int64(196)))
        bgemm = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(128), T.int64(196)))
        inverse_local = T.alloc_buffer((T.int64(128), T.int64(196), T.int64(2), T.int64(2)), scope="local")
        data_pack_local = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(128), T.int64(196)), scope="local")
        bgemm_local = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(128), T.int64(196)), scope="local")
        data_pack_shared = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(128), T.int64(196)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(128), T.int64(128)), scope="shared")
        for ci_p_fused_0 in T.thread_binding(T.int64(392), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 1024, "pragma_unroll_explicit": 1}):
            for ci_p_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(1), T.int64(4), T.int64(4)):
                    with T.block("input_tile"):
                        v_ci = T.axis.spatial(T.int64(128), (ci_p_fused_0 * T.int64(64) + ci_p_fused_1) // T.int64(196) + ax0)
                        v_p = T.axis.spatial(T.int64(196), (ci_p_fused_0 * T.int64(64) + ci_p_fused_1) % T.int64(196) + ax1)
                        v_eps, v_nu = T.axis.remap("SS", [ax2, ax3])
                        T.reads(p0[v_p // T.int64(196), v_ci, v_p % T.int64(196) // T.int64(14) * T.int64(2) + v_eps - T.int64(1), v_p % T.int64(14) * T.int64(2) + v_nu - T.int64(1)])
                        T.writes(input_tile_local[v_ci, v_p, v_eps, v_nu])
                        T.block_attr({"schedule_rule": "None"})
                        input_tile_local[v_ci, v_p, v_eps, v_nu] = T.if_then_else(T.int64(1) <= v_p % T.int64(196) // T.int64(14) * T.int64(2) + v_eps and v_p % T.int64(196) // T.int64(14) * T.int64(2) + v_eps < T.int64(29) and T.int64(1) <= v_p % T.int64(14) * T.int64(2) + v_nu and v_p % T.int64(14) * T.int64(2) + v_nu < T.int64(29), p0[v_p // T.int64(196), v_ci, v_p % T.int64(196) // T.int64(14) * T.int64(2) + v_eps - T.int64(1), v_p % T.int64(14) * T.int64(2) + v_nu - T.int64(1)], T.float32(0))
                for eps in T.unroll(T.int64(4)):
                    for nu in T.unroll(T.int64(4)):
                        with T.block("data_pack_init"):
                            v_eps, v_nu = T.axis.remap("SS", [eps, nu])
                            v_ci = T.axis.spatial(T.int64(128), (ci_p_fused_0 * T.int64(64) + ci_p_fused_1) // T.int64(196))
                            v_p = T.axis.spatial(T.int64(196), (ci_p_fused_0 * T.int64(64) + ci_p_fused_1) % T.int64(196))
                            T.reads()
                            T.writes(data_pack_local[v_eps, v_nu, v_ci, v_p])
                            T.block_attr({"schedule_rule": "conv2d_nchw_winograd_data_pack"})
                            data_pack_local[v_eps, v_nu, v_ci, v_p] = T.float32(0)
                        for r_a in T.unroll(T.int64(4)):
                            for r_b in T.unroll(T.int64(4)):
                                with T.block("data_pack_update"):
                                    v_eps, v_nu = T.axis.remap("SS", [eps, nu])
                                    v_ci = T.axis.spatial(T.int64(128), (ci_p_fused_0 * T.int64(64) + ci_p_fused_1) // T.int64(196))
                                    v_p = T.axis.spatial(T.int64(196), (ci_p_fused_0 * T.int64(64) + ci_p_fused_1) % T.int64(196))
                                    v_r_a, v_r_b = T.axis.remap("RR", [r_a, r_b])
                                    T.reads(data_pack_local[v_eps, v_nu, v_ci, v_p], input_tile_local[v_ci, v_p, v_r_a, v_r_b])
                                    T.writes(data_pack_local[v_eps, v_nu, v_ci, v_p])
                                    T.block_attr({"schedule_rule": "conv2d_nchw_winograd_data_pack"})
                                    data_pack_local[v_eps, v_nu, v_ci, v_p] = data_pack_local[v_eps, v_nu, v_ci, v_p] + input_tile_local[v_ci, v_p, v_r_a, v_r_b] * T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(3), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(1), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(0), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(3), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(1), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(0), T.float32(1), T.float32(0))))))))))))))))) * T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(3), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(1), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(0), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(3), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(1), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(0), T.float32(1), T.float32(0)))))))))))))))))
                for ax0, ax1, ax2, ax3 in T.grid(T.int64(4), T.int64(4), T.int64(1), T.int64(1)):
                    with T.block("data_pack_local"):
                        v0, v1 = T.axis.remap("SS", [ax0, ax1])
                        v2 = T.axis.spatial(T.int64(128), (ci_p_fused_0 * T.int64(64) + ci_p_fused_1) // T.int64(196) + ax2)
                        v3 = T.axis.spatial(T.int64(196), (ci_p_fused_0 * T.int64(64) + ci_p_fused_1) % T.int64(196) + ax3)
                        T.reads(data_pack_local[v0, v1, v2, v3])
                        T.writes(data_pack[v0, v1, v2, v3])
                        data_pack[v0, v1, v2, v3] = data_pack_local[v0, v1, v2, v3]
        for eps_0_nu_0_co_0_p_0_fused in T.thread_binding(T.int64(448), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 1024, "pragma_unroll_explicit": 1}):
            for eps_1_nu_1_co_1_p_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for eps_2_nu_2_co_2_p_2_fused in T.thread_binding(T.int64(112), thread="threadIdx.x"):
                    for eps_3_init, nu_3_init, co_3_init, p_3_init, eps_4_init, nu_4_init, co_4_init, p_4_init in T.grid(T.int64(1), T.int64(1), T.int64(4), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(2)):
                        with T.block("bgemm_init"):
                            v_eps = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(112) + eps_3_init + eps_4_init)
                            v_nu = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused % T.int64(112) // T.int64(28) + nu_3_init + nu_4_init)
                            v_co = T.axis.spatial(T.int64(128), eps_0_nu_0_co_0_p_0_fused % T.int64(28) // T.int64(7) * T.int64(32) + eps_2_nu_2_co_2_p_2_fused // T.int64(14) * T.int64(4) + co_3_init + co_4_init)
                            v_p = T.axis.spatial(T.int64(196), eps_0_nu_0_co_0_p_0_fused % T.int64(7) * T.int64(28) + eps_2_nu_2_co_2_p_2_fused % T.int64(14) * T.int64(2) + p_3_init * T.int64(2) + p_4_init)
                            T.reads()
                            T.writes(bgemm_local[v_eps, v_nu, v_co, v_p])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            bgemm_local[v_eps, v_nu, v_co, v_p] = T.float32(0)
                    for ci_0_fused in T.serial(T.int64(2), annotations={"software_pipeline_async_stages": [0], "software_pipeline_order": [0, 1, 2], "software_pipeline_stage": [0, 0, 2]}):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(8)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(112), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("data_pack_shared"):
                                        v0 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(112))
                                        v1 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused % T.int64(112) // T.int64(28))
                                        v2 = T.axis.spatial(T.int64(128), ci_0_fused * T.int64(64) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(224) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) // T.int64(28))
                                        v3 = T.axis.spatial(T.int64(196), eps_0_nu_0_co_0_p_0_fused % T.int64(7) * T.int64(28) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(224) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) % T.int64(28))
                                        T.reads(data_pack[v0, v1, v2, v3])
                                        T.writes(data_pack_shared[v0, v1, v2, v3])
                                        data_pack_shared[v0, v1, v2, v3] = data_pack[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(10)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(112), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("p1_shared"):
                                        v0 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(112))
                                        v1 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused % T.int64(112) // T.int64(28))
                                        v2 = T.axis.spatial(T.int64(128), ci_0_fused * T.int64(64) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(224) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) // T.int64(32))
                                        v3 = T.axis.spatial(T.int64(128), eps_0_nu_0_co_0_p_0_fused % T.int64(28) // T.int64(7) * T.int64(32) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(224) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) % T.int64(32))
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * T.int64(112) + ax0_ax1_ax2_ax3_fused_1) * T.int64(2) + ax0_ax1_ax2_ax3_fused_2 < T.int64(2048))
                                        T.reads(p1[v0, v1, v2, v3])
                                        T.writes(p1_shared[v0, v1, v2, v3])
                                        p1_shared[v0, v1, v2, v3] = p1[v0, v1, v2, v3]
                        for ci_1, eps_3, nu_3, co_3, p_3, ci_2, eps_4, nu_4, co_4, p_4 in T.grid(T.int64(16), T.int64(1), T.int64(1), T.int64(4), T.int64(1), T.int64(4), T.int64(1), T.int64(1), T.int64(1), T.int64(2)):
                            with T.block("bgemm_update"):
                                v_eps = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(112) + eps_3 + eps_4)
                                v_nu = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused % T.int64(112) // T.int64(28) + nu_3 + nu_4)
                                v_co = T.axis.spatial(T.int64(128), eps_0_nu_0_co_0_p_0_fused % T.int64(28) // T.int64(7) * T.int64(32) + eps_2_nu_2_co_2_p_2_fused // T.int64(14) * T.int64(4) + co_3 + co_4)
                                v_p = T.axis.spatial(T.int64(196), eps_0_nu_0_co_0_p_0_fused % T.int64(7) * T.int64(28) + eps_2_nu_2_co_2_p_2_fused % T.int64(14) * T.int64(2) + p_3 * T.int64(2) + p_4)
                                v_ci = T.axis.reduce(T.int64(128), ci_0_fused * T.int64(64) + ci_1 * T.int64(4) + ci_2)
                                T.reads(bgemm_local[v_eps, v_nu, v_co, v_p], data_pack_shared[v_eps, v_nu, v_ci, v_p], p1_shared[v_eps, v_nu, v_ci, v_co])
                                T.writes(bgemm_local[v_eps, v_nu, v_co, v_p])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                bgemm_local[v_eps, v_nu, v_co, v_p] = bgemm_local[v_eps, v_nu, v_co, v_p] + data_pack_shared[v_eps, v_nu, v_ci, v_p] * p1_shared[v_eps, v_nu, v_ci, v_co]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(1), T.int64(4), T.int64(2)):
                        with T.block("bgemm_local"):
                            v0 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(112) + ax0)
                            v1 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused % T.int64(112) // T.int64(28) + ax1)
                            v2 = T.axis.spatial(T.int64(128), eps_0_nu_0_co_0_p_0_fused % T.int64(28) // T.int64(7) * T.int64(32) + eps_2_nu_2_co_2_p_2_fused // T.int64(14) * T.int64(4) + ax2)
                            v3 = T.axis.spatial(T.int64(196), eps_0_nu_0_co_0_p_0_fused % T.int64(7) * T.int64(28) + eps_2_nu_2_co_2_p_2_fused % T.int64(14) * T.int64(2) + ax3)
                            T.reads(bgemm_local[v0, v1, v2, v3])
                            T.writes(bgemm[v0, v1, v2, v3])
                            bgemm[v0, v1, v2, v3] = bgemm_local[v0, v1, v2, v3]
        for n_co_h_0_w_0_fused_0 in T.thread_binding(T.int64(392), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 1024, "pragma_unroll_explicit": 1}):
            for n_co_h_0_w_0_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                    for ax2 in T.unroll(T.int64(2)):
                        for ax3 in T.unroll(T.int64(2)):
                            with T.block("inverse_init"):
                                v_co = T.axis.spatial(T.int64(128), (n_co_h_0_w_0_fused_0 * T.int64(64) + n_co_h_0_w_0_fused_1) // T.int64(196) + ax0)
                                v_p = T.axis.spatial(T.int64(196), (n_co_h_0_w_0_fused_0 * T.int64(64) + n_co_h_0_w_0_fused_1) % T.int64(196) + ax1)
                                v_vh, v_vw = T.axis.remap("SS", [ax2, ax3])
                                T.reads()
                                T.writes(inverse_local[v_co, v_p, v_vh, v_vw])
                                T.block_attr({"schedule_rule": "conv2d_nchw_winograd_inverse"})
                                inverse_local[v_co, v_p, v_vh, v_vw] = T.float32(0)
                            for ax4 in T.unroll(T.int64(4)):
                                for ax5 in T.unroll(T.int64(4)):
                                    with T.block("inverse_update"):
                                        v_co = T.axis.spatial(T.int64(128), (n_co_h_0_w_0_fused_0 * T.int64(64) + n_co_h_0_w_0_fused_1) // T.int64(196) + ax0)
                                        v_p = T.axis.spatial(T.int64(196), (n_co_h_0_w_0_fused_0 * T.int64(64) + n_co_h_0_w_0_fused_1) % T.int64(196) + ax1)
                                        v_vh, v_vw, v_r_a, v_r_b = T.axis.remap("SSRR", [ax2, ax3, ax4, ax5])
                                        T.reads(inverse_local[v_co, v_p, v_vh, v_vw], bgemm[v_r_a, v_r_b, v_co, v_p])
                                        T.writes(inverse_local[v_co, v_p, v_vh, v_vw])
                                        T.block_attr({"schedule_rule": "conv2d_nchw_winograd_inverse"})
                                        inverse_local[v_co, v_p, v_vh, v_vw] = inverse_local[v_co, v_p, v_vh, v_vw] + bgemm[v_r_a, v_r_b, v_co, v_p] * T.Select(v_r_a % T.int64(4) == T.int64(3) and v_vh % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_vh % T.int64(2) == T.int64(0), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_vh % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_vh % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_vh % T.int64(2) == T.int64(1), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_vh % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_vh % T.int64(2) == T.int64(1), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_vh % T.int64(2) == T.int64(0), T.float32(1), T.float32(0))))))))) * T.Select(v_r_b % T.int64(4) == T.int64(3) and v_vw % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_vw % T.int64(2) == T.int64(0), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_vw % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_vw % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_vw % T.int64(2) == T.int64(1), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_vw % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_vw % T.int64(2) == T.int64(1), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_vw % T.int64(2) == T.int64(0), T.float32(1), T.float32(0)))))))))
                for h_1, w_1 in T.grid(T.int64(2), T.int64(2)):
                    with T.block("conv2d_winograd"):
                        v_n = T.axis.spatial(T.int64(1), T.int64(0))
                        v_co = T.axis.spatial(T.int64(128), (n_co_h_0_w_0_fused_0 * T.int64(64) + n_co_h_0_w_0_fused_1) // T.int64(196))
                        v_h = T.axis.spatial(T.int64(28), (n_co_h_0_w_0_fused_0 * T.int64(64) + n_co_h_0_w_0_fused_1) % T.int64(196) // T.int64(14) * T.int64(2) + h_1)
                        v_w = T.axis.spatial(T.int64(28), (n_co_h_0_w_0_fused_0 * T.int64(64) + n_co_h_0_w_0_fused_1) % T.int64(14) * T.int64(2) + w_1)
                        T.reads(inverse_local[v_co, v_n * T.int64(196) + v_h // T.int64(2) * T.int64(14) + v_w // T.int64(2), v_h % T.int64(2), v_w % T.int64(2)], p2[v_n, v_co, T.int64(0), T.int64(0)])
                        T.writes(T_relu[v_n, v_co, v_h, v_w])
                        T_relu[v_n, v_co, v_h, v_w] = T.max(inverse_local[v_co, v_n * T.int64(196) + v_h // T.int64(2) * T.int64(14) + v_w // T.int64(2), v_h % T.int64(2), v_w % T.int64(2)] + p2[v_n, v_co, T.int64(0), T.int64(0)], T.float32(0))
b0 = sch.get_block(name="data_pack", func_name="main")
b1 = sch.get_block(name="bgemm", func_name="main")
b2 = sch.get_block(name="inverse", func_name="main")
b3 = sch.get_block(name="conv2d_winograd", func_name="main")
b4 = sch.get_block(name="T_add", func_name="main")
b5 = sch.get_block(name="T_relu", func_name="main")
b6 = sch.get_block(name="root", func_name="main")
b7, b8 = sch.get_producers(block=b2)
sch.compute_inline(block=b8)
b9, = sch.get_consumers(block=b2)
l10, l11, l12, l13 = sch.get_loops(block=b9)
l14, l15 = sch.split(loop=l12, factors=[None, 2], preserve_unit_iters=True)
l16, l17 = sch.split(loop=l13, factors=[None, 2], preserve_unit_iters=True)
sch.reorder(l14, l16, l15, l17)
sch.compute_at(block=b2, loop=l16, preserve_unit_loops=True, index=-1)
sch.set_scope(block=b2, buffer_index=0, storage_scope="local")
l18, l19, l20, l21, l22, l23, l24, l25, l26, l27 = sch.get_loops(block=b2)
sch.unroll(loop=l24)
sch.unroll(loop=l25)
sch.unroll(loop=l26)
sch.unroll(loop=l27)
b28, b29 = sch.get_producers(block=b0)
sch.compute_inline(block=b29)
b30, = sch.get_producers(block=b28)
l31, l32, l33, l34, l35, l36 = sch.get_loops(block=b0)
sch.reorder(l33, l34, l31, l32, l35, l36)
sch.unroll(loop=l31)
sch.unroll(loop=l32)
sch.unroll(loop=l35)
sch.unroll(loop=l36)
l37 = sch.fuse(l33, l34, preserve_unit_iters=True)
v38 = sch.sample_categorical(candidates=[32, 64, 128, 256, 512, 1024], probs=[0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666], decision=1)
l39, l40 = sch.split(loop=l37, factors=[None, v38], preserve_unit_iters=True)
sch.bind(loop=l39, thread_axis="blockIdx.x")
sch.bind(loop=l40, thread_axis="threadIdx.x")
b41 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b41, loop=l40, preserve_unit_loops=True, index=-1)
sch.compute_at(block=b28, loop=l40, preserve_unit_loops=True, index=-1)
sch.set_scope(block=b28, buffer_index=0, storage_scope="local")
sch.compute_inline(block=b30)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l42, l43, l44, l45, l46 = sch.get_loops(block=b1)
v47, v48, v49, v50, v51 = sch.sample_perfect_tile(loop=l42, n=5, max_innermost_factor=64, decision=[4, 1, 1, 1, 1])
l52, l53, l54, l55, l56 = sch.split(loop=l42, factors=[v47, v48, v49, v50, v51], preserve_unit_iters=True)
v57, v58, v59, v60, v61 = sch.sample_perfect_tile(loop=l43, n=5, max_innermost_factor=64, decision=[4, 1, 1, 1, 1])
l62, l63, l64, l65, l66 = sch.split(loop=l43, factors=[v57, v58, v59, v60, v61], preserve_unit_iters=True)
v67, v68, v69, v70, v71 = sch.sample_perfect_tile(loop=l44, n=5, max_innermost_factor=64, decision=[4, 1, 8, 4, 1])
l72, l73, l74, l75, l76 = sch.split(loop=l44, factors=[v67, v68, v69, v70, v71], preserve_unit_iters=True)
v77, v78, v79, v80, v81 = sch.sample_perfect_tile(loop=l45, n=5, max_innermost_factor=64, decision=[7, 1, 14, 1, 2])
l82, l83, l84, l85, l86 = sch.split(loop=l45, factors=[v77, v78, v79, v80, v81], preserve_unit_iters=True)
v87, v88, v89 = sch.sample_perfect_tile(loop=l46, n=3, max_innermost_factor=64, decision=[2, 16, 4])
l90, l91, l92 = sch.split(loop=l46, factors=[v87, v88, v89], preserve_unit_iters=True)
sch.reorder(l52, l62, l72, l82, l53, l63, l73, l83, l54, l64, l74, l84, l90, l91, l55, l65, l75, l85, l92, l56, l66, l76, l86)
l93 = sch.fuse(l52, l62, l72, l82, preserve_unit_iters=True)
sch.bind(loop=l93, thread_axis="blockIdx.x")
l94 = sch.fuse(l53, l63, l73, l83, preserve_unit_iters=True)
sch.bind(loop=l94, thread_axis="vthread.x")
l95 = sch.fuse(l54, l64, l74, l84, preserve_unit_iters=True)
sch.bind(loop=l95, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b96 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b96, loop=l95, preserve_unit_loops=True, index=-1)
b97 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b97, loop=l90, preserve_unit_loops=True, index=-1)
l98, l99, l100, l101, l102, l103, l104, l105 = sch.get_loops(block=b97)
l106 = sch.fuse(l102, l103, l104, l105, preserve_unit_iters=True)
v107 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b97, ann_key="meta_schedule.cooperative_fetch", ann_val=v107)
b108 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b108, loop=l90, preserve_unit_loops=True, index=-1)
l109, l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b108)
l117 = sch.fuse(l113, l114, l115, l116, preserve_unit_iters=True)
v118 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b108, ann_key="meta_schedule.cooperative_fetch", ann_val=v118)
l119 = sch.fuse(l90, preserve_unit_iters=True)
sch.annotate(block_or_loop=l119, ann_key="software_pipeline_stage", ann_val=[0, 0, 2])
sch.annotate(block_or_loop=l119, ann_key="software_pipeline_order", ann_val=[0, 1, 2])
sch.annotate(block_or_loop=l119, ann_key="software_pipeline_async_stages", ann_val=[0])
sch.reverse_compute_inline(block=b5)
sch.reverse_compute_inline(block=b4)
v120 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b6, ann_key="meta_schedule.unroll_explicit", ann_val=v120)
l121, l122, l123, l124, l125, l126 = sch.get_loops(block=b3)
l127 = sch.fuse(l121, l122, l123, l124, preserve_unit_iters=True)
v128 = sch.sample_categorical(candidates=[32, 64, 128, 256, 512, 1024], probs=[0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666], decision=1)
l129, l130 = sch.split(loop=l127, factors=[None, v128], preserve_unit_iters=True)
sch.bind(loop=l129, thread_axis="blockIdx.x")
sch.bind(loop=l130, thread_axis="threadIdx.x")
sch.enter_postproc()
sch.unannotate(block_or_loop=b97, ann_key="meta_schedule.cooperative_fetch")
l131, l132, l133, l134, l135 = sch.get_loops(block=b97)
l136, l137, l138 = sch.split(loop=l135, factors=[None, 112, 2], preserve_unit_iters=True)
sch.vectorize(loop=l138)
sch.bind(loop=l137, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b108, ann_key="meta_schedule.cooperative_fetch")
l139, l140, l141, l142, l143 = sch.get_loops(block=b108)
l144, l145, l146 = sch.split(loop=l143, factors=[None, 112, 2], preserve_unit_iters=True)
sch.vectorize(loop=l146)
sch.bind(loop=l145, thread_axis="threadIdx.x")
b147 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b147, ann_key="meta_schedule.unroll_explicit")
b148, b149, b150, b151, b152, b153, b154, b155, b156 = sch.get_child_blocks(b147)
l157, l158, l159, l160, l161, l162 = sch.get_loops(block=b148)
l163, l164, l165, l166, l167, l168 = sch.get_loops(block=b149)
sch.annotate(block_or_loop=l163, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l163, ann_key="pragma_unroll_explicit", ann_val=1)
l169, l170, l171, l172, l173, l174 = sch.get_loops(block=b150)
l175, l176, l177, l178, l179, l180, l181 = sch.get_loops(block=b151)
l182, l183, l184, l185, l186, l187, l188 = sch.get_loops(block=b152)
l189, l190, l191, l192, l193, l194, l195, l196, l197, l198, l199, l200, l201, l202 = sch.get_loops(block=b153)
sch.annotate(block_or_loop=l189, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l189, ann_key="pragma_unroll_explicit", ann_val=1)
l203, l204, l205, l206, l207, l208, l209 = sch.get_loops(block=b154)
l210, l211, l212, l213, l214, l215, l216, l217 = sch.get_loops(block=b155)
sch.annotate(block_or_loop=l210, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l210, ann_key="pragma_unroll_explicit", ann_val=1)
l218, l219, l220, l221 = sch.get_loops(block=b156)
b222 = sch.get_block(name="data_pack", func_name="main")
l223, l224, l225, l226, l227, l228 = sch.get_loops(block=b222)
b229 = sch.decompose_reduction(block=b222, loop=l227)
b230 = sch.get_block(name="bgemm", func_name="main")
l231, l232, l233, l234, l235, l236, l237, l238, l239, l240, l241, l242, l243, l244 = sch.get_loops(block=b230)
b245 = sch.decompose_reduction(block=b230, loop=l234)
b246 = sch.get_block(name="inverse", func_name="main")
l247, l248, l249, l250, l251, l252, l253, l254 = sch.get_loops(block=b246)
b255 = sch.decompose_reduction(block=b246, loop=l253)
2023-11-11 07:30:18 [INFO] [task_scheduler.cc:121] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #516: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/home2/xiachunwei/Software/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/home2/xiachunwei/Software/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/home2/xiachunwei/Software/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/home2/xiachunwei/Software/tvm/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/home2/xiachunwei/Software/tvm/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/home2/xiachunwei/Software/tvm/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/home2/xiachunwei/Software/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/home2/xiachunwei/Software/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  4: _ZN3tvm7runtime13PackedFun
  3: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  2: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  0: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  File "/home2/xiachunwei/Software/tvm/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(128), T.int64(28), T.int64(28)), "float32"), p1: T.Buffer((T.int64(4), T.int64(4), T.int64(128), T.int64(128)), "float32"), p2: T.Buffer((T.int64(1), T.int64(128), T.int64(1), T.int64(1)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(128), T.int64(28), T.int64(28)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        input_tile_local = T.alloc_buffer((T.int64(128), T.int64(196), T.int64(4), T.int64(4)), scope="local")
        data_pack = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(128), T.int64(196)))
        bgemm = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(128), T.int64(196)))
        inverse_local = T.alloc_buffer((T.int64(128), T.int64(196), T.int64(2), T.int64(2)), scope="local")
        data_pack_local = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(128), T.int64(196)), scope="local")
        bgemm_local = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(128), T.int64(196)), scope="local")
        data_pack_shared = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(128), T.int64(196)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(128), T.int64(128)), scope="shared")
        for ci_p_fused_0 in T.thread_binding(T.int64(392), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 1024, "pragma_unroll_explicit": 1}):
            for ci_p_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(1), T.int64(4), T.int64(4)):
                    with T.block("input_tile"):
                        v_ci = T.axis.spatial(T.int64(128), (ci_p_fused_0 * T.int64(64) + ci_p_fused_1) // T.int64(196) + ax0)
                        v_p = T.axis.spatial(T.int64(196), (ci_p_fused_0 * T.int64(64) + ci_p_fused_1) % T.int64(196) + ax1)
                        v_eps, v_nu = T.axis.remap("SS", [ax2, ax3])
                        T.reads(p0[v_p // T.int64(196), v_ci, v_p % T.int64(196) // T.int64(14) * T.int64(2) + v_eps - T.int64(1), v_p % T.int64(14) * T.int64(2) + v_nu - T.int64(1)])
                        T.writes(input_tile_local[v_ci, v_p, v_eps, v_nu])
                        T.block_attr({"schedule_rule": "None"})
                        input_tile_local[v_ci, v_p, v_eps, v_nu] = T.if_then_else(T.int64(1) <= v_p % T.int64(196) // T.int64(14) * T.int64(2) + v_eps and v_p % T.int64(196) // T.int64(14) * T.int64(2) + v_eps < T.int64(29) and T.int64(1) <= v_p % T.int64(14) * T.int64(2) + v_nu and v_p % T.int64(14) * T.int64(2) + v_nu < T.int64(29), p0[v_p // T.int64(196), v_ci, v_p % T.int64(196) // T.int64(14) * T.int64(2) + v_eps - T.int64(1), v_p % T.int64(14) * T.int64(2) + v_nu - T.int64(1)], T.float32(0))
                for eps in T.unroll(T.int64(4)):
                    for nu in T.unroll(T.int64(4)):
                        with T.block("data_pack_init"):
                            v_eps, v_nu = T.axis.remap("SS", [eps, nu])
                            v_ci = T.axis.spatial(T.int64(128), (ci_p_fused_0 * T.int64(64) + ci_p_fused_1) // T.int64(196))
                            v_p = T.axis.spatial(T.int64(196), (ci_p_fused_0 * T.int64(64) + ci_p_fused_1) % T.int64(196))
                            T.reads()
                            T.writes(data_pack_local[v_eps, v_nu, v_ci, v_p])
                            T.block_attr({"schedule_rule": "conv2d_nchw_winograd_data_pack"})
                            data_pack_local[v_eps, v_nu, v_ci, v_p] = T.float32(0)
                        for r_a in T.unroll(T.int64(4)):
                            for r_b in T.unroll(T.int64(4)):
                                with T.block("data_pack_update"):
                                    v_eps, v_nu = T.axis.remap("SS", [eps, nu])
                                    v_ci = T.axis.spatial(T.int64(128), (ci_p_fused_0 * T.int64(64) + ci_p_fused_1) // T.int64(196))
                                    v_p = T.axis.spatial(T.int64(196), (ci_p_fused_0 * T.int64(64) + ci_p_fused_1) % T.int64(196))
                                    v_r_a, v_r_b = T.axis.remap("RR", [r_a, r_b])
                                    T.reads(data_pack_local[v_eps, v_nu, v_ci, v_p], input_tile_local[v_ci, v_p, v_r_a, v_r_b])
                                    T.writes(data_pack_local[v_eps, v_nu, v_ci, v_p])
                                    T.block_attr({"schedule_rule": "conv2d_nchw_winograd_data_pack"})
                                    data_pack_local[v_eps, v_nu, v_ci, v_p] = data_pack_local[v_eps, v_nu, v_ci, v_p] + input_tile_local[v_ci, v_p, v_r_a, v_r_b] * T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(3), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(1), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(0), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(3), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(1), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(0), T.float32(1), T.float32(0))))))))))))))))) * T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(3), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(1), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(0), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(3), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(1), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(0), T.float32(1), T.float32(0)))))))))))))))))
                for ax0, ax1, ax2, ax3 in T.grid(T.int64(4), T.int64(4), T.int64(1), T.int64(1)):
                    with T.block("data_pack_local"):
                        v0, v1 = T.axis.remap("SS", [ax0, ax1])
                        v2 = T.axis.spatial(T.int64(128), (ci_p_fused_0 * T.int64(64) + ci_p_fused_1) // T.int64(196) + ax2)
                        v3 = T.axis.spatial(T.int64(196), (ci_p_fused_0 * T.int64(64) + ci_p_fused_1) % T.int64(196) + ax3)
                        T.reads(data_pack_local[v0, v1, v2, v3])
                        T.writes(data_pack[v0, v1, v2, v3])
                        data_pack[v0, v1, v2, v3] = data_pack_local[v0, v1, v2, v3]
        for eps_0_nu_0_co_0_p_0_fused in T.thread_binding(T.int64(448), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 1024, "pragma_unroll_explicit": 1}):
            for eps_1_nu_1_co_1_p_1_fused in T.thread_binding(T.int64(2), thread="vthread.x"):
                for eps_2_nu_2_co_2_p_2_fused in T.thread_binding(T.int64(112), thread="threadIdx.x"):
                    for eps_3_init, nu_3_init, co_3_init, p_3_init, eps_4_init, nu_4_init, co_4_init, p_4_init in T.grid(T.int64(1), T.int64(1), T.int64(2), T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(1)):
                        with T.block("bgemm_init"):
                            v_eps = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(112) + eps_3_init + eps_4_init)
                            v_nu = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused % T.int64(112) // T.int64(28) + nu_3_init + nu_4_init)
                            v_co = T.axis.spatial(T.int64(128), eps_0_nu_0_co_0_p_0_fused % T.int64(28) // T.int64(7) * T.int64(32) + eps_2_nu_2_co_2_p_2_fused // T.int64(14) * T.int64(4) + co_3_init * T.int64(2) + co_4_init)
                            v_p = T.axis.spatial(T.int64(196), eps_0_nu_0_co_0_p_0_fused % T.int64(7) * T.int64(28) + eps_1_nu_1_co_1_p_1_fused * T.int64(14) + eps_2_nu_2_co_2_p_2_fused % T.int64(14) + p_3_init + p_4_init)
                            T.reads()
                            T.writes(bgemm_local[v_eps, v_nu, v_co, v_p])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            bgemm_local[v_eps, v_nu, v_co, v_p] = T.float32(0)
                    for ci_0_fused in T.serial(T.int64(2), annotations={"software_pipeline_async_stages": [0], "software_pipeline_order": [0, 1, 2], "software_pipeline_stage": [0, 0, 2]}):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(16)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(112), thread="threadIdx.x"):
                                with T.block("data_pack_shared"):
                                    v0 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(112))
                                    v1 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused % T.int64(112) // T.int64(28))
                                    v2 = T.axis.spatial(T.int64(128), ci_0_fused * T.int64(64) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(112) + ax0_ax1_ax2_ax3_fused_1) // T.int64(28))
                                    v3 = T.axis.spatial(T.int64(196), eps_0_nu_0_co_0_p_0_fused % T.int64(7) * T.int64(28) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(112) + ax0_ax1_ax2_ax3_fused_1) % T.int64(28))
                                    T.reads(data_pack[v0, v1, v2, v3])
                                    T.writes(data_pack_shared[v0, v1, v2, v3])
                                    data_pack_shared[v0, v1, v2, v3] = data_pack[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(10)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(112), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("p1_shared"):
                                        v0 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(112))
                                        v1 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused % T.int64(112) // T.int64(28))
                                        v2 = T.axis.spatial(T.int64(128), ci_0_fused * T.int64(64) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(224) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) // T.int64(32))
                                        v3 = T.axis.spatial(T.int64(128), eps_0_nu_0_co_0_p_0_fused % T.int64(28) // T.int64(7) * T.int64(32) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(224) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) % T.int64(32))
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * T.int64(112) + ax0_ax1_ax2_ax3_fused_1) * T.int64(2) + ax0_ax1_ax2_ax3_fused_2 < T.int64(2048))
                                        T.reads(p1[v0, v1, v2, v3])
                                        T.writes(p1_shared[v0, v1, v2, v3])
                                        p1_shared[v0, v1, v2, v3] = p1[v0, v1, v2, v3]
                        for ci_1, eps_3, nu_3, co_3, p_3, ci_2, eps_4, nu_4, co_4, p_4 in T.grid(T.int64(4), T.int64(1), T.int64(1), T.int64(2), T.int64(1), T.int64(16), T.int64(1), T.int64(1), T.int64(2), T.int64(1)):
                            with T.block("bgemm_update"):
                                v_eps = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(112) + eps_3 + eps_4)
                                v_nu = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused % T.int64(112) // T.int64(28) + nu_3 + nu_4)
                                v_co = T.axis.spatial(T.int64(128), eps_0_nu_0_co_0_p_0_fused % T.int64(28) // T.int64(7) * T.int64(32) + eps_2_nu_2_co_2_p_2_fused // T.int64(14) * T.int64(4) + co_3 * T.int64(2) + co_4)
                                v_p = T.axis.spatial(T.int64(196), eps_0_nu_0_co_0_p_0_fused % T.int64(7) * T.int64(28) + eps_1_nu_1_co_1_p_1_fused * T.int64(14) + eps_2_nu_2_co_2_p_2_fused % T.int64(14) + p_3 + p_4)
                                v_ci = T.axis.reduce(T.int64(128), ci_0_fused * T.int64(64) + ci_1 * T.int64(16) + ci_2)
                                T.reads(bgemm_local[v_eps, v_nu, v_co, v_p], data_pack_shared[v_eps, v_nu, v_ci, v_p], p1_shared[v_eps, v_nu, v_ci, v_co])
                                T.writes(bgemm_local[v_eps, v_nu, v_co, v_p])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                bgemm_local[v_eps, v_nu, v_co, v_p] = bgemm_local[v_eps, v_nu, v_co, v_p] + data_pack_shared[v_eps, v_nu, v_ci, v_p] * p1_shared[v_eps, v_nu, v_ci, v_co]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(1), T.int64(4), T.int64(1)):
                        with T.block("bgemm_local"):
                            v0 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(112) + ax0)
                            v1 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused % T.int64(112) // T.int64(28) + ax1)
                            v2 = T.axis.spatial(T.int64(128), eps_0_nu_0_co_0_p_0_fused % T.int64(28) // T.int64(7) * T.int64(32) + eps_2_nu_2_co_2_p_2_fused // T.int64(14) * T.int64(4) + ax2)
                            v3 = T.axis.spatial(T.int64(196), eps_0_nu_0_co_0_p_0_fused % T.int64(7) * T.int64(28) + eps_1_nu_1_co_1_p_1_fused * T.int64(14) + eps_2_nu_2_co_2_p_2_fused % T.int64(14) + ax3)
                            T.reads(bgemm_local[v0, v1, v2, v3])
                            T.writes(bgemm[v0, v1, v2, v3])
                            bgemm[v0, v1, v2, v3] = bgemm_local[v0, v1, v2, v3]
        for n_co_h_0_w_0_fused_0 in T.thread_binding(T.int64(392), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 1024, "pragma_unroll_explicit": 1}):
            for n_co_h_0_w_0_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                    for ax2 in T.unroll(T.int64(2)):
                        for ax3 in T.unroll(T.int64(2)):
                            with T.block("inverse_init"):
                                v_co = T.axis.spatial(T.int64(128), (n_co_h_0_w_0_fused_0 * T.int64(64) + n_co_h_0_w_0_fused_1) // T.int64(196) + ax0)
                                v_p = T.axis.spatial(T.int64(196), (n_co_h_0_w_0_fused_0 * T.int64(64) + n_co_h_0_w_0_fused_1) % T.int64(196) + ax1)
                                v_vh, v_vw = T.axis.remap("SS", [ax2, ax3])
                                T.reads()
                                T.writes(inverse_local[v_co, v_p, v_vh, v_vw])
                                T.block_attr({"schedule_rule": "conv2d_nchw_winograd_inverse"})
                                inverse_local[v_co, v_p, v_vh, v_vw] = T.float32(0)
                            for ax4 in T.unroll(T.int64(4)):
                                for ax5 in T.unroll(T.int64(4)):
                                    with T.block("inverse_update"):
                                        v_co = T.axis.spatial(T.int64(128), (n_co_h_0_w_0_fused_0 * T.int64(64) + n_co_h_0_w_0_fused_1) // T.int64(196) + ax0)
                                        v_p = T.axis.spatial(T.int64(196), (n_co_h_0_w_0_fused_0 * T.int64(64) + n_co_h_0_w_0_fused_1) % T.int64(196) + ax1)
                                        v_vh, v_vw, v_r_a, v_r_b = T.axis.remap("SSRR", [ax2, ax3, ax4, ax5])
                                        T.reads(inverse_local[v_co, v_p, v_vh, v_vw], bgemm[v_r_a, v_r_b, v_co, v_p])
                                        T.writes(inverse_local[v_co, v_p, v_vh, v_vw])
                                        T.block_attr({"schedule_rule": "conv2d_nchw_winograd_inverse"})
                                        inverse_local[v_co, v_p, v_vh, v_vw] = inverse_local[v_co, v_p, v_vh, v_vw] + bgemm[v_r_a, v_r_b, v_co, v_p] * T.Select(v_r_a % T.int64(4) == T.int64(3) and v_vh % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_vh % T.int64(2) == T.int64(0), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_vh % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_vh % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_vh % T.int64(2) == T.int64(1), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_vh % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_vh % T.int64(2) == T.int64(1), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_vh % T.int64(2) == T.int64(0), T.float32(1), T.float32(0))))))))) * T.Select(v_r_b % T.int64(4) == T.int64(3) and v_vw % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_vw % T.int64(2) == T.int64(0), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_vw % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_vw % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_vw % T.int64(2) == T.int64(1), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_vw % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_vw % T.int64(2) == T.int64(1), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_vw % T.int64(2) == T.int64(0), T.float32(1), T.float32(0)))))))))
                for h_1, w_1 in T.grid(T.int64(2), T.int64(2)):
                    with T.block("conv2d_winograd"):
                        v_n = T.axis.spatial(T.int64(1), T.int64(0))
                        v_co = T.axis.spatial(T.int64(128), (n_co_h_0_w_0_fused_0 * T.int64(64) + n_co_h_0_w_0_fused_1) // T.int64(196))
                        v_h = T.axis.spatial(T.int64(28), (n_co_h_0_w_0_fused_0 * T.int64(64) + n_co_h_0_w_0_fused_1) % T.int64(196) // T.int64(14) * T.int64(2) + h_1)
                        v_w = T.axis.spatial(T.int64(28), (n_co_h_0_w_0_fused_0 * T.int64(64) + n_co_h_0_w_0_fused_1) % T.int64(14) * T.int64(2) + w_1)
                        T.reads(inverse_local[v_co, v_n * T.int64(196) + v_h // T.int64(2) * T.int64(14) + v_w // T.int64(2), v_h % T.int64(2), v_w % T.int64(2)], p2[v_n, v_co, T.int64(0), T.int64(0)])
                        T.writes(T_relu[v_n, v_co, v_h, v_w])
                        T_relu[v_n, v_co, v_h, v_w] = T.max(inverse_local[v_co, v_n * T.int64(196) + v_h // T.int64(2) * T.int64(14) + v_w // T.int64(2), v_h % T.int64(2), v_w % T.int64(2)] + p2[v_n, v_co, T.int64(0), T.int64(0)], T.float32(0))
b0 = sch.get_block(name="data_pack", func_name="main")
b1 = sch.get_block(name="bgemm", func_name="main")
b2 = sch.get_block(name="inverse", func_name="main")
b3 = sch.get_block(name="conv2d_winograd", func_name="main")
b4 = sch.get_block(name="T_add", func_name="main")
b5 = sch.get_block(name="T_relu", func_name="main")
b6 = sch.get_block(name="root", func_name="main")
b7, b8 = sch.get_producers(block=b2)
sch.compute_inline(block=b8)
b9, = sch.get_consumers(block=b2)
l10, l11, l12, l13 = sch.get_loops(block=b9)
l14, l15 = sch.split(loop=l12, factors=[None, 2], preserve_unit_iters=True)
l16, l17 = sch.split(loop=l13, factors=[None, 2], preserve_unit_iters=True)
sch.reorder(l14, l16, l15, l17)
sch.compute_at(block=b2, loop=l16, preserve_unit_loops=True, index=-1)
sch.set_scope(block=b2, buffer_index=0, storage_scope="local")
l18, l19, l20, l21, l22, l23, l24, l25, l26, l27 = sch.get_loops(block=b2)
sch.unroll(loop=l24)
sch.unroll(loop=l25)
sch.unroll(loop=l26)
sch.unroll(loop=l27)
b28, b29 = sch.get_producers(block=b0)
sch.compute_inline(block=b29)
b30, = sch.get_producers(block=b28)
l31, l32, l33, l34, l35, l36 = sch.get_loops(block=b0)
sch.reorder(l33, l34, l31, l32, l35, l36)
sch.unroll(loop=l31)
sch.unroll(loop=l32)
sch.unroll(loop=l35)
sch.unroll(loop=l36)
l37 = sch.fuse(l33, l34, preserve_unit_iters=True)
v38 = sch.sample_categorical(candidates=[32, 64, 128, 256, 512, 1024], probs=[0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666], decision=1)
l39, l40 = sch.split(loop=l37, factors=[None, v38], preserve_unit_iters=True)
sch.bind(loop=l39, thread_axis="blockIdx.x")
sch.bind(loop=l40, thread_axis="threadIdx.x")
b41 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b41, loop=l40, preserve_unit_loops=True, index=-1)
sch.compute_at(block=b28, loop=l40, preserve_unit_loops=True, index=-1)
sch.set_scope(block=b28, buffer_index=0, storage_scope="local")
sch.compute_inline(block=b30)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l42, l43, l44, l45, l46 = sch.get_loops(block=b1)
v47, v48, v49, v50, v51 = sch.sample_perfect_tile(loop=l42, n=5, max_innermost_factor=64, decision=[4, 1, 1, 1, 1])
l52, l53, l54, l55, l56 = sch.split(loop=l42, factors=[v47, v48, v49, v50, v51], preserve_unit_iters=True)
v57, v58, v59, v60, v61 = sch.sample_perfect_tile(loop=l43, n=5, max_innermost_factor=64, decision=[4, 1, 1, 1, 1])
l62, l63, l64, l65, l66 = sch.split(loop=l43, factors=[v57, v58, v59, v60, v61], preserve_unit_iters=True)
v67, v68, v69, v70, v71 = sch.sample_perfect_tile(loop=l44, n=5, max_innermost_factor=64, decision=[4, 1, 8, 2, 2])
l72, l73, l74, l75, l76 = sch.split(loop=l44, factors=[v67, v68, v69, v70, v71], preserve_unit_iters=True)
v77, v78, v79, v80, v81 = sch.sample_perfect_tile(loop=l45, n=5, max_innermost_factor=64, decision=[7, 2, 14, 1, 1])
l82, l83, l84, l85, l86 = sch.split(loop=l45, factors=[v77, v78, v79, v80, v81], preserve_unit_iters=True)
v87, v88, v89 = sch.sample_perfect_tile(loop=l46, n=3, max_innermost_factor=64, decision=[2, 4, 16])
l90, l91, l92 = sch.split(loop=l46, factors=[v87, v88, v89], preserve_unit_iters=True)
sch.reorder(l52, l62, l72, l82, l53, l63, l73, l83, l54, l64, l74, l84, l90, l91, l55, l65, l75, l85, l92, l56, l66, l76, l86)
l93 = sch.fuse(l52, l62, l72, l82, preserve_unit_iters=True)
sch.bind(loop=l93, thread_axis="blockIdx.x")
l94 = sch.fuse(l53, l63, l73, l83, preserve_unit_iters=True)
sch.bind(loop=l94, thread_axis="vthread.x")
l95 = sch.fuse(l54, l64, l74, l84, preserve_unit_iters=True)
sch.bind(loop=l95, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b96 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b96, loop=l95, preserve_unit_loops=True, index=-1)
b97 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b97, loop=l90, preserve_unit_loops=True, index=-1)
l98, l99, l100, l101, l102, l103, l104, l105 = sch.get_loops(block=b97)
l106 = sch.fuse(l102, l103, l104, l105, preserve_unit_iters=True)
v107 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b97, ann_key="meta_schedule.cooperative_fetch", ann_val=v107)
b108 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b108, loop=l90, preserve_unit_loops=True, index=-1)
l109, l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b108)
l117 = sch.fuse(l113, l114, l115, l116, preserve_unit_iters=True)
v118 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b108, ann_key="meta_schedule.cooperative_fetch", ann_val=v118)
l119 = sch.fuse(l90, preserve_unit_iters=True)
sch.annotate(block_or_loop=l119, ann_key="software_pipeline_stage", ann_val=[0, 0, 2])
sch.annotate(block_or_loop=l119, ann_key="software_pipeline_order", ann_val=[0, 1, 2])
sch.annotate(block_or_loop=l119, ann_key="software_pipeline_async_stages", ann_val=[0])
sch.reverse_compute_inline(block=b5)
sch.reverse_compute_inline(block=b4)
v120 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b6, ann_key="meta_schedule.unroll_explicit", ann_val=v120)
l121, l122, l123, l124, l125, l126 = sch.get_loops(block=b3)
l127 = sch.fuse(l121, l122, l123, l124, preserve_unit_iters=True)
v128 = sch.sample_categorical(candidates=[32, 64, 128, 256, 512, 1024], probs=[0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666], decision=1)
l129, l130 = sch.split(loop=l127, factors=[None, v128], preserve_unit_iters=True)
sch.bind(loop=l129, thread_axis="blockIdx.x")
sch.bind(loop=l130, thread_axis="threadIdx.x")
sch.enter_postproc()
sch.unannotate(block_or_loop=b97, ann_key="meta_schedule.cooperative_fetch")
l131, l132, l133, l134, l135 = sch.get_loops(block=b97)
l136, l137 = sch.split(loop=l135, factors=[None, 112], preserve_unit_iters=True)
sch.bind(loop=l137, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b108, ann_key="meta_schedule.cooperative_fetch")
l138, l139, l140, l141, l142 = sch.get_loops(block=b108)
l143, l144, l145 = sch.split(loop=l142, factors=[None, 112, 2], preserve_unit_iters=True)
sch.vectorize(loop=l145)
sch.bind(loop=l144, thread_axis="threadIdx.x")
b146 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b146, ann_key="meta_schedule.unroll_explicit")
b147, b148, b149, b150, b151, b152, b153, b154, b155 = sch.get_child_blocks(b146)
l156, l157, l158, l159, l160, l161 = sch.get_loops(block=b147)
l162, l163, l164, l165, l166, l167 = sch.get_loops(block=b148)
sch.annotate(block_or_loop=l162, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l162, ann_key="pragma_unroll_explicit", ann_val=1)
l168, l169, l170, l171, l172, l173 = sch.get_loops(block=b149)
l174, l175, l176, l177, l178, l179 = sch.get_loops(block=b150)
l180, l181, l182, l183, l184, l185, l186 = sch.get_loops(block=b151)
l187, l188, l189, l190, l191, l192, l193, l194, l195, l196, l197, l198, l199, l200 = sch.get_loops(block=b152)
sch.annotate(block_or_loop=l187, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l187, ann_key="pragma_unroll_explicit", ann_val=1)
l201, l202, l203, l204, l205, l206, l207 = sch.get_loops(block=b153)
l208, l209, l210, l211, l212, l213, l214, l215 = sch.get_loops(block=b154)
sch.annotate(block_or_loop=l208, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l208, ann_key="pragma_unroll_explicit", ann_val=1)
l216, l217, l218, l219 = sch.get_loops(block=b155)
b220 = sch.get_block(name="data_pack", func_name="main")
l221, l222, l223, l224, l225, l226 = sch.get_loops(block=b220)
b227 = sch.decompose_reduction(block=b220, loop=l225)
b228 = sch.get_block(name="bgemm", func_name="main")
l229, l230, l231, l232, l233, l234, l235, l236, l237, l238, l239, l240, l241, l242 = sch.get_loops(block=b228)
b243 = sch.decompose_reduction(block=b228, loop=l232)
b244 = sch.get_block(name="inverse", func_name="main")
l245, l246, l247, l248, l249, l250, l251, l252 = sch.get_loops(block=b244)
b253 = sch.decompose_reduction(block=b244, loop=l251)
2023-11-11 07:30:18 [INFO] [task_scheduler.cc:121] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #517: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/home2/xiachunwei/Software/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/home2/xiachunwei/Software/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/home2/xiachunwei/Software/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/home2/xiachunwei/Software/tvm/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/home2/xiachunwei/Software/tvm/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/home2/xiachunwei/Software/tvm/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/home2/xiachunwei/Software/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/home2/xiachunwei/Software/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  4: _ZN3tvm7runtime13PackedFun
  3: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  2: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  0: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  File "/home2/xiachunwei/Software/tvm/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(128), T.int64(28), T.int64(28)), "float32"), p1: T.Buffer((T.int64(4), T.int64(4), T.int64(128), T.int64(128)), "float32"), p2: T.Buffer((T.int64(1), T.int64(128), T.int64(1), T.int64(1)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(128), T.int64(28), T.int64(28)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        input_tile_local = T.alloc_buffer((T.int64(128), T.int64(196), T.int64(4), T.int64(4)), scope="local")
        data_pack = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(128), T.int64(196)))
        bgemm = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(128), T.int64(196)))
        inverse_local = T.alloc_buffer((T.int64(128), T.int64(196), T.int64(2), T.int64(2)), scope="local")
        data_pack_local = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(128), T.int64(196)), scope="local")
        bgemm_local = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(128), T.int64(196)), scope="local")
        data_pack_shared = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(128), T.int64(196)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(128), T.int64(128)), scope="shared")
        for ci_p_fused_0 in T.thread_binding(T.int64(392), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 512, "pragma_unroll_explicit": 1}):
            for ci_p_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(1), T.int64(4), T.int64(4)):
                    with T.block("input_tile"):
                        v_ci = T.axis.spatial(T.int64(128), (ci_p_fused_0 * T.int64(64) + ci_p_fused_1) // T.int64(196) + ax0)
                        v_p = T.axis.spatial(T.int64(196), (ci_p_fused_0 * T.int64(64) + ci_p_fused_1) % T.int64(196) + ax1)
                        v_eps, v_nu = T.axis.remap("SS", [ax2, ax3])
                        T.reads(p0[v_p // T.int64(196), v_ci, v_p % T.int64(196) // T.int64(14) * T.int64(2) + v_eps - T.int64(1), v_p % T.int64(14) * T.int64(2) + v_nu - T.int64(1)])
                        T.writes(input_tile_local[v_ci, v_p, v_eps, v_nu])
                        T.block_attr({"schedule_rule": "None"})
                        input_tile_local[v_ci, v_p, v_eps, v_nu] = T.if_then_else(T.int64(1) <= v_p % T.int64(196) // T.int64(14) * T.int64(2) + v_eps and v_p % T.int64(196) // T.int64(14) * T.int64(2) + v_eps < T.int64(29) and T.int64(1) <= v_p % T.int64(14) * T.int64(2) + v_nu and v_p % T.int64(14) * T.int64(2) + v_nu < T.int64(29), p0[v_p // T.int64(196), v_ci, v_p % T.int64(196) // T.int64(14) * T.int64(2) + v_eps - T.int64(1), v_p % T.int64(14) * T.int64(2) + v_nu - T.int64(1)], T.float32(0))
                for eps in T.unroll(T.int64(4)):
                    for nu in T.unroll(T.int64(4)):
                        with T.block("data_pack_init"):
                            v_eps, v_nu = T.axis.remap("SS", [eps, nu])
                            v_ci = T.axis.spatial(T.int64(128), (ci_p_fused_0 * T.int64(64) + ci_p_fused_1) // T.int64(196))
                            v_p = T.axis.spatial(T.int64(196), (ci_p_fused_0 * T.int64(64) + ci_p_fused_1) % T.int64(196))
                            T.reads()
                            T.writes(data_pack_local[v_eps, v_nu, v_ci, v_p])
                            T.block_attr({"schedule_rule": "conv2d_nchw_winograd_data_pack"})
                            data_pack_local[v_eps, v_nu, v_ci, v_p] = T.float32(0)
                        for r_a in T.unroll(T.int64(4)):
                            for r_b in T.unroll(T.int64(4)):
                                with T.block("data_pack_update"):
                                    v_eps, v_nu = T.axis.remap("SS", [eps, nu])
                                    v_ci = T.axis.spatial(T.int64(128), (ci_p_fused_0 * T.int64(64) + ci_p_fused_1) // T.int64(196))
                                    v_p = T.axis.spatial(T.int64(196), (ci_p_fused_0 * T.int64(64) + ci_p_fused_1) % T.int64(196))
                                    v_r_a, v_r_b = T.axis.remap("RR", [r_a, r_b])
                                    T.reads(data_pack_local[v_eps, v_nu, v_ci, v_p], input_tile_local[v_ci, v_p, v_r_a, v_r_b])
                                    T.writes(data_pack_local[v_eps, v_nu, v_ci, v_p])
                                    T.block_attr({"schedule_rule": "conv2d_nchw_winograd_data_pack"})
                                    data_pack_local[v_eps, v_nu, v_ci, v_p] = data_pack_local[v_eps, v_nu, v_ci, v_p] + input_tile_local[v_ci, v_p, v_r_a, v_r_b] * T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(3), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(1), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(0), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(3), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(1), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(0), T.float32(1), T.float32(0))))))))))))))))) * T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(3), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(1), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(0), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(3), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(1), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(0), T.float32(1), T.float32(0)))))))))))))))))
                for ax0, ax1, ax2, ax3 in T.grid(T.int64(4), T.int64(4), T.int64(1), T.int64(1)):
                    with T.block("data_pack_local"):
                        v0, v1 = T.axis.remap("SS", [ax0, ax1])
                        v2 = T.axis.spatial(T.int64(128), (ci_p_fused_0 * T.int64(64) + ci_p_fused_1) // T.int64(196) + ax2)
                        v3 = T.axis.spatial(T.int64(196), (ci_p_fused_0 * T.int64(64) + ci_p_fused_1) % T.int64(196) + ax3)
                        T.reads(data_pack_local[v0, v1, v2, v3])
                        T.writes(data_pack[v0, v1, v2, v3])
                        data_pack[v0, v1, v2, v3] = data_pack_local[v0, v1, v2, v3]
        for eps_0_nu_0_co_0_p_0_fused in T.thread_binding(T.int64(448), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 512, "pragma_unroll_explicit": 1}):
            for eps_1_nu_1_co_1_p_1_fused in T.thread_binding(T.int64(2), thread="vthread.x"):
                for eps_2_nu_2_co_2_p_2_fused in T.thread_binding(T.int64(112), thread="threadIdx.x"):
                    for eps_3_init, nu_3_init, co_3_init, p_3_init, eps_4_init, nu_4_init, co_4_init, p_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(4), T.int64(1)):
                        with T.block("bgemm_init"):
                            v_eps = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(112) + eps_3_init + eps_4_init)
                            v_nu = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused % T.int64(112) // T.int64(28) + nu_3_init + nu_4_init)
                            v_co = T.axis.spatial(T.int64(128), eps_0_nu_0_co_0_p_0_fused % T.int64(28) // T.int64(7) * T.int64(32) + eps_2_nu_2_co_2_p_2_fused // T.int64(14) * T.int64(4) + co_3_init * T.int64(4) + co_4_init)
                            v_p = T.axis.spatial(T.int64(196), eps_0_nu_0_co_0_p_0_fused % T.int64(7) * T.int64(28) + eps_1_nu_1_co_1_p_1_fused * T.int64(14) + eps_2_nu_2_co_2_p_2_fused % T.int64(14) + p_3_init + p_4_init)
                            T.reads()
                            T.writes(bgemm_local[v_eps, v_nu, v_co, v_p])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            bgemm_local[v_eps, v_nu, v_co, v_p] = T.float32(0)
                    for ci_0_fused in T.serial(T.int64(2), annotations={"software_pipeline_async_stages": [0], "software_pipeline_order": [0, 1, 2], "software_pipeline_stage": [0, 0, 2]}):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(16)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(112), thread="threadIdx.x"):
                                with T.block("data_pack_shared"):
                                    v0 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(112))
                                    v1 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused % T.int64(112) // T.int64(28))
                                    v2 = T.axis.spatial(T.int64(128), ci_0_fused * T.int64(64) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(112) + ax0_ax1_ax2_ax3_fused_1) // T.int64(28))
                                    v3 = T.axis.spatial(T.int64(196), eps_0_nu_0_co_0_p_0_fused % T.int64(7) * T.int64(28) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(112) + ax0_ax1_ax2_ax3_fused_1) % T.int64(28))
                                    T.reads(data_pack[v0, v1, v2, v3])
                                    T.writes(data_pack_shared[v0, v1, v2, v3])
                                    data_pack_shared[v0, v1, v2, v3] = data_pack[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(10)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(112), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("p1_shared"):
                                        v0 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(112))
                                        v1 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused % T.int64(112) // T.int64(28))
                                        v2 = T.axis.spatial(T.int64(128), ci_0_fused * T.int64(64) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(224) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) // T.int64(32))
                                        v3 = T.axis.spatial(T.int64(128), eps_0_nu_0_co_0_p_0_fused % T.int64(28) // T.int64(7) * T.int64(32) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(224) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) % T.int64(32))
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * T.int64(112) + ax0_ax1_ax2_ax3_fused_1) * T.int64(2) + ax0_ax1_ax2_ax3_fused_2 < T.int64(2048))
                                        T.reads(p1[v0, v1, v2, v3])
                                        T.writes(p1_shared[v0, v1, v2, v3])
                                        p1_shared[v0, v1, v2, v3] = p1[v0, v1, v2, v3]
                        for ci_1, eps_3, nu_3, co_3, p_3, ci_2, eps_4, nu_4, co_4, p_4 in T.grid(T.int64(8), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(8), T.int64(1), T.int64(1), T.int64(4), T.int64(1)):
                            with T.block("bgemm_update"):
                                v_eps = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(112) + eps_3 + eps_4)
                                v_nu = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused % T.int64(112) // T.int64(28) + nu_3 + nu_4)
                                v_co = T.axis.spatial(T.int64(128), eps_0_nu_0_co_0_p_0_fused % T.int64(28) // T.int64(7) * T.int64(32) + eps_2_nu_2_co_2_p_2_fused // T.int64(14) * T.int64(4) + co_3 * T.int64(4) + co_4)
                                v_p = T.axis.spatial(T.int64(196), eps_0_nu_0_co_0_p_0_fused % T.int64(7) * T.int64(28) + eps_1_nu_1_co_1_p_1_fused * T.int64(14) + eps_2_nu_2_co_2_p_2_fused % T.int64(14) + p_3 + p_4)
                                v_ci = T.axis.reduce(T.int64(128), ci_0_fused * T.int64(64) + ci_1 * T.int64(8) + ci_2)
                                T.reads(bgemm_local[v_eps, v_nu, v_co, v_p], data_pack_shared[v_eps, v_nu, v_ci, v_p], p1_shared[v_eps, v_nu, v_ci, v_co])
                                T.writes(bgemm_local[v_eps, v_nu, v_co, v_p])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                bgemm_local[v_eps, v_nu, v_co, v_p] = bgemm_local[v_eps, v_nu, v_co, v_p] + data_pack_shared[v_eps, v_nu, v_ci, v_p] * p1_shared[v_eps, v_nu, v_ci, v_co]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(1), T.int64(4), T.int64(1)):
                        with T.block("bgemm_local"):
                            v0 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(112) + ax0)
                            v1 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused % T.int64(112) // T.int64(28) + ax1)
                            v2 = T.axis.spatial(T.int64(128), eps_0_nu_0_co_0_p_0_fused % T.int64(28) // T.int64(7) * T.int64(32) + eps_2_nu_2_co_2_p_2_fused // T.int64(14) * T.int64(4) + ax2)
                            v3 = T.axis.spatial(T.int64(196), eps_0_nu_0_co_0_p_0_fused % T.int64(7) * T.int64(28) + eps_1_nu_1_co_1_p_1_fused * T.int64(14) + eps_2_nu_2_co_2_p_2_fused % T.int64(14) + ax3)
                            T.reads(bgemm_local[v0, v1, v2, v3])
                            T.writes(bgemm[v0, v1, v2, v3])
                            bgemm[v0, v1, v2, v3] = bgemm_local[v0, v1, v2, v3]
        for n_co_h_0_w_0_fused_0 in T.thread_binding(T.int64(196), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 512, "pragma_unroll_explicit": 1}):
            for n_co_h_0_w_0_fused_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                    for ax2 in T.unroll(T.int64(2)):
                        for ax3 in T.unroll(T.int64(2)):
                            with T.block("inverse_init"):
                                v_co = T.axis.spatial(T.int64(128), (n_co_h_0_w_0_fused_0 * T.int64(128) + n_co_h_0_w_0_fused_1) // T.int64(196) + ax0)
                                v_p = T.axis.spatial(T.int64(196), (n_co_h_0_w_0_fused_0 * T.int64(128) + n_co_h_0_w_0_fused_1) % T.int64(196) + ax1)
                                v_vh, v_vw = T.axis.remap("SS", [ax2, ax3])
                                T.reads()
                                T.writes(inverse_local[v_co, v_p, v_vh, v_vw])
                                T.block_attr({"schedule_rule": "conv2d_nchw_winograd_inverse"})
                                inverse_local[v_co, v_p, v_vh, v_vw] = T.float32(0)
                            for ax4 in T.unroll(T.int64(4)):
                                for ax5 in T.unroll(T.int64(4)):
                                    with T.block("inverse_update"):
                                        v_co = T.axis.spatial(T.int64(128), (n_co_h_0_w_0_fused_0 * T.int64(128) + n_co_h_0_w_0_fused_1) // T.int64(196) + ax0)
                                        v_p = T.axis.spatial(T.int64(196), (n_co_h_0_w_0_fused_0 * T.int64(128) + n_co_h_0_w_0_fused_1) % T.int64(196) + ax1)
                                        v_vh, v_vw, v_r_a, v_r_b = T.axis.remap("SSRR", [ax2, ax3, ax4, ax5])
                                        T.reads(inverse_local[v_co, v_p, v_vh, v_vw], bgemm[v_r_a, v_r_b, v_co, v_p])
                                        T.writes(inverse_local[v_co, v_p, v_vh, v_vw])
                                        T.block_attr({"schedule_rule": "conv2d_nchw_winograd_inverse"})
                                        inverse_local[v_co, v_p, v_vh, v_vw] = inverse_local[v_co, v_p, v_vh, v_vw] + bgemm[v_r_a, v_r_b, v_co, v_p] * T.Select(v_r_a % T.int64(4) == T.int64(3) and v_vh % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_vh % T.int64(2) == T.int64(0), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_vh % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_vh % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_vh % T.int64(2) == T.int64(1), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_vh % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_vh % T.int64(2) == T.int64(1), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_vh % T.int64(2) == T.int64(0), T.float32(1), T.float32(0))))))))) * T.Select(v_r_b % T.int64(4) == T.int64(3) and v_vw % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_vw % T.int64(2) == T.int64(0), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_vw % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_vw % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_vw % T.int64(2) == T.int64(1), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_vw % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_vw % T.int64(2) == T.int64(1), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_vw % T.int64(2) == T.int64(0), T.float32(1), T.float32(0)))))))))
                for h_1, w_1 in T.grid(T.int64(2), T.int64(2)):
                    with T.block("conv2d_winograd"):
                        v_n = T.axis.spatial(T.int64(1), T.int64(0))
                        v_co = T.axis.spatial(T.int64(128), (n_co_h_0_w_0_fused_0 * T.int64(128) + n_co_h_0_w_0_fused_1) // T.int64(196))
                        v_h = T.axis.spatial(T.int64(28), (n_co_h_0_w_0_fused_0 * T.int64(128) + n_co_h_0_w_0_fused_1) % T.int64(196) // T.int64(14) * T.int64(2) + h_1)
                        v_w = T.axis.spatial(T.int64(28), (n_co_h_0_w_0_fused_0 * T.int64(128) + n_co_h_0_w_0_fused_1) % T.int64(14) * T.int64(2) + w_1)
                        T.reads(inverse_local[v_co, v_n * T.int64(196) + v_h // T.int64(2) * T.int64(14) + v_w // T.int64(2), v_h % T.int64(2), v_w % T.int64(2)], p2[v_n, v_co, T.int64(0), T.int64(0)])
                        T.writes(T_relu[v_n, v_co, v_h, v_w])
                        T_relu[v_n, v_co, v_h, v_w] = T.max(inverse_local[v_co, v_n * T.int64(196) + v_h // T.int64(2) * T.int64(14) + v_w // T.int64(2), v_h % T.int64(2), v_w % T.int64(2)] + p2[v_n, v_co, T.int64(0), T.int64(0)], T.float32(0))
b0 = sch.get_block(name="data_pack", func_name="main")
b1 = sch.get_block(name="bgemm", func_name="main")
b2 = sch.get_block(name="inverse", func_name="main")
b3 = sch.get_block(name="conv2d_winograd", func_name="main")
b4 = sch.get_block(name="T_add", func_name="main")
b5 = sch.get_block(name="T_relu", func_name="main")
b6 = sch.get_block(name="root", func_name="main")
b7, b8 = sch.get_producers(block=b2)
sch.compute_inline(block=b8)
b9, = sch.get_consumers(block=b2)
l10, l11, l12, l13 = sch.get_loops(block=b9)
l14, l15 = sch.split(loop=l12, factors=[None, 2], preserve_unit_iters=True)
l16, l17 = sch.split(loop=l13, factors=[None, 2], preserve_unit_iters=True)
sch.reorder(l14, l16, l15, l17)
sch.compute_at(block=b2, loop=l16, preserve_unit_loops=True, index=-1)
sch.set_scope(block=b2, buffer_index=0, storage_scope="local")
l18, l19, l20, l21, l22, l23, l24, l25, l26, l27 = sch.get_loops(block=b2)
sch.unroll(loop=l24)
sch.unroll(loop=l25)
sch.unroll(loop=l26)
sch.unroll(loop=l27)
b28, b29 = sch.get_producers(block=b0)
sch.compute_inline(block=b29)
b30, = sch.get_producers(block=b28)
l31, l32, l33, l34, l35, l36 = sch.get_loops(block=b0)
sch.reorder(l33, l34, l31, l32, l35, l36)
sch.unroll(loop=l31)
sch.unroll(loop=l32)
sch.unroll(loop=l35)
sch.unroll(loop=l36)
l37 = sch.fuse(l33, l34, preserve_unit_iters=True)
v38 = sch.sample_categorical(candidates=[32, 64, 128, 256, 512, 1024], probs=[0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666], decision=1)
l39, l40 = sch.split(loop=l37, factors=[None, v38], preserve_unit_iters=True)
sch.bind(loop=l39, thread_axis="blockIdx.x")
sch.bind(loop=l40, thread_axis="threadIdx.x")
b41 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b41, loop=l40, preserve_unit_loops=True, index=-1)
sch.compute_at(block=b28, loop=l40, preserve_unit_loops=True, index=-1)
sch.set_scope(block=b28, buffer_index=0, storage_scope="local")
sch.compute_inline(block=b30)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l42, l43, l44, l45, l46 = sch.get_loops(block=b1)
v47, v48, v49, v50, v51 = sch.sample_perfect_tile(loop=l42, n=5, max_innermost_factor=64, decision=[4, 1, 1, 1, 1])
l52, l53, l54, l55, l56 = sch.split(loop=l42, factors=[v47, v48, v49, v50, v51], preserve_unit_iters=True)
v57, v58, v59, v60, v61 = sch.sample_perfect_tile(loop=l43, n=5, max_innermost_factor=64, decision=[4, 1, 1, 1, 1])
l62, l63, l64, l65, l66 = sch.split(loop=l43, factors=[v57, v58, v59, v60, v61], preserve_unit_iters=True)
v67, v68, v69, v70, v71 = sch.sample_perfect_tile(loop=l44, n=5, max_innermost_factor=64, decision=[4, 1, 8, 1, 4])
l72, l73, l74, l75, l76 = sch.split(loop=l44, factors=[v67, v68, v69, v70, v71], preserve_unit_iters=True)
v77, v78, v79, v80, v81 = sch.sample_perfect_tile(loop=l45, n=5, max_innermost_factor=64, decision=[7, 2, 14, 1, 1])
l82, l83, l84, l85, l86 = sch.split(loop=l45, factors=[v77, v78, v79, v80, v81], preserve_unit_iters=True)
v87, v88, v89 = sch.sample_perfect_tile(loop=l46, n=3, max_innermost_factor=64, decision=[2, 8, 8])
l90, l91, l92 = sch.split(loop=l46, factors=[v87, v88, v89], preserve_unit_iters=True)
sch.reorder(l52, l62, l72, l82, l53, l63, l73, l83, l54, l64, l74, l84, l90, l91, l55, l65, l75, l85, l92, l56, l66, l76, l86)
l93 = sch.fuse(l52, l62, l72, l82, preserve_unit_iters=True)
sch.bind(loop=l93, thread_axis="blockIdx.x")
l94 = sch.fuse(l53, l63, l73, l83, preserve_unit_iters=True)
sch.bind(loop=l94, thread_axis="vthread.x")
l95 = sch.fuse(l54, l64, l74, l84, preserve_unit_iters=True)
sch.bind(loop=l95, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b96 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b96, loop=l95, preserve_unit_loops=True, index=-1)
b97 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b97, loop=l90, preserve_unit_loops=True, index=-1)
l98, l99, l100, l101, l102, l103, l104, l105 = sch.get_loops(block=b97)
l106 = sch.fuse(l102, l103, l104, l105, preserve_unit_iters=True)
v107 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b97, ann_key="meta_schedule.cooperative_fetch", ann_val=v107)
b108 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b108, loop=l90, preserve_unit_loops=True, index=-1)
l109, l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b108)
l117 = sch.fuse(l113, l114, l115, l116, preserve_unit_iters=True)
v118 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b108, ann_key="meta_schedule.cooperative_fetch", ann_val=v118)
l119 = sch.fuse(l90, preserve_unit_iters=True)
sch.annotate(block_or_loop=l119, ann_key="software_pipeline_stage", ann_val=[0, 0, 2])
sch.annotate(block_or_loop=l119, ann_key="software_pipeline_order", ann_val=[0, 1, 2])
sch.annotate(block_or_loop=l119, ann_key="software_pipeline_async_stages", ann_val=[0])
sch.reverse_compute_inline(block=b5)
sch.reverse_compute_inline(block=b4)
v120 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b6, ann_key="meta_schedule.unroll_explicit", ann_val=v120)
l121, l122, l123, l124, l125, l126 = sch.get_loops(block=b3)
l127 = sch.fuse(l121, l122, l123, l124, preserve_unit_iters=True)
v128 = sch.sample_categorical(candidates=[32, 64, 128, 256, 512, 1024], probs=[0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666], decision=2)
l129, l130 = sch.split(loop=l127, factors=[None, v128], preserve_unit_iters=True)
sch.bind(loop=l129, thread_axis="blockIdx.x")
sch.bind(loop=l130, thread_axis="threadIdx.x")
sch.enter_postproc()
sch.unannotate(block_or_loop=b97, ann_key="meta_schedule.cooperative_fetch")
l131, l132, l133, l134, l135 = sch.get_loops(block=b97)
l136, l137 = sch.split(loop=l135, factors=[None, 112], preserve_unit_iters=True)
sch.bind(loop=l137, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b108, ann_key="meta_schedule.cooperative_fetch")
l138, l139, l140, l141, l142 = sch.get_loops(block=b108)
l143, l144, l145 = sch.split(loop=l142, factors=[None, 112, 2], preserve_unit_iters=True)
sch.vectorize(loop=l145)
sch.bind(loop=l144, thread_axis="threadIdx.x")
b146 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b146, ann_key="meta_schedule.unroll_explicit")
b147, b148, b149, b150, b151, b152, b153, b154, b155 = sch.get_child_blocks(b146)
l156, l157, l158, l159, l160, l161 = sch.get_loops(block=b147)
l162, l163, l164, l165, l166, l167 = sch.get_loops(block=b148)
sch.annotate(block_or_loop=l162, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l162, ann_key="pragma_unroll_explicit", ann_val=1)
l168, l169, l170, l171, l172, l173 = sch.get_loops(block=b149)
l174, l175, l176, l177, l178, l179 = sch.get_loops(block=b150)
l180, l181, l182, l183, l184, l185, l186 = sch.get_loops(block=b151)
l187, l188, l189, l190, l191, l192, l193, l194, l195, l196, l197, l198, l199, l200 = sch.get_loops(block=b152)
sch.annotate(block_or_loop=l187, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l187, ann_key="pragma_unroll_explicit", ann_val=1)
l201, l202, l203, l204, l205, l206, l207 = sch.get_loops(block=b153)
l208, l209, l210, l211, l212, l213, l214, l215 = sch.get_loops(block=b154)
sch.annotate(block_or_loop=l208, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l208, ann_key="pragma_unroll_explicit", ann_val=1)
l216, l217, l218, l219 = sch.get_loops(block=b155)
b220 = sch.get_block(name="data_pack", func_name="main")
l221, l222, l223, l224, l225, l226 = sch.get_loops(block=b220)
b227 = sch.decompose_reduction(block=b220, loop=l225)
b228 = sch.get_block(name="bgemm", func_name="main")
l229, l230, l231, l232, l233, l234, l235, l236, l237, l238, l239, l240, l241, l242 = sch.get_loops(block=b228)
b243 = sch.decompose_reduction(block=b228, loop=l232)
b244 = sch.get_block(name="inverse", func_name="main")
l245, l246, l247, l248, l249, l250, l251, l252 = sch.get_loops(block=b244)
b253 = sch.decompose_reduction(block=b244, loop=l251)
2023-11-11 07:30:18 [INFO] [task_scheduler.cc:121] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #518: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/home2/xiachunwei/Software/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/home2/xiachunwei/Software/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/home2/xiachunwei/Software/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/home2/xiachunwei/Software/tvm/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/home2/xiachunwei/Software/tvm/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/home2/xiachunwei/Software/tvm/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/home2/xiachunwei/Software/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/home2/xiachunwei/Software/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  4: _ZN3tvm7runtime13PackedFun
  3: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  2: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  0: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  File "/home2/xiachunwei/Software/tvm/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(128), T.int64(28), T.int64(28)), "float32"), p1: T.Buffer((T.int64(4), T.int64(4), T.int64(128), T.int64(128)), "float32"), p2: T.Buffer((T.int64(1), T.int64(128), T.int64(1), T.int64(1)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(128), T.int64(28), T.int64(28)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        input_tile_local = T.alloc_buffer((T.int64(128), T.int64(196), T.int64(4), T.int64(4)), scope="local")
        data_pack = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(128), T.int64(196)))
        bgemm = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(128), T.int64(196)))
        inverse_local = T.alloc_buffer((T.int64(128), T.int64(196), T.int64(2), T.int64(2)), scope="local")
        data_pack_local = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(128), T.int64(196)), scope="local")
        bgemm_local = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(128), T.int64(196)), scope="local")
        data_pack_shared = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(128), T.int64(196)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(128), T.int64(128)), scope="shared")
        for ci_p_fused_0 in T.thread_binding(T.int64(392), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 1024, "pragma_unroll_explicit": 1}):
            for ci_p_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(1), T.int64(4), T.int64(4)):
                    with T.block("input_tile"):
                        v_ci = T.axis.spatial(T.int64(128), (ci_p_fused_0 * T.int64(64) + ci_p_fused_1) // T.int64(196) + ax0)
                        v_p = T.axis.spatial(T.int64(196), (ci_p_fused_0 * T.int64(64) + ci_p_fused_1) % T.int64(196) + ax1)
                        v_eps, v_nu = T.axis.remap("SS", [ax2, ax3])
                        T.reads(p0[v_p // T.int64(196), v_ci, v_p % T.int64(196) // T.int64(14) * T.int64(2) + v_eps - T.int64(1), v_p % T.int64(14) * T.int64(2) + v_nu - T.int64(1)])
                        T.writes(input_tile_local[v_ci, v_p, v_eps, v_nu])
                        T.block_attr({"schedule_rule": "None"})
                        input_tile_local[v_ci, v_p, v_eps, v_nu] = T.if_then_else(T.int64(1) <= v_p % T.int64(196) // T.int64(14) * T.int64(2) + v_eps and v_p % T.int64(196) // T.int64(14) * T.int64(2) + v_eps < T.int64(29) and T.int64(1) <= v_p % T.int64(14) * T.int64(2) + v_nu and v_p % T.int64(14) * T.int64(2) + v_nu < T.int64(29), p0[v_p // T.int64(196), v_ci, v_p % T.int64(196) // T.int64(14) * T.int64(2) + v_eps - T.int64(1), v_p % T.int64(14) * T.int64(2) + v_nu - T.int64(1)], T.float32(0))
                for eps in T.unroll(T.int64(4)):
                    for nu in T.unroll(T.int64(4)):
                        with T.block("data_pack_init"):
                            v_eps, v_nu = T.axis.remap("SS", [eps, nu])
                            v_ci = T.axis.spatial(T.int64(128), (ci_p_fused_0 * T.int64(64) + ci_p_fused_1) // T.int64(196))
                            v_p = T.axis.spatial(T.int64(196), (ci_p_fused_0 * T.int64(64) + ci_p_fused_1) % T.int64(196))
                            T.reads()
                            T.writes(data_pack_local[v_eps, v_nu, v_ci, v_p])
                            T.block_attr({"schedule_rule": "conv2d_nchw_winograd_data_pack"})
                            data_pack_local[v_eps, v_nu, v_ci, v_p] = T.float32(0)
                        for r_a in T.unroll(T.int64(4)):
                            for r_b in T.unroll(T.int64(4)):
                                with T.block("data_pack_update"):
                                    v_eps, v_nu = T.axis.remap("SS", [eps, nu])
                                    v_ci = T.axis.spatial(T.int64(128), (ci_p_fused_0 * T.int64(64) + ci_p_fused_1) // T.int64(196))
                                    v_p = T.axis.spatial(T.int64(196), (ci_p_fused_0 * T.int64(64) + ci_p_fused_1) % T.int64(196))
                                    v_r_a, v_r_b = T.axis.remap("RR", [r_a, r_b])
                                    T.reads(data_pack_local[v_eps, v_nu, v_ci, v_p], input_tile_local[v_ci, v_p, v_r_a, v_r_b])
                                    T.writes(data_pack_local[v_eps, v_nu, v_ci, v_p])
                                    T.block_attr({"schedule_rule": "conv2d_nchw_winograd_data_pack"})
                                    data_pack_local[v_eps, v_nu, v_ci, v_p] = data_pack_local[v_eps, v_nu, v_ci, v_p] + input_tile_local[v_ci, v_p, v_r_a, v_r_b] * T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(3), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(1), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(0), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(3), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(1), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(0), T.float32(1), T.float32(0))))))))))))))))) * T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(3), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(1), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(0), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(3), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(1), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(0), T.float32(1), T.float32(0)))))))))))))))))
                for ax0, ax1, ax2, ax3 in T.grid(T.int64(4), T.int64(4), T.int64(1), T.int64(1)):
                    with T.block("data_pack_local"):
                        v0, v1 = T.axis.remap("SS", [ax0, ax1])
                        v2 = T.axis.spatial(T.int64(128), (ci_p_fused_0 * T.int64(64) + ci_p_fused_1) // T.int64(196) + ax2)
                        v3 = T.axis.spatial(T.int64(196), (ci_p_fused_0 * T.int64(64) + ci_p_fused_1) % T.int64(196) + ax3)
                        T.reads(data_pack_local[v0, v1, v2, v3])
                        T.writes(data_pack[v0, v1, v2, v3])
                        data_pack[v0, v1, v2, v3] = data_pack_local[v0, v1, v2, v3]
        for eps_0_nu_0_co_0_p_0_fused in T.thread_binding(T.int64(448), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 1024, "pragma_unroll_explicit": 1}):
            for eps_1_nu_1_co_1_p_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for eps_2_nu_2_co_2_p_2_fused in T.thread_binding(T.int64(112), thread="threadIdx.x"):
                    for eps_3_init, nu_3_init, co_3_init, p_3_init, eps_4_init, nu_4_init, co_4_init, p_4_init in T.grid(T.int64(1), T.int64(1), T.int64(4), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(2)):
                        with T.block("bgemm_init"):
                            v_eps = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(112) + eps_3_init + eps_4_init)
                            v_nu = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused % T.int64(112) // T.int64(28) + nu_3_init + nu_4_init)
                            v_co = T.axis.spatial(T.int64(128), eps_0_nu_0_co_0_p_0_fused % T.int64(28) // T.int64(7) * T.int64(32) + eps_2_nu_2_co_2_p_2_fused // T.int64(14) * T.int64(4) + co_3_init + co_4_init)
                            v_p = T.axis.spatial(T.int64(196), eps_0_nu_0_co_0_p_0_fused % T.int64(7) * T.int64(28) + eps_2_nu_2_co_2_p_2_fused % T.int64(14) * T.int64(2) + p_3_init * T.int64(2) + p_4_init)
                            T.reads()
                            T.writes(bgemm_local[v_eps, v_nu, v_co, v_p])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            bgemm_local[v_eps, v_nu, v_co, v_p] = T.float32(0)
                    for ci_0_fused in T.serial(T.int64(2), annotations={"software_pipeline_async_stages": [0], "software_pipeline_order": [0, 1, 2], "software_pipeline_stage": [0, 0, 2]}):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(16)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(112), thread="threadIdx.x"):
                                with T.block("data_pack_shared"):
                                    v0 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(112))
                                    v1 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused % T.int64(112) // T.int64(28))
                                    v2 = T.axis.spatial(T.int64(128), ci_0_fused * T.int64(64) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(112) + ax0_ax1_ax2_ax3_fused_1) // T.int64(28))
                                    v3 = T.axis.spatial(T.int64(196), eps_0_nu_0_co_0_p_0_fused % T.int64(7) * T.int64(28) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(112) + ax0_ax1_ax2_ax3_fused_1) % T.int64(28))
                                    T.reads(data_pack[v0, v1, v2, v3])
                                    T.writes(data_pack_shared[v0, v1, v2, v3])
                                    data_pack_shared[v0, v1, v2, v3] = data_pack[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(5)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(112), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("p1_shared"):
                                        v0 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(112))
                                        v1 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused % T.int64(112) // T.int64(28))
                                        v2 = T.axis.spatial(T.int64(128), ci_0_fused * T.int64(64) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(448) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) // T.int64(32))
                                        v3 = T.axis.spatial(T.int64(128), eps_0_nu_0_co_0_p_0_fused % T.int64(28) // T.int64(7) * T.int64(32) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(448) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) % T.int64(32))
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * T.int64(112) + ax0_ax1_ax2_ax3_fused_1) * T.int64(4) + ax0_ax1_ax2_ax3_fused_2 < T.int64(2048))
                                        T.reads(p1[v0, v1, v2, v3])
                                        T.writes(p1_shared[v0, v1, v2, v3])
                                        p1_shared[v0, v1, v2, v3] = p1[v0, v1, v2, v3]
                        for ci_1, eps_3, nu_3, co_3, p_3, ci_2, eps_4, nu_4, co_4, p_4 in T.grid(T.int64(64), T.int64(1), T.int64(1), T.int64(4), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(2)):
                            with T.block("bgemm_update"):
                                v_eps = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(112) + eps_3 + eps_4)
                                v_nu = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused % T.int64(112) // T.int64(28) + nu_3 + nu_4)
                                v_co = T.axis.spatial(T.int64(128), eps_0_nu_0_co_0_p_0_fused % T.int64(28) // T.int64(7) * T.int64(32) + eps_2_nu_2_co_2_p_2_fused // T.int64(14) * T.int64(4) + co_3 + co_4)
                                v_p = T.axis.spatial(T.int64(196), eps_0_nu_0_co_0_p_0_fused % T.int64(7) * T.int64(28) + eps_2_nu_2_co_2_p_2_fused % T.int64(14) * T.int64(2) + p_3 * T.int64(2) + p_4)
                                v_ci = T.axis.reduce(T.int64(128), ci_0_fused * T.int64(64) + ci_1 + ci_2)
                                T.reads(bgemm_local[v_eps, v_nu, v_co, v_p], data_pack_shared[v_eps, v_nu, v_ci, v_p], p1_shared[v_eps, v_nu, v_ci, v_co])
                                T.writes(bgemm_local[v_eps, v_nu, v_co, v_p])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                bgemm_local[v_eps, v_nu, v_co, v_p] = bgemm_local[v_eps, v_nu, v_co, v_p] + data_pack_shared[v_eps, v_nu, v_ci, v_p] * p1_shared[v_eps, v_nu, v_ci, v_co]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(1), T.int64(4), T.int64(2)):
                        with T.block("bgemm_local"):
                            v0 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(112) + ax0)
                            v1 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused % T.int64(112) // T.int64(28) + ax1)
                            v2 = T.axis.spatial(T.int64(128), eps_0_nu_0_co_0_p_0_fused % T.int64(28) // T.int64(7) * T.int64(32) + eps_2_nu_2_co_2_p_2_fused // T.int64(14) * T.int64(4) + ax2)
                            v3 = T.axis.spatial(T.int64(196), eps_0_nu_0_co_0_p_0_fused % T.int64(7) * T.int64(28) + eps_2_nu_2_co_2_p_2_fused % T.int64(14) * T.int64(2) + ax3)
                            T.reads(bgemm_local[v0, v1, v2, v3])
                            T.writes(bgemm[v0, v1, v2, v3])
                            bgemm[v0, v1, v2, v3] = bgemm_local[v0, v1, v2, v3]
        for n_co_h_0_w_0_fused_0 in T.thread_binding(T.int64(196), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 1024, "pragma_unroll_explicit": 1}):
            for n_co_h_0_w_0_fused_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                    for ax2 in T.unroll(T.int64(2)):
                        for ax3 in T.unroll(T.int64(2)):
                            with T.block("inverse_init"):
                                v_co = T.axis.spatial(T.int64(128), (n_co_h_0_w_0_fused_0 * T.int64(128) + n_co_h_0_w_0_fused_1) // T.int64(196) + ax0)
                                v_p = T.axis.spatial(T.int64(196), (n_co_h_0_w_0_fused_0 * T.int64(128) + n_co_h_0_w_0_fused_1) % T.int64(196) + ax1)
                                v_vh, v_vw = T.axis.remap("SS", [ax2, ax3])
                                T.reads()
                                T.writes(inverse_local[v_co, v_p, v_vh, v_vw])
                                T.block_attr({"schedule_rule": "conv2d_nchw_winograd_inverse"})
                                inverse_local[v_co, v_p, v_vh, v_vw] = T.float32(0)
                            for ax4 in T.unroll(T.int64(4)):
                                for ax5 in T.unroll(T.int64(4)):
                                    with T.block("inverse_update"):
                                        v_co = T.axis.spatial(T.int64(128), (n_co_h_0_w_0_fused_0 * T.int64(128) + n_co_h_0_w_0_fused_1) // T.int64(196) + ax0)
                                        v_p = T.axis.spatial(T.int64(196), (n_co_h_0_w_0_fused_0 * T.int64(128) + n_co_h_0_w_0_fused_1) % T.int64(196) + ax1)
                                        v_vh, v_vw, v_r_a, v_r_b = T.axis.remap("SSRR", [ax2, ax3, ax4, ax5])
                                        T.reads(inverse_local[v_co, v_p, v_vh, v_vw], bgemm[v_r_a, v_r_b, v_co, v_p])
                                        T.writes(inverse_local[v_co, v_p, v_vh, v_vw])
                                        T.block_attr({"schedule_rule": "conv2d_nchw_winograd_inverse"})
                                        inverse_local[v_co, v_p, v_vh, v_vw] = inverse_local[v_co, v_p, v_vh, v_vw] + bgemm[v_r_a, v_r_b, v_co, v_p] * T.Select(v_r_a % T.int64(4) == T.int64(3) and v_vh % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_vh % T.int64(2) == T.int64(0), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_vh % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_vh % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_vh % T.int64(2) == T.int64(1), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_vh % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_vh % T.int64(2) == T.int64(1), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_vh % T.int64(2) == T.int64(0), T.float32(1), T.float32(0))))))))) * T.Select(v_r_b % T.int64(4) == T.int64(3) and v_vw % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_vw % T.int64(2) == T.int64(0), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_vw % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_vw % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_vw % T.int64(2) == T.int64(1), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_vw % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_vw % T.int64(2) == T.int64(1), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_vw % T.int64(2) == T.int64(0), T.float32(1), T.float32(0)))))))))
                for h_1, w_1 in T.grid(T.int64(2), T.int64(2)):
                    with T.block("conv2d_winograd"):
                        v_n = T.axis.spatial(T.int64(1), T.int64(0))
                        v_co = T.axis.spatial(T.int64(128), (n_co_h_0_w_0_fused_0 * T.int64(128) + n_co_h_0_w_0_fused_1) // T.int64(196))
                        v_h = T.axis.spatial(T.int64(28), (n_co_h_0_w_0_fused_0 * T.int64(128) + n_co_h_0_w_0_fused_1) % T.int64(196) // T.int64(14) * T.int64(2) + h_1)
                        v_w = T.axis.spatial(T.int64(28), (n_co_h_0_w_0_fused_0 * T.int64(128) + n_co_h_0_w_0_fused_1) % T.int64(14) * T.int64(2) + w_1)
                        T.reads(inverse_local[v_co, v_n * T.int64(196) + v_h // T.int64(2) * T.int64(14) + v_w // T.int64(2), v_h % T.int64(2), v_w % T.int64(2)], p2[v_n, v_co, T.int64(0), T.int64(0)])
                        T.writes(T_relu[v_n, v_co, v_h, v_w])
                        T_relu[v_n, v_co, v_h, v_w] = T.max(inverse_local[v_co, v_n * T.int64(196) + v_h // T.int64(2) * T.int64(14) + v_w // T.int64(2), v_h % T.int64(2), v_w % T.int64(2)] + p2[v_n, v_co, T.int64(0), T.int64(0)], T.float32(0))
b0 = sch.get_block(name="data_pack", func_name="main")
b1 = sch.get_block(name="bgemm", func_name="main")
b2 = sch.get_block(name="inverse", func_name="main")
b3 = sch.get_block(name="conv2d_winograd", func_name="main")
b4 = sch.get_block(name="T_add", func_name="main")
b5 = sch.get_block(name="T_relu", func_name="main")
b6 = sch.get_block(name="root", func_name="main")
b7, b8 = sch.get_producers(block=b2)
sch.compute_inline(block=b8)
b9, = sch.get_consumers(block=b2)
l10, l11, l12, l13 = sch.get_loops(block=b9)
l14, l15 = sch.split(loop=l12, factors=[None, 2], preserve_unit_iters=True)
l16, l17 = sch.split(loop=l13, factors=[None, 2], preserve_unit_iters=True)
sch.reorder(l14, l16, l15, l17)
sch.compute_at(block=b2, loop=l16, preserve_unit_loops=True, index=-1)
sch.set_scope(block=b2, buffer_index=0, storage_scope="local")
l18, l19, l20, l21, l22, l23, l24, l25, l26, l27 = sch.get_loops(block=b2)
sch.unroll(loop=l24)
sch.unroll(loop=l25)
sch.unroll(loop=l26)
sch.unroll(loop=l27)
b28, b29 = sch.get_producers(block=b0)
sch.compute_inline(block=b29)
b30, = sch.get_producers(block=b28)
l31, l32, l33, l34, l35, l36 = sch.get_loops(block=b0)
sch.reorder(l33, l34, l31, l32, l35, l36)
sch.unroll(loop=l31)
sch.unroll(loop=l32)
sch.unroll(loop=l35)
sch.unroll(loop=l36)
l37 = sch.fuse(l33, l34, preserve_unit_iters=True)
v38 = sch.sample_categorical(candidates=[32, 64, 128, 256, 512, 1024], probs=[0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666], decision=1)
l39, l40 = sch.split(loop=l37, factors=[None, v38], preserve_unit_iters=True)
sch.bind(loop=l39, thread_axis="blockIdx.x")
sch.bind(loop=l40, thread_axis="threadIdx.x")
b41 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b41, loop=l40, preserve_unit_loops=True, index=-1)
sch.compute_at(block=b28, loop=l40, preserve_unit_loops=True, index=-1)
sch.set_scope(block=b28, buffer_index=0, storage_scope="local")
sch.compute_inline(block=b30)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l42, l43, l44, l45, l46 = sch.get_loops(block=b1)
v47, v48, v49, v50, v51 = sch.sample_perfect_tile(loop=l42, n=5, max_innermost_factor=64, decision=[4, 1, 1, 1, 1])
l52, l53, l54, l55, l56 = sch.split(loop=l42, factors=[v47, v48, v49, v50, v51], preserve_unit_iters=True)
v57, v58, v59, v60, v61 = sch.sample_perfect_tile(loop=l43, n=5, max_innermost_factor=64, decision=[4, 1, 1, 1, 1])
l62, l63, l64, l65, l66 = sch.split(loop=l43, factors=[v57, v58, v59, v60, v61], preserve_unit_iters=True)
v67, v68, v69, v70, v71 = sch.sample_perfect_tile(loop=l44, n=5, max_innermost_factor=64, decision=[4, 1, 8, 4, 1])
l72, l73, l74, l75, l76 = sch.split(loop=l44, factors=[v67, v68, v69, v70, v71], preserve_unit_iters=True)
v77, v78, v79, v80, v81 = sch.sample_perfect_tile(loop=l45, n=5, max_innermost_factor=64, decision=[7, 1, 14, 1, 2])
l82, l83, l84, l85, l86 = sch.split(loop=l45, factors=[v77, v78, v79, v80, v81], preserve_unit_iters=True)
v87, v88, v89 = sch.sample_perfect_tile(loop=l46, n=3, max_innermost_factor=64, decision=[2, 64, 1])
l90, l91, l92 = sch.split(loop=l46, factors=[v87, v88, v89], preserve_unit_iters=True)
sch.reorder(l52, l62, l72, l82, l53, l63, l73, l83, l54, l64, l74, l84, l90, l91, l55, l65, l75, l85, l92, l56, l66, l76, l86)
l93 = sch.fuse(l52, l62, l72, l82, preserve_unit_iters=True)
sch.bind(loop=l93, thread_axis="blockIdx.x")
l94 = sch.fuse(l53, l63, l73, l83, preserve_unit_iters=True)
sch.bind(loop=l94, thread_axis="vthread.x")
l95 = sch.fuse(l54, l64, l74, l84, preserve_unit_iters=True)
sch.bind(loop=l95, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b96 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b96, loop=l95, preserve_unit_loops=True, index=-1)
b97 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b97, loop=l90, preserve_unit_loops=True, index=-1)
l98, l99, l100, l101, l102, l103, l104, l105 = sch.get_loops(block=b97)
l106 = sch.fuse(l102, l103, l104, l105, preserve_unit_iters=True)
v107 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b97, ann_key="meta_schedule.cooperative_fetch", ann_val=v107)
b108 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b108, loop=l90, preserve_unit_loops=True, index=-1)
l109, l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b108)
l117 = sch.fuse(l113, l114, l115, l116, preserve_unit_iters=True)
v118 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b108, ann_key="meta_schedule.cooperative_fetch", ann_val=v118)
l119 = sch.fuse(l90, preserve_unit_iters=True)
sch.annotate(block_or_loop=l119, ann_key="software_pipeline_stage", ann_val=[0, 0, 2])
sch.annotate(block_or_loop=l119, ann_key="software_pipeline_order", ann_val=[0, 1, 2])
sch.annotate(block_or_loop=l119, ann_key="software_pipeline_async_stages", ann_val=[0])
sch.reverse_compute_inline(block=b5)
sch.reverse_compute_inline(block=b4)
v120 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b6, ann_key="meta_schedule.unroll_explicit", ann_val=v120)
l121, l122, l123, l124, l125, l126 = sch.get_loops(block=b3)
l127 = sch.fuse(l121, l122, l123, l124, preserve_unit_iters=True)
v128 = sch.sample_categorical(candidates=[32, 64, 128, 256, 512, 1024], probs=[0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666], decision=2)
l129, l130 = sch.split(loop=l127, factors=[None, v128], preserve_unit_iters=True)
sch.bind(loop=l129, thread_axis="blockIdx.x")
sch.bind(loop=l130, thread_axis="threadIdx.x")
sch.enter_postproc()
sch.unannotate(block_or_loop=b97, ann_key="meta_schedule.cooperative_fetch")
l131, l132, l133, l134, l135 = sch.get_loops(block=b97)
l136, l137 = sch.split(loop=l135, factors=[None, 112], preserve_unit_iters=True)
sch.bind(loop=l137, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b108, ann_key="meta_schedule.cooperative_fetch")
l138, l139, l140, l141, l142 = sch.get_loops(block=b108)
l143, l144, l145 = sch.split(loop=l142, factors=[None, 112, 4], preserve_unit_iters=True)
sch.vectorize(loop=l145)
sch.bind(loop=l144, thread_axis="threadIdx.x")
b146 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b146, ann_key="meta_schedule.unroll_explicit")
b147, b148, b149, b150, b151, b152, b153, b154, b155 = sch.get_child_blocks(b146)
l156, l157, l158, l159, l160, l161 = sch.get_loops(block=b147)
l162, l163, l164, l165, l166, l167 = sch.get_loops(block=b148)
sch.annotate(block_or_loop=l162, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l162, ann_key="pragma_unroll_explicit", ann_val=1)
l168, l169, l170, l171, l172, l173 = sch.get_loops(block=b149)
l174, l175, l176, l177, l178, l179 = sch.get_loops(block=b150)
l180, l181, l182, l183, l184, l185, l186 = sch.get_loops(block=b151)
l187, l188, l189, l190, l191, l192, l193, l194, l195, l196, l197, l198, l199, l200 = sch.get_loops(block=b152)
sch.annotate(block_or_loop=l187, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l187, ann_key="pragma_unroll_explicit", ann_val=1)
l201, l202, l203, l204, l205, l206, l207 = sch.get_loops(block=b153)
l208, l209, l210, l211, l212, l213, l214, l215 = sch.get_loops(block=b154)
sch.annotate(block_or_loop=l208, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l208, ann_key="pragma_unroll_explicit", ann_val=1)
l216, l217, l218, l219 = sch.get_loops(block=b155)
b220 = sch.get_block(name="data_pack", func_name="main")
l221, l222, l223, l224, l225, l226 = sch.get_loops(block=b220)
b227 = sch.decompose_reduction(block=b220, loop=l225)
b228 = sch.get_block(name="bgemm", func_name="main")
l229, l230, l231, l232, l233, l234, l235, l236, l237, l238, l239, l240, l241, l242 = sch.get_loops(block=b228)
b243 = sch.decompose_reduction(block=b228, loop=l232)
b244 = sch.get_block(name="inverse", func_name="main")
l245, l246, l247, l248, l249, l250, l251, l252 = sch.get_loops(block=b244)
b253 = sch.decompose_reduction(block=b244, loop=l251)
2023-11-11 07:30:18 [INFO] [task_scheduler.cc:121] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #519: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/home2/xiachunwei/Software/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/home2/xiachunwei/Software/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/home2/xiachunwei/Software/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/home2/xiachunwei/Software/tvm/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/home2/xiachunwei/Software/tvm/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/home2/xiachunwei/Software/tvm/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/home2/xiachunwei/Software/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/home2/xiachunwei/Software/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  4: _ZN3tvm7runtime13PackedFun
  3: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  2: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  0: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  File "/home2/xiachunwei/Software/tvm/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(128), T.int64(28), T.int64(28)), "float32"), p1: T.Buffer((T.int64(4), T.int64(4), T.int64(128), T.int64(128)), "float32"), p2: T.Buffer((T.int64(1), T.int64(128), T.int64(1), T.int64(1)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(128), T.int64(28), T.int64(28)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        input_tile_local = T.alloc_buffer((T.int64(128), T.int64(196), T.int64(4), T.int64(4)), scope="local")
        data_pack = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(128), T.int64(196)))
        bgemm = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(128), T.int64(196)))
        inverse_local = T.alloc_buffer((T.int64(128), T.int64(196), T.int64(2), T.int64(2)), scope="local")
        data_pack_local = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(128), T.int64(196)), scope="local")
        bgemm_local = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(128), T.int64(196)), scope="local")
        data_pack_shared = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(128), T.int64(196)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(128), T.int64(128)), scope="shared")
        for ci_p_fused_0 in T.thread_binding(T.int64(392), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 1024, "pragma_unroll_explicit": 1}):
            for ci_p_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(1), T.int64(4), T.int64(4)):
                    with T.block("input_tile"):
                        v_ci = T.axis.spatial(T.int64(128), (ci_p_fused_0 * T.int64(64) + ci_p_fused_1) // T.int64(196) + ax0)
                        v_p = T.axis.spatial(T.int64(196), (ci_p_fused_0 * T.int64(64) + ci_p_fused_1) % T.int64(196) + ax1)
                        v_eps, v_nu = T.axis.remap("SS", [ax2, ax3])
                        T.reads(p0[v_p // T.int64(196), v_ci, v_p % T.int64(196) // T.int64(14) * T.int64(2) + v_eps - T.int64(1), v_p % T.int64(14) * T.int64(2) + v_nu - T.int64(1)])
                        T.writes(input_tile_local[v_ci, v_p, v_eps, v_nu])
                        T.block_attr({"schedule_rule": "None"})
                        input_tile_local[v_ci, v_p, v_eps, v_nu] = T.if_then_else(T.int64(1) <= v_p % T.int64(196) // T.int64(14) * T.int64(2) + v_eps and v_p % T.int64(196) // T.int64(14) * T.int64(2) + v_eps < T.int64(29) and T.int64(1) <= v_p % T.int64(14) * T.int64(2) + v_nu and v_p % T.int64(14) * T.int64(2) + v_nu < T.int64(29), p0[v_p // T.int64(196), v_ci, v_p % T.int64(196) // T.int64(14) * T.int64(2) + v_eps - T.int64(1), v_p % T.int64(14) * T.int64(2) + v_nu - T.int64(1)], T.float32(0))
                for eps in T.unroll(T.int64(4)):
                    for nu in T.unroll(T.int64(4)):
                        with T.block("data_pack_init"):
                            v_eps, v_nu = T.axis.remap("SS", [eps, nu])
                            v_ci = T.axis.spatial(T.int64(128), (ci_p_fused_0 * T.int64(64) + ci_p_fused_1) // T.int64(196))
                            v_p = T.axis.spatial(T.int64(196), (ci_p_fused_0 * T.int64(64) + ci_p_fused_1) % T.int64(196))
                            T.reads()
                            T.writes(data_pack_local[v_eps, v_nu, v_ci, v_p])
                            T.block_attr({"schedule_rule": "conv2d_nchw_winograd_data_pack"})
                            data_pack_local[v_eps, v_nu, v_ci, v_p] = T.float32(0)
                        for r_a in T.unroll(T.int64(4)):
                            for r_b in T.unroll(T.int64(4)):
                                with T.block("data_pack_update"):
                                    v_eps, v_nu = T.axis.remap("SS", [eps, nu])
                                    v_ci = T.axis.spatial(T.int64(128), (ci_p_fused_0 * T.int64(64) + ci_p_fused_1) // T.int64(196))
                                    v_p = T.axis.spatial(T.int64(196), (ci_p_fused_0 * T.int64(64) + ci_p_fused_1) % T.int64(196))
                                    v_r_a, v_r_b = T.axis.remap("RR", [r_a, r_b])
                                    T.reads(data_pack_local[v_eps, v_nu, v_ci, v_p], input_tile_local[v_ci, v_p, v_r_a, v_r_b])
                                    T.writes(data_pack_local[v_eps, v_nu, v_ci, v_p])
                                    T.block_attr({"schedule_rule": "conv2d_nchw_winograd_data_pack"})
                                    data_pack_local[v_eps, v_nu, v_ci, v_p] = data_pack_local[v_eps, v_nu, v_ci, v_p] + input_tile_local[v_ci, v_p, v_r_a, v_r_b] * T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(3), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(1), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(0), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(3), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(1), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(0), T.float32(1), T.float32(0))))))))))))))))) * T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(3), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(1), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(0), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(3), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(1), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(0), T.float32(1), T.float32(0)))))))))))))))))
                for ax0, ax1, ax2, ax3 in T.grid(T.int64(4), T.int64(4), T.int64(1), T.int64(1)):
                    with T.block("data_pack_local"):
                        v0, v1 = T.axis.remap("SS", [ax0, ax1])
                        v2 = T.axis.spatial(T.int64(128), (ci_p_fused_0 * T.int64(64) + ci_p_fused_1) // T.int64(196) + ax2)
                        v3 = T.axis.spatial(T.int64(196), (ci_p_fused_0 * T.int64(64) + ci_p_fused_1) % T.int64(196) + ax3)
                        T.reads(data_pack_local[v0, v1, v2, v3])
                        T.writes(data_pack[v0, v1, v2, v3])
                        data_pack[v0, v1, v2, v3] = data_pack_local[v0, v1, v2, v3]
        for eps_0_nu_0_co_0_p_0_fused in T.thread_binding(T.int64(448), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 1024, "pragma_unroll_explicit": 1}):
            for eps_1_nu_1_co_1_p_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for eps_2_nu_2_co_2_p_2_fused in T.thread_binding(T.int64(112), thread="threadIdx.x"):
                    for eps_3_init, nu_3_init, co_3_init, p_3_init, eps_4_init, nu_4_init, co_4_init, p_4_init in T.grid(T.int64(1), T.int64(1), T.int64(2), T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(2)):
                        with T.block("bgemm_init"):
                            v_eps = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(112) + eps_3_init + eps_4_init)
                            v_nu = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused % T.int64(112) // T.int64(28) + nu_3_init + nu_4_init)
                            v_co = T.axis.spatial(T.int64(128), eps_0_nu_0_co_0_p_0_fused % T.int64(28) // T.int64(7) * T.int64(32) + eps_2_nu_2_co_2_p_2_fused // T.int64(14) * T.int64(4) + co_3_init * T.int64(2) + co_4_init)
                            v_p = T.axis.spatial(T.int64(196), eps_0_nu_0_co_0_p_0_fused % T.int64(7) * T.int64(28) + eps_2_nu_2_co_2_p_2_fused % T.int64(14) * T.int64(2) + p_3_init * T.int64(2) + p_4_init)
                            T.reads()
                            T.writes(bgemm_local[v_eps, v_nu, v_co, v_p])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            bgemm_local[v_eps, v_nu, v_co, v_p] = T.float32(0)
                    for ci_0_fused in T.serial(T.int64(2), annotations={"software_pipeline_async_stages": [0], "software_pipeline_order": [0, 1, 2], "software_pipeline_stage": [0, 0, 2]}):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(16)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(112), thread="threadIdx.x"):
                                with T.block("data_pack_shared"):
                                    v0 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(112))
                                    v1 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused % T.int64(112) // T.int64(28))
                                    v2 = T.axis.spatial(T.int64(128), ci_0_fused * T.int64(64) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(112) + ax0_ax1_ax2_ax3_fused_1) // T.int64(28))
                                    v3 = T.axis.spatial(T.int64(196), eps_0_nu_0_co_0_p_0_fused % T.int64(7) * T.int64(28) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(112) + ax0_ax1_ax2_ax3_fused_1) % T.int64(28))
                                    T.reads(data_pack[v0, v1, v2, v3])
                                    T.writes(data_pack_shared[v0, v1, v2, v3])
                                    data_pack_shared[v0, v1, v2, v3] = data_pack[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(10)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(112), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("p1_shared"):
                                        v0 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(112))
                                        v1 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused % T.int64(112) // T.int64(28))
                                        v2 = T.axis.spatial(T.int64(128), ci_0_fused * T.int64(64) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(224) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) // T.int64(32))
                                        v3 = T.axis.spatial(T.int64(128), eps_0_nu_0_co_0_p_0_fused % T.int64(28) // T.int64(7) * T.int64(32) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(224) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) % T.int64(32))
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * T.int64(112) + ax0_ax1_ax2_ax3_fused_1) * T.int64(2) + ax0_ax1_ax2_ax3_fused_2 < T.int64(2048))
                                        T.reads(p1[v0, v1, v2, v3])
                                        T.writes(p1_shared[v0, v1, v2, v3])
                                        p1_shared[v0, v1, v2, v3] = p1[v0, v1, v2, v3]
                        for ci_1, eps_3, nu_3, co_3, p_3, ci_2, eps_4, nu_4, co_4, p_4 in T.grid(T.int64(64), T.int64(1), T.int64(1), T.int64(2), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(2)):
                            with T.block("bgemm_update"):
                                v_eps = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(112) + eps_3 + eps_4)
                                v_nu = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused % T.int64(112) // T.int64(28) + nu_3 + nu_4)
                                v_co = T.axis.spatial(T.int64(128), eps_0_nu_0_co_0_p_0_fused % T.int64(28) // T.int64(7) * T.int64(32) + eps_2_nu_2_co_2_p_2_fused // T.int64(14) * T.int64(4) + co_3 * T.int64(2) + co_4)
                                v_p = T.axis.spatial(T.int64(196), eps_0_nu_0_co_0_p_0_fused % T.int64(7) * T.int64(28) + eps_2_nu_2_co_2_p_2_fused % T.int64(14) * T.int64(2) + p_3 * T.int64(2) + p_4)
                                v_ci = T.axis.reduce(T.int64(128), ci_0_fused * T.int64(64) + ci_1 + ci_2)
                                T.reads(bgemm_local[v_eps, v_nu, v_co, v_p], data_pack_shared[v_eps, v_nu, v_ci, v_p], p1_shared[v_eps, v_nu, v_ci, v_co])
                                T.writes(bgemm_local[v_eps, v_nu, v_co, v_p])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                bgemm_local[v_eps, v_nu, v_co, v_p] = bgemm_local[v_eps, v_nu, v_co, v_p] + data_pack_shared[v_eps, v_nu, v_ci, v_p] * p1_shared[v_eps, v_nu, v_ci, v_co]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(1), T.int64(4), T.int64(2)):
                        with T.block("bgemm_local"):
                            v0 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(112) + ax0)
                            v1 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused % T.int64(112) // T.int64(28) + ax1)
                            v2 = T.axis.spatial(T.int64(128), eps_0_nu_0_co_0_p_0_fused % T.int64(28) // T.int64(7) * T.int64(32) + eps_2_nu_2_co_2_p_2_fused // T.int64(14) * T.int64(4) + ax2)
                            v3 = T.axis.spatial(T.int64(196), eps_0_nu_0_co_0_p_0_fused % T.int64(7) * T.int64(28) + eps_2_nu_2_co_2_p_2_fused % T.int64(14) * T.int64(2) + ax3)
                            T.reads(bgemm_local[v0, v1, v2, v3])
                            T.writes(bgemm[v0, v1, v2, v3])
                            bgemm[v0, v1, v2, v3] = bgemm_local[v0, v1, v2, v3]
        for n_co_h_0_w_0_fused_0 in T.thread_binding(T.int64(196), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 1024, "pragma_unroll_explicit": 1}):
            for n_co_h_0_w_0_fused_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                    for ax2 in T.unroll(T.int64(2)):
                        for ax3 in T.unroll(T.int64(2)):
                            with T.block("inverse_init"):
                                v_co = T.axis.spatial(T.int64(128), (n_co_h_0_w_0_fused_0 * T.int64(128) + n_co_h_0_w_0_fused_1) // T.int64(196) + ax0)
                                v_p = T.axis.spatial(T.int64(196), (n_co_h_0_w_0_fused_0 * T.int64(128) + n_co_h_0_w_0_fused_1) % T.int64(196) + ax1)
                                v_vh, v_vw = T.axis.remap("SS", [ax2, ax3])
                                T.reads()
                                T.writes(inverse_local[v_co, v_p, v_vh, v_vw])
                                T.block_attr({"schedule_rule": "conv2d_nchw_winograd_inverse"})
                                inverse_local[v_co, v_p, v_vh, v_vw] = T.float32(0)
                            for ax4 in T.unroll(T.int64(4)):
                                for ax5 in T.unroll(T.int64(4)):
                                    with T.block("inverse_update"):
                                        v_co = T.axis.spatial(T.int64(128), (n_co_h_0_w_0_fused_0 * T.int64(128) + n_co_h_0_w_0_fused_1) // T.int64(196) + ax0)
                                        v_p = T.axis.spatial(T.int64(196), (n_co_h_0_w_0_fused_0 * T.int64(128) + n_co_h_0_w_0_fused_1) % T.int64(196) + ax1)
                                        v_vh, v_vw, v_r_a, v_r_b = T.axis.remap("SSRR", [ax2, ax3, ax4, ax5])
                                        T.reads(inverse_local[v_co, v_p, v_vh, v_vw], bgemm[v_r_a, v_r_b, v_co, v_p])
                                        T.writes(inverse_local[v_co, v_p, v_vh, v_vw])
                                        T.block_attr({"schedule_rule": "conv2d_nchw_winograd_inverse"})
                                        inverse_local[v_co, v_p, v_vh, v_vw] = inverse_local[v_co, v_p, v_vh, v_vw] + bgemm[v_r_a, v_r_b, v_co, v_p] * T.Select(v_r_a % T.int64(4) == T.int64(3) and v_vh % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_vh % T.int64(2) == T.int64(0), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_vh % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_vh % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_vh % T.int64(2) == T.int64(1), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_vh % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_vh % T.int64(2) == T.int64(1), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_vh % T.int64(2) == T.int64(0), T.float32(1), T.float32(0))))))))) * T.Select(v_r_b % T.int64(4) == T.int64(3) and v_vw % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_vw % T.int64(2) == T.int64(0), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_vw % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_vw % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_vw % T.int64(2) == T.int64(1), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_vw % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_vw % T.int64(2) == T.int64(1), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_vw % T.int64(2) == T.int64(0), T.float32(1), T.float32(0)))))))))
                for h_1, w_1 in T.grid(T.int64(2), T.int64(2)):
                    with T.block("conv2d_winograd"):
                        v_n = T.axis.spatial(T.int64(1), T.int64(0))
                        v_co = T.axis.spatial(T.int64(128), (n_co_h_0_w_0_fused_0 * T.int64(128) + n_co_h_0_w_0_fused_1) // T.int64(196))
                        v_h = T.axis.spatial(T.int64(28), (n_co_h_0_w_0_fused_0 * T.int64(128) + n_co_h_0_w_0_fused_1) % T.int64(196) // T.int64(14) * T.int64(2) + h_1)
                        v_w = T.axis.spatial(T.int64(28), (n_co_h_0_w_0_fused_0 * T.int64(128) + n_co_h_0_w_0_fused_1) % T.int64(14) * T.int64(2) + w_1)
                        T.reads(inverse_local[v_co, v_n * T.int64(196) + v_h // T.int64(2) * T.int64(14) + v_w // T.int64(2), v_h % T.int64(2), v_w % T.int64(2)], p2[v_n, v_co, T.int64(0), T.int64(0)])
                        T.writes(T_relu[v_n, v_co, v_h, v_w])
                        T_relu[v_n, v_co, v_h, v_w] = T.max(inverse_local[v_co, v_n * T.int64(196) + v_h // T.int64(2) * T.int64(14) + v_w // T.int64(2), v_h % T.int64(2), v_w % T.int64(2)] + p2[v_n, v_co, T.int64(0), T.int64(0)], T.float32(0))
b0 = sch.get_block(name="data_pack", func_name="main")
b1 = sch.get_block(name="bgemm", func_name="main")
b2 = sch.get_block(name="inverse", func_name="main")
b3 = sch.get_block(name="conv2d_winograd", func_name="main")
b4 = sch.get_block(name="T_add", func_name="main")
b5 = sch.get_block(name="T_relu", func_name="main")
b6 = sch.get_block(name="root", func_name="main")
b7, b8 = sch.get_producers(block=b2)
sch.compute_inline(block=b8)
b9, = sch.get_consumers(block=b2)
l10, l11, l12, l13 = sch.get_loops(block=b9)
l14, l15 = sch.split(loop=l12, factors=[None, 2], preserve_unit_iters=True)
l16, l17 = sch.split(loop=l13, factors=[None, 2], preserve_unit_iters=True)
sch.reorder(l14, l16, l15, l17)
sch.compute_at(block=b2, loop=l16, preserve_unit_loops=True, index=-1)
sch.set_scope(block=b2, buffer_index=0, storage_scope="local")
l18, l19, l20, l21, l22, l23, l24, l25, l26, l27 = sch.get_loops(block=b2)
sch.unroll(loop=l24)
sch.unroll(loop=l25)
sch.unroll(loop=l26)
sch.unroll(loop=l27)
b28, b29 = sch.get_producers(block=b0)
sch.compute_inline(block=b29)
b30, = sch.get_producers(block=b28)
l31, l32, l33, l34, l35, l36 = sch.get_loops(block=b0)
sch.reorder(l33, l34, l31, l32, l35, l36)
sch.unroll(loop=l31)
sch.unroll(loop=l32)
sch.unroll(loop=l35)
sch.unroll(loop=l36)
l37 = sch.fuse(l33, l34, preserve_unit_iters=True)
v38 = sch.sample_categorical(candidates=[32, 64, 128, 256, 512, 1024], probs=[0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666], decision=1)
l39, l40 = sch.split(loop=l37, factors=[None, v38], preserve_unit_iters=True)
sch.bind(loop=l39, thread_axis="blockIdx.x")
sch.bind(loop=l40, thread_axis="threadIdx.x")
b41 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b41, loop=l40, preserve_unit_loops=True, index=-1)
sch.compute_at(block=b28, loop=l40, preserve_unit_loops=True, index=-1)
sch.set_scope(block=b28, buffer_index=0, storage_scope="local")
sch.compute_inline(block=b30)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l42, l43, l44, l45, l46 = sch.get_loops(block=b1)
v47, v48, v49, v50, v51 = sch.sample_perfect_tile(loop=l42, n=5, max_innermost_factor=64, decision=[4, 1, 1, 1, 1])
l52, l53, l54, l55, l56 = sch.split(loop=l42, factors=[v47, v48, v49, v50, v51], preserve_unit_iters=True)
v57, v58, v59, v60, v61 = sch.sample_perfect_tile(loop=l43, n=5, max_innermost_factor=64, decision=[4, 1, 1, 1, 1])
l62, l63, l64, l65, l66 = sch.split(loop=l43, factors=[v57, v58, v59, v60, v61], preserve_unit_iters=True)
v67, v68, v69, v70, v71 = sch.sample_perfect_tile(loop=l44, n=5, max_innermost_factor=64, decision=[4, 1, 8, 2, 2])
l72, l73, l74, l75, l76 = sch.split(loop=l44, factors=[v67, v68, v69, v70, v71], preserve_unit_iters=True)
v77, v78, v79, v80, v81 = sch.sample_perfect_tile(loop=l45, n=5, max_innermost_factor=64, decision=[7, 1, 14, 1, 2])
l82, l83, l84, l85, l86 = sch.split(loop=l45, factors=[v77, v78, v79, v80, v81], preserve_unit_iters=True)
v87, v88, v89 = sch.sample_perfect_tile(loop=l46, n=3, max_innermost_factor=64, decision=[2, 64, 1])
l90, l91, l92 = sch.split(loop=l46, factors=[v87, v88, v89], preserve_unit_iters=True)
sch.reorder(l52, l62, l72, l82, l53, l63, l73, l83, l54, l64, l74, l84, l90, l91, l55, l65, l75, l85, l92, l56, l66, l76, l86)
l93 = sch.fuse(l52, l62, l72, l82, preserve_unit_iters=True)
sch.bind(loop=l93, thread_axis="blockIdx.x")
l94 = sch.fuse(l53, l63, l73, l83, preserve_unit_iters=True)
sch.bind(loop=l94, thread_axis="vthread.x")
l95 = sch.fuse(l54, l64, l74, l84, preserve_unit_iters=True)
sch.bind(loop=l95, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b96 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b96, loop=l95, preserve_unit_loops=True, index=-1)
b97 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b97, loop=l90, preserve_unit_loops=True, index=-1)
l98, l99, l100, l101, l102, l103, l104, l105 = sch.get_loops(block=b97)
l106 = sch.fuse(l102, l103, l104, l105, preserve_unit_iters=True)
v107 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b97, ann_key="meta_schedule.cooperative_fetch", ann_val=v107)
b108 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b108, loop=l90, preserve_unit_loops=True, index=-1)
l109, l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b108)
l117 = sch.fuse(l113, l114, l115, l116, preserve_unit_iters=True)
v118 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b108, ann_key="meta_schedule.cooperative_fetch", ann_val=v118)
l119 = sch.fuse(l90, preserve_unit_iters=True)
sch.annotate(block_or_loop=l119, ann_key="software_pipeline_stage", ann_val=[0, 0, 2])
sch.annotate(block_or_loop=l119, ann_key="software_pipeline_order", ann_val=[0, 1, 2])
sch.annotate(block_or_loop=l119, ann_key="software_pipeline_async_stages", ann_val=[0])
sch.reverse_compute_inline(block=b5)
sch.reverse_compute_inline(block=b4)
v120 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b6, ann_key="meta_schedule.unroll_explicit", ann_val=v120)
l121, l122, l123, l124, l125, l126 = sch.get_loops(block=b3)
l127 = sch.fuse(l121, l122, l123, l124, preserve_unit_iters=True)
v128 = sch.sample_categorical(candidates=[32, 64, 128, 256, 512, 1024], probs=[0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666], decision=2)
l129, l130 = sch.split(loop=l127, factors=[None, v128], preserve_unit_iters=True)
sch.bind(loop=l129, thread_axis="blockIdx.x")
sch.bind(loop=l130, thread_axis="threadIdx.x")
sch.enter_postproc()
sch.unannotate(block_or_loop=b97, ann_key="meta_schedule.cooperative_fetch")
l131, l132, l133, l134, l135 = sch.get_loops(block=b97)
l136, l137 = sch.split(loop=l135, factors=[None, 112], preserve_unit_iters=True)
sch.bind(loop=l137, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b108, ann_key="meta_schedule.cooperative_fetch")
l138, l139, l140, l141, l142 = sch.get_loops(block=b108)
l143, l144, l145 = sch.split(loop=l142, factors=[None, 112, 2], preserve_unit_iters=True)
sch.vectorize(loop=l145)
sch.bind(loop=l144, thread_axis="threadIdx.x")
b146 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b146, ann_key="meta_schedule.unroll_explicit")
b147, b148, b149, b150, b151, b152, b153, b154, b155 = sch.get_child_blocks(b146)
l156, l157, l158, l159, l160, l161 = sch.get_loops(block=b147)
l162, l163, l164, l165, l166, l167 = sch.get_loops(block=b148)
sch.annotate(block_or_loop=l162, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l162, ann_key="pragma_unroll_explicit", ann_val=1)
l168, l169, l170, l171, l172, l173 = sch.get_loops(block=b149)
l174, l175, l176, l177, l178, l179 = sch.get_loops(block=b150)
l180, l181, l182, l183, l184, l185, l186 = sch.get_loops(block=b151)
l187, l188, l189, l190, l191, l192, l193, l194, l195, l196, l197, l198, l199, l200 = sch.get_loops(block=b152)
sch.annotate(block_or_loop=l187, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l187, ann_key="pragma_unroll_explicit", ann_val=1)
l201, l202, l203, l204, l205, l206, l207 = sch.get_loops(block=b153)
l208, l209, l210, l211, l212, l213, l214, l215 = sch.get_loops(block=b154)
sch.annotate(block_or_loop=l208, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l208, ann_key="pragma_unroll_explicit", ann_val=1)
l216, l217, l218, l219 = sch.get_loops(block=b155)
b220 = sch.get_block(name="data_pack", func_name="main")
l221, l222, l223, l224, l225, l226 = sch.get_loops(block=b220)
b227 = sch.decompose_reduction(block=b220, loop=l225)
b228 = sch.get_block(name="bgemm", func_name="main")
l229, l230, l231, l232, l233, l234, l235, l236, l237, l238, l239, l240, l241, l242 = sch.get_loops(block=b228)
b243 = sch.decompose_reduction(block=b228, loop=l232)
b244 = sch.get_block(name="inverse", func_name="main")
l245, l246, l247, l248, l249, l250, l251, l252 = sch.get_loops(block=b244)
b253 = sch.decompose_reduction(block=b244, loop=l251)
2023-11-11 07:30:18 [INFO] [task_scheduler.cc:121] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #520: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/home2/xiachunwei/Software/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/home2/xiachunwei/Software/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/home2/xiachunwei/Software/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/home2/xiachunwei/Software/tvm/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/home2/xiachunwei/Software/tvm/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/home2/xiachunwei/Software/tvm/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/home2/xiachunwei/Software/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/home2/xiachunwei/Software/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  4: _ZN3tvm7runtime13PackedFun
  3: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  2: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  0: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  File "/home2/xiachunwei/Software/tvm/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(128), T.int64(28), T.int64(28)), "float32"), p1: T.Buffer((T.int64(4), T.int64(4), T.int64(128), T.int64(128)), "float32"), p2: T.Buffer((T.int64(1), T.int64(128), T.int64(1), T.int64(1)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(128), T.int64(28), T.int64(28)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        input_tile_local = T.alloc_buffer((T.int64(128), T.int64(196), T.int64(4), T.int64(4)), scope="local")
        data_pack = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(128), T.int64(196)))
        bgemm = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(128), T.int64(196)))
        inverse_local = T.alloc_buffer((T.int64(128), T.int64(196), T.int64(2), T.int64(2)), scope="local")
        data_pack_local = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(128), T.int64(196)), scope="local")
        bgemm_local = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(128), T.int64(196)), scope="local")
        data_pack_shared = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(128), T.int64(196)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(128), T.int64(128)), scope="shared")
        for ci_p_fused_0 in T.thread_binding(T.int64(392), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 1024, "pragma_unroll_explicit": 1}):
            for ci_p_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(1), T.int64(4), T.int64(4)):
                    with T.block("input_tile"):
                        v_ci = T.axis.spatial(T.int64(128), (ci_p_fused_0 * T.int64(64) + ci_p_fused_1) // T.int64(196) + ax0)
                        v_p = T.axis.spatial(T.int64(196), (ci_p_fused_0 * T.int64(64) + ci_p_fused_1) % T.int64(196) + ax1)
                        v_eps, v_nu = T.axis.remap("SS", [ax2, ax3])
                        T.reads(p0[v_p // T.int64(196), v_ci, v_p % T.int64(196) // T.int64(14) * T.int64(2) + v_eps - T.int64(1), v_p % T.int64(14) * T.int64(2) + v_nu - T.int64(1)])
                        T.writes(input_tile_local[v_ci, v_p, v_eps, v_nu])
                        T.block_attr({"schedule_rule": "None"})
                        input_tile_local[v_ci, v_p, v_eps, v_nu] = T.if_then_else(T.int64(1) <= v_p % T.int64(196) // T.int64(14) * T.int64(2) + v_eps and v_p % T.int64(196) // T.int64(14) * T.int64(2) + v_eps < T.int64(29) and T.int64(1) <= v_p % T.int64(14) * T.int64(2) + v_nu and v_p % T.int64(14) * T.int64(2) + v_nu < T.int64(29), p0[v_p // T.int64(196), v_ci, v_p % T.int64(196) // T.int64(14) * T.int64(2) + v_eps - T.int64(1), v_p % T.int64(14) * T.int64(2) + v_nu - T.int64(1)], T.float32(0))
                for eps in T.unroll(T.int64(4)):
                    for nu in T.unroll(T.int64(4)):
                        with T.block("data_pack_init"):
                            v_eps, v_nu = T.axis.remap("SS", [eps, nu])
                            v_ci = T.axis.spatial(T.int64(128), (ci_p_fused_0 * T.int64(64) + ci_p_fused_1) // T.int64(196))
                            v_p = T.axis.spatial(T.int64(196), (ci_p_fused_0 * T.int64(64) + ci_p_fused_1) % T.int64(196))
                            T.reads()
                            T.writes(data_pack_local[v_eps, v_nu, v_ci, v_p])
                            T.block_attr({"schedule_rule": "conv2d_nchw_winograd_data_pack"})
                            data_pack_local[v_eps, v_nu, v_ci, v_p] = T.float32(0)
                        for r_a in T.unroll(T.int64(4)):
                            for r_b in T.unroll(T.int64(4)):
                                with T.block("data_pack_update"):
                                    v_eps, v_nu = T.axis.remap("SS", [eps, nu])
                                    v_ci = T.axis.spatial(T.int64(128), (ci_p_fused_0 * T.int64(64) + ci_p_fused_1) // T.int64(196))
                                    v_p = T.axis.spatial(T.int64(196), (ci_p_fused_0 * T.int64(64) + ci_p_fused_1) % T.int64(196))
                                    v_r_a, v_r_b = T.axis.remap("RR", [r_a, r_b])
                                    T.reads(data_pack_local[v_eps, v_nu, v_ci, v_p], input_tile_local[v_ci, v_p, v_r_a, v_r_b])
                                    T.writes(data_pack_local[v_eps, v_nu, v_ci, v_p])
                                    T.block_attr({"schedule_rule": "conv2d_nchw_winograd_data_pack"})
                                    data_pack_local[v_eps, v_nu, v_ci, v_p] = data_pack_local[v_eps, v_nu, v_ci, v_p] + input_tile_local[v_ci, v_p, v_r_a, v_r_b] * T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(3), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(1), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(0), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(3), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(1), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(0), T.float32(1), T.float32(0))))))))))))))))) * T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(3), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(1), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(0), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(3), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(1), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(0), T.float32(1), T.float32(0)))))))))))))))))
                for ax0, ax1, ax2, ax3 in T.grid(T.int64(4), T.int64(4), T.int64(1), T.int64(1)):
                    with T.block("data_pack_local"):
                        v0, v1 = T.axis.remap("SS", [ax0, ax1])
                        v2 = T.axis.spatial(T.int64(128), (ci_p_fused_0 * T.int64(64) + ci_p_fused_1) // T.int64(196) + ax2)
                        v3 = T.axis.spatial(T.int64(196), (ci_p_fused_0 * T.int64(64) + ci_p_fused_1) % T.int64(196) + ax3)
                        T.reads(data_pack_local[v0, v1, v2, v3])
                        T.writes(data_pack[v0, v1, v2, v3])
                        data_pack[v0, v1, v2, v3] = data_pack_local[v0, v1, v2, v3]
        for eps_0_nu_0_co_0_p_0_fused in T.thread_binding(T.int64(448), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 1024, "pragma_unroll_explicit": 1}):
            for eps_1_nu_1_co_1_p_1_fused in T.thread_binding(T.int64(2), thread="vthread.x"):
                for eps_2_nu_2_co_2_p_2_fused in T.thread_binding(T.int64(112), thread="threadIdx.x"):
                    for eps_3_init, nu_3_init, co_3_init, p_3_init, eps_4_init, nu_4_init, co_4_init, p_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(4), T.int64(1)):
                        with T.block("bgemm_init"):
                            v_eps = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(112) + eps_3_init + eps_4_init)
                            v_nu = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused % T.int64(112) // T.int64(28) + nu_3_init + nu_4_init)
                            v_co = T.axis.spatial(T.int64(128), eps_0_nu_0_co_0_p_0_fused % T.int64(28) // T.int64(7) * T.int64(32) + eps_2_nu_2_co_2_p_2_fused // T.int64(14) * T.int64(4) + co_3_init * T.int64(4) + co_4_init)
                            v_p = T.axis.spatial(T.int64(196), eps_0_nu_0_co_0_p_0_fused % T.int64(7) * T.int64(28) + eps_1_nu_1_co_1_p_1_fused * T.int64(14) + eps_2_nu_2_co_2_p_2_fused % T.int64(14) + p_3_init + p_4_init)
                            T.reads()
                            T.writes(bgemm_local[v_eps, v_nu, v_co, v_p])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            bgemm_local[v_eps, v_nu, v_co, v_p] = T.float32(0)
                    for ci_0_fused in T.serial(T.int64(2), annotations={"software_pipeline_async_stages": [0], "software_pipeline_order": [0, 1, 2], "software_pipeline_stage": [0, 0, 2]}):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(16)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(112), thread="threadIdx.x"):
                                with T.block("data_pack_shared"):
                                    v0 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(112))
                                    v1 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused % T.int64(112) // T.int64(28))
                                    v2 = T.axis.spatial(T.int64(128), ci_0_fused * T.int64(64) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(112) + ax0_ax1_ax2_ax3_fused_1) // T.int64(28))
                                    v3 = T.axis.spatial(T.int64(196), eps_0_nu_0_co_0_p_0_fused % T.int64(7) * T.int64(28) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(112) + ax0_ax1_ax2_ax3_fused_1) % T.int64(28))
                                    T.reads(data_pack[v0, v1, v2, v3])
                                    T.writes(data_pack_shared[v0, v1, v2, v3])
                                    data_pack_shared[v0, v1, v2, v3] = data_pack[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(19)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(112), thread="threadIdx.x"):
                                with T.block("p1_shared"):
                                    v0 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(112))
                                    v1 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused % T.int64(112) // T.int64(28))
                                    v2 = T.axis.spatial(T.int64(128), ci_0_fused * T.int64(64) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(112) + ax0_ax1_ax2_ax3_fused_1) // T.int64(32))
                                    v3 = T.axis.spatial(T.int64(128), eps_0_nu_0_co_0_p_0_fused % T.int64(28) // T.int64(7) * T.int64(32) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(112) + ax0_ax1_ax2_ax3_fused_1) % T.int64(32))
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * T.int64(112) + ax0_ax1_ax2_ax3_fused_1 < T.int64(2048))
                                    T.reads(p1[v0, v1, v2, v3])
                                    T.writes(p1_shared[v0, v1, v2, v3])
                                    p1_shared[v0, v1, v2, v3] = p1[v0, v1, v2, v3]
                        for ci_1, eps_3, nu_3, co_3, p_3, ci_2, eps_4, nu_4, co_4, p_4 in T.grid(T.int64(16), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(4), T.int64(1), T.int64(1), T.int64(4), T.int64(1)):
                            with T.block("bgemm_update"):
                                v_eps = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(112) + eps_3 + eps_4)
                                v_nu = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused % T.int64(112) // T.int64(28) + nu_3 + nu_4)
                                v_co = T.axis.spatial(T.int64(128), eps_0_nu_0_co_0_p_0_fused % T.int64(28) // T.int64(7) * T.int64(32) + eps_2_nu_2_co_2_p_2_fused // T.int64(14) * T.int64(4) + co_3 * T.int64(4) + co_4)
                                v_p = T.axis.spatial(T.int64(196), eps_0_nu_0_co_0_p_0_fused % T.int64(7) * T.int64(28) + eps_1_nu_1_co_1_p_1_fused * T.int64(14) + eps_2_nu_2_co_2_p_2_fused % T.int64(14) + p_3 + p_4)
                                v_ci = T.axis.reduce(T.int64(128), ci_0_fused * T.int64(64) + ci_1 * T.int64(4) + ci_2)
                                T.reads(bgemm_local[v_eps, v_nu, v_co, v_p], data_pack_shared[v_eps, v_nu, v_ci, v_p], p1_shared[v_eps, v_nu, v_ci, v_co])
                                T.writes(bgemm_local[v_eps, v_nu, v_co, v_p])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                bgemm_local[v_eps, v_nu, v_co, v_p] = bgemm_local[v_eps, v_nu, v_co, v_p] + data_pack_shared[v_eps, v_nu, v_ci, v_p] * p1_shared[v_eps, v_nu, v_ci, v_co]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(1), T.int64(4), T.int64(1)):
                        with T.block("bgemm_local"):
                            v0 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(112) + ax0)
                            v1 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused % T.int64(112) // T.int64(28) + ax1)
                            v2 = T.axis.spatial(T.int64(128), eps_0_nu_0_co_0_p_0_fused % T.int64(28) // T.int64(7) * T.int64(32) + eps_2_nu_2_co_2_p_2_fused // T.int64(14) * T.int64(4) + ax2)
                            v3 = T.axis.spatial(T.int64(196), eps_0_nu_0_co_0_p_0_fused % T.int64(7) * T.int64(28) + eps_1_nu_1_co_1_p_1_fused * T.int64(14) + eps_2_nu_2_co_2_p_2_fused % T.int64(14) + ax3)
                            T.reads(bgemm_local[v0, v1, v2, v3])
                            T.writes(bgemm[v0, v1, v2, v3])
                            bgemm[v0, v1, v2, v3] = bgemm_local[v0, v1, v2, v3]
        for n_co_h_0_w_0_fused_0 in T.thread_binding(T.int64(196), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 1024, "pragma_unroll_explicit": 1}):
            for n_co_h_0_w_0_fused_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                    for ax2 in T.unroll(T.int64(2)):
                        for ax3 in T.unroll(T.int64(2)):
                            with T.block("inverse_init"):
                                v_co = T.axis.spatial(T.int64(128), (n_co_h_0_w_0_fused_0 * T.int64(128) + n_co_h_0_w_0_fused_1) // T.int64(196) + ax0)
                                v_p = T.axis.spatial(T.int64(196), (n_co_h_0_w_0_fused_0 * T.int64(128) + n_co_h_0_w_0_fused_1) % T.int64(196) + ax1)
                                v_vh, v_vw = T.axis.remap("SS", [ax2, ax3])
                                T.reads()
                                T.writes(inverse_local[v_co, v_p, v_vh, v_vw])
                                T.block_attr({"schedule_rule": "conv2d_nchw_winograd_inverse"})
                                inverse_local[v_co, v_p, v_vh, v_vw] = T.float32(0)
                            for ax4 in T.unroll(T.int64(4)):
                                for ax5 in T.unroll(T.int64(4)):
                                    with T.block("inverse_update"):
                                        v_co = T.axis.spatial(T.int64(128), (n_co_h_0_w_0_fused_0 * T.int64(128) + n_co_h_0_w_0_fused_1) // T.int64(196) + ax0)
                                        v_p = T.axis.spatial(T.int64(196), (n_co_h_0_w_0_fused_0 * T.int64(128) + n_co_h_0_w_0_fused_1) % T.int64(196) + ax1)
                                        v_vh, v_vw, v_r_a, v_r_b = T.axis.remap("SSRR", [ax2, ax3, ax4, ax5])
                                        T.reads(inverse_local[v_co, v_p, v_vh, v_vw], bgemm[v_r_a, v_r_b, v_co, v_p])
                                        T.writes(inverse_local[v_co, v_p, v_vh, v_vw])
                                        T.block_attr({"schedule_rule": "conv2d_nchw_winograd_inverse"})
                                        inverse_local[v_co, v_p, v_vh, v_vw] = inverse_local[v_co, v_p, v_vh, v_vw] + bgemm[v_r_a, v_r_b, v_co, v_p] * T.Select(v_r_a % T.int64(4) == T.int64(3) and v_vh % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_vh % T.int64(2) == T.int64(0), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_vh % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_vh % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_vh % T.int64(2) == T.int64(1), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_vh % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_vh % T.int64(2) == T.int64(1), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_vh % T.int64(2) == T.int64(0), T.float32(1), T.float32(0))))))))) * T.Select(v_r_b % T.int64(4) == T.int64(3) and v_vw % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_vw % T.int64(2) == T.int64(0), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_vw % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_vw % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_vw % T.int64(2) == T.int64(1), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_vw % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_vw % T.int64(2) == T.int64(1), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_vw % T.int64(2) == T.int64(0), T.float32(1), T.float32(0)))))))))
                for h_1, w_1 in T.grid(T.int64(2), T.int64(2)):
                    with T.block("conv2d_winograd"):
                        v_n = T.axis.spatial(T.int64(1), T.int64(0))
                        v_co = T.axis.spatial(T.int64(128), (n_co_h_0_w_0_fused_0 * T.int64(128) + n_co_h_0_w_0_fused_1) // T.int64(196))
                        v_h = T.axis.spatial(T.int64(28), (n_co_h_0_w_0_fused_0 * T.int64(128) + n_co_h_0_w_0_fused_1) % T.int64(196) // T.int64(14) * T.int64(2) + h_1)
                        v_w = T.axis.spatial(T.int64(28), (n_co_h_0_w_0_fused_0 * T.int64(128) + n_co_h_0_w_0_fused_1) % T.int64(14) * T.int64(2) + w_1)
                        T.reads(inverse_local[v_co, v_n * T.int64(196) + v_h // T.int64(2) * T.int64(14) + v_w // T.int64(2), v_h % T.int64(2), v_w % T.int64(2)], p2[v_n, v_co, T.int64(0), T.int64(0)])
                        T.writes(T_relu[v_n, v_co, v_h, v_w])
                        T_relu[v_n, v_co, v_h, v_w] = T.max(inverse_local[v_co, v_n * T.int64(196) + v_h // T.int64(2) * T.int64(14) + v_w // T.int64(2), v_h % T.int64(2), v_w % T.int64(2)] + p2[v_n, v_co, T.int64(0), T.int64(0)], T.float32(0))
b0 = sch.get_block(name="data_pack", func_name="main")
b1 = sch.get_block(name="bgemm", func_name="main")
b2 = sch.get_block(name="inverse", func_name="main")
b3 = sch.get_block(name="conv2d_winograd", func_name="main")
b4 = sch.get_block(name="T_add", func_name="main")
b5 = sch.get_block(name="T_relu", func_name="main")
b6 = sch.get_block(name="root", func_name="main")
b7, b8 = sch.get_producers(block=b2)
sch.compute_inline(block=b8)
b9, = sch.get_consumers(block=b2)
l10, l11, l12, l13 = sch.get_loops(block=b9)
l14, l15 = sch.split(loop=l12, factors=[None, 2], preserve_unit_iters=True)
l16, l17 = sch.split(loop=l13, factors=[None, 2], preserve_unit_iters=True)
sch.reorder(l14, l16, l15, l17)
sch.compute_at(block=b2, loop=l16, preserve_unit_loops=True, index=-1)
sch.set_scope(block=b2, buffer_index=0, storage_scope="local")
l18, l19, l20, l21, l22, l23, l24, l25, l26, l27 = sch.get_loops(block=b2)
sch.unroll(loop=l24)
sch.unroll(loop=l25)
sch.unroll(loop=l26)
sch.unroll(loop=l27)
b28, b29 = sch.get_producers(block=b0)
sch.compute_inline(block=b29)
b30, = sch.get_producers(block=b28)
l31, l32, l33, l34, l35, l36 = sch.get_loops(block=b0)
sch.reorder(l33, l34, l31, l32, l35, l36)
sch.unroll(loop=l31)
sch.unroll(loop=l32)
sch.unroll(loop=l35)
sch.unroll(loop=l36)
l37 = sch.fuse(l33, l34, preserve_unit_iters=True)
v38 = sch.sample_categorical(candidates=[32, 64, 128, 256, 512, 1024], probs=[0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666], decision=1)
l39, l40 = sch.split(loop=l37, factors=[None, v38], preserve_unit_iters=True)
sch.bind(loop=l39, thread_axis="blockIdx.x")
sch.bind(loop=l40, thread_axis="threadIdx.x")
b41 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b41, loop=l40, preserve_unit_loops=True, index=-1)
sch.compute_at(block=b28, loop=l40, preserve_unit_loops=True, index=-1)
sch.set_scope(block=b28, buffer_index=0, storage_scope="local")
sch.compute_inline(block=b30)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l42, l43, l44, l45, l46 = sch.get_loops(block=b1)
v47, v48, v49, v50, v51 = sch.sample_perfect_tile(loop=l42, n=5, max_innermost_factor=64, decision=[4, 1, 1, 1, 1])
l52, l53, l54, l55, l56 = sch.split(loop=l42, factors=[v47, v48, v49, v50, v51], preserve_unit_iters=True)
v57, v58, v59, v60, v61 = sch.sample_perfect_tile(loop=l43, n=5, max_innermost_factor=64, decision=[4, 1, 1, 1, 1])
l62, l63, l64, l65, l66 = sch.split(loop=l43, factors=[v57, v58, v59, v60, v61], preserve_unit_iters=True)
v67, v68, v69, v70, v71 = sch.sample_perfect_tile(loop=l44, n=5, max_innermost_factor=64, decision=[4, 1, 8, 1, 4])
l72, l73, l74, l75, l76 = sch.split(loop=l44, factors=[v67, v68, v69, v70, v71], preserve_unit_iters=True)
v77, v78, v79, v80, v81 = sch.sample_perfect_tile(loop=l45, n=5, max_innermost_factor=64, decision=[7, 2, 14, 1, 1])
l82, l83, l84, l85, l86 = sch.split(loop=l45, factors=[v77, v78, v79, v80, v81], preserve_unit_iters=True)
v87, v88, v89 = sch.sample_perfect_tile(loop=l46, n=3, max_innermost_factor=64, decision=[2, 16, 4])
l90, l91, l92 = sch.split(loop=l46, factors=[v87, v88, v89], preserve_unit_iters=True)
sch.reorder(l52, l62, l72, l82, l53, l63, l73, l83, l54, l64, l74, l84, l90, l91, l55, l65, l75, l85, l92, l56, l66, l76, l86)
l93 = sch.fuse(l52, l62, l72, l82, preserve_unit_iters=True)
sch.bind(loop=l93, thread_axis="blockIdx.x")
l94 = sch.fuse(l53, l63, l73, l83, preserve_unit_iters=True)
sch.bind(loop=l94, thread_axis="vthread.x")
l95 = sch.fuse(l54, l64, l74, l84, preserve_unit_iters=True)
sch.bind(loop=l95, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b96 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b96, loop=l95, preserve_unit_loops=True, index=-1)
b97 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b97, loop=l90, preserve_unit_loops=True, index=-1)
l98, l99, l100, l101, l102, l103, l104, l105 = sch.get_loops(block=b97)
l106 = sch.fuse(l102, l103, l104, l105, preserve_unit_iters=True)
v107 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b97, ann_key="meta_schedule.cooperative_fetch", ann_val=v107)
b108 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b108, loop=l90, preserve_unit_loops=True, index=-1)
l109, l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b108)
l117 = sch.fuse(l113, l114, l115, l116, preserve_unit_iters=True)
v118 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b108, ann_key="meta_schedule.cooperative_fetch", ann_val=v118)
l119 = sch.fuse(l90, preserve_unit_iters=True)
sch.annotate(block_or_loop=l119, ann_key="software_pipeline_stage", ann_val=[0, 0, 2])
sch.annotate(block_or_loop=l119, ann_key="software_pipeline_order", ann_val=[0, 1, 2])
sch.annotate(block_or_loop=l119, ann_key="software_pipeline_async_stages", ann_val=[0])
sch.reverse_compute_inline(block=b5)
sch.reverse_compute_inline(block=b4)
v120 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b6, ann_key="meta_schedule.unroll_explicit", ann_val=v120)
l121, l122, l123, l124, l125, l126 = sch.get_loops(block=b3)
l127 = sch.fuse(l121, l122, l123, l124, preserve_unit_iters=True)
v128 = sch.sample_categorical(candidates=[32, 64, 128, 256, 512, 1024], probs=[0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666], decision=2)
l129, l130 = sch.split(loop=l127, factors=[None, v128], preserve_unit_iters=True)
sch.bind(loop=l129, thread_axis="blockIdx.x")
sch.bind(loop=l130, thread_axis="threadIdx.x")
sch.enter_postproc()
sch.unannotate(block_or_loop=b97, ann_key="meta_schedule.cooperative_fetch")
l131, l132, l133, l134, l135 = sch.get_loops(block=b97)
l136, l137 = sch.split(loop=l135, factors=[None, 112], preserve_unit_iters=True)
sch.bind(loop=l137, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b108, ann_key="meta_schedule.cooperative_fetch")
l138, l139, l140, l141, l142 = sch.get_loops(block=b108)
l143, l144 = sch.split(loop=l142, factors=[None, 112], preserve_unit_iters=True)
sch.bind(loop=l144, thread_axis="threadIdx.x")
b145 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b145, ann_key="meta_schedule.unroll_explicit")
b146, b147, b148, b149, b150, b151, b152, b153, b154 = sch.get_child_blocks(b145)
l155, l156, l157, l158, l159, l160 = sch.get_loops(block=b146)
l161, l162, l163, l164, l165, l166 = sch.get_loops(block=b147)
sch.annotate(block_or_loop=l161, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l161, ann_key="pragma_unroll_explicit", ann_val=1)
l167, l168, l169, l170, l171, l172 = sch.get_loops(block=b148)
l173, l174, l175, l176, l177, l178 = sch.get_loops(block=b149)
l179, l180, l181, l182, l183, l184 = sch.get_loops(block=b150)
l185, l186, l187, l188, l189, l190, l191, l192, l193, l194, l195, l196, l197, l198 = sch.get_loops(block=b151)
sch.annotate(block_or_loop=l185, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l185, ann_key="pragma_unroll_explicit", ann_val=1)
l199, l200, l201, l202, l203, l204, l205 = sch.get_loops(block=b152)
l206, l207, l208, l209, l210, l211, l212, l213 = sch.get_loops(block=b153)
sch.annotate(block_or_loop=l206, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l206, ann_key="pragma_unroll_explicit", ann_val=1)
l214, l215, l216, l217 = sch.get_loops(block=b154)
b218 = sch.get_block(name="data_pack", func_name="main")
l219, l220, l221, l222, l223, l224 = sch.get_loops(block=b218)
b225 = sch.decompose_reduction(block=b218, loop=l223)
b226 = sch.get_block(name="bgemm", func_name="main")
l227, l228, l229, l230, l231, l232, l233, l234, l235, l236, l237, l238, l239, l240 = sch.get_loops(block=b226)
b241 = sch.decompose_reduction(block=b226, loop=l230)
b242 = sch.get_block(name="inverse", func_name="main")
l243, l244, l245, l246, l247, l248, l249, l250 = sch.get_loops(block=b242)
b251 = sch.decompose_reduction(block=b242, loop=l249)
2023-11-11 07:30:18 [INFO] [task_scheduler.cc:121] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #521: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/home2/xiachunwei/Software/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/home2/xiachunwei/Software/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/home2/xiachunwei/Software/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/home2/xiachunwei/Software/tvm/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/home2/xiachunwei/Software/tvm/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/home2/xiachunwei/Software/tvm/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/home2/xiachunwei/Software/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/home2/xiachunwei/Software/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  4: _ZN3tvm7runtime13PackedFun
  3: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  2: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  0: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  File "/home2/xiachunwei/Software/tvm/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(128), T.int64(28), T.int64(28)), "float32"), p1: T.Buffer((T.int64(4), T.int64(4), T.int64(128), T.int64(128)), "float32"), p2: T.Buffer((T.int64(1), T.int64(128), T.int64(1), T.int64(1)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(128), T.int64(28), T.int64(28)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        input_tile_local = T.alloc_buffer((T.int64(128), T.int64(196), T.int64(4), T.int64(4)), scope="local")
        data_pack = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(128), T.int64(196)))
        bgemm = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(128), T.int64(196)))
        inverse_local = T.alloc_buffer((T.int64(128), T.int64(196), T.int64(2), T.int64(2)), scope="local")
        data_pack_local = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(128), T.int64(196)), scope="local")
        bgemm_local = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(128), T.int64(196)), scope="local")
        data_pack_shared = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(128), T.int64(196)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(128), T.int64(128)), scope="shared")
        for ci_p_fused_0 in T.thread_binding(T.int64(392), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 1024, "pragma_unroll_explicit": 1}):
            for ci_p_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(1), T.int64(4), T.int64(4)):
                    with T.block("input_tile"):
                        v_ci = T.axis.spatial(T.int64(128), (ci_p_fused_0 * T.int64(64) + ci_p_fused_1) // T.int64(196) + ax0)
                        v_p = T.axis.spatial(T.int64(196), (ci_p_fused_0 * T.int64(64) + ci_p_fused_1) % T.int64(196) + ax1)
                        v_eps, v_nu = T.axis.remap("SS", [ax2, ax3])
                        T.reads(p0[v_p // T.int64(196), v_ci, v_p % T.int64(196) // T.int64(14) * T.int64(2) + v_eps - T.int64(1), v_p % T.int64(14) * T.int64(2) + v_nu - T.int64(1)])
                        T.writes(input_tile_local[v_ci, v_p, v_eps, v_nu])
                        T.block_attr({"schedule_rule": "None"})
                        input_tile_local[v_ci, v_p, v_eps, v_nu] = T.if_then_else(T.int64(1) <= v_p % T.int64(196) // T.int64(14) * T.int64(2) + v_eps and v_p % T.int64(196) // T.int64(14) * T.int64(2) + v_eps < T.int64(29) and T.int64(1) <= v_p % T.int64(14) * T.int64(2) + v_nu and v_p % T.int64(14) * T.int64(2) + v_nu < T.int64(29), p0[v_p // T.int64(196), v_ci, v_p % T.int64(196) // T.int64(14) * T.int64(2) + v_eps - T.int64(1), v_p % T.int64(14) * T.int64(2) + v_nu - T.int64(1)], T.float32(0))
                for eps in T.unroll(T.int64(4)):
                    for nu in T.unroll(T.int64(4)):
                        with T.block("data_pack_init"):
                            v_eps, v_nu = T.axis.remap("SS", [eps, nu])
                            v_ci = T.axis.spatial(T.int64(128), (ci_p_fused_0 * T.int64(64) + ci_p_fused_1) // T.int64(196))
                            v_p = T.axis.spatial(T.int64(196), (ci_p_fused_0 * T.int64(64) + ci_p_fused_1) % T.int64(196))
                            T.reads()
                            T.writes(data_pack_local[v_eps, v_nu, v_ci, v_p])
                            T.block_attr({"schedule_rule": "conv2d_nchw_winograd_data_pack"})
                            data_pack_local[v_eps, v_nu, v_ci, v_p] = T.float32(0)
                        for r_a in T.unroll(T.int64(4)):
                            for r_b in T.unroll(T.int64(4)):
                                with T.block("data_pack_update"):
                                    v_eps, v_nu = T.axis.remap("SS", [eps, nu])
                                    v_ci = T.axis.spatial(T.int64(128), (ci_p_fused_0 * T.int64(64) + ci_p_fused_1) // T.int64(196))
                                    v_p = T.axis.spatial(T.int64(196), (ci_p_fused_0 * T.int64(64) + ci_p_fused_1) % T.int64(196))
                                    v_r_a, v_r_b = T.axis.remap("RR", [r_a, r_b])
                                    T.reads(data_pack_local[v_eps, v_nu, v_ci, v_p], input_tile_local[v_ci, v_p, v_r_a, v_r_b])
                                    T.writes(data_pack_local[v_eps, v_nu, v_ci, v_p])
                                    T.block_attr({"schedule_rule": "conv2d_nchw_winograd_data_pack"})
                                    data_pack_local[v_eps, v_nu, v_ci, v_p] = data_pack_local[v_eps, v_nu, v_ci, v_p] + input_tile_local[v_ci, v_p, v_r_a, v_r_b] * T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(3), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(1), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(0), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(3), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(1), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(0), T.float32(1), T.float32(0))))))))))))))))) * T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(3), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(1), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(0), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(3), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(1), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(0), T.float32(1), T.float32(0)))))))))))))))))
                for ax0, ax1, ax2, ax3 in T.grid(T.int64(4), T.int64(4), T.int64(1), T.int64(1)):
                    with T.block("data_pack_local"):
                        v0, v1 = T.axis.remap("SS", [ax0, ax1])
                        v2 = T.axis.spatial(T.int64(128), (ci_p_fused_0 * T.int64(64) + ci_p_fused_1) // T.int64(196) + ax2)
                        v3 = T.axis.spatial(T.int64(196), (ci_p_fused_0 * T.int64(64) + ci_p_fused_1) % T.int64(196) + ax3)
                        T.reads(data_pack_local[v0, v1, v2, v3])
                        T.writes(data_pack[v0, v1, v2, v3])
                        data_pack[v0, v1, v2, v3] = data_pack_local[v0, v1, v2, v3]
        for eps_0_nu_0_co_0_p_0_fused in T.thread_binding(T.int64(448), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 1024, "pragma_unroll_explicit": 1}):
            for eps_1_nu_1_co_1_p_1_fused in T.thread_binding(T.int64(2), thread="vthread.x"):
                for eps_2_nu_2_co_2_p_2_fused in T.thread_binding(T.int64(112), thread="threadIdx.x"):
                    for eps_3_init, nu_3_init, co_3_init, p_3_init, eps_4_init, nu_4_init, co_4_init, p_4_init in T.grid(T.int64(1), T.int64(1), T.int64(2), T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(1)):
                        with T.block("bgemm_init"):
                            v_eps = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(112) + eps_3_init + eps_4_init)
                            v_nu = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused % T.int64(112) // T.int64(28) + nu_3_init + nu_4_init)
                            v_co = T.axis.spatial(T.int64(128), eps_0_nu_0_co_0_p_0_fused % T.int64(28) // T.int64(7) * T.int64(32) + eps_2_nu_2_co_2_p_2_fused // T.int64(14) * T.int64(4) + co_3_init * T.int64(2) + co_4_init)
                            v_p = T.axis.spatial(T.int64(196), eps_0_nu_0_co_0_p_0_fused % T.int64(7) * T.int64(28) + eps_1_nu_1_co_1_p_1_fused * T.int64(14) + eps_2_nu_2_co_2_p_2_fused % T.int64(14) + p_3_init + p_4_init)
                            T.reads()
                            T.writes(bgemm_local[v_eps, v_nu, v_co, v_p])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            bgemm_local[v_eps, v_nu, v_co, v_p] = T.float32(0)
                    for ci_0_fused in T.serial(T.int64(2), annotations={"software_pipeline_async_stages": [0], "software_pipeline_order": [0, 1, 2], "software_pipeline_stage": [0, 0, 2]}):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(8)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(112), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("data_pack_shared"):
                                        v0 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(112))
                                        v1 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused % T.int64(112) // T.int64(28))
                                        v2 = T.axis.spatial(T.int64(128), ci_0_fused * T.int64(64) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(224) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) // T.int64(28))
                                        v3 = T.axis.spatial(T.int64(196), eps_0_nu_0_co_0_p_0_fused % T.int64(7) * T.int64(28) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(224) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) % T.int64(28))
                                        T.reads(data_pack[v0, v1, v2, v3])
                                        T.writes(data_pack_shared[v0, v1, v2, v3])
                                        data_pack_shared[v0, v1, v2, v3] = data_pack[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(10)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(112), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("p1_shared"):
                                        v0 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(112))
                                        v1 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused % T.int64(112) // T.int64(28))
                                        v2 = T.axis.spatial(T.int64(128), ci_0_fused * T.int64(64) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(224) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) // T.int64(32))
                                        v3 = T.axis.spatial(T.int64(128), eps_0_nu_0_co_0_p_0_fused % T.int64(28) // T.int64(7) * T.int64(32) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(224) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) % T.int64(32))
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * T.int64(112) + ax0_ax1_ax2_ax3_fused_1) * T.int64(2) + ax0_ax1_ax2_ax3_fused_2 < T.int64(2048))
                                        T.reads(p1[v0, v1, v2, v3])
                                        T.writes(p1_shared[v0, v1, v2, v3])
                                        p1_shared[v0, v1, v2, v3] = p1[v0, v1, v2, v3]
                        for ci_1, eps_3, nu_3, co_3, p_3, ci_2, eps_4, nu_4, co_4, p_4 in T.grid(T.int64(4), T.int64(1), T.int64(1), T.int64(2), T.int64(1), T.int64(16), T.int64(1), T.int64(1), T.int64(2), T.int64(1)):
                            with T.block("bgemm_update"):
                                v_eps = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(112) + eps_3 + eps_4)
                                v_nu = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused % T.int64(112) // T.int64(28) + nu_3 + nu_4)
                                v_co = T.axis.spatial(T.int64(128), eps_0_nu_0_co_0_p_0_fused % T.int64(28) // T.int64(7) * T.int64(32) + eps_2_nu_2_co_2_p_2_fused // T.int64(14) * T.int64(4) + co_3 * T.int64(2) + co_4)
                                v_p = T.axis.spatial(T.int64(196), eps_0_nu_0_co_0_p_0_fused % T.int64(7) * T.int64(28) + eps_1_nu_1_co_1_p_1_fused * T.int64(14) + eps_2_nu_2_co_2_p_2_fused % T.int64(14) + p_3 + p_4)
                                v_ci = T.axis.reduce(T.int64(128), ci_0_fused * T.int64(64) + ci_1 * T.int64(16) + ci_2)
                                T.reads(bgemm_local[v_eps, v_nu, v_co, v_p], data_pack_shared[v_eps, v_nu, v_ci, v_p], p1_shared[v_eps, v_nu, v_ci, v_co])
                                T.writes(bgemm_local[v_eps, v_nu, v_co, v_p])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                bgemm_local[v_eps, v_nu, v_co, v_p] = bgemm_local[v_eps, v_nu, v_co, v_p] + data_pack_shared[v_eps, v_nu, v_ci, v_p] * p1_shared[v_eps, v_nu, v_ci, v_co]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(1), T.int64(4), T.int64(1)):
                        with T.block("bgemm_local"):
                            v0 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(112) + ax0)
                            v1 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused % T.int64(112) // T.int64(28) + ax1)
                            v2 = T.axis.spatial(T.int64(128), eps_0_nu_0_co_0_p_0_fused % T.int64(28) // T.int64(7) * T.int64(32) + eps_2_nu_2_co_2_p_2_fused // T.int64(14) * T.int64(4) + ax2)
                            v3 = T.axis.spatial(T.int64(196), eps_0_nu_0_co_0_p_0_fused % T.int64(7) * T.int64(28) + eps_1_nu_1_co_1_p_1_fused * T.int64(14) + eps_2_nu_2_co_2_p_2_fused % T.int64(14) + ax3)
                            T.reads(bgemm_local[v0, v1, v2, v3])
                            T.writes(bgemm[v0, v1, v2, v3])
                            bgemm[v0, v1, v2, v3] = bgemm_local[v0, v1, v2, v3]
        for n_co_h_0_w_0_fused_0 in T.thread_binding(T.int64(392), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 1024, "pragma_unroll_explicit": 1}):
            for n_co_h_0_w_0_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                    for ax2 in T.unroll(T.int64(2)):
                        for ax3 in T.unroll(T.int64(2)):
                            with T.block("inverse_init"):
                                v_co = T.axis.spatial(T.int64(128), (n_co_h_0_w_0_fused_0 * T.int64(64) + n_co_h_0_w_0_fused_1) // T.int64(196) + ax0)
                                v_p = T.axis.spatial(T.int64(196), (n_co_h_0_w_0_fused_0 * T.int64(64) + n_co_h_0_w_0_fused_1) % T.int64(196) + ax1)
                                v_vh, v_vw = T.axis.remap("SS", [ax2, ax3])
                                T.reads()
                                T.writes(inverse_local[v_co, v_p, v_vh, v_vw])
                                T.block_attr({"schedule_rule": "conv2d_nchw_winograd_inverse"})
                                inverse_local[v_co, v_p, v_vh, v_vw] = T.float32(0)
                            for ax4 in T.unroll(T.int64(4)):
                                for ax5 in T.unroll(T.int64(4)):
                                    with T.block("inverse_update"):
                                        v_co = T.axis.spatial(T.int64(128), (n_co_h_0_w_0_fused_0 * T.int64(64) + n_co_h_0_w_0_fused_1) // T.int64(196) + ax0)
                                        v_p = T.axis.spatial(T.int64(196), (n_co_h_0_w_0_fused_0 * T.int64(64) + n_co_h_0_w_0_fused_1) % T.int64(196) + ax1)
                                        v_vh, v_vw, v_r_a, v_r_b = T.axis.remap("SSRR", [ax2, ax3, ax4, ax5])
                                        T.reads(inverse_local[v_co, v_p, v_vh, v_vw], bgemm[v_r_a, v_r_b, v_co, v_p])
                                        T.writes(inverse_local[v_co, v_p, v_vh, v_vw])
                                        T.block_attr({"schedule_rule": "conv2d_nchw_winograd_inverse"})
                                        inverse_local[v_co, v_p, v_vh, v_vw] = inverse_local[v_co, v_p, v_vh, v_vw] + bgemm[v_r_a, v_r_b, v_co, v_p] * T.Select(v_r_a % T.int64(4) == T.int64(3) and v_vh % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_vh % T.int64(2) == T.int64(0), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_vh % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_vh % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_vh % T.int64(2) == T.int64(1), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_vh % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_vh % T.int64(2) == T.int64(1), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_vh % T.int64(2) == T.int64(0), T.float32(1), T.float32(0))))))))) * T.Select(v_r_b % T.int64(4) == T.int64(3) and v_vw % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_vw % T.int64(2) == T.int64(0), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_vw % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_vw % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_vw % T.int64(2) == T.int64(1), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_vw % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_vw % T.int64(2) == T.int64(1), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_vw % T.int64(2) == T.int64(0), T.float32(1), T.float32(0)))))))))
                for h_1, w_1 in T.grid(T.int64(2), T.int64(2)):
                    with T.block("conv2d_winograd"):
                        v_n = T.axis.spatial(T.int64(1), T.int64(0))
                        v_co = T.axis.spatial(T.int64(128), (n_co_h_0_w_0_fused_0 * T.int64(64) + n_co_h_0_w_0_fused_1) // T.int64(196))
                        v_h = T.axis.spatial(T.int64(28), (n_co_h_0_w_0_fused_0 * T.int64(64) + n_co_h_0_w_0_fused_1) % T.int64(196) // T.int64(14) * T.int64(2) + h_1)
                        v_w = T.axis.spatial(T.int64(28), (n_co_h_0_w_0_fused_0 * T.int64(64) + n_co_h_0_w_0_fused_1) % T.int64(14) * T.int64(2) + w_1)
                        T.reads(inverse_local[v_co, v_n * T.int64(196) + v_h // T.int64(2) * T.int64(14) + v_w // T.int64(2), v_h % T.int64(2), v_w % T.int64(2)], p2[v_n, v_co, T.int64(0), T.int64(0)])
                        T.writes(T_relu[v_n, v_co, v_h, v_w])
                        T_relu[v_n, v_co, v_h, v_w] = T.max(inverse_local[v_co, v_n * T.int64(196) + v_h // T.int64(2) * T.int64(14) + v_w // T.int64(2), v_h % T.int64(2), v_w % T.int64(2)] + p2[v_n, v_co, T.int64(0), T.int64(0)], T.float32(0))
b0 = sch.get_block(name="data_pack", func_name="main")
b1 = sch.get_block(name="bgemm", func_name="main")
b2 = sch.get_block(name="inverse", func_name="main")
b3 = sch.get_block(name="conv2d_winograd", func_name="main")
b4 = sch.get_block(name="T_add", func_name="main")
b5 = sch.get_block(name="T_relu", func_name="main")
b6 = sch.get_block(name="root", func_name="main")
b7, b8 = sch.get_producers(block=b2)
sch.compute_inline(block=b8)
b9, = sch.get_consumers(block=b2)
l10, l11, l12, l13 = sch.get_loops(block=b9)
l14, l15 = sch.split(loop=l12, factors=[None, 2], preserve_unit_iters=True)
l16, l17 = sch.split(loop=l13, factors=[None, 2], preserve_unit_iters=True)
sch.reorder(l14, l16, l15, l17)
sch.compute_at(block=b2, loop=l16, preserve_unit_loops=True, index=-1)
sch.set_scope(block=b2, buffer_index=0, storage_scope="local")
l18, l19, l20, l21, l22, l23, l24, l25, l26, l27 = sch.get_loops(block=b2)
sch.unroll(loop=l24)
sch.unroll(loop=l25)
sch.unroll(loop=l26)
sch.unroll(loop=l27)
b28, b29 = sch.get_producers(block=b0)
sch.compute_inline(block=b29)
b30, = sch.get_producers(block=b28)
l31, l32, l33, l34, l35, l36 = sch.get_loops(block=b0)
sch.reorder(l33, l34, l31, l32, l35, l36)
sch.unroll(loop=l31)
sch.unroll(loop=l32)
sch.unroll(loop=l35)
sch.unroll(loop=l36)
l37 = sch.fuse(l33, l34, preserve_unit_iters=True)
v38 = sch.sample_categorical(candidates=[32, 64, 128, 256, 512, 1024], probs=[0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666], decision=1)
l39, l40 = sch.split(loop=l37, factors=[None, v38], preserve_unit_iters=True)
sch.bind(loop=l39, thread_axis="blockIdx.x")
sch.bind(loop=l40, thread_axis="threadIdx.x")
b41 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b41, loop=l40, preserve_unit_loops=True, index=-1)
sch.compute_at(block=b28, loop=l40, preserve_unit_loops=True, index=-1)
sch.set_scope(block=b28, buffer_index=0, storage_scope="local")
sch.compute_inline(block=b30)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l42, l43, l44, l45, l46 = sch.get_loops(block=b1)
v47, v48, v49, v50, v51 = sch.sample_perfect_tile(loop=l42, n=5, max_innermost_factor=64, decision=[4, 1, 1, 1, 1])
l52, l53, l54, l55, l56 = sch.split(loop=l42, factors=[v47, v48, v49, v50, v51], preserve_unit_iters=True)
v57, v58, v59, v60, v61 = sch.sample_perfect_tile(loop=l43, n=5, max_innermost_factor=64, decision=[4, 1, 1, 1, 1])
l62, l63, l64, l65, l66 = sch.split(loop=l43, factors=[v57, v58, v59, v60, v61], preserve_unit_iters=True)
v67, v68, v69, v70, v71 = sch.sample_perfect_tile(loop=l44, n=5, max_innermost_factor=64, decision=[4, 1, 8, 2, 2])
l72, l73, l74, l75, l76 = sch.split(loop=l44, factors=[v67, v68, v69, v70, v71], preserve_unit_iters=True)
v77, v78, v79, v80, v81 = sch.sample_perfect_tile(loop=l45, n=5, max_innermost_factor=64, decision=[7, 2, 14, 1, 1])
l82, l83, l84, l85, l86 = sch.split(loop=l45, factors=[v77, v78, v79, v80, v81], preserve_unit_iters=True)
v87, v88, v89 = sch.sample_perfect_tile(loop=l46, n=3, max_innermost_factor=64, decision=[2, 4, 16])
l90, l91, l92 = sch.split(loop=l46, factors=[v87, v88, v89], preserve_unit_iters=True)
sch.reorder(l52, l62, l72, l82, l53, l63, l73, l83, l54, l64, l74, l84, l90, l91, l55, l65, l75, l85, l92, l56, l66, l76, l86)
l93 = sch.fuse(l52, l62, l72, l82, preserve_unit_iters=True)
sch.bind(loop=l93, thread_axis="blockIdx.x")
l94 = sch.fuse(l53, l63, l73, l83, preserve_unit_iters=True)
sch.bind(loop=l94, thread_axis="vthread.x")
l95 = sch.fuse(l54, l64, l74, l84, preserve_unit_iters=True)
sch.bind(loop=l95, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b96 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b96, loop=l95, preserve_unit_loops=True, index=-1)
b97 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b97, loop=l90, preserve_unit_loops=True, index=-1)
l98, l99, l100, l101, l102, l103, l104, l105 = sch.get_loops(block=b97)
l106 = sch.fuse(l102, l103, l104, l105, preserve_unit_iters=True)
v107 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b97, ann_key="meta_schedule.cooperative_fetch", ann_val=v107)
b108 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b108, loop=l90, preserve_unit_loops=True, index=-1)
l109, l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b108)
l117 = sch.fuse(l113, l114, l115, l116, preserve_unit_iters=True)
v118 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b108, ann_key="meta_schedule.cooperative_fetch", ann_val=v118)
l119 = sch.fuse(l90, preserve_unit_iters=True)
sch.annotate(block_or_loop=l119, ann_key="software_pipeline_stage", ann_val=[0, 0, 2])
sch.annotate(block_or_loop=l119, ann_key="software_pipeline_order", ann_val=[0, 1, 2])
sch.annotate(block_or_loop=l119, ann_key="software_pipeline_async_stages", ann_val=[0])
sch.reverse_compute_inline(block=b5)
sch.reverse_compute_inline(block=b4)
v120 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b6, ann_key="meta_schedule.unroll_explicit", ann_val=v120)
l121, l122, l123, l124, l125, l126 = sch.get_loops(block=b3)
l127 = sch.fuse(l121, l122, l123, l124, preserve_unit_iters=True)
v128 = sch.sample_categorical(candidates=[32, 64, 128, 256, 512, 1024], probs=[0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666], decision=1)
l129, l130 = sch.split(loop=l127, factors=[None, v128], preserve_unit_iters=True)
sch.bind(loop=l129, thread_axis="blockIdx.x")
sch.bind(loop=l130, thread_axis="threadIdx.x")
sch.enter_postproc()
sch.unannotate(block_or_loop=b97, ann_key="meta_schedule.cooperative_fetch")
l131, l132, l133, l134, l135 = sch.get_loops(block=b97)
l136, l137, l138 = sch.split(loop=l135, factors=[None, 112, 2], preserve_unit_iters=True)
sch.vectorize(loop=l138)
sch.bind(loop=l137, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b108, ann_key="meta_schedule.cooperative_fetch")
l139, l140, l141, l142, l143 = sch.get_loops(block=b108)
l144, l145, l146 = sch.split(loop=l143, factors=[None, 112, 2], preserve_unit_iters=True)
sch.vectorize(loop=l146)
sch.bind(loop=l145, thread_axis="threadIdx.x")
b147 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b147, ann_key="meta_schedule.unroll_explicit")
b148, b149, b150, b151, b152, b153, b154, b155, b156 = sch.get_child_blocks(b147)
l157, l158, l159, l160, l161, l162 = sch.get_loops(block=b148)
l163, l164, l165, l166, l167, l168 = sch.get_loops(block=b149)
sch.annotate(block_or_loop=l163, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l163, ann_key="pragma_unroll_explicit", ann_val=1)
l169, l170, l171, l172, l173, l174 = sch.get_loops(block=b150)
l175, l176, l177, l178, l179, l180, l181 = sch.get_loops(block=b151)
l182, l183, l184, l185, l186, l187, l188 = sch.get_loops(block=b152)
l189, l190, l191, l192, l193, l194, l195, l196, l197, l198, l199, l200, l201, l202 = sch.get_loops(block=b153)
sch.annotate(block_or_loop=l189, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l189, ann_key="pragma_unroll_explicit", ann_val=1)
l203, l204, l205, l206, l207, l208, l209 = sch.get_loops(block=b154)
l210, l211, l212, l213, l214, l215, l216, l217 = sch.get_loops(block=b155)
sch.annotate(block_or_loop=l210, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l210, ann_key="pragma_unroll_explicit", ann_val=1)
l218, l219, l220, l221 = sch.get_loops(block=b156)
b222 = sch.get_block(name="data_pack", func_name="main")
l223, l224, l225, l226, l227, l228 = sch.get_loops(block=b222)
b229 = sch.decompose_reduction(block=b222, loop=l227)
b230 = sch.get_block(name="bgemm", func_name="main")
l231, l232, l233, l234, l235, l236, l237, l238, l239, l240, l241, l242, l243, l244 = sch.get_loops(block=b230)
b245 = sch.decompose_reduction(block=b230, loop=l234)
b246 = sch.get_block(name="inverse", func_name="main")
l247, l248, l249, l250, l251, l252, l253, l254 = sch.get_loops(block=b246)
b255 = sch.decompose_reduction(block=b246, loop=l253)
2023-11-11 07:30:18 [INFO] [task_scheduler.cc:121] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #522: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/home2/xiachunwei/Software/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/home2/xiachunwei/Software/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/home2/xiachunwei/Software/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/home2/xiachunwei/Software/tvm/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/home2/xiachunwei/Software/tvm/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/home2/xiachunwei/Software/tvm/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/home2/xiachunwei/Software/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/home2/xiachunwei/Software/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  4: _ZN3tvm7runtime13PackedFun
  3: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  2: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  0: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  File "/home2/xiachunwei/Software/tvm/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(128), T.int64(28), T.int64(28)), "float32"), p1: T.Buffer((T.int64(4), T.int64(4), T.int64(128), T.int64(128)), "float32"), p2: T.Buffer((T.int64(1), T.int64(128), T.int64(1), T.int64(1)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(128), T.int64(28), T.int64(28)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        input_tile_local = T.alloc_buffer((T.int64(128), T.int64(196), T.int64(4), T.int64(4)), scope="local")
        data_pack = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(128), T.int64(196)))
        bgemm = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(128), T.int64(196)))
        inverse_local = T.alloc_buffer((T.int64(128), T.int64(196), T.int64(2), T.int64(2)), scope="local")
        data_pack_local = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(128), T.int64(196)), scope="local")
        bgemm_local = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(128), T.int64(196)), scope="local")
        data_pack_shared = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(128), T.int64(196)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(128), T.int64(128)), scope="shared")
        for ci_p_fused_0 in T.thread_binding(T.int64(392), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 1024, "pragma_unroll_explicit": 1}):
            for ci_p_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(1), T.int64(4), T.int64(4)):
                    with T.block("input_tile"):
                        v_ci = T.axis.spatial(T.int64(128), (ci_p_fused_0 * T.int64(64) + ci_p_fused_1) // T.int64(196) + ax0)
                        v_p = T.axis.spatial(T.int64(196), (ci_p_fused_0 * T.int64(64) + ci_p_fused_1) % T.int64(196) + ax1)
                        v_eps, v_nu = T.axis.remap("SS", [ax2, ax3])
                        T.reads(p0[v_p // T.int64(196), v_ci, v_p % T.int64(196) // T.int64(14) * T.int64(2) + v_eps - T.int64(1), v_p % T.int64(14) * T.int64(2) + v_nu - T.int64(1)])
                        T.writes(input_tile_local[v_ci, v_p, v_eps, v_nu])
                        T.block_attr({"schedule_rule": "None"})
                        input_tile_local[v_ci, v_p, v_eps, v_nu] = T.if_then_else(T.int64(1) <= v_p % T.int64(196) // T.int64(14) * T.int64(2) + v_eps and v_p % T.int64(196) // T.int64(14) * T.int64(2) + v_eps < T.int64(29) and T.int64(1) <= v_p % T.int64(14) * T.int64(2) + v_nu and v_p % T.int64(14) * T.int64(2) + v_nu < T.int64(29), p0[v_p // T.int64(196), v_ci, v_p % T.int64(196) // T.int64(14) * T.int64(2) + v_eps - T.int64(1), v_p % T.int64(14) * T.int64(2) + v_nu - T.int64(1)], T.float32(0))
                for eps in T.unroll(T.int64(4)):
                    for nu in T.unroll(T.int64(4)):
                        with T.block("data_pack_init"):
                            v_eps, v_nu = T.axis.remap("SS", [eps, nu])
                            v_ci = T.axis.spatial(T.int64(128), (ci_p_fused_0 * T.int64(64) + ci_p_fused_1) // T.int64(196))
                            v_p = T.axis.spatial(T.int64(196), (ci_p_fused_0 * T.int64(64) + ci_p_fused_1) % T.int64(196))
                            T.reads()
                            T.writes(data_pack_local[v_eps, v_nu, v_ci, v_p])
                            T.block_attr({"schedule_rule": "conv2d_nchw_winograd_data_pack"})
                            data_pack_local[v_eps, v_nu, v_ci, v_p] = T.float32(0)
                        for r_a in T.unroll(T.int64(4)):
                            for r_b in T.unroll(T.int64(4)):
                                with T.block("data_pack_update"):
                                    v_eps, v_nu = T.axis.remap("SS", [eps, nu])
                                    v_ci = T.axis.spatial(T.int64(128), (ci_p_fused_0 * T.int64(64) + ci_p_fused_1) // T.int64(196))
                                    v_p = T.axis.spatial(T.int64(196), (ci_p_fused_0 * T.int64(64) + ci_p_fused_1) % T.int64(196))
                                    v_r_a, v_r_b = T.axis.remap("RR", [r_a, r_b])
                                    T.reads(data_pack_local[v_eps, v_nu, v_ci, v_p], input_tile_local[v_ci, v_p, v_r_a, v_r_b])
                                    T.writes(data_pack_local[v_eps, v_nu, v_ci, v_p])
                                    T.block_attr({"schedule_rule": "conv2d_nchw_winograd_data_pack"})
                                    data_pack_local[v_eps, v_nu, v_ci, v_p] = data_pack_local[v_eps, v_nu, v_ci, v_p] + input_tile_local[v_ci, v_p, v_r_a, v_r_b] * T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(3), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(1), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(0), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(3), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(1), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(0), T.float32(1), T.float32(0))))))))))))))))) * T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(3), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(1), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(0), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(3), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(1), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(0), T.float32(1), T.float32(0)))))))))))))))))
                for ax0, ax1, ax2, ax3 in T.grid(T.int64(4), T.int64(4), T.int64(1), T.int64(1)):
                    with T.block("data_pack_local"):
                        v0, v1 = T.axis.remap("SS", [ax0, ax1])
                        v2 = T.axis.spatial(T.int64(128), (ci_p_fused_0 * T.int64(64) + ci_p_fused_1) // T.int64(196) + ax2)
                        v3 = T.axis.spatial(T.int64(196), (ci_p_fused_0 * T.int64(64) + ci_p_fused_1) % T.int64(196) + ax3)
                        T.reads(data_pack_local[v0, v1, v2, v3])
                        T.writes(data_pack[v0, v1, v2, v3])
                        data_pack[v0, v1, v2, v3] = data_pack_local[v0, v1, v2, v3]
        for eps_0_nu_0_co_0_p_0_fused in T.thread_binding(T.int64(448), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 1024, "pragma_unroll_explicit": 1}):
            for eps_1_nu_1_co_1_p_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for eps_2_nu_2_co_2_p_2_fused in T.thread_binding(T.int64(112), thread="threadIdx.x"):
                    for eps_3_init, nu_3_init, co_3_init, p_3_init, eps_4_init, nu_4_init, co_4_init, p_4_init in T.grid(T.int64(1), T.int64(1), T.int64(2), T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(2)):
                        with T.block("bgemm_init"):
                            v_eps = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(112) + eps_3_init + eps_4_init)
                            v_nu = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused % T.int64(112) // T.int64(28) + nu_3_init + nu_4_init)
                            v_co = T.axis.spatial(T.int64(128), eps_0_nu_0_co_0_p_0_fused % T.int64(28) // T.int64(7) * T.int64(32) + eps_2_nu_2_co_2_p_2_fused // T.int64(14) * T.int64(4) + co_3_init * T.int64(2) + co_4_init)
                            v_p = T.axis.spatial(T.int64(196), eps_0_nu_0_co_0_p_0_fused % T.int64(7) * T.int64(28) + eps_2_nu_2_co_2_p_2_fused % T.int64(14) * T.int64(2) + p_3_init * T.int64(2) + p_4_init)
                            T.reads()
                            T.writes(bgemm_local[v_eps, v_nu, v_co, v_p])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            bgemm_local[v_eps, v_nu, v_co, v_p] = T.float32(0)
                    for ci_0_fused in T.serial(T.int64(2), annotations={"software_pipeline_async_stages": [0], "software_pipeline_order": [0, 1, 2], "software_pipeline_stage": [0, 0, 2]}):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(16)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(112), thread="threadIdx.x"):
                                with T.block("data_pack_shared"):
                                    v0 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(112))
                                    v1 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused % T.int64(112) // T.int64(28))
                                    v2 = T.axis.spatial(T.int64(128), ci_0_fused * T.int64(64) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(112) + ax0_ax1_ax2_ax3_fused_1) // T.int64(28))
                                    v3 = T.axis.spatial(T.int64(196), eps_0_nu_0_co_0_p_0_fused % T.int64(7) * T.int64(28) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(112) + ax0_ax1_ax2_ax3_fused_1) % T.int64(28))
                                    T.reads(data_pack[v0, v1, v2, v3])
                                    T.writes(data_pack_shared[v0, v1, v2, v3])
                                    data_pack_shared[v0, v1, v2, v3] = data_pack[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(5)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(112), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("p1_shared"):
                                        v0 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(112))
                                        v1 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused % T.int64(112) // T.int64(28))
                                        v2 = T.axis.spatial(T.int64(128), ci_0_fused * T.int64(64) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(448) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) // T.int64(32))
                                        v3 = T.axis.spatial(T.int64(128), eps_0_nu_0_co_0_p_0_fused % T.int64(28) // T.int64(7) * T.int64(32) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(448) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) % T.int64(32))
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * T.int64(112) + ax0_ax1_ax2_ax3_fused_1) * T.int64(4) + ax0_ax1_ax2_ax3_fused_2 < T.int64(2048))
                                        T.reads(p1[v0, v1, v2, v3])
                                        T.writes(p1_shared[v0, v1, v2, v3])
                                        p1_shared[v0, v1, v2, v3] = p1[v0, v1, v2, v3]
                        for ci_1, eps_3, nu_3, co_3, p_3, ci_2, eps_4, nu_4, co_4, p_4 in T.grid(T.int64(64), T.int64(1), T.int64(1), T.int64(2), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(2)):
                            with T.block("bgemm_update"):
                                v_eps = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(112) + eps_3 + eps_4)
                                v_nu = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused % T.int64(112) // T.int64(28) + nu_3 + nu_4)
                                v_co = T.axis.spatial(T.int64(128), eps_0_nu_0_co_0_p_0_fused % T.int64(28) // T.int64(7) * T.int64(32) + eps_2_nu_2_co_2_p_2_fused // T.int64(14) * T.int64(4) + co_3 * T.int64(2) + co_4)
                                v_p = T.axis.spatial(T.int64(196), eps_0_nu_0_co_0_p_0_fused % T.int64(7) * T.int64(28) + eps_2_nu_2_co_2_p_2_fused % T.int64(14) * T.int64(2) + p_3 * T.int64(2) + p_4)
                                v_ci = T.axis.reduce(T.int64(128), ci_0_fused * T.int64(64) + ci_1 + ci_2)
                                T.reads(bgemm_local[v_eps, v_nu, v_co, v_p], data_pack_shared[v_eps, v_nu, v_ci, v_p], p1_shared[v_eps, v_nu, v_ci, v_co])
                                T.writes(bgemm_local[v_eps, v_nu, v_co, v_p])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                bgemm_local[v_eps, v_nu, v_co, v_p] = bgemm_local[v_eps, v_nu, v_co, v_p] + data_pack_shared[v_eps, v_nu, v_ci, v_p] * p1_shared[v_eps, v_nu, v_ci, v_co]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(1), T.int64(4), T.int64(2)):
                        with T.block("bgemm_local"):
                            v0 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(112) + ax0)
                            v1 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused % T.int64(112) // T.int64(28) + ax1)
                            v2 = T.axis.spatial(T.int64(128), eps_0_nu_0_co_0_p_0_fused % T.int64(28) // T.int64(7) * T.int64(32) + eps_2_nu_2_co_2_p_2_fused // T.int64(14) * T.int64(4) + ax2)
                            v3 = T.axis.spatial(T.int64(196), eps_0_nu_0_co_0_p_0_fused % T.int64(7) * T.int64(28) + eps_2_nu_2_co_2_p_2_fused % T.int64(14) * T.int64(2) + ax3)
                            T.reads(bgemm_local[v0, v1, v2, v3])
                            T.writes(bgemm[v0, v1, v2, v3])
                            bgemm[v0, v1, v2, v3] = bgemm_local[v0, v1, v2, v3]
        for n_co_h_0_w_0_fused_0 in T.thread_binding(T.int64(196), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 1024, "pragma_unroll_explicit": 1}):
            for n_co_h_0_w_0_fused_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                    for ax2 in T.unroll(T.int64(2)):
                        for ax3 in T.unroll(T.int64(2)):
                            with T.block("inverse_init"):
                                v_co = T.axis.spatial(T.int64(128), (n_co_h_0_w_0_fused_0 * T.int64(128) + n_co_h_0_w_0_fused_1) // T.int64(196) + ax0)
                                v_p = T.axis.spatial(T.int64(196), (n_co_h_0_w_0_fused_0 * T.int64(128) + n_co_h_0_w_0_fused_1) % T.int64(196) + ax1)
                                v_vh, v_vw = T.axis.remap("SS", [ax2, ax3])
                                T.reads()
                                T.writes(inverse_local[v_co, v_p, v_vh, v_vw])
                                T.block_attr({"schedule_rule": "conv2d_nchw_winograd_inverse"})
                                inverse_local[v_co, v_p, v_vh, v_vw] = T.float32(0)
                            for ax4 in T.unroll(T.int64(4)):
                                for ax5 in T.unroll(T.int64(4)):
                                    with T.block("inverse_update"):
                                        v_co = T.axis.spatial(T.int64(128), (n_co_h_0_w_0_fused_0 * T.int64(128) + n_co_h_0_w_0_fused_1) // T.int64(196) + ax0)
                                        v_p = T.axis.spatial(T.int64(196), (n_co_h_0_w_0_fused_0 * T.int64(128) + n_co_h_0_w_0_fused_1) % T.int64(196) + ax1)
                                        v_vh, v_vw, v_r_a, v_r_b = T.axis.remap("SSRR", [ax2, ax3, ax4, ax5])
                                        T.reads(inverse_local[v_co, v_p, v_vh, v_vw], bgemm[v_r_a, v_r_b, v_co, v_p])
                                        T.writes(inverse_local[v_co, v_p, v_vh, v_vw])
                                        T.block_attr({"schedule_rule": "conv2d_nchw_winograd_inverse"})
                                        inverse_local[v_co, v_p, v_vh, v_vw] = inverse_local[v_co, v_p, v_vh, v_vw] + bgemm[v_r_a, v_r_b, v_co, v_p] * T.Select(v_r_a % T.int64(4) == T.int64(3) and v_vh % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_vh % T.int64(2) == T.int64(0), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_vh % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_vh % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_vh % T.int64(2) == T.int64(1), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_vh % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_vh % T.int64(2) == T.int64(1), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_vh % T.int64(2) == T.int64(0), T.float32(1), T.float32(0))))))))) * T.Select(v_r_b % T.int64(4) == T.int64(3) and v_vw % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_vw % T.int64(2) == T.int64(0), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_vw % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_vw % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_vw % T.int64(2) == T.int64(1), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_vw % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_vw % T.int64(2) == T.int64(1), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_vw % T.int64(2) == T.int64(0), T.float32(1), T.float32(0)))))))))
                for h_1, w_1 in T.grid(T.int64(2), T.int64(2)):
                    with T.block("conv2d_winograd"):
                        v_n = T.axis.spatial(T.int64(1), T.int64(0))
                        v_co = T.axis.spatial(T.int64(128), (n_co_h_0_w_0_fused_0 * T.int64(128) + n_co_h_0_w_0_fused_1) // T.int64(196))
                        v_h = T.axis.spatial(T.int64(28), (n_co_h_0_w_0_fused_0 * T.int64(128) + n_co_h_0_w_0_fused_1) % T.int64(196) // T.int64(14) * T.int64(2) + h_1)
                        v_w = T.axis.spatial(T.int64(28), (n_co_h_0_w_0_fused_0 * T.int64(128) + n_co_h_0_w_0_fused_1) % T.int64(14) * T.int64(2) + w_1)
                        T.reads(inverse_local[v_co, v_n * T.int64(196) + v_h // T.int64(2) * T.int64(14) + v_w // T.int64(2), v_h % T.int64(2), v_w % T.int64(2)], p2[v_n, v_co, T.int64(0), T.int64(0)])
                        T.writes(T_relu[v_n, v_co, v_h, v_w])
                        T_relu[v_n, v_co, v_h, v_w] = T.max(inverse_local[v_co, v_n * T.int64(196) + v_h // T.int64(2) * T.int64(14) + v_w // T.int64(2), v_h % T.int64(2), v_w % T.int64(2)] + p2[v_n, v_co, T.int64(0), T.int64(0)], T.float32(0))
b0 = sch.get_block(name="data_pack", func_name="main")
b1 = sch.get_block(name="bgemm", func_name="main")
b2 = sch.get_block(name="inverse", func_name="main")
b3 = sch.get_block(name="conv2d_winograd", func_name="main")
b4 = sch.get_block(name="T_add", func_name="main")
b5 = sch.get_block(name="T_relu", func_name="main")
b6 = sch.get_block(name="root", func_name="main")
b7, b8 = sch.get_producers(block=b2)
sch.compute_inline(block=b8)
b9, = sch.get_consumers(block=b2)
l10, l11, l12, l13 = sch.get_loops(block=b9)
l14, l15 = sch.split(loop=l12, factors=[None, 2], preserve_unit_iters=True)
l16, l17 = sch.split(loop=l13, factors=[None, 2], preserve_unit_iters=True)
sch.reorder(l14, l16, l15, l17)
sch.compute_at(block=b2, loop=l16, preserve_unit_loops=True, index=-1)
sch.set_scope(block=b2, buffer_index=0, storage_scope="local")
l18, l19, l20, l21, l22, l23, l24, l25, l26, l27 = sch.get_loops(block=b2)
sch.unroll(loop=l24)
sch.unroll(loop=l25)
sch.unroll(loop=l26)
sch.unroll(loop=l27)
b28, b29 = sch.get_producers(block=b0)
sch.compute_inline(block=b29)
b30, = sch.get_producers(block=b28)
l31, l32, l33, l34, l35, l36 = sch.get_loops(block=b0)
sch.reorder(l33, l34, l31, l32, l35, l36)
sch.unroll(loop=l31)
sch.unroll(loop=l32)
sch.unroll(loop=l35)
sch.unroll(loop=l36)
l37 = sch.fuse(l33, l34, preserve_unit_iters=True)
v38 = sch.sample_categorical(candidates=[32, 64, 128, 256, 512, 1024], probs=[0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666], decision=1)
l39, l40 = sch.split(loop=l37, factors=[None, v38], preserve_unit_iters=True)
sch.bind(loop=l39, thread_axis="blockIdx.x")
sch.bind(loop=l40, thread_axis="threadIdx.x")
b41 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b41, loop=l40, preserve_unit_loops=True, index=-1)
sch.compute_at(block=b28, loop=l40, preserve_unit_loops=True, index=-1)
sch.set_scope(block=b28, buffer_index=0, storage_scope="local")
sch.compute_inline(block=b30)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l42, l43, l44, l45, l46 = sch.get_loops(block=b1)
v47, v48, v49, v50, v51 = sch.sample_perfect_tile(loop=l42, n=5, max_innermost_factor=64, decision=[4, 1, 1, 1, 1])
l52, l53, l54, l55, l56 = sch.split(loop=l42, factors=[v47, v48, v49, v50, v51], preserve_unit_iters=True)
v57, v58, v59, v60, v61 = sch.sample_perfect_tile(loop=l43, n=5, max_innermost_factor=64, decision=[4, 1, 1, 1, 1])
l62, l63, l64, l65, l66 = sch.split(loop=l43, factors=[v57, v58, v59, v60, v61], preserve_unit_iters=True)
v67, v68, v69, v70, v71 = sch.sample_perfect_tile(loop=l44, n=5, max_innermost_factor=64, decision=[4, 1, 8, 2, 2])
l72, l73, l74, l75, l76 = sch.split(loop=l44, factors=[v67, v68, v69, v70, v71], preserve_unit_iters=True)
v77, v78, v79, v80, v81 = sch.sample_perfect_tile(loop=l45, n=5, max_innermost_factor=64, decision=[7, 1, 14, 1, 2])
l82, l83, l84, l85, l86 = sch.split(loop=l45, factors=[v77, v78, v79, v80, v81], preserve_unit_iters=True)
v87, v88, v89 = sch.sample_perfect_tile(loop=l46, n=3, max_innermost_factor=64, decision=[2, 64, 1])
l90, l91, l92 = sch.split(loop=l46, factors=[v87, v88, v89], preserve_unit_iters=True)
sch.reorder(l52, l62, l72, l82, l53, l63, l73, l83, l54, l64, l74, l84, l90, l91, l55, l65, l75, l85, l92, l56, l66, l76, l86)
l93 = sch.fuse(l52, l62, l72, l82, preserve_unit_iters=True)
sch.bind(loop=l93, thread_axis="blockIdx.x")
l94 = sch.fuse(l53, l63, l73, l83, preserve_unit_iters=True)
sch.bind(loop=l94, thread_axis="vthread.x")
l95 = sch.fuse(l54, l64, l74, l84, preserve_unit_iters=True)
sch.bind(loop=l95, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b96 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b96, loop=l95, preserve_unit_loops=True, index=-1)
b97 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b97, loop=l90, preserve_unit_loops=True, index=-1)
l98, l99, l100, l101, l102, l103, l104, l105 = sch.get_loops(block=b97)
l106 = sch.fuse(l102, l103, l104, l105, preserve_unit_iters=True)
v107 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b97, ann_key="meta_schedule.cooperative_fetch", ann_val=v107)
b108 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b108, loop=l90, preserve_unit_loops=True, index=-1)
l109, l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b108)
l117 = sch.fuse(l113, l114, l115, l116, preserve_unit_iters=True)
v118 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b108, ann_key="meta_schedule.cooperative_fetch", ann_val=v118)
l119 = sch.fuse(l90, preserve_unit_iters=True)
sch.annotate(block_or_loop=l119, ann_key="software_pipeline_stage", ann_val=[0, 0, 2])
sch.annotate(block_or_loop=l119, ann_key="software_pipeline_order", ann_val=[0, 1, 2])
sch.annotate(block_or_loop=l119, ann_key="software_pipeline_async_stages", ann_val=[0])
sch.reverse_compute_inline(block=b5)
sch.reverse_compute_inline(block=b4)
v120 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b6, ann_key="meta_schedule.unroll_explicit", ann_val=v120)
l121, l122, l123, l124, l125, l126 = sch.get_loops(block=b3)
l127 = sch.fuse(l121, l122, l123, l124, preserve_unit_iters=True)
v128 = sch.sample_categorical(candidates=[32, 64, 128, 256, 512, 1024], probs=[0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666], decision=2)
l129, l130 = sch.split(loop=l127, factors=[None, v128], preserve_unit_iters=True)
sch.bind(loop=l129, thread_axis="blockIdx.x")
sch.bind(loop=l130, thread_axis="threadIdx.x")
sch.enter_postproc()
sch.unannotate(block_or_loop=b97, ann_key="meta_schedule.cooperative_fetch")
l131, l132, l133, l134, l135 = sch.get_loops(block=b97)
l136, l137 = sch.split(loop=l135, factors=[None, 112], preserve_unit_iters=True)
sch.bind(loop=l137, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b108, ann_key="meta_schedule.cooperative_fetch")
l138, l139, l140, l141, l142 = sch.get_loops(block=b108)
l143, l144, l145 = sch.split(loop=l142, factors=[None, 112, 4], preserve_unit_iters=True)
sch.vectorize(loop=l145)
sch.bind(loop=l144, thread_axis="threadIdx.x")
b146 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b146, ann_key="meta_schedule.unroll_explicit")
b147, b148, b149, b150, b151, b152, b153, b154, b155 = sch.get_child_blocks(b146)
l156, l157, l158, l159, l160, l161 = sch.get_loops(block=b147)
l162, l163, l164, l165, l166, l167 = sch.get_loops(block=b148)
sch.annotate(block_or_loop=l162, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l162, ann_key="pragma_unroll_explicit", ann_val=1)
l168, l169, l170, l171, l172, l173 = sch.get_loops(block=b149)
l174, l175, l176, l177, l178, l179 = sch.get_loops(block=b150)
l180, l181, l182, l183, l184, l185, l186 = sch.get_loops(block=b151)
l187, l188, l189, l190, l191, l192, l193, l194, l195, l196, l197, l198, l199, l200 = sch.get_loops(block=b152)
sch.annotate(block_or_loop=l187, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l187, ann_key="pragma_unroll_explicit", ann_val=1)
l201, l202, l203, l204, l205, l206, l207 = sch.get_loops(block=b153)
l208, l209, l210, l211, l212, l213, l214, l215 = sch.get_loops(block=b154)
sch.annotate(block_or_loop=l208, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l208, ann_key="pragma_unroll_explicit", ann_val=1)
l216, l217, l218, l219 = sch.get_loops(block=b155)
b220 = sch.get_block(name="data_pack", func_name="main")
l221, l222, l223, l224, l225, l226 = sch.get_loops(block=b220)
b227 = sch.decompose_reduction(block=b220, loop=l225)
b228 = sch.get_block(name="bgemm", func_name="main")
l229, l230, l231, l232, l233, l234, l235, l236, l237, l238, l239, l240, l241, l242 = sch.get_loops(block=b228)
b243 = sch.decompose_reduction(block=b228, loop=l232)
b244 = sch.get_block(name="inverse", func_name="main")
l245, l246, l247, l248, l249, l250, l251, l252 = sch.get_loops(block=b244)
b253 = sch.decompose_reduction(block=b244, loop=l251)
2023-11-11 07:30:18 [INFO] [task_scheduler.cc:121] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #523: Error in running:
LocalRunner: An exception occurred
Subprocess terminated
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(128), T.int64(28), T.int64(28)), "float32"), p1: T.Buffer((T.int64(4), T.int64(4), T.int64(128), T.int64(128)), "float32"), p2: T.Buffer((T.int64(1), T.int64(128), T.int64(1), T.int64(1)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(128), T.int64(28), T.int64(28)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        input_tile_local = T.alloc_buffer((T.int64(128), T.int64(196), T.int64(4), T.int64(4)), scope="local")
        data_pack = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(128), T.int64(196)))
        bgemm = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(128), T.int64(196)))
        inverse_local = T.alloc_buffer((T.int64(128), T.int64(196), T.int64(2), T.int64(2)), scope="local")
        data_pack_local = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(128), T.int64(196)), scope="local")
        bgemm_local = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(128), T.int64(196)), scope="local")
        data_pack_shared = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(128), T.int64(196)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(128), T.int64(128)), scope="shared")
        for ci_p_fused_0 in T.thread_binding(T.int64(392), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 1024, "pragma_unroll_explicit": 1}):
            for ci_p_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(1), T.int64(4), T.int64(4)):
                    with T.block("input_tile"):
                        v_ci = T.axis.spatial(T.int64(128), (ci_p_fused_0 * T.int64(64) + ci_p_fused_1) // T.int64(196) + ax0)
                        v_p = T.axis.spatial(T.int64(196), (ci_p_fused_0 * T.int64(64) + ci_p_fused_1) % T.int64(196) + ax1)
                        v_eps, v_nu = T.axis.remap("SS", [ax2, ax3])
                        T.reads(p0[v_p // T.int64(196), v_ci, v_p % T.int64(196) // T.int64(14) * T.int64(2) + v_eps - T.int64(1), v_p % T.int64(14) * T.int64(2) + v_nu - T.int64(1)])
                        T.writes(input_tile_local[v_ci, v_p, v_eps, v_nu])
                        T.block_attr({"schedule_rule": "None"})
                        input_tile_local[v_ci, v_p, v_eps, v_nu] = T.if_then_else(T.int64(1) <= v_p % T.int64(196) // T.int64(14) * T.int64(2) + v_eps and v_p % T.int64(196) // T.int64(14) * T.int64(2) + v_eps < T.int64(29) and T.int64(1) <= v_p % T.int64(14) * T.int64(2) + v_nu and v_p % T.int64(14) * T.int64(2) + v_nu < T.int64(29), p0[v_p // T.int64(196), v_ci, v_p % T.int64(196) // T.int64(14) * T.int64(2) + v_eps - T.int64(1), v_p % T.int64(14) * T.int64(2) + v_nu - T.int64(1)], T.float32(0))
                for eps in T.unroll(T.int64(4)):
                    for nu in T.unroll(T.int64(4)):
                        with T.block("data_pack_init"):
                            v_eps, v_nu = T.axis.remap("SS", [eps, nu])
                            v_ci = T.axis.spatial(T.int64(128), (ci_p_fused_0 * T.int64(64) + ci_p_fused_1) // T.int64(196))
                            v_p = T.axis.spatial(T.int64(196), (ci_p_fused_0 * T.int64(64) + ci_p_fused_1) % T.int64(196))
                            T.reads()
                            T.writes(data_pack_local[v_eps, v_nu, v_ci, v_p])
                            T.block_attr({"schedule_rule": "conv2d_nchw_winograd_data_pack"})
                            data_pack_local[v_eps, v_nu, v_ci, v_p] = T.float32(0)
                        for r_a in T.unroll(T.int64(4)):
                            for r_b in T.unroll(T.int64(4)):
                                with T.block("data_pack_update"):
                                    v_eps, v_nu = T.axis.remap("SS", [eps, nu])
                                    v_ci = T.axis.spatial(T.int64(128), (ci_p_fused_0 * T.int64(64) + ci_p_fused_1) // T.int64(196))
                                    v_p = T.axis.spatial(T.int64(196), (ci_p_fused_0 * T.int64(64) + ci_p_fused_1) % T.int64(196))
                                    v_r_a, v_r_b = T.axis.remap("RR", [r_a, r_b])
                                    T.reads(data_pack_local[v_eps, v_nu, v_ci, v_p], input_tile_local[v_ci, v_p, v_r_a, v_r_b])
                                    T.writes(data_pack_local[v_eps, v_nu, v_ci, v_p])
                                    T.block_attr({"schedule_rule": "conv2d_nchw_winograd_data_pack"})
                                    data_pack_local[v_eps, v_nu, v_ci, v_p] = data_pack_local[v_eps, v_nu, v_ci, v_p] + input_tile_local[v_ci, v_p, v_r_a, v_r_b] * T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(3), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(1), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(0), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(3), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(1), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(0), T.float32(1), T.float32(0))))))))))))))))) * T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(3), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(1), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(0), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(3), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(1), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(0), T.float32(1), T.float32(0)))))))))))))))))
                for ax0, ax1, ax2, ax3 in T.grid(T.int64(4), T.int64(4), T.int64(1), T.int64(1)):
                    with T.block("data_pack_local"):
                        v0, v1 = T.axis.remap("SS", [ax0, ax1])
                        v2 = T.axis.spatial(T.int64(128), (ci_p_fused_0 * T.int64(64) + ci_p_fused_1) // T.int64(196) + ax2)
                        v3 = T.axis.spatial(T.int64(196), (ci_p_fused_0 * T.int64(64) + ci_p_fused_1) % T.int64(196) + ax3)
                        T.reads(data_pack_local[v0, v1, v2, v3])
                        T.writes(data_pack[v0, v1, v2, v3])
                        data_pack[v0, v1, v2, v3] = data_pack_local[v0, v1, v2, v3]
        for eps_0_nu_0_co_0_p_0_fused in T.thread_binding(T.int64(448), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 1024, "pragma_unroll_explicit": 1}):
            for eps_1_nu_1_co_1_p_1_fused in T.thread_binding(T.int64(2), thread="vthread.x"):
                for eps_2_nu_2_co_2_p_2_fused in T.thread_binding(T.int64(112), thread="threadIdx.x"):
                    for eps_3_init, nu_3_init, co_3_init, p_3_init, eps_4_init, nu_4_init, co_4_init, p_4_init in T.grid(T.int64(1), T.int64(1), T.int64(2), T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(1)):
                        with T.block("bgemm_init"):
                            v_eps = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(112) + eps_3_init + eps_4_init)
                            v_nu = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused % T.int64(112) // T.int64(28) + nu_3_init + nu_4_init)
                            v_co = T.axis.spatial(T.int64(128), eps_0_nu_0_co_0_p_0_fused % T.int64(28) // T.int64(7) * T.int64(32) + eps_2_nu_2_co_2_p_2_fused // T.int64(14) * T.int64(4) + co_3_init * T.int64(2) + co_4_init)
                            v_p = T.axis.spatial(T.int64(196), eps_0_nu_0_co_0_p_0_fused % T.int64(7) * T.int64(28) + eps_1_nu_1_co_1_p_1_fused * T.int64(14) + eps_2_nu_2_co_2_p_2_fused % T.int64(14) + p_3_init + p_4_init)
                            T.reads()
                            T.writes(bgemm_local[v_eps, v_nu, v_co, v_p])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            bgemm_local[v_eps, v_nu, v_co, v_p] = T.float32(0)
                    for ci_0_fused in T.serial(T.int64(2), annotations={"software_pipeline_async_stages": [0], "software_pipeline_order": [0, 1, 2], "software_pipeline_stage": [0, 0, 2]}):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(4)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(112), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("data_pack_shared"):
                                        v0 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(112))
                                        v1 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused % T.int64(112) // T.int64(28))
                                        v2 = T.axis.spatial(T.int64(128), ci_0_fused * T.int64(64) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(448) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) // T.int64(28))
                                        v3 = T.axis.spatial(T.int64(196), eps_0_nu_0_co_0_p_0_fused % T.int64(7) * T.int64(28) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(448) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) % T.int64(28))
                                        T.reads(data_pack[v0, v1, v2, v3])
                                        T.writes(data_pack_shared[v0, v1, v2, v3])
                                        data_pack_shared[v0, v1, v2, v3] = data_pack[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(10)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(112), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("p1_shared"):
                                        v0 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(112))
                                        v1 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused % T.int64(112) // T.int64(28))
                                        v2 = T.axis.spatial(T.int64(128), ci_0_fused * T.int64(64) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(224) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) // T.int64(32))
                                        v3 = T.axis.spatial(T.int64(128), eps_0_nu_0_co_0_p_0_fused % T.int64(28) // T.int64(7) * T.int64(32) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(224) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) % T.int64(32))
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * T.int64(112) + ax0_ax1_ax2_ax3_fused_1) * T.int64(2) + ax0_ax1_ax2_ax3_fused_2 < T.int64(2048))
                                        T.reads(p1[v0, v1, v2, v3])
                                        T.writes(p1_shared[v0, v1, v2, v3])
                                        p1_shared[v0, v1, v2, v3] = p1[v0, v1, v2, v3]
                        for ci_1, eps_3, nu_3, co_3, p_3, ci_2, eps_4, nu_4, co_4, p_4 in T.grid(T.int64(4), T.int64(1), T.int64(1), T.int64(2), T.int64(1), T.int64(16), T.int64(1), T.int64(1), T.int64(2), T.int64(1)):
                            with T.block("bgemm_update"):
                                v_eps = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(112) + eps_3 + eps_4)
                                v_nu = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused % T.int64(112) // T.int64(28) + nu_3 + nu_4)
                                v_co = T.axis.spatial(T.int64(128), eps_0_nu_0_co_0_p_0_fused % T.int64(28) // T.int64(7) * T.int64(32) + eps_2_nu_2_co_2_p_2_fused // T.int64(14) * T.int64(4) + co_3 * T.int64(2) + co_4)
                                v_p = T.axis.spatial(T.int64(196), eps_0_nu_0_co_0_p_0_fused % T.int64(7) * T.int64(28) + eps_1_nu_1_co_1_p_1_fused * T.int64(14) + eps_2_nu_2_co_2_p_2_fused % T.int64(14) + p_3 + p_4)
                                v_ci = T.axis.reduce(T.int64(128), ci_0_fused * T.int64(64) + ci_1 * T.int64(16) + ci_2)
                                T.reads(bgemm_local[v_eps, v_nu, v_co, v_p], data_pack_shared[v_eps, v_nu, v_ci, v_p], p1_shared[v_eps, v_nu, v_ci, v_co])
                                T.writes(bgemm_local[v_eps, v_nu, v_co, v_p])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                bgemm_local[v_eps, v_nu, v_co, v_p] = bgemm_local[v_eps, v_nu, v_co, v_p] + data_pack_shared[v_eps, v_nu, v_ci, v_p] * p1_shared[v_eps, v_nu, v_ci, v_co]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(1), T.int64(4), T.int64(1)):
                        with T.block("bgemm_local"):
                            v0 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(112) + ax0)
                            v1 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused % T.int64(112) // T.int64(28) + ax1)
                            v2 = T.axis.spatial(T.int64(128), eps_0_nu_0_co_0_p_0_fused % T.int64(28) // T.int64(7) * T.int64(32) + eps_2_nu_2_co_2_p_2_fused // T.int64(14) * T.int64(4) + ax2)
                            v3 = T.axis.spatial(T.int64(196), eps_0_nu_0_co_0_p_0_fused % T.int64(7) * T.int64(28) + eps_1_nu_1_co_1_p_1_fused * T.int64(14) + eps_2_nu_2_co_2_p_2_fused % T.int64(14) + ax3)
                            T.reads(bgemm_local[v0, v1, v2, v3])
                            T.writes(bgemm[v0, v1, v2, v3])
                            bgemm[v0, v1, v2, v3] = bgemm_local[v0, v1, v2, v3]
        for n_co_h_0_w_0_fused_0 in T.thread_binding(T.int64(392), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 1024, "pragma_unroll_explicit": 1}):
            for n_co_h_0_w_0_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                    for ax2 in T.unroll(T.int64(2)):
                        for ax3 in T.unroll(T.int64(2)):
                            with T.block("inverse_init"):
                                v_co = T.axis.spatial(T.int64(128), (n_co_h_0_w_0_fused_0 * T.int64(64) + n_co_h_0_w_0_fused_1) // T.int64(196) + ax0)
                                v_p = T.axis.spatial(T.int64(196), (n_co_h_0_w_0_fused_0 * T.int64(64) + n_co_h_0_w_0_fused_1) % T.int64(196) + ax1)
                                v_vh, v_vw = T.axis.remap("SS", [ax2, ax3])
                                T.reads()
                                T.writes(inverse_local[v_co, v_p, v_vh, v_vw])
                                T.block_attr({"schedule_rule": "conv2d_nchw_winograd_inverse"})
                                inverse_local[v_co, v_p, v_vh, v_vw] = T.float32(0)
                            for ax4 in T.unroll(T.int64(4)):
                                for ax5 in T.unroll(T.int64(4)):
                                    with T.block("inverse_update"):
                                        v_co = T.axis.spatial(T.int64(128), (n_co_h_0_w_0_fused_0 * T.int64(64) + n_co_h_0_w_0_fused_1) // T.int64(196) + ax0)
                                        v_p = T.axis.spatial(T.int64(196), (n_co_h_0_w_0_fused_0 * T.int64(64) + n_co_h_0_w_0_fused_1) % T.int64(196) + ax1)
                                        v_vh, v_vw, v_r_a, v_r_b = T.axis.remap("SSRR", [ax2, ax3, ax4, ax5])
                                        T.reads(inverse_local[v_co, v_p, v_vh, v_vw], bgemm[v_r_a, v_r_b, v_co, v_p])
                                        T.writes(inverse_local[v_co, v_p, v_vh, v_vw])
                                        T.block_attr({"schedule_rule": "conv2d_nchw_winograd_inverse"})
                                        inverse_local[v_co, v_p, v_vh, v_vw] = inverse_local[v_co, v_p, v_vh, v_vw] + bgemm[v_r_a, v_r_b, v_co, v_p] * T.Select(v_r_a % T.int64(4) == T.int64(3) and v_vh % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_vh % T.int64(2) == T.int64(0), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_vh % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_vh % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_vh % T.int64(2) == T.int64(1), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_vh % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_vh % T.int64(2) == T.int64(1), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_vh % T.int64(2) == T.int64(0), T.float32(1), T.float32(0))))))))) * T.Select(v_r_b % T.int64(4) == T.int64(3) and v_vw % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_vw % T.int64(2) == T.int64(0), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_vw % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_vw % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_vw % T.int64(2) == T.int64(1), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_vw % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_vw % T.int64(2) == T.int64(1), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_vw % T.int64(2) == T.int64(0), T.float32(1), T.float32(0)))))))))
                for h_1, w_1 in T.grid(T.int64(2), T.int64(2)):
                    with T.block("conv2d_winograd"):
                        v_n = T.axis.spatial(T.int64(1), T.int64(0))
                        v_co = T.axis.spatial(T.int64(128), (n_co_h_0_w_0_fused_0 * T.int64(64) + n_co_h_0_w_0_fused_1) // T.int64(196))
                        v_h = T.axis.spatial(T.int64(28), (n_co_h_0_w_0_fused_0 * T.int64(64) + n_co_h_0_w_0_fused_1) % T.int64(196) // T.int64(14) * T.int64(2) + h_1)
                        v_w = T.axis.spatial(T.int64(28), (n_co_h_0_w_0_fused_0 * T.int64(64) + n_co_h_0_w_0_fused_1) % T.int64(14) * T.int64(2) + w_1)
                        T.reads(inverse_local[v_co, v_n * T.int64(196) + v_h // T.int64(2) * T.int64(14) + v_w // T.int64(2), v_h % T.int64(2), v_w % T.int64(2)], p2[v_n, v_co, T.int64(0), T.int64(0)])
                        T.writes(T_relu[v_n, v_co, v_h, v_w])
                        T_relu[v_n, v_co, v_h, v_w] = T.max(inverse_local[v_co, v_n * T.int64(196) + v_h // T.int64(2) * T.int64(14) + v_w // T.int64(2), v_h % T.int64(2), v_w % T.int64(2)] + p2[v_n, v_co, T.int64(0), T.int64(0)], T.float32(0))
b0 = sch.get_block(name="data_pack", func_name="main")
b1 = sch.get_block(name="bgemm", func_name="main")
b2 = sch.get_block(name="inverse", func_name="main")
b3 = sch.get_block(name="conv2d_winograd", func_name="main")
b4 = sch.get_block(name="T_add", func_name="main")
b5 = sch.get_block(name="T_relu", func_name="main")
b6 = sch.get_block(name="root", func_name="main")
b7, b8 = sch.get_producers(block=b2)
sch.compute_inline(block=b8)
b9, = sch.get_consumers(block=b2)
l10, l11, l12, l13 = sch.get_loops(block=b9)
l14, l15 = sch.split(loop=l12, factors=[None, 2], preserve_unit_iters=True)
l16, l17 = sch.split(loop=l13, factors=[None, 2], preserve_unit_iters=True)
sch.reorder(l14, l16, l15, l17)
sch.compute_at(block=b2, loop=l16, preserve_unit_loops=True, index=-1)
sch.set_scope(block=b2, buffer_index=0, storage_scope="local")
l18, l19, l20, l21, l22, l23, l24, l25, l26, l27 = sch.get_loops(block=b2)
sch.unroll(loop=l24)
sch.unroll(loop=l25)
sch.unroll(loop=l26)
sch.unroll(loop=l27)
b28, b29 = sch.get_producers(block=b0)
sch.compute_inline(block=b29)
b30, = sch.get_producers(block=b28)
l31, l32, l33, l34, l35, l36 = sch.get_loops(block=b0)
sch.reorder(l33, l34, l31, l32, l35, l36)
sch.unroll(loop=l31)
sch.unroll(loop=l32)
sch.unroll(loop=l35)
sch.unroll(loop=l36)
l37 = sch.fuse(l33, l34, preserve_unit_iters=True)
v38 = sch.sample_categorical(candidates=[32, 64, 128, 256, 512, 1024], probs=[0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666], decision=1)
l39, l40 = sch.split(loop=l37, factors=[None, v38], preserve_unit_iters=True)
sch.bind(loop=l39, thread_axis="blockIdx.x")
sch.bind(loop=l40, thread_axis="threadIdx.x")
b41 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b41, loop=l40, preserve_unit_loops=True, index=-1)
sch.compute_at(block=b28, loop=l40, preserve_unit_loops=True, index=-1)
sch.set_scope(block=b28, buffer_index=0, storage_scope="local")
sch.compute_inline(block=b30)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l42, l43, l44, l45, l46 = sch.get_loops(block=b1)
v47, v48, v49, v50, v51 = sch.sample_perfect_tile(loop=l42, n=5, max_innermost_factor=64, decision=[4, 1, 1, 1, 1])
l52, l53, l54, l55, l56 = sch.split(loop=l42, factors=[v47, v48, v49, v50, v51], preserve_unit_iters=True)
v57, v58, v59, v60, v61 = sch.sample_perfect_tile(loop=l43, n=5, max_innermost_factor=64, decision=[4, 1, 1, 1, 1])
l62, l63, l64, l65, l66 = sch.split(loop=l43, factors=[v57, v58, v59, v60, v61], preserve_unit_iters=True)
v67, v68, v69, v70, v71 = sch.sample_perfect_tile(loop=l44, n=5, max_innermost_factor=64, decision=[4, 1, 8, 2, 2])
l72, l73, l74, l75, l76 = sch.split(loop=l44, factors=[v67, v68, v69, v70, v71], preserve_unit_iters=True)
v77, v78, v79, v80, v81 = sch.sample_perfect_tile(loop=l45, n=5, max_innermost_factor=64, decision=[7, 2, 14, 1, 1])
l82, l83, l84, l85, l86 = sch.split(loop=l45, factors=[v77, v78, v79, v80, v81], preserve_unit_iters=True)
v87, v88, v89 = sch.sample_perfect_tile(loop=l46, n=3, max_innermost_factor=64, decision=[2, 4, 16])
l90, l91, l92 = sch.split(loop=l46, factors=[v87, v88, v89], preserve_unit_iters=True)
sch.reorder(l52, l62, l72, l82, l53, l63, l73, l83, l54, l64, l74, l84, l90, l91, l55, l65, l75, l85, l92, l56, l66, l76, l86)
l93 = sch.fuse(l52, l62, l72, l82, preserve_unit_iters=True)
sch.bind(loop=l93, thread_axis="blockIdx.x")
l94 = sch.fuse(l53, l63, l73, l83, preserve_unit_iters=True)
sch.bind(loop=l94, thread_axis="vthread.x")
l95 = sch.fuse(l54, l64, l74, l84, preserve_unit_iters=True)
sch.bind(loop=l95, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b96 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b96, loop=l95, preserve_unit_loops=True, index=-1)
b97 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b97, loop=l90, preserve_unit_loops=True, index=-1)
l98, l99, l100, l101, l102, l103, l104, l105 = sch.get_loops(block=b97)
l106 = sch.fuse(l102, l103, l104, l105, preserve_unit_iters=True)
v107 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b97, ann_key="meta_schedule.cooperative_fetch", ann_val=v107)
b108 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b108, loop=l90, preserve_unit_loops=True, index=-1)
l109, l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b108)
l117 = sch.fuse(l113, l114, l115, l116, preserve_unit_iters=True)
v118 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b108, ann_key="meta_schedule.cooperative_fetch", ann_val=v118)
l119 = sch.fuse(l90, preserve_unit_iters=True)
sch.annotate(block_or_loop=l119, ann_key="software_pipeline_stage", ann_val=[0, 0, 2])
sch.annotate(block_or_loop=l119, ann_key="software_pipeline_order", ann_val=[0, 1, 2])
sch.annotate(block_or_loop=l119, ann_key="software_pipeline_async_stages", ann_val=[0])
sch.reverse_compute_inline(block=b5)
sch.reverse_compute_inline(block=b4)
v120 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b6, ann_key="meta_schedule.unroll_explicit", ann_val=v120)
l121, l122, l123, l124, l125, l126 = sch.get_loops(block=b3)
l127 = sch.fuse(l121, l122, l123, l124, preserve_unit_iters=True)
v128 = sch.sample_categorical(candidates=[32, 64, 128, 256, 512, 1024], probs=[0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666], decision=1)
l129, l130 = sch.split(loop=l127, factors=[None, v128], preserve_unit_iters=True)
sch.bind(loop=l129, thread_axis="blockIdx.x")
sch.bind(loop=l130, thread_axis="threadIdx.x")
sch.enter_postproc()
sch.unannotate(block_or_loop=b97, ann_key="meta_schedule.cooperative_fetch")
l131, l132, l133, l134, l135 = sch.get_loops(block=b97)
l136, l137, l138 = sch.split(loop=l135, factors=[None, 112, 4], preserve_unit_iters=True)
sch.vectorize(loop=l138)
sch.bind(loop=l137, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b108, ann_key="meta_schedule.cooperative_fetch")
l139, l140, l141, l142, l143 = sch.get_loops(block=b108)
l144, l145, l146 = sch.split(loop=l143, factors=[None, 112, 2], preserve_unit_iters=True)
sch.vectorize(loop=l146)
sch.bind(loop=l145, thread_axis="threadIdx.x")
b147 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b147, ann_key="meta_schedule.unroll_explicit")
b148, b149, b150, b151, b152, b153, b154, b155, b156 = sch.get_child_blocks(b147)
l157, l158, l159, l160, l161, l162 = sch.get_loops(block=b148)
l163, l164, l165, l166, l167, l168 = sch.get_loops(block=b149)
sch.annotate(block_or_loop=l163, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l163, ann_key="pragma_unroll_explicit", ann_val=1)
l169, l170, l171, l172, l173, l174 = sch.get_loops(block=b150)
l175, l176, l177, l178, l179, l180, l181 = sch.get_loops(block=b151)
l182, l183, l184, l185, l186, l187, l188 = sch.get_loops(block=b152)
l189, l190, l191, l192, l193, l194, l195, l196, l197, l198, l199, l200, l201, l202 = sch.get_loops(block=b153)
sch.annotate(block_or_loop=l189, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l189, ann_key="pragma_unroll_explicit", ann_val=1)
l203, l204, l205, l206, l207, l208, l209 = sch.get_loops(block=b154)
l210, l211, l212, l213, l214, l215, l216, l217 = sch.get_loops(block=b155)
sch.annotate(block_or_loop=l210, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l210, ann_key="pragma_unroll_explicit", ann_val=1)
l218, l219, l220, l221 = sch.get_loops(block=b156)
b222 = sch.get_block(name="data_pack", func_name="main")
l223, l224, l225, l226, l227, l228 = sch.get_loops(block=b222)
b229 = sch.decompose_reduction(block=b222, loop=l227)
b230 = sch.get_block(name="bgemm", func_name="main")
l231, l232, l233, l234, l235, l236, l237, l238, l239, l240, l241, l242, l243, l244 = sch.get_loops(block=b230)
b245 = sch.decompose_reduction(block=b230, loop=l234)
b246 = sch.get_block(name="inverse", func_name="main")
l247, l248, l249, l250, l251, l252, l253, l254 = sch.get_loops(block=b246)
b255 = sch.decompose_reduction(block=b246, loop=l253)
2023-11-11 07:30:18 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #524: GFLOPs: 5992.7927. Time: 21.1997 us. Best GFLOPs: 6044.2565
2023-11-11 07:30:18 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #525: GFLOPs: 5840.1007. Time: 21.7540 us. Best GFLOPs: 6044.2565
2023-11-11 07:30:18 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #526: GFLOPs: 5926.3887. Time: 21.4373 us. Best GFLOPs: 6044.2565
2023-11-11 07:30:18 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #527: GFLOPs: 5884.0797. Time: 21.5914 us. Best GFLOPs: 6044.2565
2023-11-11 07:30:18 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #528: GFLOPs: 5956.4292. Time: 21.3292 us. Best GFLOPs: 6044.2565
2023-11-11 07:30:18 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #529: GFLOPs: 5879.4844. Time: 21.6083 us. Best GFLOPs: 6044.2565
2023-11-11 07:30:18 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #530: GFLOPs: 5939.9332. Time: 21.3884 us. Best GFLOPs: 6044.2565
2023-11-11 07:30:18 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #531: GFLOPs: 5850.4307. Time: 21.7156 us. Best GFLOPs: 6044.2565
2023-11-11 07:30:18 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #532: GFLOPs: 5904.9793. Time: 21.5150 us. Best GFLOPs: 6044.2565
2023-11-11 07:30:18 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #533: GFLOPs: 5994.0940. Time: 21.1951 us. Best GFLOPs: 6044.2565
2023-11-11 07:30:18 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #534: GFLOPs: 5880.5318. Time: 21.6044 us. Best GFLOPs: 6044.2565
2023-11-11 07:30:18 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #535: GFLOPs: 5919.7939. Time: 21.4612 us. Best GFLOPs: 6044.2565
2023-11-11 07:30:18 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #536: GFLOPs: 5868.8939. Time: 21.6473 us. Best GFLOPs: 6044.2565
2023-11-11 07:30:18 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #537: GFLOPs: 5945.2944. Time: 21.3691 us. Best GFLOPs: 6044.2565
2023-11-11 07:30:18 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #538: GFLOPs: 5964.6321. Time: 21.2998 us. Best GFLOPs: 6044.2565
2023-11-11 07:30:18 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #539: GFLOPs: 5847.5940. Time: 21.7261 us. Best GFLOPs: 6044.2565
2023-11-11 07:30:18 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #540: GFLOPs: 5849.4565. Time: 21.7192 us. Best GFLOPs: 6044.2565
2023-11-11 07:30:18 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #541: GFLOPs: 5900.1767. Time: 21.5325 us. Best GFLOPs: 6044.2565
2023-11-11 07:30:18 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #542: GFLOPs: 5934.4414. Time: 21.4082 us. Best GFLOPs: 6044.2565
2023-11-11 07:30:18 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #543: GFLOPs: 5944.9958. Time: 21.3702 us. Best GFLOPs: 6044.2565
2023-11-11 07:30:18 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #544: GFLOPs: 5921.1917. Time: 21.4561 us. Best GFLOPs: 6044.2565
2023-11-11 07:30:18 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #545: GFLOPs: 5881.5637. Time: 21.6007 us. Best GFLOPs: 6044.2565
2023-11-11 07:30:18 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #546: GFLOPs: 5811.2072. Time: 21.8622 us. Best GFLOPs: 6044.2565
2023-11-11 07:30:18 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #547: GFLOPs: 5911.7964. Time: 21.4902 us. Best GFLOPs: 6044.2565
2023-11-11 07:30:18 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #548: GFLOPs: 5870.6995. Time: 21.6406 us. Best GFLOPs: 6044.2565
2023-11-11 07:30:18 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #549: GFLOPs: 5786.9425. Time: 21.9538 us. Best GFLOPs: 6044.2565
2023-11-11 07:30:18 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #550: GFLOPs: 5820.0191. Time: 21.8291 us. Best GFLOPs: 6044.2565
2023-11-11 07:30:18 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #551: GFLOPs: 5872.0199. Time: 21.6358 us. Best GFLOPs: 6044.2565
2023-11-11 07:30:18 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #552: GFLOPs: 5884.1190. Time: 21.5913 us. Best GFLOPs: 6044.2565
2023-11-11 07:30:18 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #553: GFLOPs: 5839.9941. Time: 21.7544 us. Best GFLOPs: 6044.2565
2023-11-11 07:30:18 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #554: GFLOPs: 6000.4605. Time: 21.1726 us. Best GFLOPs: 6044.2565
2023-11-11 07:30:18 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #555: GFLOPs: 5920.6323. Time: 21.4581 us. Best GFLOPs: 6044.2565
2023-11-11 07:30:18 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #556: GFLOPs: 5849.5343. Time: 21.7189 us. Best GFLOPs: 6044.2565
2023-11-11 07:30:18 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #557: GFLOPs: 5920.9120. Time: 21.4571 us. Best GFLOPs: 6044.2565
2023-11-11 07:30:18 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #558: GFLOPs: 5860.8213. Time: 21.6771 us. Best GFLOPs: 6044.2565
2023-11-11 07:30:18 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #559: GFLOPs: 5891.5799. Time: 21.5639 us. Best GFLOPs: 6044.2565
2023-11-11 07:30:18 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #560: GFLOPs: 6009.8901. Time: 21.1394 us. Best GFLOPs: 6044.2565
2023-11-11 07:30:18 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #561: GFLOPs: 5965.5503. Time: 21.2965 us. Best GFLOPs: 6044.2565
2023-11-11 07:30:18 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #562: GFLOPs: 5931.6348. Time: 21.4183 us. Best GFLOPs: 6044.2565
2023-11-11 07:30:18 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #563: GFLOPs: 5961.1602. Time: 21.3122 us. Best GFLOPs: 6044.2565
2023-11-11 07:30:18 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #564: GFLOPs: 5835.8446. Time: 21.7699 us. Best GFLOPs: 6044.2565
2023-11-11 07:30:18 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #565: GFLOPs: 5925.7297. Time: 21.4397 us. Best GFLOPs: 6044.2565
2023-11-11 07:30:18 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #566: GFLOPs: 5910.9040. Time: 21.4934 us. Best GFLOPs: 6044.2565
2023-11-11 07:30:18 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #567: GFLOPs: 5809.7417. Time: 21.8677 us. Best GFLOPs: 6044.2565
2023-11-11 07:30:18 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #568: GFLOPs: 5817.5128. Time: 21.8385 us. Best GFLOPs: 6044.2565
2023-11-11 07:30:18 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #569: GFLOPs: 5929.1505. Time: 21.4273 us. Best GFLOPs: 6044.2565
2023-11-11 07:30:18 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #570: GFLOPs: 5871.2625. Time: 21.6386 us. Best GFLOPs: 6044.2565
2023-11-11 07:30:18 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #571: GFLOPs: 5827.6261. Time: 21.8006 us. Best GFLOPs: 6044.2565
2023-11-11 07:30:18 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #572: GFLOPs: 5814.8143. Time: 21.8486 us. Best GFLOPs: 6044.2565
2023-11-11 07:30:18 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #573: GFLOPs: 5910.4629. Time: 21.4950 us. Best GFLOPs: 6044.2565
2023-11-11 07:30:18 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #574: GFLOPs: 827.7114. Time: 153.4902 us. Best GFLOPs: 6044.2565
2023-11-11 07:30:18 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #575: GFLOPs: 160.6492. Time: 790.8263 us. Best GFLOPs: 6044.2565
2023-11-11 07:30:18 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #576: GFLOPs: 54.4702. Time: 2332.3863 us. Best GFLOPs: 6044.2565
2023-11-11 08:19:48 [INFO] [evolutionary_search.cc:713] Generating candidates......
2023-11-11 08:19:52 [INFO] [evolutionary_search.cc:715] Picked top 102 candidate(s) from database
2023-11-11 08:19:58 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310a03fb48)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310e893e78)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310e893578)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310ec9ace8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56310e986408)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310eb1cfb8)]: 396 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310dc6f7b8)]: 0 failure(s)
2023-11-11 08:20:04 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310a03fb48)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310e893e78)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310e893578)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310ec9ace8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56310e986408)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310eb1cfb8)]: 791 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310dc6f7b8)]: 0 failure(s)
2023-11-11 08:20:11 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310a03fb48)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310e893e78)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310e893578)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310ec9ace8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56310e986408)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310eb1cfb8)]: 1188 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310dc6f7b8)]: 0 failure(s)
2023-11-11 08:20:17 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310a03fb48)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310e893e78)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310e893578)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310ec9ace8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56310e986408)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310eb1cfb8)]: 1588 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310dc6f7b8)]: 0 failure(s)
2023-11-11 08:20:17 [INFO] [evolutionary_search.cc:723] Sampled 52 candidate(s)
2023-11-11 08:20:41 [INFO] [evolutionary_search.cc:621] Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310a03fb48)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310e893e78)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310e893578)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310ec9ace8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56310e986408)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310eb1cfb8)]: 293 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310dc6f7b8)]: 0 failure(s)
2023-11-11 08:21:06 [INFO] [evolutionary_search.cc:621] Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310a03fb48)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310e893e78)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310e893578)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310ec9ace8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56310e986408)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310eb1cfb8)]: 242 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310dc6f7b8)]: 0 failure(s)
2023-11-11 08:21:32 [INFO] [evolutionary_search.cc:621] Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310a03fb48)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310e893e78)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310e893578)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310ec9ace8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56310e986408)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310eb1cfb8)]: 253 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310dc6f7b8)]: 0 failure(s)
2023-11-11 08:21:56 [INFO] [evolutionary_search.cc:621] Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310a03fb48)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310e893e78)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310e893578)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310ec9ace8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56310e986408)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310eb1cfb8)]: 217 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310dc6f7b8)]: 0 failure(s)
2023-11-11 08:22:02 [INFO] [evolutionary_search.cc:649] Scores of the best 64 candidates:
[1 : 16]:	1.1392  1.1006  0.9989  0.9914  0.9891  0.9889  0.9888  0.9887  0.9874  0.9870  0.9867  0.9863  0.9859  0.9853  0.9852  0.9852
[17 : 32]:	0.9850  0.9842  0.9838  0.9837  0.9832  0.9824  0.9824  0.9823  0.9823  0.9823  0.9822  0.9822  0.9817  0.9817  0.9816  0.9814
[33 : 48]:	0.9814  0.9813  0.9813  0.9813  0.9812  0.9808  0.9803  0.9803  0.9802  0.9801  0.9801  0.9801  0.9800  0.9800  0.9800  0.9798
[49 : 64]:	0.9797  0.9795  0.9794  0.9791  0.9786  0.9786  0.9786  0.9784  0.9781  0.9781  0.9781  0.9778  0.9777  0.9777  0.9773  0.9773
2023-11-11 08:22:03 [INFO] [evolutionary_search.cc:727] Got 64 candidate(s) with evolutionary search
2023-11-11 08:22:03 [INFO] [evolutionary_search.cc:730] Sending 64 candidates(s) for measurement
2023-11-11 08:22:36 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #577: GFLOPs: 1585.5310. Time: 80.1281 us. Best GFLOPs: 6044.2565
2023-11-11 08:22:36 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #578: GFLOPs: 1578.3427. Time: 80.4931 us. Best GFLOPs: 6044.2565
2023-11-11 08:22:36 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #579: GFLOPs: 5920.2330. Time: 21.4596 us. Best GFLOPs: 6044.2565
2023-11-11 08:22:36 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #580: GFLOPs: 5878.4052. Time: 21.6123 us. Best GFLOPs: 6044.2565
2023-11-11 08:22:36 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #581: GFLOPs: 5926.7488. Time: 21.4360 us. Best GFLOPs: 6044.2565
2023-11-11 08:22:36 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #582: GFLOPs: 5931.8259. Time: 21.4176 us. Best GFLOPs: 6044.2565
2023-11-11 08:22:36 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #583: GFLOPs: 5881.3635. Time: 21.6014 us. Best GFLOPs: 6044.2565
2023-11-11 08:22:36 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #584: GFLOPs: 5923.3165. Time: 21.4484 us. Best GFLOPs: 6044.2565
2023-11-11 08:22:36 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #585: GFLOPs: 5919.6339. Time: 21.4617 us. Best GFLOPs: 6044.2565
2023-11-11 08:22:36 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #586: GFLOPs: 6023.0113. Time: 21.0934 us. Best GFLOPs: 6044.2565
2023-11-11 08:22:36 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #587: GFLOPs: 5901.3666. Time: 21.5282 us. Best GFLOPs: 6044.2565
2023-11-11 08:22:36 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #588: GFLOPs: 5918.1572. Time: 21.4671 us. Best GFLOPs: 6044.2565
2023-11-11 08:22:36 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #589: GFLOPs: 5868.9331. Time: 21.6471 us. Best GFLOPs: 6044.2565
2023-11-11 08:22:36 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #590: GFLOPs: 6015.6519. Time: 21.1192 us. Best GFLOPs: 6044.2565
2023-11-11 08:22:36 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #591: GFLOPs: 5902.5603. Time: 21.5238 us. Best GFLOPs: 6044.2565
2023-11-11 08:22:36 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #592: GFLOPs: 5903.6294. Time: 21.5199 us. Best GFLOPs: 6044.2565
2023-11-11 08:22:36 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #593: GFLOPs: 5904.8539. Time: 21.5155 us. Best GFLOPs: 6044.2565
2023-11-11 08:22:36 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #594: GFLOPs: 5886.1710. Time: 21.5837 us. Best GFLOPs: 6044.2565
2023-11-11 08:22:36 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #595: GFLOPs: 5926.5884. Time: 21.4366 us. Best GFLOPs: 6044.2565
2023-11-11 08:22:36 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #596: GFLOPs: 5856.2345. Time: 21.6941 us. Best GFLOPs: 6044.2565
2023-11-11 08:22:36 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #597: GFLOPs: 5823.4279. Time: 21.8163 us. Best GFLOPs: 6044.2565
2023-11-11 08:22:36 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #598: GFLOPs: 5842.2929. Time: 21.7459 us. Best GFLOPs: 6044.2565
2023-11-11 08:22:36 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #599: GFLOPs: 5969.1072. Time: 21.2839 us. Best GFLOPs: 6044.2565
2023-11-11 08:22:36 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #600: GFLOPs: 5901.0491. Time: 21.5293 us. Best GFLOPs: 6044.2565
2023-11-11 08:22:36 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #601: GFLOPs: 5916.6414. Time: 21.4726 us. Best GFLOPs: 6044.2565
2023-11-11 08:22:36 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #602: GFLOPs: 5922.3502. Time: 21.4519 us. Best GFLOPs: 6044.2565
2023-11-11 08:22:36 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #603: GFLOPs: 5906.8535. Time: 21.5082 us. Best GFLOPs: 6044.2565
2023-11-11 08:22:36 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #604: GFLOPs: 5887.6492. Time: 21.5783 us. Best GFLOPs: 6044.2565
2023-11-11 08:22:36 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #605: GFLOPs: 5900.1372. Time: 21.5327 us. Best GFLOPs: 6044.2565
2023-11-11 08:22:36 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #606: GFLOPs: 5911.1430. Time: 21.4926 us. Best GFLOPs: 6044.2565
2023-11-11 08:22:36 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #607: GFLOPs: 5932.1920. Time: 21.4163 us. Best GFLOPs: 6044.2565
2023-11-11 08:22:36 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #608: GFLOPs: 5964.5241. Time: 21.3002 us. Best GFLOPs: 6044.2565
2023-11-11 08:22:36 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #609: GFLOPs: 5408.6169. Time: 23.4895 us. Best GFLOPs: 6044.2565
2023-11-11 08:22:36 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #610: GFLOPs: 5851.4837. Time: 21.7117 us. Best GFLOPs: 6044.2565
2023-11-11 08:22:36 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #611: GFLOPs: 5869.5490. Time: 21.6449 us. Best GFLOPs: 6044.2565
2023-11-11 08:22:36 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #612: GFLOPs: 5850.3917. Time: 21.7157 us. Best GFLOPs: 6044.2565
2023-11-11 08:22:36 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #613: GFLOPs: 5856.8296. Time: 21.6919 us. Best GFLOPs: 6044.2565
2023-11-11 08:22:36 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #614: GFLOPs: 5901.5499. Time: 21.5275 us. Best GFLOPs: 6044.2565
2023-11-11 08:22:36 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #615: GFLOPs: 5865.6389. Time: 21.6593 us. Best GFLOPs: 6044.2565
2023-11-11 08:22:36 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #616: GFLOPs: 5875.6113. Time: 21.6225 us. Best GFLOPs: 6044.2565
2023-11-11 08:22:36 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #617: GFLOPs: 5846.6937. Time: 21.7295 us. Best GFLOPs: 6044.2565
2023-11-11 08:22:36 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #618: GFLOPs: 5898.9478. Time: 21.5370 us. Best GFLOPs: 6044.2565
2023-11-11 08:22:36 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #619: GFLOPs: 5911.7554. Time: 21.4903 us. Best GFLOPs: 6044.2565
2023-11-11 08:22:36 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #620: GFLOPs: 5909.3725. Time: 21.4990 us. Best GFLOPs: 6044.2565
2023-11-11 08:22:36 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #621: GFLOPs: 5987.6783. Time: 21.2178 us. Best GFLOPs: 6044.2565
2023-11-11 08:22:36 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #622: GFLOPs: 5935.3648. Time: 21.4049 us. Best GFLOPs: 6044.2565
2023-11-11 08:22:36 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #623: GFLOPs: 5914.2740. Time: 21.4812 us. Best GFLOPs: 6044.2565
2023-11-11 08:22:36 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #624: GFLOPs: 5959.2919. Time: 21.3189 us. Best GFLOPs: 6044.2565
2023-11-11 08:22:36 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #625: GFLOPs: 5857.2625. Time: 21.6903 us. Best GFLOPs: 6044.2565
2023-11-11 08:22:36 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #626: GFLOPs: 5865.1307. Time: 21.6612 us. Best GFLOPs: 6044.2565
2023-11-11 08:22:36 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #627: GFLOPs: 5865.0508. Time: 21.6615 us. Best GFLOPs: 6044.2565
2023-11-11 08:22:36 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #628: GFLOPs: 5964.2403. Time: 21.3012 us. Best GFLOPs: 6044.2565
2023-11-11 08:22:36 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #629: GFLOPs: 5786.3138. Time: 21.9562 us. Best GFLOPs: 6044.2565
2023-11-11 08:22:36 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #630: GFLOPs: 5871.0232. Time: 21.6394 us. Best GFLOPs: 6044.2565
2023-11-11 08:22:36 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #631: GFLOPs: 5896.3722. Time: 21.5464 us. Best GFLOPs: 6044.2565
2023-11-11 08:22:36 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #632: GFLOPs: 5879.6440. Time: 21.6077 us. Best GFLOPs: 6044.2565
2023-11-11 08:22:36 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #633: GFLOPs: 5938.5150. Time: 21.3935 us. Best GFLOPs: 6044.2565
2023-11-11 08:22:36 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #634: GFLOPs: 6078.0641. Time: 20.9023 us. Best GFLOPs: 6078.0641
2023-11-11 08:22:36 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #635: GFLOPs: 5945.2140. Time: 21.3694 us. Best GFLOPs: 6078.0641
2023-11-11 08:22:36 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #636: GFLOPs: 5810.5950. Time: 21.8645 us. Best GFLOPs: 6078.0641
2023-11-11 08:22:36 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #637: GFLOPs: 5885.5256. Time: 21.5861 us. Best GFLOPs: 6078.0641
2023-11-11 08:22:36 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #638: GFLOPs: 123.2699. Time: 1030.6298 us. Best GFLOPs: 6078.0641
2023-11-11 08:22:36 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #639: GFLOPs: 79.8069. Time: 1591.9136 us. Best GFLOPs: 6078.0641
2023-11-11 08:22:36 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #640: GFLOPs: 68.7335. Time: 1848.3788 us. Best GFLOPs: 6078.0641
2023-11-11 09:11:34 [INFO] [evolutionary_search.cc:713] Generating candidates......
2023-11-11 09:11:38 [INFO] [evolutionary_search.cc:715] Picked top 102 candidate(s) from database
2023-11-11 09:11:44 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310a03fb48)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310e893e78)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310e893578)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310ec9ace8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56310e986408)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310eb1cfb8)]: 395 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310dc6f7b8)]: 0 failure(s)
2023-11-11 09:11:51 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310a03fb48)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310e893e78)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310e893578)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310ec9ace8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56310e986408)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310eb1cfb8)]: 796 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310dc6f7b8)]: 0 failure(s)
2023-11-11 09:11:58 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310a03fb48)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310e893e78)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310e893578)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310ec9ace8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56310e986408)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310eb1cfb8)]: 1190 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310dc6f7b8)]: 0 failure(s)
2023-11-11 09:12:05 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310a03fb48)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310e893e78)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310e893578)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310ec9ace8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56310e986408)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310eb1cfb8)]: 1587 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310dc6f7b8)]: 0 failure(s)
2023-11-11 09:12:05 [INFO] [evolutionary_search.cc:723] Sampled 53 candidate(s)
2023-11-11 09:12:29 [INFO] [evolutionary_search.cc:621] Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310a03fb48)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310e893e78)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310e893578)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310ec9ace8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56310e986408)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310eb1cfb8)]: 283 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310dc6f7b8)]: 0 failure(s)
2023-11-11 09:12:55 [INFO] [evolutionary_search.cc:621] Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310a03fb48)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310e893e78)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310e893578)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310ec9ace8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56310e986408)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310eb1cfb8)]: 281 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310dc6f7b8)]: 0 failure(s)
2023-11-11 09:13:21 [INFO] [evolutionary_search.cc:621] Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310a03fb48)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310e893e78)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310e893578)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310ec9ace8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56310e986408)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310eb1cfb8)]: 254 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310dc6f7b8)]: 0 failure(s)
2023-11-11 09:13:47 [INFO] [evolutionary_search.cc:621] Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310a03fb48)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310e893e78)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310e893578)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310ec9ace8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56310e986408)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310eb1cfb8)]: 270 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310dc6f7b8)]: 0 failure(s)
2023-11-11 09:13:53 [INFO] [evolutionary_search.cc:649] Scores of the best 64 candidates:
[1 : 16]:	0.9960  0.9936  0.9934  0.9917  0.9909  0.9907  0.9905  0.9899  0.9899  0.9899  0.9894  0.9888  0.9887  0.9879  0.9874  0.9870
[17 : 32]:	0.9865  0.9864  0.9863  0.9862  0.9858  0.9852  0.9846  0.9846  0.9843  0.9843  0.9841  0.9834  0.9834  0.9834  0.9833  0.9831
[33 : 48]:	0.9830  0.9828  0.9822  0.9822  0.9817  0.9816  0.9816  0.9812  0.9812  0.9810  0.9810  0.9802  0.9801  0.9801  0.9801  0.9800
[49 : 64]:	0.9799  0.9795  0.9793  0.9793  0.9792  0.9791  0.9790  0.9789  0.9788  0.9788  0.9786  0.9784  0.9783  0.9782  0.9781  0.9781
2023-11-11 09:13:54 [INFO] [evolutionary_search.cc:727] Got 64 candidate(s) with evolutionary search
2023-11-11 09:13:54 [INFO] [evolutionary_search.cc:730] Sending 64 candidates(s) for measurement
2023-11-11 09:14:50 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #641: GFLOPs: 6005.5765. Time: 21.1546 us. Best GFLOPs: 6078.0641
2023-11-11 09:14:50 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #642: GFLOPs: 5900.2559. Time: 21.5322 us. Best GFLOPs: 6078.0641
2023-11-11 09:14:50 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #643: GFLOPs: 5866.8149. Time: 21.6550 us. Best GFLOPs: 6078.0641
2023-11-11 09:14:50 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #644: GFLOPs: 5922.0703. Time: 21.4529 us. Best GFLOPs: 6078.0641
2023-11-11 09:14:50 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #645: GFLOPs: 5911.1033. Time: 21.4927 us. Best GFLOPs: 6078.0641
2023-11-11 09:14:50 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #646: GFLOPs: 5898.8285. Time: 21.5374 us. Best GFLOPs: 6078.0641
2023-11-11 09:14:50 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #647: GFLOPs: 5930.9718. Time: 21.4207 us. Best GFLOPs: 6078.0641
2023-11-11 09:14:50 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #648: GFLOPs: 5915.0199. Time: 21.4785 us. Best GFLOPs: 6078.0641
2023-11-11 09:14:50 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #649: GFLOPs: 5900.2559. Time: 21.5322 us. Best GFLOPs: 6078.0641
2023-11-11 09:14:50 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #650: GFLOPs: 5862.0694. Time: 21.6725 us. Best GFLOPs: 6078.0641
2023-11-11 09:14:50 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #651: GFLOPs: 5968.5798. Time: 21.2857 us. Best GFLOPs: 6078.0641
2023-11-11 09:14:50 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #652: GFLOPs: 6008.4544. Time: 21.1445 us. Best GFLOPs: 6078.0641
2023-11-11 09:14:50 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #653: GFLOPs: 5954.3279. Time: 21.3367 us. Best GFLOPs: 6078.0641
2023-11-11 09:14:50 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #654: GFLOPs: 5863.8580. Time: 21.6659 us. Best GFLOPs: 6078.0641
2023-11-11 09:14:50 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #655: GFLOPs: 5955.0657. Time: 21.3340 us. Best GFLOPs: 6078.0641
2023-11-11 09:14:50 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #656: GFLOPs: 6028.3498. Time: 21.0747 us. Best GFLOPs: 6078.0641
2023-11-11 09:14:50 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #657: GFLOPs: 5956.9954. Time: 21.3271 us. Best GFLOPs: 6078.0641
2023-11-11 09:14:50 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #658: GFLOPs: 5892.1415. Time: 21.5619 us. Best GFLOPs: 6078.0641
2023-11-11 09:14:50 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #659: GFLOPs: 5991.1409. Time: 21.2056 us. Best GFLOPs: 6078.0641
2023-11-11 09:14:50 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #660: GFLOPs: 6004.8370. Time: 21.1572 us. Best GFLOPs: 6078.0641
2023-11-11 09:14:50 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #661: GFLOPs: 5864.1102. Time: 21.6649 us. Best GFLOPs: 6078.0641
2023-11-11 09:14:50 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #662: GFLOPs: 5859.2502. Time: 21.6829 us. Best GFLOPs: 6078.0641
2023-11-11 09:14:50 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #663: GFLOPs: 6007.2970. Time: 21.1486 us. Best GFLOPs: 6078.0641
2023-11-11 09:14:50 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #664: GFLOPs: 5908.8750. Time: 21.5008 us. Best GFLOPs: 6078.0641
2023-11-11 09:14:50 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #665: GFLOPs: 5947.3940. Time: 21.3616 us. Best GFLOPs: 6078.0641
2023-11-11 09:14:50 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #666: GFLOPs: 5936.0472. Time: 21.4024 us. Best GFLOPs: 6078.0641
2023-11-11 09:14:50 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #667: GFLOPs: 5937.2916. Time: 21.3979 us. Best GFLOPs: 6078.0641
2023-11-11 09:14:50 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #668: GFLOPs: 5935.7260. Time: 21.4036 us. Best GFLOPs: 6078.0641
2023-11-11 09:14:50 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #669: GFLOPs: 5936.5689. Time: 21.4005 us. Best GFLOPs: 6078.0641
2023-11-11 09:14:50 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #670: GFLOPs: 6078.8933. Time: 20.8995 us. Best GFLOPs: 6078.8933
2023-11-11 09:14:50 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #671: GFLOPs: 5930.4916. Time: 21.4224 us. Best GFLOPs: 6078.8933
2023-11-11 09:14:50 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #672: GFLOPs: 5956.9760. Time: 21.3272 us. Best GFLOPs: 6078.8933
2023-11-11 09:14:50 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #673: GFLOPs: 5799.3352. Time: 21.9069 us. Best GFLOPs: 6078.8933
2023-11-11 09:14:50 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #674: GFLOPs: 5935.0012. Time: 21.4062 us. Best GFLOPs: 6078.8933
2023-11-11 09:14:50 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #675: GFLOPs: 5968.7473. Time: 21.2851 us. Best GFLOPs: 6078.8933
2023-11-11 09:14:50 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #676: GFLOPs: 5840.5946. Time: 21.7522 us. Best GFLOPs: 6078.8933
2023-11-11 09:14:50 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #677: GFLOPs: 6009.8901. Time: 21.1394 us. Best GFLOPs: 6078.8933
2023-11-11 09:14:50 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #678: GFLOPs: 5950.4935. Time: 21.3504 us. Best GFLOPs: 6078.8933
2023-11-11 09:14:50 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #679: GFLOPs: 5937.2014. Time: 21.3982 us. Best GFLOPs: 6078.8933
2023-11-11 09:14:50 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #680: GFLOPs: 5880.1379. Time: 21.6059 us. Best GFLOPs: 6078.8933
2023-11-11 09:14:50 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #681: GFLOPs: 5946.1802. Time: 21.3659 us. Best GFLOPs: 6078.8933
2023-11-11 09:14:50 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #682: GFLOPs: 6078.7491. Time: 20.9000 us. Best GFLOPs: 6078.8933
2023-11-11 09:14:50 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #683: GFLOPs: 5969.0667. Time: 21.2840 us. Best GFLOPs: 6078.8933
2023-11-11 09:14:50 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #684: GFLOPs: 5822.5284. Time: 21.8197 us. Best GFLOPs: 6078.8933
2023-11-11 09:14:50 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #685: GFLOPs: 5883.2117. Time: 21.5946 us. Best GFLOPs: 6078.8933
2023-11-11 09:14:50 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #686: GFLOPs: 5927.9636. Time: 21.4316 us. Best GFLOPs: 6078.8933
2023-11-11 09:14:50 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #687: GFLOPs: 5940.4662. Time: 21.3865 us. Best GFLOPs: 6078.8933
2023-11-11 09:14:50 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #688: GFLOPs: 5676.6303. Time: 22.3805 us. Best GFLOPs: 6078.8933
2023-11-11 09:14:50 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #689: GFLOPs: 5871.7407. Time: 21.6368 us. Best GFLOPs: 6078.8933
2023-11-11 09:14:50 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #690: GFLOPs: 5930.3927. Time: 21.4228 us. Best GFLOPs: 6078.8933
2023-11-11 09:14:50 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #691: GFLOPs: 5980.5761. Time: 21.2430 us. Best GFLOPs: 6078.8933
2023-11-11 09:14:50 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #692: GFLOPs: 5956.7886. Time: 21.3279 us. Best GFLOPs: 6078.8933
2023-11-11 09:14:50 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #693: GFLOPs: 5917.8784. Time: 21.4681 us. Best GFLOPs: 6078.8933
2023-11-11 09:14:50 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #694: GFLOPs: 5968.5876. Time: 21.2857 us. Best GFLOPs: 6078.8933
2023-11-11 09:14:50 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #695: GFLOPs: 5845.2106. Time: 21.7350 us. Best GFLOPs: 6078.8933
2023-11-11 09:14:50 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #696: GFLOPs: 6031.0777. Time: 21.0652 us. Best GFLOPs: 6078.8933
2023-11-11 09:14:50 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #697: GFLOPs: 5901.1530. Time: 21.5290 us. Best GFLOPs: 6078.8933
2023-11-11 09:14:50 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #698: GFLOPs: 5927.9893. Time: 21.4315 us. Best GFLOPs: 6078.8933
2023-11-11 09:14:50 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #699: GFLOPs: 5924.7553. Time: 21.4432 us. Best GFLOPs: 6078.8933
2023-11-11 09:14:50 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #700: GFLOPs: 5472.2318. Time: 23.2164 us. Best GFLOPs: 6078.8933
2023-11-11 09:14:50 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #701: GFLOPs: 5931.7148. Time: 21.4180 us. Best GFLOPs: 6078.8933
2023-11-11 09:14:50 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #702: GFLOPs: 7.9146. Time: 16052.0781 us. Best GFLOPs: 6078.8933
2023-11-11 09:14:50 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #703: GFLOPs: 70.5168. Time: 1801.6366 us. Best GFLOPs: 6078.8933
2023-11-11 09:14:50 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #704: GFLOPs: 370.7950. Time: 342.6304 us. Best GFLOPs: 6078.8933
2023-11-11 09:47:42 [INFO] [evolutionary_search.cc:713] Generating candidates......
2023-11-11 09:47:45 [INFO] [evolutionary_search.cc:715] Picked top 102 candidate(s) from database
2023-11-11 09:47:51 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310a03fb48)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310e893e78)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310e893578)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310ec9ace8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56310e986408)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310eb1cfb8)]: 394 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310dc6f7b8)]: 0 failure(s)
2023-11-11 09:47:57 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310a03fb48)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310e893e78)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310e893578)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310ec9ace8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56310e986408)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310eb1cfb8)]: 784 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310dc6f7b8)]: 0 failure(s)
2023-11-11 09:48:04 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310a03fb48)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310e893e78)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310e893578)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310ec9ace8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56310e986408)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310eb1cfb8)]: 1186 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310dc6f7b8)]: 0 failure(s)
2023-11-11 09:48:10 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310a03fb48)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310e893e78)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310e893578)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310ec9ace8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56310e986408)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310eb1cfb8)]: 1579 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310dc6f7b8)]: 0 failure(s)
2023-11-11 09:48:10 [INFO] [evolutionary_search.cc:723] Sampled 61 candidate(s)
2023-11-11 09:48:34 [INFO] [evolutionary_search.cc:621] Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310a03fb48)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310e893e78)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310e893578)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310ec9ace8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56310e986408)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310eb1cfb8)]: 294 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310dc6f7b8)]: 0 failure(s)
2023-11-11 09:49:00 [INFO] [evolutionary_search.cc:621] Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310a03fb48)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310e893e78)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310e893578)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310ec9ace8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56310e986408)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310eb1cfb8)]: 264 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310dc6f7b8)]: 0 failure(s)
2023-11-11 09:49:27 [INFO] [evolutionary_search.cc:621] Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310a03fb48)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310e893e78)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310e893578)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310ec9ace8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56310e986408)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310eb1cfb8)]: 256 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310dc6f7b8)]: 0 failure(s)
2023-11-11 09:49:52 [INFO] [evolutionary_search.cc:621] Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310a03fb48)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310e893e78)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310e893578)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310ec9ace8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56310e986408)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310eb1cfb8)]: 221 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310dc6f7b8)]: 0 failure(s)
2023-11-11 09:49:59 [INFO] [evolutionary_search.cc:649] Scores of the best 64 candidates:
[1 : 16]:	0.9989  0.9960  0.9918  0.9910  0.9908  0.9907  0.9896  0.9888  0.9875  0.9863  0.9862  0.9851  0.9847  0.9847  0.9846  0.9846
[17 : 32]:	0.9846  0.9846  0.9840  0.9836  0.9835  0.9835  0.9833  0.9828  0.9824  0.9824  0.9823  0.9822  0.9819  0.9814  0.9813  0.9813
[33 : 48]:	0.9811  0.9810  0.9810  0.9810  0.9806  0.9806  0.9803  0.9803  0.9801  0.9801  0.9801  0.9801  0.9799  0.9799  0.9798  0.9797
[49 : 64]:	0.9796  0.9794  0.9790  0.9788  0.9786  0.9781  0.9781  0.9777  0.9777  0.9777  0.9777  0.9773  0.9773  0.9773  0.9773  0.9771
2023-11-11 09:49:59 [INFO] [evolutionary_search.cc:727] Got 64 candidate(s) with evolutionary search
2023-11-11 09:49:59 [INFO] [evolutionary_search.cc:730] Sending 64 candidates(s) for measurement
2023-11-11 09:54:38 [INFO] [task_scheduler.cc:121] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #705: Error in running:
LocalRunner: An exception occurred
Subprocess terminated
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(128), T.int64(28), T.int64(28)), "float32"), p1: T.Buffer((T.int64(4), T.int64(4), T.int64(128), T.int64(128)), "float32"), p2: T.Buffer((T.int64(1), T.int64(128), T.int64(1), T.int64(1)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(128), T.int64(28), T.int64(28)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        input_tile_local = T.alloc_buffer((T.int64(128), T.int64(196), T.int64(4), T.int64(4)), scope="local")
        data_pack = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(128), T.int64(196)))
        bgemm = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(128), T.int64(196)))
        inverse_local = T.alloc_buffer((T.int64(128), T.int64(196), T.int64(2), T.int64(2)), scope="local")
        data_pack_local = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(128), T.int64(196)), scope="local")
        bgemm_local = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(128), T.int64(196)), scope="local")
        data_pack_shared = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(128), T.int64(196)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(128), T.int64(128)), scope="shared")
        for ci_p_fused_0 in T.thread_binding(T.int64(392), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 512, "pragma_unroll_explicit": 1}):
            for ci_p_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(1), T.int64(4), T.int64(4)):
                    with T.block("input_tile"):
                        v_ci = T.axis.spatial(T.int64(128), (ci_p_fused_0 * T.int64(64) + ci_p_fused_1) // T.int64(196) + ax0)
                        v_p = T.axis.spatial(T.int64(196), (ci_p_fused_0 * T.int64(64) + ci_p_fused_1) % T.int64(196) + ax1)
                        v_eps, v_nu = T.axis.remap("SS", [ax2, ax3])
                        T.reads(p0[v_p // T.int64(196), v_ci, v_p % T.int64(196) // T.int64(14) * T.int64(2) + v_eps - T.int64(1), v_p % T.int64(14) * T.int64(2) + v_nu - T.int64(1)])
                        T.writes(input_tile_local[v_ci, v_p, v_eps, v_nu])
                        T.block_attr({"schedule_rule": "None"})
                        input_tile_local[v_ci, v_p, v_eps, v_nu] = T.if_then_else(T.int64(1) <= v_p % T.int64(196) // T.int64(14) * T.int64(2) + v_eps and v_p % T.int64(196) // T.int64(14) * T.int64(2) + v_eps < T.int64(29) and T.int64(1) <= v_p % T.int64(14) * T.int64(2) + v_nu and v_p % T.int64(14) * T.int64(2) + v_nu < T.int64(29), p0[v_p // T.int64(196), v_ci, v_p % T.int64(196) // T.int64(14) * T.int64(2) + v_eps - T.int64(1), v_p % T.int64(14) * T.int64(2) + v_nu - T.int64(1)], T.float32(0))
                for eps in T.unroll(T.int64(4)):
                    for nu in T.unroll(T.int64(4)):
                        with T.block("data_pack_init"):
                            v_eps, v_nu = T.axis.remap("SS", [eps, nu])
                            v_ci = T.axis.spatial(T.int64(128), (ci_p_fused_0 * T.int64(64) + ci_p_fused_1) // T.int64(196))
                            v_p = T.axis.spatial(T.int64(196), (ci_p_fused_0 * T.int64(64) + ci_p_fused_1) % T.int64(196))
                            T.reads()
                            T.writes(data_pack_local[v_eps, v_nu, v_ci, v_p])
                            T.block_attr({"schedule_rule": "conv2d_nchw_winograd_data_pack"})
                            data_pack_local[v_eps, v_nu, v_ci, v_p] = T.float32(0)
                        for r_a in T.unroll(T.int64(4)):
                            for r_b in T.unroll(T.int64(4)):
                                with T.block("data_pack_update"):
                                    v_eps, v_nu = T.axis.remap("SS", [eps, nu])
                                    v_ci = T.axis.spatial(T.int64(128), (ci_p_fused_0 * T.int64(64) + ci_p_fused_1) // T.int64(196))
                                    v_p = T.axis.spatial(T.int64(196), (ci_p_fused_0 * T.int64(64) + ci_p_fused_1) % T.int64(196))
                                    v_r_a, v_r_b = T.axis.remap("RR", [r_a, r_b])
                                    T.reads(data_pack_local[v_eps, v_nu, v_ci, v_p], input_tile_local[v_ci, v_p, v_r_a, v_r_b])
                                    T.writes(data_pack_local[v_eps, v_nu, v_ci, v_p])
                                    T.block_attr({"schedule_rule": "conv2d_nchw_winograd_data_pack"})
                                    data_pack_local[v_eps, v_nu, v_ci, v_p] = data_pack_local[v_eps, v_nu, v_ci, v_p] + input_tile_local[v_ci, v_p, v_r_a, v_r_b] * T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(3), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(1), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(0), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(3), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(1), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(0), T.float32(1), T.float32(0))))))))))))))))) * T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(3), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(1), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(0), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(3), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(1), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(0), T.float32(1), T.float32(0)))))))))))))))))
                for ax0, ax1, ax2, ax3 in T.grid(T.int64(4), T.int64(4), T.int64(1), T.int64(1)):
                    with T.block("data_pack_local"):
                        v0, v1 = T.axis.remap("SS", [ax0, ax1])
                        v2 = T.axis.spatial(T.int64(128), (ci_p_fused_0 * T.int64(64) + ci_p_fused_1) // T.int64(196) + ax2)
                        v3 = T.axis.spatial(T.int64(196), (ci_p_fused_0 * T.int64(64) + ci_p_fused_1) % T.int64(196) + ax3)
                        T.reads(data_pack_local[v0, v1, v2, v3])
                        T.writes(data_pack[v0, v1, v2, v3])
                        data_pack[v0, v1, v2, v3] = data_pack_local[v0, v1, v2, v3]
        for eps_0_nu_0_co_0_p_0_fused in T.thread_binding(T.int64(448), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 512, "pragma_unroll_explicit": 1}):
            for eps_1_nu_1_co_1_p_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for eps_2_nu_2_co_2_p_2_fused in T.thread_binding(T.int64(112), thread="threadIdx.x"):
                    for eps_3_init, nu_3_init, co_3_init, p_3_init, eps_4_init, nu_4_init, co_4_init, p_4_init in T.grid(T.int64(1), T.int64(1), T.int64(2), T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(2)):
                        with T.block("bgemm_init"):
                            v_eps = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(112) + eps_3_init + eps_4_init)
                            v_nu = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused % T.int64(112) // T.int64(28) + nu_3_init + nu_4_init)
                            v_co = T.axis.spatial(T.int64(128), eps_0_nu_0_co_0_p_0_fused % T.int64(28) // T.int64(7) * T.int64(32) + eps_2_nu_2_co_2_p_2_fused // T.int64(14) * T.int64(4) + co_3_init * T.int64(2) + co_4_init)
                            v_p = T.axis.spatial(T.int64(196), eps_0_nu_0_co_0_p_0_fused % T.int64(7) * T.int64(28) + eps_2_nu_2_co_2_p_2_fused % T.int64(14) * T.int64(2) + p_3_init * T.int64(2) + p_4_init)
                            T.reads()
                            T.writes(bgemm_local[v_eps, v_nu, v_co, v_p])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            bgemm_local[v_eps, v_nu, v_co, v_p] = T.float32(0)
                    for ci_0_fused in T.serial(T.int64(2), annotations={"software_pipeline_async_stages": [0], "software_pipeline_order": [0, 1, 2], "software_pipeline_stage": [0, 0, 2]}):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(4)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(112), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("data_pack_shared"):
                                        v0 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(112))
                                        v1 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused % T.int64(112) // T.int64(28))
                                        v2 = T.axis.spatial(T.int64(128), ci_0_fused * T.int64(64) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(448) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) // T.int64(28))
                                        v3 = T.axis.spatial(T.int64(196), eps_0_nu_0_co_0_p_0_fused % T.int64(7) * T.int64(28) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(448) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) % T.int64(28))
                                        T.reads(data_pack[v0, v1, v2, v3])
                                        T.writes(data_pack_shared[v0, v1, v2, v3])
                                        data_pack_shared[v0, v1, v2, v3] = data_pack[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(5)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(112), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("p1_shared"):
                                        v0 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(112))
                                        v1 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused % T.int64(112) // T.int64(28))
                                        v2 = T.axis.spatial(T.int64(128), ci_0_fused * T.int64(64) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(448) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) // T.int64(32))
                                        v3 = T.axis.spatial(T.int64(128), eps_0_nu_0_co_0_p_0_fused % T.int64(28) // T.int64(7) * T.int64(32) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(448) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) % T.int64(32))
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * T.int64(112) + ax0_ax1_ax2_ax3_fused_1) * T.int64(4) + ax0_ax1_ax2_ax3_fused_2 < T.int64(2048))
                                        T.reads(p1[v0, v1, v2, v3])
                                        T.writes(p1_shared[v0, v1, v2, v3])
                                        p1_shared[v0, v1, v2, v3] = p1[v0, v1, v2, v3]
                        for ci_1, eps_3, nu_3, co_3, p_3, ci_2, eps_4, nu_4, co_4, p_4 in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(1), T.int64(64), T.int64(1), T.int64(1), T.int64(2), T.int64(2)):
                            with T.block("bgemm_update"):
                                v_eps = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(112) + eps_3 + eps_4)
                                v_nu = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused % T.int64(112) // T.int64(28) + nu_3 + nu_4)
                                v_co = T.axis.spatial(T.int64(128), eps_0_nu_0_co_0_p_0_fused % T.int64(28) // T.int64(7) * T.int64(32) + eps_2_nu_2_co_2_p_2_fused // T.int64(14) * T.int64(4) + co_3 * T.int64(2) + co_4)
                                v_p = T.axis.spatial(T.int64(196), eps_0_nu_0_co_0_p_0_fused % T.int64(7) * T.int64(28) + eps_2_nu_2_co_2_p_2_fused % T.int64(14) * T.int64(2) + p_3 * T.int64(2) + p_4)
                                v_ci = T.axis.reduce(T.int64(128), ci_0_fused * T.int64(64) + ci_1 * T.int64(64) + ci_2)
                                T.reads(bgemm_local[v_eps, v_nu, v_co, v_p], data_pack_shared[v_eps, v_nu, v_ci, v_p], p1_shared[v_eps, v_nu, v_ci, v_co])
                                T.writes(bgemm_local[v_eps, v_nu, v_co, v_p])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                bgemm_local[v_eps, v_nu, v_co, v_p] = bgemm_local[v_eps, v_nu, v_co, v_p] + data_pack_shared[v_eps, v_nu, v_ci, v_p] * p1_shared[v_eps, v_nu, v_ci, v_co]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(1), T.int64(4), T.int64(2)):
                        with T.block("bgemm_local"):
                            v0 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(112) + ax0)
                            v1 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused % T.int64(112) // T.int64(28) + ax1)
                            v2 = T.axis.spatial(T.int64(128), eps_0_nu_0_co_0_p_0_fused % T.int64(28) // T.int64(7) * T.int64(32) + eps_2_nu_2_co_2_p_2_fused // T.int64(14) * T.int64(4) + ax2)
                            v3 = T.axis.spatial(T.int64(196), eps_0_nu_0_co_0_p_0_fused % T.int64(7) * T.int64(28) + eps_2_nu_2_co_2_p_2_fused % T.int64(14) * T.int64(2) + ax3)
                            T.reads(bgemm_local[v0, v1, v2, v3])
                            T.writes(bgemm[v0, v1, v2, v3])
                            bgemm[v0, v1, v2, v3] = bgemm_local[v0, v1, v2, v3]
        for n_co_h_0_w_0_fused_0 in T.thread_binding(T.int64(392), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 512, "pragma_unroll_explicit": 1}):
            for n_co_h_0_w_0_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                    for ax2 in T.unroll(T.int64(2)):
                        for ax3 in T.unroll(T.int64(2)):
                            with T.block("inverse_init"):
                                v_co = T.axis.spatial(T.int64(128), (n_co_h_0_w_0_fused_0 * T.int64(64) + n_co_h_0_w_0_fused_1) // T.int64(196) + ax0)
                                v_p = T.axis.spatial(T.int64(196), (n_co_h_0_w_0_fused_0 * T.int64(64) + n_co_h_0_w_0_fused_1) % T.int64(196) + ax1)
                                v_vh, v_vw = T.axis.remap("SS", [ax2, ax3])
                                T.reads()
                                T.writes(inverse_local[v_co, v_p, v_vh, v_vw])
                                T.block_attr({"schedule_rule": "conv2d_nchw_winograd_inverse"})
                                inverse_local[v_co, v_p, v_vh, v_vw] = T.float32(0)
                            for ax4 in T.unroll(T.int64(4)):
                                for ax5 in T.unroll(T.int64(4)):
                                    with T.block("inverse_update"):
                                        v_co = T.axis.spatial(T.int64(128), (n_co_h_0_w_0_fused_0 * T.int64(64) + n_co_h_0_w_0_fused_1) // T.int64(196) + ax0)
                                        v_p = T.axis.spatial(T.int64(196), (n_co_h_0_w_0_fused_0 * T.int64(64) + n_co_h_0_w_0_fused_1) % T.int64(196) + ax1)
                                        v_vh, v_vw, v_r_a, v_r_b = T.axis.remap("SSRR", [ax2, ax3, ax4, ax5])
                                        T.reads(inverse_local[v_co, v_p, v_vh, v_vw], bgemm[v_r_a, v_r_b, v_co, v_p])
                                        T.writes(inverse_local[v_co, v_p, v_vh, v_vw])
                                        T.block_attr({"schedule_rule": "conv2d_nchw_winograd_inverse"})
                                        inverse_local[v_co, v_p, v_vh, v_vw] = inverse_local[v_co, v_p, v_vh, v_vw] + bgemm[v_r_a, v_r_b, v_co, v_p] * T.Select(v_r_a % T.int64(4) == T.int64(3) and v_vh % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_vh % T.int64(2) == T.int64(0), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_vh % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_vh % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_vh % T.int64(2) == T.int64(1), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_vh % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_vh % T.int64(2) == T.int64(1), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_vh % T.int64(2) == T.int64(0), T.float32(1), T.float32(0))))))))) * T.Select(v_r_b % T.int64(4) == T.int64(3) and v_vw % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_vw % T.int64(2) == T.int64(0), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_vw % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_vw % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_vw % T.int64(2) == T.int64(1), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_vw % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_vw % T.int64(2) == T.int64(1), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_vw % T.int64(2) == T.int64(0), T.float32(1), T.float32(0)))))))))
                for h_1, w_1 in T.grid(T.int64(2), T.int64(2)):
                    with T.block("conv2d_winograd"):
                        v_n = T.axis.spatial(T.int64(1), T.int64(0))
                        v_co = T.axis.spatial(T.int64(128), (n_co_h_0_w_0_fused_0 * T.int64(64) + n_co_h_0_w_0_fused_1) // T.int64(196))
                        v_h = T.axis.spatial(T.int64(28), (n_co_h_0_w_0_fused_0 * T.int64(64) + n_co_h_0_w_0_fused_1) % T.int64(196) // T.int64(14) * T.int64(2) + h_1)
                        v_w = T.axis.spatial(T.int64(28), (n_co_h_0_w_0_fused_0 * T.int64(64) + n_co_h_0_w_0_fused_1) % T.int64(14) * T.int64(2) + w_1)
                        T.reads(inverse_local[v_co, v_n * T.int64(196) + v_h // T.int64(2) * T.int64(14) + v_w // T.int64(2), v_h % T.int64(2), v_w % T.int64(2)], p2[v_n, v_co, T.int64(0), T.int64(0)])
                        T.writes(T_relu[v_n, v_co, v_h, v_w])
                        T_relu[v_n, v_co, v_h, v_w] = T.max(inverse_local[v_co, v_n * T.int64(196) + v_h // T.int64(2) * T.int64(14) + v_w // T.int64(2), v_h % T.int64(2), v_w % T.int64(2)] + p2[v_n, v_co, T.int64(0), T.int64(0)], T.float32(0))
b0 = sch.get_block(name="data_pack", func_name="main")
b1 = sch.get_block(name="bgemm", func_name="main")
b2 = sch.get_block(name="inverse", func_name="main")
b3 = sch.get_block(name="conv2d_winograd", func_name="main")
b4 = sch.get_block(name="T_add", func_name="main")
b5 = sch.get_block(name="T_relu", func_name="main")
b6 = sch.get_block(name="root", func_name="main")
b7, b8 = sch.get_producers(block=b2)
sch.compute_inline(block=b8)
b9, = sch.get_consumers(block=b2)
l10, l11, l12, l13 = sch.get_loops(block=b9)
l14, l15 = sch.split(loop=l12, factors=[None, 2], preserve_unit_iters=True)
l16, l17 = sch.split(loop=l13, factors=[None, 2], preserve_unit_iters=True)
sch.reorder(l14, l16, l15, l17)
sch.compute_at(block=b2, loop=l16, preserve_unit_loops=True, index=-1)
sch.set_scope(block=b2, buffer_index=0, storage_scope="local")
l18, l19, l20, l21, l22, l23, l24, l25, l26, l27 = sch.get_loops(block=b2)
sch.unroll(loop=l24)
sch.unroll(loop=l25)
sch.unroll(loop=l26)
sch.unroll(loop=l27)
b28, b29 = sch.get_producers(block=b0)
sch.compute_inline(block=b29)
b30, = sch.get_producers(block=b28)
l31, l32, l33, l34, l35, l36 = sch.get_loops(block=b0)
sch.reorder(l33, l34, l31, l32, l35, l36)
sch.unroll(loop=l31)
sch.unroll(loop=l32)
sch.unroll(loop=l35)
sch.unroll(loop=l36)
l37 = sch.fuse(l33, l34, preserve_unit_iters=True)
v38 = sch.sample_categorical(candidates=[32, 64, 128, 256, 512, 1024], probs=[0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666], decision=1)
l39, l40 = sch.split(loop=l37, factors=[None, v38], preserve_unit_iters=True)
sch.bind(loop=l39, thread_axis="blockIdx.x")
sch.bind(loop=l40, thread_axis="threadIdx.x")
b41 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b41, loop=l40, preserve_unit_loops=True, index=-1)
sch.compute_at(block=b28, loop=l40, preserve_unit_loops=True, index=-1)
sch.set_scope(block=b28, buffer_index=0, storage_scope="local")
sch.compute_inline(block=b30)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l42, l43, l44, l45, l46 = sch.get_loops(block=b1)
v47, v48, v49, v50, v51 = sch.sample_perfect_tile(loop=l42, n=5, max_innermost_factor=64, decision=[4, 1, 1, 1, 1])
l52, l53, l54, l55, l56 = sch.split(loop=l42, factors=[v47, v48, v49, v50, v51], preserve_unit_iters=True)
v57, v58, v59, v60, v61 = sch.sample_perfect_tile(loop=l43, n=5, max_innermost_factor=64, decision=[4, 1, 1, 1, 1])
l62, l63, l64, l65, l66 = sch.split(loop=l43, factors=[v57, v58, v59, v60, v61], preserve_unit_iters=True)
v67, v68, v69, v70, v71 = sch.sample_perfect_tile(loop=l44, n=5, max_innermost_factor=64, decision=[4, 1, 8, 2, 2])
l72, l73, l74, l75, l76 = sch.split(loop=l44, factors=[v67, v68, v69, v70, v71], preserve_unit_iters=True)
v77, v78, v79, v80, v81 = sch.sample_perfect_tile(loop=l45, n=5, max_innermost_factor=64, decision=[7, 1, 14, 1, 2])
l82, l83, l84, l85, l86 = sch.split(loop=l45, factors=[v77, v78, v79, v80, v81], preserve_unit_iters=True)
v87, v88, v89 = sch.sample_perfect_tile(loop=l46, n=3, max_innermost_factor=64, decision=[2, 1, 64])
l90, l91, l92 = sch.split(loop=l46, factors=[v87, v88, v89], preserve_unit_iters=True)
sch.reorder(l52, l62, l72, l82, l53, l63, l73, l83, l54, l64, l74, l84, l90, l91, l55, l65, l75, l85, l92, l56, l66, l76, l86)
l93 = sch.fuse(l52, l62, l72, l82, preserve_unit_iters=True)
sch.bind(loop=l93, thread_axis="blockIdx.x")
l94 = sch.fuse(l53, l63, l73, l83, preserve_unit_iters=True)
sch.bind(loop=l94, thread_axis="vthread.x")
l95 = sch.fuse(l54, l64, l74, l84, preserve_unit_iters=True)
sch.bind(loop=l95, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b96 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b96, loop=l95, preserve_unit_loops=True, index=-1)
b97 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b97, loop=l90, preserve_unit_loops=True, index=-1)
l98, l99, l100, l101, l102, l103, l104, l105 = sch.get_loops(block=b97)
l106 = sch.fuse(l102, l103, l104, l105, preserve_unit_iters=True)
v107 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b97, ann_key="meta_schedule.cooperative_fetch", ann_val=v107)
b108 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b108, loop=l90, preserve_unit_loops=True, index=-1)
l109, l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b108)
l117 = sch.fuse(l113, l114, l115, l116, preserve_unit_iters=True)
v118 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b108, ann_key="meta_schedule.cooperative_fetch", ann_val=v118)
l119 = sch.fuse(l90, preserve_unit_iters=True)
sch.annotate(block_or_loop=l119, ann_key="software_pipeline_stage", ann_val=[0, 0, 2])
sch.annotate(block_or_loop=l119, ann_key="software_pipeline_order", ann_val=[0, 1, 2])
sch.annotate(block_or_loop=l119, ann_key="software_pipeline_async_stages", ann_val=[0])
sch.reverse_compute_inline(block=b5)
sch.reverse_compute_inline(block=b4)
v120 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b6, ann_key="meta_schedule.unroll_explicit", ann_val=v120)
l121, l122, l123, l124, l125, l126 = sch.get_loops(block=b3)
l127 = sch.fuse(l121, l122, l123, l124, preserve_unit_iters=True)
v128 = sch.sample_categorical(candidates=[32, 64, 128, 256, 512, 1024], probs=[0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666], decision=1)
l129, l130 = sch.split(loop=l127, factors=[None, v128], preserve_unit_iters=True)
sch.bind(loop=l129, thread_axis="blockIdx.x")
sch.bind(loop=l130, thread_axis="threadIdx.x")
sch.enter_postproc()
sch.unannotate(block_or_loop=b97, ann_key="meta_schedule.cooperative_fetch")
l131, l132, l133, l134, l135 = sch.get_loops(block=b97)
l136, l137, l138 = sch.split(loop=l135, factors=[None, 112, 4], preserve_unit_iters=True)
sch.vectorize(loop=l138)
sch.bind(loop=l137, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b108, ann_key="meta_schedule.cooperative_fetch")
l139, l140, l141, l142, l143 = sch.get_loops(block=b108)
l144, l145, l146 = sch.split(loop=l143, factors=[None, 112, 4], preserve_unit_iters=True)
sch.vectorize(loop=l146)
sch.bind(loop=l145, thread_axis="threadIdx.x")
b147 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b147, ann_key="meta_schedule.unroll_explicit")
b148, b149, b150, b151, b152, b153, b154, b155, b156 = sch.get_child_blocks(b147)
l157, l158, l159, l160, l161, l162 = sch.get_loops(block=b148)
l163, l164, l165, l166, l167, l168 = sch.get_loops(block=b149)
sch.annotate(block_or_loop=l163, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l163, ann_key="pragma_unroll_explicit", ann_val=1)
l169, l170, l171, l172, l173, l174 = sch.get_loops(block=b150)
l175, l176, l177, l178, l179, l180, l181 = sch.get_loops(block=b151)
l182, l183, l184, l185, l186, l187, l188 = sch.get_loops(block=b152)
l189, l190, l191, l192, l193, l194, l195, l196, l197, l198, l199, l200, l201, l202 = sch.get_loops(block=b153)
sch.annotate(block_or_loop=l189, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l189, ann_key="pragma_unroll_explicit", ann_val=1)
l203, l204, l205, l206, l207, l208, l209 = sch.get_loops(block=b154)
l210, l211, l212, l213, l214, l215, l216, l217 = sch.get_loops(block=b155)
sch.annotate(block_or_loop=l210, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l210, ann_key="pragma_unroll_explicit", ann_val=1)
l218, l219, l220, l221 = sch.get_loops(block=b156)
b222 = sch.get_block(name="data_pack", func_name="main")
l223, l224, l225, l226, l227, l228 = sch.get_loops(block=b222)
b229 = sch.decompose_reduction(block=b222, loop=l227)
b230 = sch.get_block(name="bgemm", func_name="main")
l231, l232, l233, l234, l235, l236, l237, l238, l239, l240, l241, l242, l243, l244 = sch.get_loops(block=b230)
b245 = sch.decompose_reduction(block=b230, loop=l234)
b246 = sch.get_block(name="inverse", func_name="main")
l247, l248, l249, l250, l251, l252, l253, l254 = sch.get_loops(block=b246)
b255 = sch.decompose_reduction(block=b246, loop=l253)
2023-11-11 09:54:38 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #706: GFLOPs: 5992.8250. Time: 21.1996 us. Best GFLOPs: 6078.8933
2023-11-11 09:54:38 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #707: GFLOPs: 5988.1214. Time: 21.2163 us. Best GFLOPs: 6078.8933
2023-11-11 09:54:38 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #708: GFLOPs: 5828.9712. Time: 21.7955 us. Best GFLOPs: 6078.8933
2023-11-11 09:54:38 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #709: GFLOPs: 5408.5644. Time: 23.4897 us. Best GFLOPs: 6078.8933
2023-11-11 09:54:38 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #710: GFLOPs: 5840.2582. Time: 21.7534 us. Best GFLOPs: 6078.8933
2023-11-11 09:54:38 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #711: GFLOPs: 5925.9956. Time: 21.4387 us. Best GFLOPs: 6078.8933
2023-11-11 09:54:38 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #712: GFLOPs: 5971.1378. Time: 21.2766 us. Best GFLOPs: 6078.8933
2023-11-11 09:54:38 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #713: GFLOPs: 5893.5870. Time: 21.5566 us. Best GFLOPs: 6078.8933
2023-11-11 09:54:38 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #714: GFLOPs: 5910.2279. Time: 21.4959 us. Best GFLOPs: 6078.8933
2023-11-11 09:54:38 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #715: GFLOPs: 5836.2777. Time: 21.7683 us. Best GFLOPs: 6078.8933
2023-11-11 09:54:38 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #716: GFLOPs: 5841.7577. Time: 21.7478 us. Best GFLOPs: 6078.8933
2023-11-11 09:54:38 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #717: GFLOPs: 5647.2263. Time: 22.4970 us. Best GFLOPs: 6078.8933
2023-11-11 09:54:38 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #718: GFLOPs: 5361.5594. Time: 23.6956 us. Best GFLOPs: 6078.8933
2023-11-11 09:54:38 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #719: GFLOPs: 5988.9211. Time: 21.2134 us. Best GFLOPs: 6078.8933
2023-11-11 09:54:38 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #720: GFLOPs: 5824.4477. Time: 21.8125 us. Best GFLOPs: 6078.8933
2023-11-11 09:54:38 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #721: GFLOPs: 5863.3017. Time: 21.6679 us. Best GFLOPs: 6078.8933
2023-11-11 09:54:38 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #722: GFLOPs: 5984.3679. Time: 21.2296 us. Best GFLOPs: 6078.8933
2023-11-11 09:54:38 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #723: GFLOPs: 5858.7485. Time: 21.6848 us. Best GFLOPs: 6078.8933
2023-11-11 09:54:38 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #724: GFLOPs: 5821.5465. Time: 21.8233 us. Best GFLOPs: 6078.8933
2023-11-11 09:54:38 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #725: GFLOPs: 5853.7775. Time: 21.7032 us. Best GFLOPs: 6078.8933
2023-11-11 09:54:38 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #726: GFLOPs: 5790.9476. Time: 21.9387 us. Best GFLOPs: 6078.8933
2023-11-11 09:54:38 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #727: GFLOPs: 5913.5341. Time: 21.4839 us. Best GFLOPs: 6078.8933
2023-11-11 09:54:38 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #728: GFLOPs: 5849.1446. Time: 21.7204 us. Best GFLOPs: 6078.8933
2023-11-11 09:54:38 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #729: GFLOPs: 5776.4315. Time: 21.9938 us. Best GFLOPs: 6078.8933
2023-11-11 09:54:38 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #730: GFLOPs: 5914.6664. Time: 21.4798 us. Best GFLOPs: 6078.8933
2023-11-11 09:54:38 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #731: GFLOPs: 5775.9619. Time: 21.9956 us. Best GFLOPs: 6078.8933
2023-11-11 09:54:38 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #732: GFLOPs: 5830.5731. Time: 21.7896 us. Best GFLOPs: 6078.8933
2023-11-11 09:54:38 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #733: GFLOPs: 5836.5824. Time: 21.7671 us. Best GFLOPs: 6078.8933
2023-11-11 09:54:38 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #734: GFLOPs: 5858.7338. Time: 21.6848 us. Best GFLOPs: 6078.8933
2023-11-11 09:54:38 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #735: GFLOPs: 5786.2984. Time: 21.9563 us. Best GFLOPs: 6078.8933
2023-11-11 09:54:38 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #736: GFLOPs: 5827.7443. Time: 21.8001 us. Best GFLOPs: 6078.8933
2023-11-11 09:54:38 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #737: GFLOPs: 5773.8100. Time: 22.0038 us. Best GFLOPs: 6078.8933
2023-11-11 09:54:38 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #738: GFLOPs: 5981.2878. Time: 21.2405 us. Best GFLOPs: 6078.8933
2023-11-11 09:54:38 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #739: GFLOPs: 5790.0908. Time: 21.9419 us. Best GFLOPs: 6078.8933
2023-11-11 09:54:38 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #740: GFLOPs: 5780.2315. Time: 21.9793 us. Best GFLOPs: 6078.8933
2023-11-11 09:54:38 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #741: GFLOPs: 5900.5337. Time: 21.5312 us. Best GFLOPs: 6078.8933
2023-11-11 09:54:38 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #742: GFLOPs: 5888.3428. Time: 21.5758 us. Best GFLOPs: 6078.8933
2023-11-11 09:54:38 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #743: GFLOPs: 5828.2114. Time: 21.7984 us. Best GFLOPs: 6078.8933
2023-11-11 09:54:38 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #744: GFLOPs: 5800.5019. Time: 21.9025 us. Best GFLOPs: 6078.8933
2023-11-11 09:54:38 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #745: GFLOPs: 5832.6174. Time: 21.7819 us. Best GFLOPs: 6078.8933
2023-11-11 09:54:38 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #746: GFLOPs: 5844.7579. Time: 21.7367 us. Best GFLOPs: 6078.8933
2023-11-11 09:54:38 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #747: GFLOPs: 5990.2545. Time: 21.2087 us. Best GFLOPs: 6078.8933
2023-11-11 09:54:38 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #748: GFLOPs: 5527.0218. Time: 22.9863 us. Best GFLOPs: 6078.8933
2023-11-11 09:54:38 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #749: GFLOPs: 5837.8932. Time: 21.7622 us. Best GFLOPs: 6078.8933
2023-11-11 09:54:38 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #750: GFLOPs: 5831.2415. Time: 21.7871 us. Best GFLOPs: 6078.8933
2023-11-11 09:54:38 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #751: GFLOPs: 5858.4961. Time: 21.6857 us. Best GFLOPs: 6078.8933
2023-11-11 09:54:38 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #752: GFLOPs: 5812.5083. Time: 21.8573 us. Best GFLOPs: 6078.8933
2023-11-11 09:54:38 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #753: GFLOPs: 5792.7632. Time: 21.9318 us. Best GFLOPs: 6078.8933
2023-11-11 09:54:38 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #754: GFLOPs: 5829.7866. Time: 21.7925 us. Best GFLOPs: 6078.8933
2023-11-11 09:54:38 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #755: GFLOPs: 5904.9747. Time: 21.5150 us. Best GFLOPs: 6078.8933
2023-11-11 09:54:38 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #756: GFLOPs: 5798.2855. Time: 21.9109 us. Best GFLOPs: 6078.8933
2023-11-11 09:54:38 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #757: GFLOPs: 5760.9707. Time: 22.0528 us. Best GFLOPs: 6078.8933
2023-11-11 09:54:38 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #758: GFLOPs: 5922.3900. Time: 21.4518 us. Best GFLOPs: 6078.8933
2023-11-11 09:54:38 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #759: GFLOPs: 5927.4292. Time: 21.4335 us. Best GFLOPs: 6078.8933
2023-11-11 09:54:38 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #760: GFLOPs: 5786.4707. Time: 21.9556 us. Best GFLOPs: 6078.8933
2023-11-11 09:54:38 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #761: GFLOPs: 5842.1916. Time: 21.7462 us. Best GFLOPs: 6078.8933
2023-11-11 09:54:38 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #762: GFLOPs: 5844.9550. Time: 21.7359 us. Best GFLOPs: 6078.8933
2023-11-11 09:54:38 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #763: GFLOPs: 5901.8828. Time: 21.5263 us. Best GFLOPs: 6078.8933
2023-11-11 09:54:38 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #764: GFLOPs: 5831.2043. Time: 21.7872 us. Best GFLOPs: 6078.8933
2023-11-11 09:54:38 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #765: GFLOPs: 5925.4436. Time: 21.4407 us. Best GFLOPs: 6078.8933
2023-11-11 09:54:38 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #766: GFLOPs: 130.5752. Time: 972.9690 us. Best GFLOPs: 6078.8933
2023-11-11 09:54:38 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #767: GFLOPs: 506.0622. Time: 251.0475 us. Best GFLOPs: 6078.8933
2023-11-11 09:54:38 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #768: GFLOPs: 82.4389. Time: 1541.0885 us. Best GFLOPs: 6078.8933
2023-11-11 10:28:09 [INFO] [evolutionary_search.cc:713] Generating candidates......
2023-11-11 10:28:12 [INFO] [evolutionary_search.cc:715] Picked top 102 candidate(s) from database
2023-11-11 10:28:19 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310a03fb48)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310e893e78)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310e893578)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310ec9ace8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56310e986408)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310eb1cfb8)]: 394 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310dc6f7b8)]: 0 failure(s)
2023-11-11 10:28:25 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310a03fb48)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310e893e78)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310e893578)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310ec9ace8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56310e986408)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310eb1cfb8)]: 788 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310dc6f7b8)]: 0 failure(s)
2023-11-11 10:28:32 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310a03fb48)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310e893e78)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310e893578)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310ec9ace8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56310e986408)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310eb1cfb8)]: 1178 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310dc6f7b8)]: 0 failure(s)
2023-11-11 10:28:32 [INFO] [evolutionary_search.cc:723] Sampled 52 candidate(s)
2023-11-11 10:28:55 [INFO] [evolutionary_search.cc:621] Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310a03fb48)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310e893e78)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310e893578)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310ec9ace8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56310e986408)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310eb1cfb8)]: 294 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310dc6f7b8)]: 0 failure(s)
2023-11-11 10:29:20 [INFO] [evolutionary_search.cc:621] Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310a03fb48)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310e893e78)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310e893578)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310ec9ace8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56310e986408)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310eb1cfb8)]: 235 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310dc6f7b8)]: 0 failure(s)
2023-11-11 10:29:46 [INFO] [evolutionary_search.cc:621] Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310a03fb48)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310e893e78)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310e893578)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310ec9ace8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56310e986408)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310eb1cfb8)]: 275 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310dc6f7b8)]: 0 failure(s)
2023-11-11 10:30:11 [INFO] [evolutionary_search.cc:621] Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310a03fb48)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310e893e78)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310e893578)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310ec9ace8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56310e986408)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310eb1cfb8)]: 270 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310dc6f7b8)]: 0 failure(s)
2023-11-11 10:30:17 [INFO] [evolutionary_search.cc:649] Scores of the best 64 candidates:
[1 : 16]:	0.9829  0.9824  0.9822  0.9822  0.9818  0.9816  0.9810  0.9805  0.9805  0.9800  0.9799  0.9796  0.9794  0.9794  0.9792  0.9791
[17 : 32]:	0.9788  0.9788  0.9787  0.9786  0.9786  0.9781  0.9781  0.9781  0.9781  0.9779  0.9779  0.9779  0.9776  0.9775  0.9772  0.9770
[33 : 48]:	0.9766  0.9765  0.9765  0.9762  0.9762  0.9759  0.9759  0.9759  0.9757  0.9756  0.9754  0.9754  0.9753  0.9753  0.9753  0.9752
[49 : 64]:	0.9751  0.9751  0.9749  0.9749  0.9749  0.9749  0.9748  0.9747  0.9744  0.9741  0.9741  0.9740  0.9740  0.9740  0.9739  0.9739
2023-11-11 10:30:18 [INFO] [evolutionary_search.cc:727] Got 64 candidate(s) with evolutionary search
2023-11-11 10:30:18 [INFO] [evolutionary_search.cc:730] Sending 64 candidates(s) for measurement
2023-11-11 10:30:59 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #769: GFLOPs: 5826.7414. Time: 21.8039 us. Best GFLOPs: 6078.8933
2023-11-11 10:30:59 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #770: GFLOPs: 5853.5101. Time: 21.7042 us. Best GFLOPs: 6078.8933
2023-11-11 10:30:59 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #771: GFLOPs: 5849.4225. Time: 21.7193 us. Best GFLOPs: 6078.8933
2023-11-11 10:30:59 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #772: GFLOPs: 5974.7712. Time: 21.2637 us. Best GFLOPs: 6078.8933
2023-11-11 10:30:59 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #773: GFLOPs: 6031.2696. Time: 21.0645 us. Best GFLOPs: 6078.8933
2023-11-11 10:30:59 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #774: GFLOPs: 5876.1085. Time: 21.6207 us. Best GFLOPs: 6078.8933
2023-11-11 10:30:59 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #775: GFLOPs: 6035.2091. Time: 21.0507 us. Best GFLOPs: 6078.8933
2023-11-11 10:30:59 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #776: GFLOPs: 6032.6757. Time: 21.0596 us. Best GFLOPs: 6078.8933
2023-11-11 10:30:59 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #777: GFLOPs: 6037.8264. Time: 21.0416 us. Best GFLOPs: 6078.8933
2023-11-11 10:30:59 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #778: GFLOPs: 5887.0393. Time: 21.5806 us. Best GFLOPs: 6078.8933
2023-11-11 10:30:59 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #779: GFLOPs: 5939.9332. Time: 21.3884 us. Best GFLOPs: 6078.8933
2023-11-11 10:30:59 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #780: GFLOPs: 5935.1638. Time: 21.4056 us. Best GFLOPs: 6078.8933
2023-11-11 10:30:59 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #781: GFLOPs: 5968.4577. Time: 21.2862 us. Best GFLOPs: 6078.8933
2023-11-11 10:30:59 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #782: GFLOPs: 5967.1596. Time: 21.2908 us. Best GFLOPs: 6078.8933
2023-11-11 10:30:59 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #783: GFLOPs: 5848.4430. Time: 21.7230 us. Best GFLOPs: 6078.8933
2023-11-11 10:30:59 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #784: GFLOPs: 5880.9959. Time: 21.6027 us. Best GFLOPs: 6078.8933
2023-11-11 10:30:59 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #785: GFLOPs: 5863.7973. Time: 21.6661 us. Best GFLOPs: 6078.8933
2023-11-11 10:30:59 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #786: GFLOPs: 5882.4835. Time: 21.5973 us. Best GFLOPs: 6078.8933
2023-11-11 10:30:59 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #787: GFLOPs: 5896.5604. Time: 21.5457 us. Best GFLOPs: 6078.8933
2023-11-11 10:30:59 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #788: GFLOPs: 5902.0330. Time: 21.5257 us. Best GFLOPs: 6078.8933
2023-11-11 10:30:59 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #789: GFLOPs: 5971.9077. Time: 21.2739 us. Best GFLOPs: 6078.8933
2023-11-11 10:30:59 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #790: GFLOPs: 5967.0683. Time: 21.2911 us. Best GFLOPs: 6078.8933
2023-11-11 10:30:59 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #791: GFLOPs: 5966.2265. Time: 21.2941 us. Best GFLOPs: 6078.8933
2023-11-11 10:30:59 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #792: GFLOPs: 5888.4108. Time: 21.5755 us. Best GFLOPs: 6078.8933
2023-11-11 10:30:59 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #793: GFLOPs: 5889.8046. Time: 21.5704 us. Best GFLOPs: 6078.8933
2023-11-11 10:30:59 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #794: GFLOPs: 5920.1931. Time: 21.4597 us. Best GFLOPs: 6078.8933
2023-11-11 10:30:59 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #795: GFLOPs: 5913.0894. Time: 21.4855 us. Best GFLOPs: 6078.8933
2023-11-11 10:30:59 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #796: GFLOPs: 5928.5445. Time: 21.4295 us. Best GFLOPs: 6078.8933
2023-11-11 10:30:59 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #797: GFLOPs: 5971.0155. Time: 21.2771 us. Best GFLOPs: 6078.8933
2023-11-11 10:30:59 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #798: GFLOPs: 6019.1254. Time: 21.1070 us. Best GFLOPs: 6078.8933
2023-11-11 10:30:59 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #799: GFLOPs: 5929.6315. Time: 21.4256 us. Best GFLOPs: 6078.8933
2023-11-11 10:30:59 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #800: GFLOPs: 6021.0985. Time: 21.1001 us. Best GFLOPs: 6078.8933
2023-11-11 10:30:59 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #801: GFLOPs: 5901.6445. Time: 21.5272 us. Best GFLOPs: 6078.8933
2023-11-11 10:30:59 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #802: GFLOPs: 5935.5657. Time: 21.4041 us. Best GFLOPs: 6078.8933
2023-11-11 10:30:59 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #803: GFLOPs: 5973.9871. Time: 21.2665 us. Best GFLOPs: 6078.8933
2023-11-11 10:30:59 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #804: GFLOPs: 5836.0359. Time: 21.7692 us. Best GFLOPs: 6078.8933
2023-11-11 10:30:59 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #805: GFLOPs: 5866.0851. Time: 21.6577 us. Best GFLOPs: 6078.8933
2023-11-11 10:30:59 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #806: GFLOPs: 5818.2564. Time: 21.8357 us. Best GFLOPs: 6078.8933
2023-11-11 10:30:59 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #807: GFLOPs: 5874.8936. Time: 21.6252 us. Best GFLOPs: 6078.8933
2023-11-11 10:30:59 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #808: GFLOPs: 5896.1585. Time: 21.5472 us. Best GFLOPs: 6078.8933
2023-11-11 10:30:59 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #809: GFLOPs: 5871.7805. Time: 21.6366 us. Best GFLOPs: 6078.8933
2023-11-11 10:30:59 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #810: GFLOPs: 5819.3134. Time: 21.8317 us. Best GFLOPs: 6078.8933
2023-11-11 10:30:59 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #811: GFLOPs: 6025.1163. Time: 21.0860 us. Best GFLOPs: 6078.8933
2023-11-11 10:30:59 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #812: GFLOPs: 6013.9709. Time: 21.1251 us. Best GFLOPs: 6078.8933
2023-11-11 10:30:59 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #813: GFLOPs: 5861.5649. Time: 21.6744 us. Best GFLOPs: 6078.8933
2023-11-11 10:30:59 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #814: GFLOPs: 5858.3180. Time: 21.6864 us. Best GFLOPs: 6078.8933
2023-11-11 10:30:59 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #815: GFLOPs: 5853.2621. Time: 21.7051 us. Best GFLOPs: 6078.8933
2023-11-11 10:30:59 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #816: GFLOPs: 6015.3675. Time: 21.1202 us. Best GFLOPs: 6078.8933
2023-11-11 10:30:59 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #817: GFLOPs: 5877.4210. Time: 21.6159 us. Best GFLOPs: 6078.8933
2023-11-11 10:30:59 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #818: GFLOPs: 5847.9591. Time: 21.7248 us. Best GFLOPs: 6078.8933
2023-11-11 10:30:59 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #819: GFLOPs: 5876.4485. Time: 21.6195 us. Best GFLOPs: 6078.8933
2023-11-11 10:30:59 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #820: GFLOPs: 5869.1688. Time: 21.6463 us. Best GFLOPs: 6078.8933
2023-11-11 10:30:59 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #821: GFLOPs: 5850.1347. Time: 21.7167 us. Best GFLOPs: 6078.8933
2023-11-11 10:30:59 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #822: GFLOPs: 5937.0912. Time: 21.3986 us. Best GFLOPs: 6078.8933
2023-11-11 10:30:59 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #823: GFLOPs: 5907.0062. Time: 21.5076 us. Best GFLOPs: 6078.8933
2023-11-11 10:30:59 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #824: GFLOPs: 5891.1386. Time: 21.5655 us. Best GFLOPs: 6078.8933
2023-11-11 10:30:59 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #825: GFLOPs: 5854.4345. Time: 21.7008 us. Best GFLOPs: 6078.8933
2023-11-11 10:30:59 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #826: GFLOPs: 5925.0683. Time: 21.4421 us. Best GFLOPs: 6078.8933
2023-11-11 10:30:59 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #827: GFLOPs: 5824.9182. Time: 21.8107 us. Best GFLOPs: 6078.8933
2023-11-11 10:30:59 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #828: GFLOPs: 5947.9934. Time: 21.3594 us. Best GFLOPs: 6078.8933
2023-11-11 10:30:59 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #829: GFLOPs: 5955.0551. Time: 21.3341 us. Best GFLOPs: 6078.8933
2023-11-11 10:30:59 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #830: GFLOPs: 262.5436. Time: 483.9030 us. Best GFLOPs: 6078.8933
2023-11-11 10:30:59 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #831: GFLOPs: 1165.9074. Time: 108.9672 us. Best GFLOPs: 6078.8933
2023-11-11 10:30:59 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #832: GFLOPs: 790.8578. Time: 160.6428 us. Best GFLOPs: 6078.8933
2023-11-11 10:54:30 [INFO] [evolutionary_search.cc:713] Generating candidates......
2023-11-11 10:54:33 [INFO] [evolutionary_search.cc:715] Picked top 102 candidate(s) from database
2023-11-11 10:54:40 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310a03fb48)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310e893e78)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310e893578)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310ec9ace8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56310e986408)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310eb1cfb8)]: 399 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310dc6f7b8)]: 0 failure(s)
2023-11-11 10:54:47 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310a03fb48)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310e893e78)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310e893578)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310ec9ace8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56310e986408)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310eb1cfb8)]: 794 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310dc6f7b8)]: 0 failure(s)
2023-11-11 10:54:53 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310a03fb48)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310e893e78)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310e893578)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310ec9ace8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56310e986408)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310eb1cfb8)]: 1186 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310dc6f7b8)]: 0 failure(s)
2023-11-11 10:55:00 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310a03fb48)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310e893e78)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310e893578)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310ec9ace8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56310e986408)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310eb1cfb8)]: 1583 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310dc6f7b8)]: 0 failure(s)
2023-11-11 10:55:00 [INFO] [evolutionary_search.cc:723] Sampled 57 candidate(s)
2023-11-11 10:55:25 [INFO] [evolutionary_search.cc:621] Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310a03fb48)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310e893e78)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310e893578)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310ec9ace8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56310e986408)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310eb1cfb8)]: 325 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310dc6f7b8)]: 0 failure(s)
2023-11-11 10:55:54 [INFO] [evolutionary_search.cc:621] Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310a03fb48)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310e893e78)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310e893578)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310ec9ace8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56310e986408)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310eb1cfb8)]: 298 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310dc6f7b8)]: 0 failure(s)
2023-11-11 10:56:22 [INFO] [evolutionary_search.cc:621] Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310a03fb48)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310e893e78)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310e893578)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310ec9ace8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56310e986408)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310eb1cfb8)]: 274 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310dc6f7b8)]: 0 failure(s)
2023-11-11 10:56:49 [INFO] [evolutionary_search.cc:621] Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310a03fb48)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310e893e78)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310e893578)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310ec9ace8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56310e986408)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310eb1cfb8)]: 249 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310dc6f7b8)]: 0 failure(s)
2023-11-11 10:56:55 [INFO] [evolutionary_search.cc:649] Scores of the best 64 candidates:
[1 : 16]:	0.9824  0.9800  0.9796  0.9788  0.9787  0.9787  0.9784  0.9784  0.9782  0.9781  0.9781  0.9780  0.9779  0.9779  0.9777  0.9776
[17 : 32]:	0.9774  0.9773  0.9773  0.9770  0.9766  0.9766  0.9765  0.9765  0.9765  0.9763  0.9763  0.9762  0.9761  0.9761  0.9760  0.9760
[33 : 48]:	0.9759  0.9759  0.9758  0.9757  0.9755  0.9754  0.9752  0.9751  0.9751  0.9751  0.9750  0.9749  0.9748  0.9747  0.9747  0.9746
[49 : 64]:	0.9743  0.9742  0.9742  0.9742  0.9741  0.9740  0.9740  0.9740  0.9740  0.9739  0.9739  0.9739  0.9739  0.9738  0.9737  0.9737
2023-11-11 10:56:56 [INFO] [evolutionary_search.cc:727] Got 64 candidate(s) with evolutionary search
2023-11-11 10:56:56 [INFO] [evolutionary_search.cc:730] Sending 64 candidates(s) for measurement
2023-11-11 10:57:36 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #833: GFLOPs: 5940.4158. Time: 21.3867 us. Best GFLOPs: 6078.8933
2023-11-11 10:57:36 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #834: GFLOPs: 5896.9624. Time: 21.5443 us. Best GFLOPs: 6078.8933
2023-11-11 10:57:36 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #835: GFLOPs: 5892.2624. Time: 21.5614 us. Best GFLOPs: 6078.8933
2023-11-11 10:57:36 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #836: GFLOPs: 5948.4775. Time: 21.3577 us. Best GFLOPs: 6078.8933
2023-11-11 10:57:36 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #837: GFLOPs: 5886.1710. Time: 21.5837 us. Best GFLOPs: 6078.8933
2023-11-11 10:57:36 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #838: GFLOPs: 5895.1442. Time: 21.5509 us. Best GFLOPs: 6078.8933
2023-11-11 10:57:36 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #839: GFLOPs: 5924.1064. Time: 21.4455 us. Best GFLOPs: 6078.8933
2023-11-11 10:57:36 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #840: GFLOPs: 5929.1905. Time: 21.4271 us. Best GFLOPs: 6078.8933
2023-11-11 10:57:36 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #841: GFLOPs: 5916.5129. Time: 21.4731 us. Best GFLOPs: 6078.8933
2023-11-11 10:57:36 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #842: GFLOPs: 5872.2587. Time: 21.6349 us. Best GFLOPs: 6078.8933
2023-11-11 10:57:36 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #843: GFLOPs: 5907.4434. Time: 21.5060 us. Best GFLOPs: 6078.8933
2023-11-11 10:57:36 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #844: GFLOPs: 5947.8306. Time: 21.3600 us. Best GFLOPs: 6078.8933
2023-11-11 10:57:36 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #845: GFLOPs: 6008.3098. Time: 21.1450 us. Best GFLOPs: 6078.8933
2023-11-11 10:57:36 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #846: GFLOPs: 5910.1882. Time: 21.4960 us. Best GFLOPs: 6078.8933
2023-11-11 10:57:36 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #847: GFLOPs: 5989.4493. Time: 21.2116 us. Best GFLOPs: 6078.8933
2023-11-11 10:57:36 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #848: GFLOPs: 5878.9960. Time: 21.6101 us. Best GFLOPs: 6078.8933
2023-11-11 10:57:36 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #849: GFLOPs: 5891.7106. Time: 21.5635 us. Best GFLOPs: 6078.8933
2023-11-11 10:57:36 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #850: GFLOPs: 5876.1230. Time: 21.6207 us. Best GFLOPs: 6078.8933
2023-11-11 10:57:36 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #851: GFLOPs: 5872.4278. Time: 21.6343 us. Best GFLOPs: 6078.8933
2023-11-11 10:57:36 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #852: GFLOPs: 5892.0975. Time: 21.5620 us. Best GFLOPs: 6078.8933
2023-11-11 10:57:36 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #853: GFLOPs: 5855.5047. Time: 21.6968 us. Best GFLOPs: 6078.8933
2023-11-11 10:57:36 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #854: GFLOPs: 5878.0509. Time: 21.6136 us. Best GFLOPs: 6078.8933
2023-11-11 10:57:36 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #855: GFLOPs: 5875.6501. Time: 21.6224 us. Best GFLOPs: 6078.8933
2023-11-11 10:57:36 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #856: GFLOPs: 5889.3334. Time: 21.5722 us. Best GFLOPs: 6078.8933
2023-11-11 10:57:36 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #857: GFLOPs: 5915.4852. Time: 21.4768 us. Best GFLOPs: 6078.8933
2023-11-11 10:57:36 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #858: GFLOPs: 5946.4386. Time: 21.3650 us. Best GFLOPs: 6078.8933
2023-11-11 10:57:36 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #859: GFLOPs: 5938.0146. Time: 21.3953 us. Best GFLOPs: 6078.8933
2023-11-11 10:57:36 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #860: GFLOPs: 5886.5273. Time: 21.5824 us. Best GFLOPs: 6078.8933
2023-11-11 10:57:36 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #861: GFLOPs: 5874.8902. Time: 21.6252 us. Best GFLOPs: 6078.8933
2023-11-11 10:57:36 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #862: GFLOPs: 5891.9812. Time: 21.5625 us. Best GFLOPs: 6078.8933
2023-11-11 10:57:36 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #863: GFLOPs: 5882.6439. Time: 21.5967 us. Best GFLOPs: 6078.8933
2023-11-11 10:57:36 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #864: GFLOPs: 5876.4770. Time: 21.6194 us. Best GFLOPs: 6078.8933
2023-11-11 10:57:36 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #865: GFLOPs: 5839.3122. Time: 21.7570 us. Best GFLOPs: 6078.8933
2023-11-11 10:57:36 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #866: GFLOPs: 5838.0899. Time: 21.7615 us. Best GFLOPs: 6078.8933
2023-11-11 10:57:36 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #867: GFLOPs: 5877.2245. Time: 21.6166 us. Best GFLOPs: 6078.8933
2023-11-11 10:57:36 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #868: GFLOPs: 5891.0178. Time: 21.5660 us. Best GFLOPs: 6078.8933
2023-11-11 10:57:36 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #869: GFLOPs: 5881.9898. Time: 21.5991 us. Best GFLOPs: 6078.8933
2023-11-11 10:57:36 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #870: GFLOPs: 5877.2470. Time: 21.6165 us. Best GFLOPs: 6078.8933
2023-11-11 10:57:36 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #871: GFLOPs: 5955.4347. Time: 21.3327 us. Best GFLOPs: 6078.8933
2023-11-11 10:57:36 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #872: GFLOPs: 5889.9192. Time: 21.5700 us. Best GFLOPs: 6078.8933
2023-11-11 10:57:36 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #873: GFLOPs: 5905.4961. Time: 21.5131 us. Best GFLOPs: 6078.8933
2023-11-11 10:57:36 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #874: GFLOPs: 6007.7430. Time: 21.1470 us. Best GFLOPs: 6078.8933
2023-11-11 10:57:36 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #875: GFLOPs: 5995.6961. Time: 21.1895 us. Best GFLOPs: 6078.8933
2023-11-11 10:57:36 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #876: GFLOPs: 5926.3082. Time: 21.4376 us. Best GFLOPs: 6078.8933
2023-11-11 10:57:36 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #877: GFLOPs: 6005.0299. Time: 21.1565 us. Best GFLOPs: 6078.8933
2023-11-11 10:57:36 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #878: GFLOPs: 5866.4836. Time: 21.6562 us. Best GFLOPs: 6078.8933
2023-11-11 10:57:36 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #879: GFLOPs: 5852.5372. Time: 21.7078 us. Best GFLOPs: 6078.8933
2023-11-11 10:57:36 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #880: GFLOPs: 5875.9260. Time: 21.6214 us. Best GFLOPs: 6078.8933
2023-11-11 10:57:36 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #881: GFLOPs: 5882.3237. Time: 21.5979 us. Best GFLOPs: 6078.8933
2023-11-11 10:57:36 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #882: GFLOPs: 5870.8563. Time: 21.6401 us. Best GFLOPs: 6078.8933
2023-11-11 10:57:36 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #883: GFLOPs: 5888.1677. Time: 21.5764 us. Best GFLOPs: 6078.8933
2023-11-11 10:57:36 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #884: GFLOPs: 5876.1230. Time: 21.6207 us. Best GFLOPs: 6078.8933
2023-11-11 10:57:36 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #885: GFLOPs: 5922.7656. Time: 21.4504 us. Best GFLOPs: 6078.8933
2023-11-11 10:57:36 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #886: GFLOPs: 5923.0296. Time: 21.4494 us. Best GFLOPs: 6078.8933
2023-11-11 10:57:36 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #887: GFLOPs: 5919.3145. Time: 21.4629 us. Best GFLOPs: 6078.8933
2023-11-11 10:57:36 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #888: GFLOPs: 5911.1497. Time: 21.4925 us. Best GFLOPs: 6078.8933
2023-11-11 10:57:36 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #889: GFLOPs: 5886.9210. Time: 21.5810 us. Best GFLOPs: 6078.8933
2023-11-11 10:57:36 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #890: GFLOPs: 5916.9606. Time: 21.4714 us. Best GFLOPs: 6078.8933
2023-11-11 10:57:36 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #891: GFLOPs: 5887.4341. Time: 21.5791 us. Best GFLOPs: 6078.8933
2023-11-11 10:57:36 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #892: GFLOPs: 5879.3111. Time: 21.6089 us. Best GFLOPs: 6078.8933
2023-11-11 10:57:36 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #893: GFLOPs: 5867.5017. Time: 21.6524 us. Best GFLOPs: 6078.8933
2023-11-11 10:57:36 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #894: GFLOPs: 14.0773. Time: 9024.8533 us. Best GFLOPs: 6078.8933
2023-11-11 10:57:36 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #895: GFLOPs: 1400.4756. Time: 90.7161 us. Best GFLOPs: 6078.8933
2023-11-11 10:57:36 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #896: GFLOPs: 81.1142. Time: 1566.2560 us. Best GFLOPs: 6078.8933
2023-11-11 11:09:27 [INFO] [evolutionary_search.cc:713] Generating candidates......
2023-11-11 11:09:30 [INFO] [evolutionary_search.cc:715] Picked top 102 candidate(s) from database
2023-11-11 11:09:37 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310a03fb48)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310e893e78)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310e893578)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310ec9ace8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56310e986408)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310eb1cfb8)]: 399 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310dc6f7b8)]: 0 failure(s)
2023-11-11 11:09:44 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310a03fb48)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310e893e78)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310e893578)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310ec9ace8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56310e986408)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310eb1cfb8)]: 796 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310dc6f7b8)]: 0 failure(s)
2023-11-11 11:09:50 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310a03fb48)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310e893e78)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310e893578)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310ec9ace8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56310e986408)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310eb1cfb8)]: 1201 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310dc6f7b8)]: 0 failure(s)
2023-11-11 11:09:57 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310a03fb48)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310e893e78)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310e893578)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310ec9ace8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56310e986408)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310eb1cfb8)]: 1588 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310dc6f7b8)]: 0 failure(s)
2023-11-11 11:09:57 [INFO] [evolutionary_search.cc:723] Sampled 52 candidate(s)
2023-11-11 11:10:22 [INFO] [evolutionary_search.cc:621] Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310a03fb48)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310e893e78)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310e893578)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310ec9ace8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56310e986408)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310eb1cfb8)]: 328 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310dc6f7b8)]: 0 failure(s)
2023-11-11 11:10:50 [INFO] [evolutionary_search.cc:621] Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310a03fb48)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310e893e78)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310e893578)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310ec9ace8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56310e986408)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310eb1cfb8)]: 321 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310dc6f7b8)]: 0 failure(s)
2023-11-11 11:11:16 [INFO] [evolutionary_search.cc:621] Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310a03fb48)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310e893e78)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310e893578)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310ec9ace8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56310e986408)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310eb1cfb8)]: 245 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310dc6f7b8)]: 0 failure(s)
2023-11-11 11:11:41 [INFO] [evolutionary_search.cc:621] Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310a03fb48)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310e893e78)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310e893578)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310ec9ace8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56310e986408)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310eb1cfb8)]: 230 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310dc6f7b8)]: 0 failure(s)
2023-11-11 11:11:48 [INFO] [evolutionary_search.cc:649] Scores of the best 64 candidates:
[1 : 16]:	0.9813  0.9813  0.9808  0.9800  0.9799  0.9798  0.9798  0.9797  0.9791  0.9791  0.9788  0.9787  0.9787  0.9787  0.9781  0.9780
[17 : 32]:	0.9779  0.9775  0.9765  0.9765  0.9764  0.9763  0.9763  0.9760  0.9756  0.9755  0.9753  0.9753  0.9751  0.9751  0.9751  0.9749
[33 : 48]:	0.9747  0.9742  0.9742  0.9741  0.9741  0.9740  0.9740  0.9739  0.9738  0.9737  0.9737  0.9736  0.9734  0.9734  0.9734  0.9732
[49 : 64]:	0.9728  0.9728  0.9727  0.9727  0.9726  0.9726  0.9726  0.9726  0.9726  0.9725  0.9725  0.9724  0.9724  0.9723  0.9723  0.9722
2023-11-11 11:11:48 [INFO] [evolutionary_search.cc:727] Got 64 candidate(s) with evolutionary search
2023-11-11 11:11:48 [INFO] [evolutionary_search.cc:730] Sending 64 candidates(s) for measurement
2023-11-11 11:12:23 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #897: GFLOPs: 6112.8394. Time: 20.7834 us. Best GFLOPs: 6112.8394
2023-11-11 11:12:23 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #898: GFLOPs: 6072.9559. Time: 20.9199 us. Best GFLOPs: 6112.8394
2023-11-11 11:12:23 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #899: GFLOPs: 6079.9341. Time: 20.8959 us. Best GFLOPs: 6112.8394
2023-11-11 11:12:23 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #900: GFLOPs: 5974.5111. Time: 21.2646 us. Best GFLOPs: 6112.8394
2023-11-11 11:12:23 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #901: GFLOPs: 6005.2729. Time: 21.1557 us. Best GFLOPs: 6112.8394
2023-11-11 11:12:23 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #902: GFLOPs: 5939.3007. Time: 21.3907 us. Best GFLOPs: 6112.8394
2023-11-11 11:12:23 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #903: GFLOPs: 5960.8382. Time: 21.3134 us. Best GFLOPs: 6112.8394
2023-11-11 11:12:23 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #904: GFLOPs: 6071.7742. Time: 20.9240 us. Best GFLOPs: 6112.8394
2023-11-11 11:12:23 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #905: GFLOPs: 6016.4849. Time: 21.1163 us. Best GFLOPs: 6112.8394
2023-11-11 11:12:23 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #906: GFLOPs: 6015.0425. Time: 21.1213 us. Best GFLOPs: 6112.8394
2023-11-11 11:12:23 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #907: GFLOPs: 6137.4284. Time: 20.7001 us. Best GFLOPs: 6137.4284
2023-11-11 11:12:23 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #908: GFLOPs: 6014.7949. Time: 21.1222 us. Best GFLOPs: 6137.4284
2023-11-11 11:12:23 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #909: GFLOPs: 6065.7523. Time: 20.9447 us. Best GFLOPs: 6137.4284
2023-11-11 11:12:23 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #910: GFLOPs: 6011.9988. Time: 21.1320 us. Best GFLOPs: 6137.4284
2023-11-11 11:12:23 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #911: GFLOPs: 6065.6690. Time: 20.9450 us. Best GFLOPs: 6137.4284
2023-11-11 11:12:23 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #912: GFLOPs: 5934.4822. Time: 21.4080 us. Best GFLOPs: 6137.4284
2023-11-11 11:12:23 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #913: GFLOPs: 6071.3554. Time: 20.9254 us. Best GFLOPs: 6137.4284
2023-11-11 11:12:23 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #914: GFLOPs: 6015.1236. Time: 21.1210 us. Best GFLOPs: 6137.4284
2023-11-11 11:12:23 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #915: GFLOPs: 5969.1072. Time: 21.2839 us. Best GFLOPs: 6137.4284
2023-11-11 11:12:23 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #916: GFLOPs: 6073.2589. Time: 20.9189 us. Best GFLOPs: 6137.4284
2023-11-11 11:12:23 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #917: GFLOPs: 5973.7094. Time: 21.2675 us. Best GFLOPs: 6137.4284
2023-11-11 11:12:23 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #918: GFLOPs: 6068.9639. Time: 20.9337 us. Best GFLOPs: 6137.4284
2023-11-11 11:12:23 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #919: GFLOPs: 6031.0357. Time: 21.0653 us. Best GFLOPs: 6137.4284
2023-11-11 11:12:23 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #920: GFLOPs: 6026.1075. Time: 21.0825 us. Best GFLOPs: 6137.4284
2023-11-11 11:12:23 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #921: GFLOPs: 6079.3215. Time: 20.8980 us. Best GFLOPs: 6137.4284
2023-11-11 11:12:23 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #922: GFLOPs: 6013.6000. Time: 21.1264 us. Best GFLOPs: 6137.4284
2023-11-11 11:12:23 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #923: GFLOPs: 6003.1126. Time: 21.1633 us. Best GFLOPs: 6137.4284
2023-11-11 11:12:23 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #924: GFLOPs: 5973.0474. Time: 21.2698 us. Best GFLOPs: 6137.4284
2023-11-11 11:12:23 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #925: GFLOPs: 6010.2954. Time: 21.1380 us. Best GFLOPs: 6137.4284
2023-11-11 11:12:23 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #926: GFLOPs: 5949.5253. Time: 21.3539 us. Best GFLOPs: 6137.4284
2023-11-11 11:12:23 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #927: GFLOPs: 6065.7757. Time: 20.9447 us. Best GFLOPs: 6137.4284
2023-11-11 11:12:23 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #928: GFLOPs: 5964.7119. Time: 21.2995 us. Best GFLOPs: 6137.4284
2023-11-11 11:12:23 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #929: GFLOPs: 5980.7795. Time: 21.2423 us. Best GFLOPs: 6137.4284
2023-11-11 11:12:23 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #930: GFLOPs: 5935.6859. Time: 21.4037 us. Best GFLOPs: 6137.4284
2023-11-11 11:12:23 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #931: GFLOPs: 5938.4163. Time: 21.3939 us. Best GFLOPs: 6137.4284
2023-11-11 11:12:23 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #932: GFLOPs: 5937.9612. Time: 21.3955 us. Best GFLOPs: 6137.4284
2023-11-11 11:12:23 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #933: GFLOPs: 6028.3435. Time: 21.0747 us. Best GFLOPs: 6137.4284
2023-11-11 11:12:23 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #934: GFLOPs: 5915.8440. Time: 21.4755 us. Best GFLOPs: 6137.4284
2023-11-11 11:12:23 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #935: GFLOPs: 6032.9617. Time: 21.0586 us. Best GFLOPs: 6137.4284
2023-11-11 11:12:23 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #936: GFLOPs: 5955.6221. Time: 21.3321 us. Best GFLOPs: 6137.4284
2023-11-11 11:12:23 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #937: GFLOPs: 6047.6681. Time: 21.0074 us. Best GFLOPs: 6137.4284
2023-11-11 11:12:23 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #938: GFLOPs: 5987.2663. Time: 21.2193 us. Best GFLOPs: 6137.4284
2023-11-11 11:12:23 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #939: GFLOPs: 6028.5941. Time: 21.0738 us. Best GFLOPs: 6137.4284
2023-11-11 11:12:23 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #940: GFLOPs: 5944.9142. Time: 21.3705 us. Best GFLOPs: 6137.4284
2023-11-11 11:12:23 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #941: GFLOPs: 5936.7699. Time: 21.3998 us. Best GFLOPs: 6137.4284
2023-11-11 11:12:23 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #942: GFLOPs: 6064.5789. Time: 20.9488 us. Best GFLOPs: 6137.4284
2023-11-11 11:12:23 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #943: GFLOPs: 6028.1782. Time: 21.0753 us. Best GFLOPs: 6137.4284
2023-11-11 11:12:23 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #944: GFLOPs: 5893.0652. Time: 21.5585 us. Best GFLOPs: 6137.4284
2023-11-11 11:12:23 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #945: GFLOPs: 5947.8988. Time: 21.3598 us. Best GFLOPs: 6137.4284
2023-11-11 11:12:23 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #946: GFLOPs: 6006.1111. Time: 21.1527 us. Best GFLOPs: 6137.4284
2023-11-11 11:12:23 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #947: GFLOPs: 5930.3209. Time: 21.4231 us. Best GFLOPs: 6137.4284
2023-11-11 11:12:23 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #948: GFLOPs: 5988.6547. Time: 21.2144 us. Best GFLOPs: 6137.4284
2023-11-11 11:12:23 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #949: GFLOPs: 6009.8121. Time: 21.1397 us. Best GFLOPs: 6137.4284
2023-11-11 11:12:23 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #950: GFLOPs: 5912.3618. Time: 21.4881 us. Best GFLOPs: 6137.4284
2023-11-11 11:12:23 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #951: GFLOPs: 6010.0207. Time: 21.1390 us. Best GFLOPs: 6137.4284
2023-11-11 11:12:23 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #952: GFLOPs: 5930.5927. Time: 21.4221 us. Best GFLOPs: 6137.4284
2023-11-11 11:12:23 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #953: GFLOPs: 6065.8581. Time: 20.9444 us. Best GFLOPs: 6137.4284
2023-11-11 11:12:23 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #954: GFLOPs: 5950.1903. Time: 21.3515 us. Best GFLOPs: 6137.4284
2023-11-11 11:12:23 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #955: GFLOPs: 5987.0210. Time: 21.2202 us. Best GFLOPs: 6137.4284
2023-11-11 11:12:23 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #956: GFLOPs: 5940.2966. Time: 21.3871 us. Best GFLOPs: 6137.4284
2023-11-11 11:12:23 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #957: GFLOPs: 6014.3928. Time: 21.1236 us. Best GFLOPs: 6137.4284
2023-11-11 11:12:23 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #958: GFLOPs: 1189.8566. Time: 106.7739 us. Best GFLOPs: 6137.4284
2023-11-11 11:12:23 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #959: GFLOPs: 417.1427. Time: 304.5616 us. Best GFLOPs: 6137.4284
2023-11-11 11:12:23 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #960: GFLOPs: 31.1879. Time: 4073.5538 us. Best GFLOPs: 6137.4284
2023-11-11 11:24:13 [INFO] [evolutionary_search.cc:713] Generating candidates......
2023-11-11 11:24:16 [INFO] [evolutionary_search.cc:715] Picked top 102 candidate(s) from database
2023-11-11 11:24:23 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310a03fb48)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310e893e78)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310e893578)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310ec9ace8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56310e986408)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310eb1cfb8)]: 389 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310dc6f7b8)]: 0 failure(s)
2023-11-11 11:24:30 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310a03fb48)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310e893e78)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310e893578)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310ec9ace8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56310e986408)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310eb1cfb8)]: 774 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310dc6f7b8)]: 0 failure(s)
2023-11-11 11:24:36 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310a03fb48)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310e893e78)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310e893578)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310ec9ace8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56310e986408)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310eb1cfb8)]: 1168 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310dc6f7b8)]: 0 failure(s)
2023-11-11 11:24:36 [INFO] [evolutionary_search.cc:723] Sampled 62 candidate(s)
2023-11-11 11:25:00 [INFO] [evolutionary_search.cc:621] Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310a03fb48)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310e893e78)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310e893578)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310ec9ace8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56310e986408)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310eb1cfb8)]: 318 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310dc6f7b8)]: 0 failure(s)
2023-11-11 11:25:27 [INFO] [evolutionary_search.cc:621] Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310a03fb48)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310e893e78)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310e893578)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310ec9ace8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56310e986408)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310eb1cfb8)]: 321 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310dc6f7b8)]: 0 failure(s)
2023-11-11 11:25:53 [INFO] [evolutionary_search.cc:621] Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310a03fb48)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310e893e78)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310e893578)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310ec9ace8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56310e986408)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310eb1cfb8)]: 303 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310dc6f7b8)]: 0 failure(s)
2023-11-11 11:26:21 [INFO] [evolutionary_search.cc:621] Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310a03fb48)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310e893e78)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310e893578)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310ec9ace8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56310e986408)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310eb1cfb8)]: 340 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310dc6f7b8)]: 0 failure(s)
2023-11-11 11:26:27 [INFO] [evolutionary_search.cc:649] Scores of the best 40 candidates:
[1 : 16]:	0.9824  0.9820  0.9813  0.9805  0.9800  0.9799  0.9792  0.9790  0.9790  0.9787  0.9787  0.9787  0.9785  0.9784  0.9782  0.9780
[17 : 32]:	0.9780  0.9777  0.9776  0.9776  0.9774  0.9773  0.9772  0.9767  0.9767  0.9765  0.9765  0.9761  0.9761  0.9754  0.9753  0.9753
[33 : 40]:	0.9752  0.9750  0.9749  0.9748  0.9747  0.9747  0.9747  0.9746
2023-11-11 11:26:27 [INFO] [evolutionary_search.cc:727] Got 40 candidate(s) with evolutionary search
2023-11-11 11:26:27 [INFO] [evolutionary_search.cc:730] Sending 40 candidates(s) for measurement
2023-11-11 11:26:52 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #961: GFLOPs: 5898.6117. Time: 21.5382 us. Best GFLOPs: 6137.4284
2023-11-11 11:26:52 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #962: GFLOPs: 5897.0031. Time: 21.5441 us. Best GFLOPs: 6137.4284
2023-11-11 11:26:52 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #963: GFLOPs: 5952.2010. Time: 21.3443 us. Best GFLOPs: 6137.4284
2023-11-11 11:26:52 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #964: GFLOPs: 5949.9291. Time: 21.3525 us. Best GFLOPs: 6137.4284
2023-11-11 11:26:52 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #965: GFLOPs: 5864.3846. Time: 21.6639 us. Best GFLOPs: 6137.4284
2023-11-11 11:26:52 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #966: GFLOPs: 5863.9537. Time: 21.6655 us. Best GFLOPs: 6137.4284
2023-11-11 11:26:52 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #967: GFLOPs: 5891.5041. Time: 21.5642 us. Best GFLOPs: 6137.4284
2023-11-11 11:26:52 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #968: GFLOPs: 5880.9237. Time: 21.6030 us. Best GFLOPs: 6137.4284
2023-11-11 11:26:52 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #969: GFLOPs: 5896.8078. Time: 21.5448 us. Best GFLOPs: 6137.4284
2023-11-11 11:26:52 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #970: GFLOPs: 5880.4532. Time: 21.6047 us. Best GFLOPs: 6137.4284
2023-11-11 11:26:52 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #971: GFLOPs: 5882.1871. Time: 21.5984 us. Best GFLOPs: 6137.4284
2023-11-11 11:26:52 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #972: GFLOPs: 5862.2287. Time: 21.6719 us. Best GFLOPs: 6137.4284
2023-11-11 11:26:52 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #973: GFLOPs: 5949.6064. Time: 21.3536 us. Best GFLOPs: 6137.4284
2023-11-11 11:26:52 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #974: GFLOPs: 5880.7684. Time: 21.6036 us. Best GFLOPs: 6137.4284
2023-11-11 11:26:52 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #975: GFLOPs: 5867.0007. Time: 21.6543 us. Best GFLOPs: 6137.4284
2023-11-11 11:26:52 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #976: GFLOPs: 5896.2530. Time: 21.5468 us. Best GFLOPs: 6137.4284
2023-11-11 11:26:52 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #977: GFLOPs: 5820.9977. Time: 21.8254 us. Best GFLOPs: 6137.4284
2023-11-11 11:26:52 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #978: GFLOPs: 5856.5518. Time: 21.6929 us. Best GFLOPs: 6137.4284
2023-11-11 11:26:52 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #979: GFLOPs: 5849.4954. Time: 21.7191 us. Best GFLOPs: 6137.4284
2023-11-11 11:26:52 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #980: GFLOPs: 5846.9648. Time: 21.7285 us. Best GFLOPs: 6137.4284
2023-11-11 11:26:52 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #981: GFLOPs: 5842.0729. Time: 21.7467 us. Best GFLOPs: 6137.4284
2023-11-11 11:26:52 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #982: GFLOPs: 5962.6199. Time: 21.3070 us. Best GFLOPs: 6137.4284
2023-11-11 11:26:52 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #983: GFLOPs: 5954.0053. Time: 21.3378 us. Best GFLOPs: 6137.4284
2023-11-11 11:26:52 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #984: GFLOPs: 5957.5611. Time: 21.3251 us. Best GFLOPs: 6137.4284
2023-11-11 11:26:52 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #985: GFLOPs: 5946.7588. Time: 21.3638 us. Best GFLOPs: 6137.4284
2023-11-11 11:26:52 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #986: GFLOPs: 5863.3757. Time: 21.6677 us. Best GFLOPs: 6137.4284
2023-11-11 11:26:52 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #987: GFLOPs: 5957.6822. Time: 21.3247 us. Best GFLOPs: 6137.4284
2023-11-11 11:26:52 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #988: GFLOPs: 5845.9429. Time: 21.7323 us. Best GFLOPs: 6137.4284
2023-11-11 11:26:52 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #989: GFLOPs: 5966.7541. Time: 21.2923 us. Best GFLOPs: 6137.4284
2023-11-11 11:26:52 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #990: GFLOPs: 5806.4962. Time: 21.8799 us. Best GFLOPs: 6137.4284
2023-11-11 11:26:52 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #991: GFLOPs: 5888.8517. Time: 21.5739 us. Best GFLOPs: 6137.4284
2023-11-11 11:26:52 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #992: GFLOPs: 5956.2268. Time: 21.3299 us. Best GFLOPs: 6137.4284
2023-11-11 11:26:52 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #993: GFLOPs: 5831.2807. Time: 21.7869 us. Best GFLOPs: 6137.4284
2023-11-11 11:26:52 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #994: GFLOPs: 5829.4096. Time: 21.7939 us. Best GFLOPs: 6137.4284
2023-11-11 11:26:52 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #995: GFLOPs: 5887.3947. Time: 21.5793 us. Best GFLOPs: 6137.4284
2023-11-11 11:26:52 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #996: GFLOPs: 5967.4032. Time: 21.2899 us. Best GFLOPs: 6137.4284
2023-11-11 11:26:52 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #997: GFLOPs: 5904.9747. Time: 21.5150 us. Best GFLOPs: 6137.4284
2023-11-11 11:26:52 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #998: GFLOPs: 5900.2164. Time: 21.5324 us. Best GFLOPs: 6137.4284
2023-11-11 11:26:52 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #999: GFLOPs: 215.7320. Time: 588.9050 us. Best GFLOPs: 6137.4284
2023-11-11 11:26:52 [INFO] [task_scheduler.cc:131] [Task #14: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_nn_relu_1] Trial #1000: GFLOPs: 308.3758. Time: 411.9832 us. Best GFLOPs: 6137.4284
