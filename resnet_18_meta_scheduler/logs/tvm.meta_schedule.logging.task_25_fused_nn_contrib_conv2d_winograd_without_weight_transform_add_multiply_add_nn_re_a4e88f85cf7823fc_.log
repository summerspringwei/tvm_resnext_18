2023-11-10 23:34:21 [INFO] [task_scheduler.cc:160] Initializing Task #25: "fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_"
2023-11-10 23:34:21 [INFO] [task_scheduler.cc:35] 
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(512), T.int64(7), T.int64(7)), "float32"), p1: T.Buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(512)), "float32"), p2: T.Buffer((T.int64(1), T.int64(512), T.int64(7), T.int64(7)), "float32"), p3: T.Buffer((T.int64(1), T.int64(512), T.int64(1), T.int64(1)), "float32"), p4: T.Buffer((T.int64(1), T.int64(512), T.int64(1), T.int64(1)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(512), T.int64(7), T.int64(7)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        data_pad = T.alloc_buffer((T.int64(1), T.int64(512), T.int64(9), T.int64(9)))
        input_tile = T.alloc_buffer((T.int64(512), T.int64(16), T.int64(4), T.int64(4)))
        B = T.alloc_buffer((T.int64(4), T.int64(4)))
        data_pack = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(16)))
        bgemm = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(16)))
        A = T.alloc_buffer((T.int64(4), T.int64(2)))
        inverse = T.alloc_buffer((T.int64(512), T.int64(16), T.int64(2), T.int64(2)))
        conv2d_winograd = T.alloc_buffer((T.int64(1), T.int64(512), T.int64(7), T.int64(7)))
        T_add = T.alloc_buffer((T.int64(1), T.int64(512), T.int64(7), T.int64(7)))
        T_multiply = T.alloc_buffer((T.int64(1), T.int64(512), T.int64(7), T.int64(7)))
        T_add_1 = T.alloc_buffer((T.int64(1), T.int64(512), T.int64(7), T.int64(7)))
        for i0, i1, i2, i3 in T.grid(T.int64(1), T.int64(512), T.int64(9), T.int64(9)):
            with T.block("data_pad"):
                v_i0, v_i1, v_i2, v_i3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(p0[v_i0, v_i1, v_i2 - T.int64(1), v_i3 - T.int64(1)])
                T.writes(data_pad[v_i0, v_i1, v_i2, v_i3])
                data_pad[v_i0, v_i1, v_i2, v_i3] = T.if_then_else(T.int64(1) <= v_i2 and v_i2 < T.int64(8) and T.int64(1) <= v_i3 and v_i3 < T.int64(8), p0[v_i0, v_i1, v_i2 - T.int64(1), v_i3 - T.int64(1)], T.float32(0))
        for ci, p, eps, nu in T.grid(T.int64(512), T.int64(16), T.int64(4), T.int64(4)):
            with T.block("input_tile"):
                v_ci, v_p, v_eps, v_nu = T.axis.remap("SSSS", [ci, p, eps, nu])
                T.reads(data_pad[v_p // T.int64(16), v_ci, v_p % T.int64(16) // T.int64(4) * T.int64(2) + v_eps, v_p % T.int64(4) * T.int64(2) + v_nu])
                T.writes(input_tile[v_ci, v_p, v_eps, v_nu])
                T.block_attr({"schedule_rule": "None"})
                input_tile[v_ci, v_p, v_eps, v_nu] = data_pad[v_p // T.int64(16), v_ci, v_p % T.int64(16) // T.int64(4) * T.int64(2) + v_eps, v_p % T.int64(4) * T.int64(2) + v_nu]
        for i, j in T.grid(T.int64(4), T.int64(4)):
            with T.block("B"):
                v_i, v_j = T.axis.remap("SS", [i, j])
                T.reads()
                T.writes(B[v_i, v_j])
                T.block_attr({"schedule_rule": "None"})
                B[v_i, v_j] = T.Select(v_i % T.int64(4) == T.int64(3) and v_j % T.int64(4) == T.int64(3), T.float32(1), T.Select(v_i % T.int64(4) == T.int64(3) and v_j % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_i % T.int64(4) == T.int64(3) and v_j % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_i % T.int64(4) == T.int64(3) and v_j % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_i % T.int64(4) == T.int64(2) and v_j % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_i % T.int64(4) == T.int64(2) and v_j % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_i % T.int64(4) == T.int64(2) and v_j % T.int64(4) == T.int64(1), T.float32(1), T.Select(v_i % T.int64(4) == T.int64(2) and v_j % T.int64(4) == T.int64(0), T.float32(-1), T.Select(v_i % T.int64(4) == T.int64(1) and v_j % T.int64(4) == T.int64(3), T.float32(-1), T.Select(v_i % T.int64(4) == T.int64(1) and v_j % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_i % T.int64(4) == T.int64(1) and v_j % T.int64(4) == T.int64(1), T.float32(-1), T.Select(v_i % T.int64(4) == T.int64(1) and v_j % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_i % T.int64(4) == T.int64(0) and v_j % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_i % T.int64(4) == T.int64(0) and v_j % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_i % T.int64(4) == T.int64(0) and v_j % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_i % T.int64(4) == T.int64(0) and v_j % T.int64(4) == T.int64(0), T.float32(1), T.float32(0)))))))))))))))))
        for eps, nu, ci, p, r_a, r_b in T.grid(T.int64(4), T.int64(4), T.int64(512), T.int64(16), T.int64(4), T.int64(4)):
            with T.block("data_pack"):
                v_eps, v_nu, v_ci, v_p, v_r_a, v_r_b = T.axis.remap("SSSSRR", [eps, nu, ci, p, r_a, r_b])
                T.reads(input_tile[v_ci, v_p, v_r_a, v_r_b], B[T.min(v_r_a, v_r_b):T.min(v_r_a, v_r_b) + (T.max(v_r_a, v_r_b) + T.int64(1) - T.min(v_r_a, v_r_b)), T.min(v_eps, v_nu):T.min(v_eps, v_nu) + (T.max(v_eps, v_nu) + T.int64(1) - T.min(v_eps, v_nu))])
                T.writes(data_pack[v_eps, v_nu, v_ci, v_p])
                T.block_attr({"schedule_rule": "conv2d_nchw_winograd_data_pack"})
                with T.init():
                    data_pack[v_eps, v_nu, v_ci, v_p] = T.float32(0)
                data_pack[v_eps, v_nu, v_ci, v_p] = data_pack[v_eps, v_nu, v_ci, v_p] + input_tile[v_ci, v_p, v_r_a, v_r_b] * B[v_r_a, v_eps] * B[v_r_b, v_nu]
        for eps, nu, co, p, ci in T.grid(T.int64(4), T.int64(4), T.int64(512), T.int64(16), T.int64(512)):
            with T.block("bgemm"):
                v_eps, v_nu, v_co, v_p, v_ci = T.axis.remap("SSSSR", [eps, nu, co, p, ci])
                T.reads(data_pack[v_eps, v_nu, v_ci, v_p], p1[v_eps, v_nu, v_ci, v_co])
                T.writes(bgemm[v_eps, v_nu, v_co, v_p])
                with T.init():
                    bgemm[v_eps, v_nu, v_co, v_p] = T.float32(0)
                bgemm[v_eps, v_nu, v_co, v_p] = bgemm[v_eps, v_nu, v_co, v_p] + data_pack[v_eps, v_nu, v_ci, v_p] * p1[v_eps, v_nu, v_ci, v_co]
        for i, j in T.grid(T.int64(4), T.int64(2)):
            with T.block("A"):
                v_i, v_j = T.axis.remap("SS", [i, j])
                T.reads()
                T.writes(A[v_i, v_j])
                T.block_attr({"schedule_rule": "None"})
                A[v_i, v_j] = T.Select(v_i % T.int64(4) == T.int64(3) and v_j % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_i % T.int64(4) == T.int64(3) and v_j % T.int64(2) == T.int64(0), T.float32(0), T.Select(v_i % T.int64(4) == T.int64(2) and v_j % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_i % T.int64(4) == T.int64(2) and v_j % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_i % T.int64(4) == T.int64(1) and v_j % T.int64(2) == T.int64(1), T.float32(-1), T.Select(v_i % T.int64(4) == T.int64(1) and v_j % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_i % T.int64(4) == T.int64(0) and v_j % T.int64(2) == T.int64(1), T.float32(0), T.Select(v_i % T.int64(4) == T.int64(0) and v_j % T.int64(2) == T.int64(0), T.float32(1), T.float32(0)))))))))
        for co, p, vh, vw, r_a, r_b in T.grid(T.int64(512), T.int64(16), T.int64(2), T.int64(2), T.int64(4), T.int64(4)):
            with T.block("inverse"):
                v_co, v_p, v_vh, v_vw, v_r_a, v_r_b = T.axis.remap("SSSSRR", [co, p, vh, vw, r_a, r_b])
                T.reads(bgemm[v_r_a, v_r_b, v_co, v_p], A[T.min(v_r_a, v_r_b):T.min(v_r_a, v_r_b) + (T.max(v_r_a, v_r_b) + T.int64(1) - T.min(v_r_a, v_r_b)), T.min(v_vh, v_vw):T.min(v_vh, v_vw) + (T.max(v_vh, v_vw) + T.int64(1) - T.min(v_vh, v_vw))])
                T.writes(inverse[v_co, v_p, v_vh, v_vw])
                T.block_attr({"schedule_rule": "conv2d_nchw_winograd_inverse"})
                with T.init():
                    inverse[v_co, v_p, v_vh, v_vw] = T.float32(0)
                inverse[v_co, v_p, v_vh, v_vw] = inverse[v_co, v_p, v_vh, v_vw] + bgemm[v_r_a, v_r_b, v_co, v_p] * A[v_r_a, v_vh] * A[v_r_b, v_vw]
        for n, co, h, w in T.grid(T.int64(1), T.int64(512), T.int64(7), T.int64(7)):
            with T.block("conv2d_winograd"):
                v_n, v_co, v_h, v_w = T.axis.remap("SSSS", [n, co, h, w])
                T.reads(inverse[v_co, v_n * T.int64(16) + v_h // T.int64(2) * T.int64(4) + v_w // T.int64(2), v_h % T.int64(2), v_w % T.int64(2)])
                T.writes(conv2d_winograd[v_n, v_co, v_h, v_w])
                conv2d_winograd[v_n, v_co, v_h, v_w] = inverse[v_co, v_n * T.int64(16) + v_h // T.int64(2) * T.int64(4) + v_w // T.int64(2), v_h % T.int64(2), v_w % T.int64(2)]
        for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(512), T.int64(7), T.int64(7)):
            with T.block("T_add"):
                v_ax0, v_ax1, v_ax2, v_ax3 = T.axis.remap("SSSS", [ax0, ax1, ax2, ax3])
                T.reads(conv2d_winograd[v_ax0, v_ax1, v_ax2, v_ax3], p2[v_ax0, v_ax1, v_ax2, v_ax3])
                T.writes(T_add[v_ax0, v_ax1, v_ax2, v_ax3])
                T_add[v_ax0, v_ax1, v_ax2, v_ax3] = conv2d_winograd[v_ax0, v_ax1, v_ax2, v_ax3] + p2[v_ax0, v_ax1, v_ax2, v_ax3]
        for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(512), T.int64(7), T.int64(7)):
            with T.block("T_multiply"):
                v_ax0, v_ax1, v_ax2, v_ax3 = T.axis.remap("SSSS", [ax0, ax1, ax2, ax3])
                T.reads(T_add[v_ax0, v_ax1, v_ax2, v_ax3], p3[v_ax0, v_ax1, T.int64(0), T.int64(0)])
                T.writes(T_multiply[v_ax0, v_ax1, v_ax2, v_ax3])
                T_multiply[v_ax0, v_ax1, v_ax2, v_ax3] = T_add[v_ax0, v_ax1, v_ax2, v_ax3] * p3[v_ax0, v_ax1, T.int64(0), T.int64(0)]
        for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(512), T.int64(7), T.int64(7)):
            with T.block("T_add_1"):
                v_ax0, v_ax1, v_ax2, v_ax3 = T.axis.remap("SSSS", [ax0, ax1, ax2, ax3])
                T.reads(T_multiply[v_ax0, v_ax1, v_ax2, v_ax3], p4[v_ax0, v_ax1, T.int64(0), T.int64(0)])
                T.writes(T_add_1[v_ax0, v_ax1, v_ax2, v_ax3])
                T_add_1[v_ax0, v_ax1, v_ax2, v_ax3] = T_multiply[v_ax0, v_ax1, v_ax2, v_ax3] + p4[v_ax0, v_ax1, T.int64(0), T.int64(0)]
        for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(512), T.int64(7), T.int64(7)):
            with T.block("T_relu"):
                v_ax0, v_ax1, v_ax2, v_ax3 = T.axis.remap("SSSS", [ax0, ax1, ax2, ax3])
                T.reads(T_add_1[v_ax0, v_ax1, v_ax2, v_ax3])
                T.writes(T_relu[v_ax0, v_ax1, v_ax2, v_ax3])
                T_relu[v_ax0, v_ax1, v_ax2, v_ax3] = T.max(T_add_1[v_ax0, v_ax1, v_ax2, v_ax3], T.float32(0))
2023-11-10 23:34:21 [INFO] [task_scheduler.cc:164] Total 3 design space(s) generated
2023-11-10 23:34:21 [INFO] [task_scheduler.cc:170] Design space #0:
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(512), T.int64(7), T.int64(7)), "float32"), p1: T.Buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(512)), "float32"), p2: T.Buffer((T.int64(1), T.int64(512), T.int64(7), T.int64(7)), "float32"), p3: T.Buffer((T.int64(1), T.int64(512), T.int64(1), T.int64(1)), "float32"), p4: T.Buffer((T.int64(1), T.int64(512), T.int64(1), T.int64(1)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(512), T.int64(7), T.int64(7)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit": 16})
            input_tile_local = T.alloc_buffer((T.int64(512), T.int64(16), T.int64(4), T.int64(4)), scope="local")
            data_pack = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(16)))
            bgemm = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(16)))
            inverse_local = T.alloc_buffer((T.int64(512), T.int64(16), T.int64(2), T.int64(2)), scope="local")
            data_pack_local = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(16)), scope="local")
            bgemm_local = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(16)), scope="local")
            data_pack_shared = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(16)), scope="shared")
            p1_shared = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(512)), scope="shared")
            for ci_p_fused_0 in T.thread_binding(T.int64(256), thread="blockIdx.x"):
                for ci_p_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(1), T.int64(4), T.int64(4)):
                        with T.block("input_tile"):
                            v_ci = T.axis.spatial(T.int64(512), ci_p_fused_0 * T.int64(2) + ci_p_fused_1 // T.int64(16) + ax0)
                            v_p = T.axis.spatial(T.int64(16), ci_p_fused_1 % T.int64(16) + ax1)
                            v_eps, v_nu = T.axis.remap("SS", [ax2, ax3])
                            T.reads(p0[v_p // T.int64(16), v_ci, v_p % T.int64(16) // T.int64(4) * T.int64(2) + v_eps - T.int64(1), v_p % T.int64(4) * T.int64(2) + v_nu - T.int64(1)])
                            T.writes(input_tile_local[v_ci, v_p, v_eps, v_nu])
                            T.block_attr({"schedule_rule": "None"})
                            input_tile_local[v_ci, v_p, v_eps, v_nu] = T.if_then_else(T.int64(1) <= v_p % T.int64(16) // T.int64(4) * T.int64(2) + v_eps and v_p % T.int64(16) // T.int64(4) * T.int64(2) + v_eps < T.int64(8) and T.int64(1) <= v_p % T.int64(4) * T.int64(2) + v_nu and v_p % T.int64(4) * T.int64(2) + v_nu < T.int64(8), p0[v_p // T.int64(16), v_ci, v_p % T.int64(16) // T.int64(4) * T.int64(2) + v_eps - T.int64(1), v_p % T.int64(4) * T.int64(2) + v_nu - T.int64(1)], T.float32(0))
                    for eps in T.unroll(T.int64(4)):
                        for nu in T.unroll(T.int64(4)):
                            for r_a in T.unroll(T.int64(4)):
                                for r_b in T.unroll(T.int64(4)):
                                    with T.block("data_pack"):
                                        v_eps, v_nu = T.axis.remap("SS", [eps, nu])
                                        v_ci = T.axis.spatial(T.int64(512), (ci_p_fused_0 * T.int64(32) + ci_p_fused_1) // T.int64(16))
                                        v_p = T.axis.spatial(T.int64(16), (ci_p_fused_0 * T.int64(32) + ci_p_fused_1) % T.int64(16))
                                        v_r_a, v_r_b = T.axis.remap("RR", [r_a, r_b])
                                        T.reads(input_tile_local[v_ci, v_p, v_r_a, v_r_b])
                                        T.writes(data_pack_local[v_eps, v_nu, v_ci, v_p])
                                        T.block_attr({"schedule_rule": "conv2d_nchw_winograd_data_pack"})
                                        with T.init():
                                            data_pack_local[v_eps, v_nu, v_ci, v_p] = T.float32(0)
                                        data_pack_local[v_eps, v_nu, v_ci, v_p] = data_pack_local[v_eps, v_nu, v_ci, v_p] + input_tile_local[v_ci, v_p, v_r_a, v_r_b] * T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(3), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(1), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(0), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(3), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(1), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(0), T.float32(1), T.float32(0))))))))))))))))) * T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(3), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(1), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(0), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(3), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(1), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(0), T.float32(1), T.float32(0)))))))))))))))))
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(4), T.int64(4), T.int64(1), T.int64(1)):
                        with T.block("data_pack_local"):
                            v0, v1 = T.axis.remap("SS", [ax0, ax1])
                            v2 = T.axis.spatial(T.int64(512), ci_p_fused_0 * T.int64(2) + ci_p_fused_1 // T.int64(16) + ax2)
                            v3 = T.axis.spatial(T.int64(16), ci_p_fused_1 % T.int64(16) + ax3)
                            T.reads(data_pack_local[v0, v1, v2, v3])
                            T.writes(data_pack[v0, v1, v2, v3])
                            data_pack[v0, v1, v2, v3] = data_pack_local[v0, v1, v2, v3]
            for eps_0_nu_0_co_0_p_0_fused in T.thread_binding(T.int64(8), thread="blockIdx.x"):
                for eps_1_nu_1_co_1_p_1_fused in T.thread_binding(T.int64(8), thread="vthread.x"):
                    for eps_2_nu_2_co_2_p_2_fused in T.thread_binding(T.int64(8), thread="threadIdx.x"):
                        for ci_0 in range(T.int64(2)):
                            for ax0_ax1_ax2_ax3_fused in range(T.int64(65536)):
                                with T.block("data_pack_shared"):
                                    v0 = T.axis.spatial(T.int64(4), ax0_ax1_ax2_ax3_fused // T.int64(16384))
                                    v1 = T.axis.spatial(T.int64(4), ax0_ax1_ax2_ax3_fused % T.int64(16384) // T.int64(4096))
                                    v2 = T.axis.spatial(T.int64(512), ci_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused % T.int64(4096) // T.int64(16))
                                    v3 = T.axis.spatial(T.int64(16), ax0_ax1_ax2_ax3_fused % T.int64(16))
                                    T.reads(data_pack[v0, v1, v2, v3])
                                    T.writes(data_pack_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch": 3})
                                    data_pack_shared[v0, v1, v2, v3] = data_pack[v0, v1, v2, v3]
                            for ax0_ax1_ax2_ax3_fused in range(T.int64(262144)):
                                with T.block("p1_shared"):
                                    v0 = T.axis.spatial(T.int64(4), ax0_ax1_ax2_ax3_fused // T.int64(65536))
                                    v1 = T.axis.spatial(T.int64(4), ax0_ax1_ax2_ax3_fused % T.int64(65536) // T.int64(16384))
                                    v2 = T.axis.spatial(T.int64(512), ci_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused % T.int64(16384) // T.int64(64))
                                    v3 = T.axis.spatial(T.int64(512), eps_0_nu_0_co_0_p_0_fused * T.int64(64) + ax0_ax1_ax2_ax3_fused % T.int64(64))
                                    T.reads(p1[v0, v1, v2, v3])
                                    T.writes(p1_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch": 2})
                                    p1_shared[v0, v1, v2, v3] = p1[v0, v1, v2, v3]
                            for ci_1, eps_3, nu_3, co_3, p_3, ci_2, eps_4, nu_4, co_4, p_4 in T.grid(T.int64(64), T.int64(1), T.int64(2), T.int64(1), T.int64(4), T.int64(4), T.int64(2), T.int64(1), T.int64(16), T.int64(1)):
                                with T.block("bgemm"):
                                    v_eps = T.axis.spatial(T.int64(4), eps_1_nu_1_co_1_p_1_fused // T.int64(4) * T.int64(2) + eps_3 * T.int64(2) + eps_4)
                                    v_nu = T.axis.spatial(T.int64(4), eps_2_nu_2_co_2_p_2_fused // T.int64(4) * T.int64(2) + nu_3 + nu_4)
                                    v_co = T.axis.spatial(T.int64(512), eps_0_nu_0_co_0_p_0_fused * T.int64(64) + eps_2_nu_2_co_2_p_2_fused % T.int64(4) * T.int64(16) + co_3 * T.int64(16) + co_4)
                                    v_p = T.axis.spatial(T.int64(16), eps_1_nu_1_co_1_p_1_fused % T.int64(4) * T.int64(4) + p_3 + p_4)
                                    v_ci = T.axis.reduce(T.int64(512), ci_0 * T.int64(256) + ci_1 * T.int64(4) + ci_2)
                                    T.reads(data_pack_shared[v_eps, v_nu, v_ci, v_p], p1_shared[v_eps, v_nu, v_ci, v_co])
                                    T.writes(bgemm_local[v_eps, v_nu, v_co, v_p])
                                    T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                    with T.init():
                                        bgemm_local[v_eps, v_nu, v_co, v_p] = T.float32(0)
                                    bgemm_local[v_eps, v_nu, v_co, v_p] = bgemm_local[v_eps, v_nu, v_co, v_p] + data_pack_shared[v_eps, v_nu, v_ci, v_p] * p1_shared[v_eps, v_nu, v_ci, v_co]
                        for ax0, ax1, ax2, ax3 in T.grid(T.int64(2), T.int64(2), T.int64(16), T.int64(4)):
                            with T.block("bgemm_local"):
                                v0 = T.axis.spatial(T.int64(4), eps_1_nu_1_co_1_p_1_fused // T.int64(4) * T.int64(2) + ax0)
                                v1 = T.axis.spatial(T.int64(4), eps_2_nu_2_co_2_p_2_fused // T.int64(4) * T.int64(2) + ax1)
                                v2 = T.axis.spatial(T.int64(512), eps_0_nu_0_co_0_p_0_fused * T.int64(64) + eps_2_nu_2_co_2_p_2_fused % T.int64(4) * T.int64(16) + ax2)
                                v3 = T.axis.spatial(T.int64(16), eps_1_nu_1_co_1_p_1_fused % T.int64(4) * T.int64(4) + ax3)
                                T.reads(bgemm_local[v0, v1, v2, v3])
                                T.writes(bgemm[v0, v1, v2, v3])
                                bgemm[v0, v1, v2, v3] = bgemm_local[v0, v1, v2, v3]
            for n_co_h_0_w_0_fused_0 in T.thread_binding(T.int64(256), thread="blockIdx.x"):
                for n_co_h_0_w_0_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                    for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                        for ax2 in T.unroll(T.int64(2)):
                            for ax3 in T.unroll(T.int64(2)):
                                for ax4 in T.unroll(T.int64(4)):
                                    for ax5 in T.unroll(T.int64(4)):
                                        with T.block("inverse"):
                                            v_co = T.axis.spatial(T.int64(512), (n_co_h_0_w_0_fused_0 * T.int64(32) + n_co_h_0_w_0_fused_1) // T.int64(16) + ax0)
                                            v_p = T.axis.spatial(T.int64(16), (n_co_h_0_w_0_fused_0 * T.int64(32) + n_co_h_0_w_0_fused_1) % T.int64(16) + ax1)
                                            v_vh, v_vw, v_r_a, v_r_b = T.axis.remap("SSRR", [ax2, ax3, ax4, ax5])
                                            T.reads(bgemm[v_r_a, v_r_b, v_co, v_p])
                                            T.writes(inverse_local[v_co, v_p, v_vh, v_vw])
                                            T.block_attr({"schedule_rule": "conv2d_nchw_winograd_inverse"})
                                            with T.init():
                                                inverse_local[v_co, v_p, v_vh, v_vw] = T.float32(0)
                                            inverse_local[v_co, v_p, v_vh, v_vw] = inverse_local[v_co, v_p, v_vh, v_vw] + bgemm[v_r_a, v_r_b, v_co, v_p] * T.Select(v_r_a % T.int64(4) == T.int64(3) and v_vh % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_vh % T.int64(2) == T.int64(0), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_vh % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_vh % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_vh % T.int64(2) == T.int64(1), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_vh % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_vh % T.int64(2) == T.int64(1), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_vh % T.int64(2) == T.int64(0), T.float32(1), T.float32(0))))))))) * T.Select(v_r_b % T.int64(4) == T.int64(3) and v_vw % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_vw % T.int64(2) == T.int64(0), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_vw % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_vw % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_vw % T.int64(2) == T.int64(1), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_vw % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_vw % T.int64(2) == T.int64(1), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_vw % T.int64(2) == T.int64(0), T.float32(1), T.float32(0)))))))))
                    for h_1, w_1 in T.grid(T.int64(2), T.int64(2)):
                        with T.block("conv2d_winograd"):
                            v_n = T.axis.spatial(T.int64(1), T.int64(0))
                            v_co = T.axis.spatial(T.int64(512), (n_co_h_0_w_0_fused_0 * T.int64(32) + n_co_h_0_w_0_fused_1) // T.int64(16))
                            v_h = T.axis.spatial(T.int64(7), (n_co_h_0_w_0_fused_0 * T.int64(32) + n_co_h_0_w_0_fused_1) % T.int64(16) // T.int64(4) * T.int64(2) + h_1)
                            v_w = T.axis.spatial(T.int64(7), (n_co_h_0_w_0_fused_0 * T.int64(32) + n_co_h_0_w_0_fused_1) % T.int64(4) * T.int64(2) + w_1)
                            T.where((n_co_h_0_w_0_fused_0 * T.int64(32) + n_co_h_0_w_0_fused_1) % T.int64(16) // T.int64(4) * T.int64(2) + h_1 < T.int64(7) and (n_co_h_0_w_0_fused_0 * T.int64(32) + n_co_h_0_w_0_fused_1) % T.int64(4) * T.int64(2) + w_1 < T.int64(7))
                            T.reads(inverse_local[v_co, v_n * T.int64(16) + v_h // T.int64(2) * T.int64(4) + v_w // T.int64(2), v_h % T.int64(2), v_w % T.int64(2)], p2[v_n, v_co, v_h, v_w], p3[v_n, v_co, T.int64(0), T.int64(0)], p4[v_n, v_co, T.int64(0), T.int64(0)])
                            T.writes(T_relu[v_n, v_co, v_h, v_w])
                            T_relu[v_n, v_co, v_h, v_w] = T.max((inverse_local[v_co, v_n * T.int64(16) + v_h // T.int64(2) * T.int64(4) + v_w // T.int64(2), v_h % T.int64(2), v_w % T.int64(2)] + p2[v_n, v_co, v_h, v_w]) * p3[v_n, v_co, T.int64(0), T.int64(0)] + p4[v_n, v_co, T.int64(0), T.int64(0)], T.float32(0))
b0 = sch.get_block(name="data_pack", func_name="main")
b1 = sch.get_block(name="bgemm", func_name="main")
b2 = sch.get_block(name="inverse", func_name="main")
b3 = sch.get_block(name="conv2d_winograd", func_name="main")
b4 = sch.get_block(name="T_add", func_name="main")
b5 = sch.get_block(name="T_multiply", func_name="main")
b6 = sch.get_block(name="T_add_1", func_name="main")
b7 = sch.get_block(name="T_relu", func_name="main")
b8 = sch.get_block(name="root", func_name="main")
b9, b10 = sch.get_producers(block=b2)
sch.compute_inline(block=b10)
b11, = sch.get_consumers(block=b2)
l12, l13, l14, l15 = sch.get_loops(block=b11)
l16, l17 = sch.split(loop=l14, factors=[None, 2], preserve_unit_iters=True)
l18, l19 = sch.split(loop=l15, factors=[None, 2], preserve_unit_iters=True)
sch.reorder(l16, l18, l17, l19)
sch.compute_at(block=b2, loop=l18, preserve_unit_loops=True, index=-1)
sch.set_scope(block=b2, buffer_index=0, storage_scope="local")
l20, l21, l22, l23, l24, l25, l26, l27, l28, l29 = sch.get_loops(block=b2)
sch.unroll(loop=l26)
sch.unroll(loop=l27)
sch.unroll(loop=l28)
sch.unroll(loop=l29)
b30, b31 = sch.get_producers(block=b0)
sch.compute_inline(block=b31)
b32, = sch.get_producers(block=b30)
l33, l34, l35, l36, l37, l38 = sch.get_loops(block=b0)
sch.reorder(l35, l36, l33, l34, l37, l38)
sch.unroll(loop=l33)
sch.unroll(loop=l34)
sch.unroll(loop=l37)
sch.unroll(loop=l38)
l39 = sch.fuse(l35, l36, preserve_unit_iters=True)
v40 = sch.sample_categorical(candidates=[32, 64, 128, 256, 512, 1024], probs=[0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666], decision=0)
l41, l42 = sch.split(loop=l39, factors=[None, v40], preserve_unit_iters=True)
sch.bind(loop=l41, thread_axis="blockIdx.x")
sch.bind(loop=l42, thread_axis="threadIdx.x")
b43 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b43, loop=l42, preserve_unit_loops=True, index=-1)
sch.compute_at(block=b30, loop=l42, preserve_unit_loops=True, index=-1)
sch.set_scope(block=b30, buffer_index=0, storage_scope="local")
sch.compute_inline(block=b32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l44, l45, l46, l47, l48 = sch.get_loops(block=b1)
v49, v50, v51, v52, v53 = sch.sample_perfect_tile(loop=l44, n=5, max_innermost_factor=64, decision=[1, 2, 1, 1, 2])
l54, l55, l56, l57, l58 = sch.split(loop=l44, factors=[v49, v50, v51, v52, v53], preserve_unit_iters=True)
v59, v60, v61, v62, v63 = sch.sample_perfect_tile(loop=l45, n=5, max_innermost_factor=64, decision=[1, 1, 2, 2, 1])
l64, l65, l66, l67, l68 = sch.split(loop=l45, factors=[v59, v60, v61, v62, v63], preserve_unit_iters=True)
v69, v70, v71, v72, v73 = sch.sample_perfect_tile(loop=l46, n=5, max_innermost_factor=64, decision=[8, 1, 4, 1, 16])
l74, l75, l76, l77, l78 = sch.split(loop=l46, factors=[v69, v70, v71, v72, v73], preserve_unit_iters=True)
v79, v80, v81, v82, v83 = sch.sample_perfect_tile(loop=l47, n=5, max_innermost_factor=64, decision=[1, 4, 1, 4, 1])
l84, l85, l86, l87, l88 = sch.split(loop=l47, factors=[v79, v80, v81, v82, v83], preserve_unit_iters=True)
v89, v90, v91 = sch.sample_perfect_tile(loop=l48, n=3, max_innermost_factor=64, decision=[2, 64, 4])
l92, l93, l94 = sch.split(loop=l48, factors=[v89, v90, v91], preserve_unit_iters=True)
sch.reorder(l54, l64, l74, l84, l55, l65, l75, l85, l56, l66, l76, l86, l92, l93, l57, l67, l77, l87, l94, l58, l68, l78, l88)
l95 = sch.fuse(l54, l64, l74, l84, preserve_unit_iters=True)
sch.bind(loop=l95, thread_axis="blockIdx.x")
l96 = sch.fuse(l55, l65, l75, l85, preserve_unit_iters=True)
sch.bind(loop=l96, thread_axis="vthread.x")
l97 = sch.fuse(l56, l66, l76, l86, preserve_unit_iters=True)
sch.bind(loop=l97, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b98 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b98, loop=l97, preserve_unit_loops=True, index=-1)
b99 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b99, loop=l92, preserve_unit_loops=True, index=-1)
l100, l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b99)
l108 = sch.fuse(l104, l105, l106, l107, preserve_unit_iters=True)
v109 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b99, ann_key="meta_schedule.cooperative_fetch", ann_val=v109)
b110 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b110, loop=l92, preserve_unit_loops=True, index=-1)
l111, l112, l113, l114, l115, l116, l117, l118 = sch.get_loops(block=b110)
l119 = sch.fuse(l115, l116, l117, l118, preserve_unit_iters=True)
v120 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b110, ann_key="meta_schedule.cooperative_fetch", ann_val=v120)
sch.reverse_compute_inline(block=b7)
sch.reverse_compute_inline(block=b6)
sch.reverse_compute_inline(block=b5)
sch.reverse_compute_inline(block=b4)
v121 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=1)
sch.annotate(block_or_loop=b8, ann_key="meta_schedule.unroll_explicit", ann_val=v121)
l122, l123, l124, l125, l126, l127 = sch.get_loops(block=b3)
l128 = sch.fuse(l122, l123, l124, l125, preserve_unit_iters=True)
v129 = sch.sample_categorical(candidates=[32, 64, 128, 256, 512, 1024], probs=[0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666], decision=0)
l130, l131 = sch.split(loop=l128, factors=[None, v129], preserve_unit_iters=True)
sch.bind(loop=l130, thread_axis="blockIdx.x")
sch.bind(loop=l131, thread_axis="threadIdx.x")
2023-11-10 23:34:21 [INFO] [task_scheduler.cc:170] Design space #1:
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(512), T.int64(7), T.int64(7)), "float32"), p1: T.Buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(512)), "float32"), p2: T.Buffer((T.int64(1), T.int64(512), T.int64(7), T.int64(7)), "float32"), p3: T.Buffer((T.int64(1), T.int64(512), T.int64(1), T.int64(1)), "float32"), p4: T.Buffer((T.int64(1), T.int64(512), T.int64(1), T.int64(1)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(512), T.int64(7), T.int64(7)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit": 1024})
            input_tile_local = T.alloc_buffer((T.int64(512), T.int64(16), T.int64(4), T.int64(4)), scope="local")
            data_pack = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(16)))
            bgemm = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(16)))
            inverse_local = T.alloc_buffer((T.int64(512), T.int64(16), T.int64(2), T.int64(2)), scope="local")
            data_pack_local = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(16)), scope="local")
            bgemm_local = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(16)), scope="local")
            data_pack_shared = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(16)), scope="shared")
            p1_shared = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(512)), scope="shared")
            for ci_p_fused_0 in T.thread_binding(T.int64(256), thread="blockIdx.x"):
                for ci_p_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(1), T.int64(4), T.int64(4)):
                        with T.block("input_tile"):
                            v_ci = T.axis.spatial(T.int64(512), ci_p_fused_0 * T.int64(2) + ci_p_fused_1 // T.int64(16) + ax0)
                            v_p = T.axis.spatial(T.int64(16), ci_p_fused_1 % T.int64(16) + ax1)
                            v_eps, v_nu = T.axis.remap("SS", [ax2, ax3])
                            T.reads(p0[v_p // T.int64(16), v_ci, v_p % T.int64(16) // T.int64(4) * T.int64(2) + v_eps - T.int64(1), v_p % T.int64(4) * T.int64(2) + v_nu - T.int64(1)])
                            T.writes(input_tile_local[v_ci, v_p, v_eps, v_nu])
                            T.block_attr({"schedule_rule": "None"})
                            input_tile_local[v_ci, v_p, v_eps, v_nu] = T.if_then_else(T.int64(1) <= v_p % T.int64(16) // T.int64(4) * T.int64(2) + v_eps and v_p % T.int64(16) // T.int64(4) * T.int64(2) + v_eps < T.int64(8) and T.int64(1) <= v_p % T.int64(4) * T.int64(2) + v_nu and v_p % T.int64(4) * T.int64(2) + v_nu < T.int64(8), p0[v_p // T.int64(16), v_ci, v_p % T.int64(16) // T.int64(4) * T.int64(2) + v_eps - T.int64(1), v_p % T.int64(4) * T.int64(2) + v_nu - T.int64(1)], T.float32(0))
                    for eps in T.unroll(T.int64(4)):
                        for nu in T.unroll(T.int64(4)):
                            for r_a in T.unroll(T.int64(4)):
                                for r_b in T.unroll(T.int64(4)):
                                    with T.block("data_pack"):
                                        v_eps, v_nu = T.axis.remap("SS", [eps, nu])
                                        v_ci = T.axis.spatial(T.int64(512), (ci_p_fused_0 * T.int64(32) + ci_p_fused_1) // T.int64(16))
                                        v_p = T.axis.spatial(T.int64(16), (ci_p_fused_0 * T.int64(32) + ci_p_fused_1) % T.int64(16))
                                        v_r_a, v_r_b = T.axis.remap("RR", [r_a, r_b])
                                        T.reads(input_tile_local[v_ci, v_p, v_r_a, v_r_b])
                                        T.writes(data_pack_local[v_eps, v_nu, v_ci, v_p])
                                        T.block_attr({"schedule_rule": "conv2d_nchw_winograd_data_pack"})
                                        with T.init():
                                            data_pack_local[v_eps, v_nu, v_ci, v_p] = T.float32(0)
                                        data_pack_local[v_eps, v_nu, v_ci, v_p] = data_pack_local[v_eps, v_nu, v_ci, v_p] + input_tile_local[v_ci, v_p, v_r_a, v_r_b] * T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(3), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(1), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(0), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(3), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(1), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(0), T.float32(1), T.float32(0))))))))))))))))) * T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(3), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(1), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(0), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(3), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(1), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(0), T.float32(1), T.float32(0)))))))))))))))))
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(4), T.int64(4), T.int64(1), T.int64(1)):
                        with T.block("data_pack_local"):
                            v0, v1 = T.axis.remap("SS", [ax0, ax1])
                            v2 = T.axis.spatial(T.int64(512), ci_p_fused_0 * T.int64(2) + ci_p_fused_1 // T.int64(16) + ax2)
                            v3 = T.axis.spatial(T.int64(16), ci_p_fused_1 % T.int64(16) + ax3)
                            T.reads(data_pack_local[v0, v1, v2, v3])
                            T.writes(data_pack[v0, v1, v2, v3])
                            data_pack[v0, v1, v2, v3] = data_pack_local[v0, v1, v2, v3]
            for eps_0_nu_0_co_0_p_0_fused in T.thread_binding(T.int64(8), thread="blockIdx.x"):
                for eps_1_nu_1_co_1_p_1_fused in T.thread_binding(T.int64(8), thread="vthread.x"):
                    for eps_2_nu_2_co_2_p_2_fused in T.thread_binding(T.int64(8), thread="threadIdx.x"):
                        for ci_0_fused in T.serial(T.int64(2), annotations={"software_pipeline_async_stages": [0], "software_pipeline_order": [0, 1, 2], "software_pipeline_stage": [0, 0, 2]}):
                            for ax0_ax1_ax2_ax3_fused in range(T.int64(65536)):
                                with T.block("data_pack_shared"):
                                    v0 = T.axis.spatial(T.int64(4), ax0_ax1_ax2_ax3_fused // T.int64(16384))
                                    v1 = T.axis.spatial(T.int64(4), ax0_ax1_ax2_ax3_fused % T.int64(16384) // T.int64(4096))
                                    v2 = T.axis.spatial(T.int64(512), ci_0_fused * T.int64(256) + ax0_ax1_ax2_ax3_fused % T.int64(4096) // T.int64(16))
                                    v3 = T.axis.spatial(T.int64(16), ax0_ax1_ax2_ax3_fused % T.int64(16))
                                    T.reads(data_pack[v0, v1, v2, v3])
                                    T.writes(data_pack_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch": 3})
                                    data_pack_shared[v0, v1, v2, v3] = data_pack[v0, v1, v2, v3]
                            for ax0_ax1_ax2_ax3_fused in range(T.int64(262144)):
                                with T.block("p1_shared"):
                                    v0 = T.axis.spatial(T.int64(4), ax0_ax1_ax2_ax3_fused // T.int64(65536))
                                    v1 = T.axis.spatial(T.int64(4), ax0_ax1_ax2_ax3_fused % T.int64(65536) // T.int64(16384))
                                    v2 = T.axis.spatial(T.int64(512), ci_0_fused * T.int64(256) + ax0_ax1_ax2_ax3_fused % T.int64(16384) // T.int64(64))
                                    v3 = T.axis.spatial(T.int64(512), eps_0_nu_0_co_0_p_0_fused * T.int64(64) + ax0_ax1_ax2_ax3_fused % T.int64(64))
                                    T.reads(p1[v0, v1, v2, v3])
                                    T.writes(p1_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch": 2})
                                    p1_shared[v0, v1, v2, v3] = p1[v0, v1, v2, v3]
                            for ci_1, eps_3, nu_3, co_3, p_3, ci_2, eps_4, nu_4, co_4, p_4 in T.grid(T.int64(64), T.int64(1), T.int64(2), T.int64(1), T.int64(4), T.int64(4), T.int64(2), T.int64(1), T.int64(16), T.int64(1)):
                                with T.block("bgemm"):
                                    v_eps = T.axis.spatial(T.int64(4), eps_1_nu_1_co_1_p_1_fused // T.int64(4) * T.int64(2) + eps_3 * T.int64(2) + eps_4)
                                    v_nu = T.axis.spatial(T.int64(4), eps_2_nu_2_co_2_p_2_fused // T.int64(4) * T.int64(2) + nu_3 + nu_4)
                                    v_co = T.axis.spatial(T.int64(512), eps_0_nu_0_co_0_p_0_fused * T.int64(64) + eps_2_nu_2_co_2_p_2_fused % T.int64(4) * T.int64(16) + co_3 * T.int64(16) + co_4)
                                    v_p = T.axis.spatial(T.int64(16), eps_1_nu_1_co_1_p_1_fused % T.int64(4) * T.int64(4) + p_3 + p_4)
                                    v_ci = T.axis.reduce(T.int64(512), ci_0_fused * T.int64(256) + ci_1 * T.int64(4) + ci_2)
                                    T.reads(data_pack_shared[v_eps, v_nu, v_ci, v_p], p1_shared[v_eps, v_nu, v_ci, v_co])
                                    T.writes(bgemm_local[v_eps, v_nu, v_co, v_p])
                                    T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                    with T.init():
                                        bgemm_local[v_eps, v_nu, v_co, v_p] = T.float32(0)
                                    bgemm_local[v_eps, v_nu, v_co, v_p] = bgemm_local[v_eps, v_nu, v_co, v_p] + data_pack_shared[v_eps, v_nu, v_ci, v_p] * p1_shared[v_eps, v_nu, v_ci, v_co]
                        for ax0, ax1, ax2, ax3 in T.grid(T.int64(2), T.int64(2), T.int64(16), T.int64(4)):
                            with T.block("bgemm_local"):
                                v0 = T.axis.spatial(T.int64(4), eps_1_nu_1_co_1_p_1_fused // T.int64(4) * T.int64(2) + ax0)
                                v1 = T.axis.spatial(T.int64(4), eps_2_nu_2_co_2_p_2_fused // T.int64(4) * T.int64(2) + ax1)
                                v2 = T.axis.spatial(T.int64(512), eps_0_nu_0_co_0_p_0_fused * T.int64(64) + eps_2_nu_2_co_2_p_2_fused % T.int64(4) * T.int64(16) + ax2)
                                v3 = T.axis.spatial(T.int64(16), eps_1_nu_1_co_1_p_1_fused % T.int64(4) * T.int64(4) + ax3)
                                T.reads(bgemm_local[v0, v1, v2, v3])
                                T.writes(bgemm[v0, v1, v2, v3])
                                bgemm[v0, v1, v2, v3] = bgemm_local[v0, v1, v2, v3]
            for n_co_h_0_w_0_fused_0 in T.thread_binding(T.int64(128), thread="blockIdx.x"):
                for n_co_h_0_w_0_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                    for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                        for ax2 in T.unroll(T.int64(2)):
                            for ax3 in T.unroll(T.int64(2)):
                                for ax4 in T.unroll(T.int64(4)):
                                    for ax5 in T.unroll(T.int64(4)):
                                        with T.block("inverse"):
                                            v_co = T.axis.spatial(T.int64(512), (n_co_h_0_w_0_fused_0 * T.int64(64) + n_co_h_0_w_0_fused_1) // T.int64(16) + ax0)
                                            v_p = T.axis.spatial(T.int64(16), (n_co_h_0_w_0_fused_0 * T.int64(64) + n_co_h_0_w_0_fused_1) % T.int64(16) + ax1)
                                            v_vh, v_vw, v_r_a, v_r_b = T.axis.remap("SSRR", [ax2, ax3, ax4, ax5])
                                            T.reads(bgemm[v_r_a, v_r_b, v_co, v_p])
                                            T.writes(inverse_local[v_co, v_p, v_vh, v_vw])
                                            T.block_attr({"schedule_rule": "conv2d_nchw_winograd_inverse"})
                                            with T.init():
                                                inverse_local[v_co, v_p, v_vh, v_vw] = T.float32(0)
                                            inverse_local[v_co, v_p, v_vh, v_vw] = inverse_local[v_co, v_p, v_vh, v_vw] + bgemm[v_r_a, v_r_b, v_co, v_p] * T.Select(v_r_a % T.int64(4) == T.int64(3) and v_vh % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_vh % T.int64(2) == T.int64(0), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_vh % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_vh % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_vh % T.int64(2) == T.int64(1), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_vh % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_vh % T.int64(2) == T.int64(1), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_vh % T.int64(2) == T.int64(0), T.float32(1), T.float32(0))))))))) * T.Select(v_r_b % T.int64(4) == T.int64(3) and v_vw % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_vw % T.int64(2) == T.int64(0), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_vw % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_vw % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_vw % T.int64(2) == T.int64(1), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_vw % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_vw % T.int64(2) == T.int64(1), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_vw % T.int64(2) == T.int64(0), T.float32(1), T.float32(0)))))))))
                    for h_1, w_1 in T.grid(T.int64(2), T.int64(2)):
                        with T.block("conv2d_winograd"):
                            v_n = T.axis.spatial(T.int64(1), T.int64(0))
                            v_co = T.axis.spatial(T.int64(512), (n_co_h_0_w_0_fused_0 * T.int64(64) + n_co_h_0_w_0_fused_1) // T.int64(16))
                            v_h = T.axis.spatial(T.int64(7), (n_co_h_0_w_0_fused_0 * T.int64(64) + n_co_h_0_w_0_fused_1) % T.int64(16) // T.int64(4) * T.int64(2) + h_1)
                            v_w = T.axis.spatial(T.int64(7), (n_co_h_0_w_0_fused_0 * T.int64(64) + n_co_h_0_w_0_fused_1) % T.int64(4) * T.int64(2) + w_1)
                            T.where((n_co_h_0_w_0_fused_0 * T.int64(64) + n_co_h_0_w_0_fused_1) % T.int64(16) // T.int64(4) * T.int64(2) + h_1 < T.int64(7) and (n_co_h_0_w_0_fused_0 * T.int64(64) + n_co_h_0_w_0_fused_1) % T.int64(4) * T.int64(2) + w_1 < T.int64(7))
                            T.reads(inverse_local[v_co, v_n * T.int64(16) + v_h // T.int64(2) * T.int64(4) + v_w // T.int64(2), v_h % T.int64(2), v_w % T.int64(2)], p2[v_n, v_co, v_h, v_w], p3[v_n, v_co, T.int64(0), T.int64(0)], p4[v_n, v_co, T.int64(0), T.int64(0)])
                            T.writes(T_relu[v_n, v_co, v_h, v_w])
                            T_relu[v_n, v_co, v_h, v_w] = T.max((inverse_local[v_co, v_n * T.int64(16) + v_h // T.int64(2) * T.int64(4) + v_w // T.int64(2), v_h % T.int64(2), v_w % T.int64(2)] + p2[v_n, v_co, v_h, v_w]) * p3[v_n, v_co, T.int64(0), T.int64(0)] + p4[v_n, v_co, T.int64(0), T.int64(0)], T.float32(0))
b0 = sch.get_block(name="data_pack", func_name="main")
b1 = sch.get_block(name="bgemm", func_name="main")
b2 = sch.get_block(name="inverse", func_name="main")
b3 = sch.get_block(name="conv2d_winograd", func_name="main")
b4 = sch.get_block(name="T_add", func_name="main")
b5 = sch.get_block(name="T_multiply", func_name="main")
b6 = sch.get_block(name="T_add_1", func_name="main")
b7 = sch.get_block(name="T_relu", func_name="main")
b8 = sch.get_block(name="root", func_name="main")
b9, b10 = sch.get_producers(block=b2)
sch.compute_inline(block=b10)
b11, = sch.get_consumers(block=b2)
l12, l13, l14, l15 = sch.get_loops(block=b11)
l16, l17 = sch.split(loop=l14, factors=[None, 2], preserve_unit_iters=True)
l18, l19 = sch.split(loop=l15, factors=[None, 2], preserve_unit_iters=True)
sch.reorder(l16, l18, l17, l19)
sch.compute_at(block=b2, loop=l18, preserve_unit_loops=True, index=-1)
sch.set_scope(block=b2, buffer_index=0, storage_scope="local")
l20, l21, l22, l23, l24, l25, l26, l27, l28, l29 = sch.get_loops(block=b2)
sch.unroll(loop=l26)
sch.unroll(loop=l27)
sch.unroll(loop=l28)
sch.unroll(loop=l29)
b30, b31 = sch.get_producers(block=b0)
sch.compute_inline(block=b31)
b32, = sch.get_producers(block=b30)
l33, l34, l35, l36, l37, l38 = sch.get_loops(block=b0)
sch.reorder(l35, l36, l33, l34, l37, l38)
sch.unroll(loop=l33)
sch.unroll(loop=l34)
sch.unroll(loop=l37)
sch.unroll(loop=l38)
l39 = sch.fuse(l35, l36, preserve_unit_iters=True)
v40 = sch.sample_categorical(candidates=[32, 64, 128, 256, 512, 1024], probs=[0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666], decision=0)
l41, l42 = sch.split(loop=l39, factors=[None, v40], preserve_unit_iters=True)
sch.bind(loop=l41, thread_axis="blockIdx.x")
sch.bind(loop=l42, thread_axis="threadIdx.x")
b43 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b43, loop=l42, preserve_unit_loops=True, index=-1)
sch.compute_at(block=b30, loop=l42, preserve_unit_loops=True, index=-1)
sch.set_scope(block=b30, buffer_index=0, storage_scope="local")
sch.compute_inline(block=b32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l44, l45, l46, l47, l48 = sch.get_loops(block=b1)
v49, v50, v51, v52, v53 = sch.sample_perfect_tile(loop=l44, n=5, max_innermost_factor=64, decision=[1, 2, 1, 1, 2])
l54, l55, l56, l57, l58 = sch.split(loop=l44, factors=[v49, v50, v51, v52, v53], preserve_unit_iters=True)
v59, v60, v61, v62, v63 = sch.sample_perfect_tile(loop=l45, n=5, max_innermost_factor=64, decision=[1, 1, 2, 2, 1])
l64, l65, l66, l67, l68 = sch.split(loop=l45, factors=[v59, v60, v61, v62, v63], preserve_unit_iters=True)
v69, v70, v71, v72, v73 = sch.sample_perfect_tile(loop=l46, n=5, max_innermost_factor=64, decision=[8, 1, 4, 1, 16])
l74, l75, l76, l77, l78 = sch.split(loop=l46, factors=[v69, v70, v71, v72, v73], preserve_unit_iters=True)
v79, v80, v81, v82, v83 = sch.sample_perfect_tile(loop=l47, n=5, max_innermost_factor=64, decision=[1, 4, 1, 4, 1])
l84, l85, l86, l87, l88 = sch.split(loop=l47, factors=[v79, v80, v81, v82, v83], preserve_unit_iters=True)
v89, v90, v91 = sch.sample_perfect_tile(loop=l48, n=3, max_innermost_factor=64, decision=[2, 64, 4])
l92, l93, l94 = sch.split(loop=l48, factors=[v89, v90, v91], preserve_unit_iters=True)
sch.reorder(l54, l64, l74, l84, l55, l65, l75, l85, l56, l66, l76, l86, l92, l93, l57, l67, l77, l87, l94, l58, l68, l78, l88)
l95 = sch.fuse(l54, l64, l74, l84, preserve_unit_iters=True)
sch.bind(loop=l95, thread_axis="blockIdx.x")
l96 = sch.fuse(l55, l65, l75, l85, preserve_unit_iters=True)
sch.bind(loop=l96, thread_axis="vthread.x")
l97 = sch.fuse(l56, l66, l76, l86, preserve_unit_iters=True)
sch.bind(loop=l97, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b98 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b98, loop=l97, preserve_unit_loops=True, index=-1)
b99 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b99, loop=l92, preserve_unit_loops=True, index=-1)
l100, l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b99)
l108 = sch.fuse(l104, l105, l106, l107, preserve_unit_iters=True)
v109 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b99, ann_key="meta_schedule.cooperative_fetch", ann_val=v109)
b110 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b110, loop=l92, preserve_unit_loops=True, index=-1)
l111, l112, l113, l114, l115, l116, l117, l118 = sch.get_loops(block=b110)
l119 = sch.fuse(l115, l116, l117, l118, preserve_unit_iters=True)
v120 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b110, ann_key="meta_schedule.cooperative_fetch", ann_val=v120)
l121 = sch.fuse(l92, preserve_unit_iters=True)
sch.annotate(block_or_loop=l121, ann_key="software_pipeline_stage", ann_val=[0, 0, 2])
sch.annotate(block_or_loop=l121, ann_key="software_pipeline_order", ann_val=[0, 1, 2])
sch.annotate(block_or_loop=l121, ann_key="software_pipeline_async_stages", ann_val=[0])
sch.reverse_compute_inline(block=b7)
sch.reverse_compute_inline(block=b6)
sch.reverse_compute_inline(block=b5)
sch.reverse_compute_inline(block=b4)
v122 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b8, ann_key="meta_schedule.unroll_explicit", ann_val=v122)
l123, l124, l125, l126, l127, l128 = sch.get_loops(block=b3)
l129 = sch.fuse(l123, l124, l125, l126, preserve_unit_iters=True)
v130 = sch.sample_categorical(candidates=[32, 64, 128, 256, 512, 1024], probs=[0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666], decision=1)
l131, l132 = sch.split(loop=l129, factors=[None, v130], preserve_unit_iters=True)
sch.bind(loop=l131, thread_axis="blockIdx.x")
sch.bind(loop=l132, thread_axis="threadIdx.x")
2023-11-10 23:34:21 [INFO] [task_scheduler.cc:170] Design space #2:
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(512), T.int64(7), T.int64(7)), "float32"), p1: T.Buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(512)), "float32"), p2: T.Buffer((T.int64(1), T.int64(512), T.int64(7), T.int64(7)), "float32"), p3: T.Buffer((T.int64(1), T.int64(512), T.int64(1), T.int64(1)), "float32"), p4: T.Buffer((T.int64(1), T.int64(512), T.int64(1), T.int64(1)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(512), T.int64(7), T.int64(7)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit": 16})
            input_tile_local = T.alloc_buffer((T.int64(512), T.int64(16), T.int64(4), T.int64(4)), scope="local")
            data_pack = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(16)))
            bgemm = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(16)))
            inverse_local = T.alloc_buffer((T.int64(512), T.int64(16), T.int64(2), T.int64(2)), scope="local")
            data_pack_local = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(16)), scope="local")
            bgemm_local = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(16)), scope="local")
            data_pack_shared = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(16)), scope="shared")
            p1_shared = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(512)), scope="shared")
            for ci_p_fused_0 in T.thread_binding(T.int64(256), thread="blockIdx.x"):
                for ci_p_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(1), T.int64(4), T.int64(4)):
                        with T.block("input_tile"):
                            v_ci = T.axis.spatial(T.int64(512), ci_p_fused_0 * T.int64(2) + ci_p_fused_1 // T.int64(16) + ax0)
                            v_p = T.axis.spatial(T.int64(16), ci_p_fused_1 % T.int64(16) + ax1)
                            v_eps, v_nu = T.axis.remap("SS", [ax2, ax3])
                            T.reads(p0[v_p // T.int64(16), v_ci, v_p % T.int64(16) // T.int64(4) * T.int64(2) + v_eps - T.int64(1), v_p % T.int64(4) * T.int64(2) + v_nu - T.int64(1)])
                            T.writes(input_tile_local[v_ci, v_p, v_eps, v_nu])
                            T.block_attr({"schedule_rule": "None"})
                            input_tile_local[v_ci, v_p, v_eps, v_nu] = T.if_then_else(T.int64(1) <= v_p % T.int64(16) // T.int64(4) * T.int64(2) + v_eps and v_p % T.int64(16) // T.int64(4) * T.int64(2) + v_eps < T.int64(8) and T.int64(1) <= v_p % T.int64(4) * T.int64(2) + v_nu and v_p % T.int64(4) * T.int64(2) + v_nu < T.int64(8), p0[v_p // T.int64(16), v_ci, v_p % T.int64(16) // T.int64(4) * T.int64(2) + v_eps - T.int64(1), v_p % T.int64(4) * T.int64(2) + v_nu - T.int64(1)], T.float32(0))
                    for eps in T.unroll(T.int64(4)):
                        for nu in T.unroll(T.int64(4)):
                            for r_a in T.unroll(T.int64(4)):
                                for r_b in T.unroll(T.int64(4)):
                                    with T.block("data_pack"):
                                        v_eps, v_nu = T.axis.remap("SS", [eps, nu])
                                        v_ci = T.axis.spatial(T.int64(512), (ci_p_fused_0 * T.int64(32) + ci_p_fused_1) // T.int64(16))
                                        v_p = T.axis.spatial(T.int64(16), (ci_p_fused_0 * T.int64(32) + ci_p_fused_1) % T.int64(16))
                                        v_r_a, v_r_b = T.axis.remap("RR", [r_a, r_b])
                                        T.reads(input_tile_local[v_ci, v_p, v_r_a, v_r_b])
                                        T.writes(data_pack_local[v_eps, v_nu, v_ci, v_p])
                                        T.block_attr({"schedule_rule": "conv2d_nchw_winograd_data_pack"})
                                        with T.init():
                                            data_pack_local[v_eps, v_nu, v_ci, v_p] = T.float32(0)
                                        data_pack_local[v_eps, v_nu, v_ci, v_p] = data_pack_local[v_eps, v_nu, v_ci, v_p] + input_tile_local[v_ci, v_p, v_r_a, v_r_b] * T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(3), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(1), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(0), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(3), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(1), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(0), T.float32(1), T.float32(0))))))))))))))))) * T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(3), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(1), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(0), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(3), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(1), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(0), T.float32(1), T.float32(0)))))))))))))))))
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(4), T.int64(4), T.int64(1), T.int64(1)):
                        with T.block("data_pack_local"):
                            v0, v1 = T.axis.remap("SS", [ax0, ax1])
                            v2 = T.axis.spatial(T.int64(512), ci_p_fused_0 * T.int64(2) + ci_p_fused_1 // T.int64(16) + ax2)
                            v3 = T.axis.spatial(T.int64(16), ci_p_fused_1 % T.int64(16) + ax3)
                            T.reads(data_pack_local[v0, v1, v2, v3])
                            T.writes(data_pack[v0, v1, v2, v3])
                            data_pack[v0, v1, v2, v3] = data_pack_local[v0, v1, v2, v3]
            for eps_0_nu_0_co_0_p_0_fused in T.thread_binding(T.int64(8), thread="blockIdx.x"):
                for eps_1_nu_1_co_1_p_1_fused in T.thread_binding(T.int64(8), thread="vthread.x"):
                    for eps_2_nu_2_co_2_p_2_fused in T.thread_binding(T.int64(8), thread="threadIdx.x"):
                        for ci_0_fused in T.serial(T.int64(2), annotations={"software_pipeline_async_stages": [0], "software_pipeline_order": [0, 1, 2], "software_pipeline_stage": [0, 0, 3]}):
                            for ax0_ax1_ax2_ax3_fused in range(T.int64(65536)):
                                with T.block("data_pack_shared"):
                                    v0 = T.axis.spatial(T.int64(4), ax0_ax1_ax2_ax3_fused // T.int64(16384))
                                    v1 = T.axis.spatial(T.int64(4), ax0_ax1_ax2_ax3_fused % T.int64(16384) // T.int64(4096))
                                    v2 = T.axis.spatial(T.int64(512), ci_0_fused * T.int64(256) + ax0_ax1_ax2_ax3_fused % T.int64(4096) // T.int64(16))
                                    v3 = T.axis.spatial(T.int64(16), ax0_ax1_ax2_ax3_fused % T.int64(16))
                                    T.reads(data_pack[v0, v1, v2, v3])
                                    T.writes(data_pack_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch": 3})
                                    data_pack_shared[v0, v1, v2, v3] = data_pack[v0, v1, v2, v3]
                            for ax0_ax1_ax2_ax3_fused in range(T.int64(262144)):
                                with T.block("p1_shared"):
                                    v0 = T.axis.spatial(T.int64(4), ax0_ax1_ax2_ax3_fused // T.int64(65536))
                                    v1 = T.axis.spatial(T.int64(4), ax0_ax1_ax2_ax3_fused % T.int64(65536) // T.int64(16384))
                                    v2 = T.axis.spatial(T.int64(512), ci_0_fused * T.int64(256) + ax0_ax1_ax2_ax3_fused % T.int64(16384) // T.int64(64))
                                    v3 = T.axis.spatial(T.int64(512), eps_0_nu_0_co_0_p_0_fused * T.int64(64) + ax0_ax1_ax2_ax3_fused % T.int64(64))
                                    T.reads(p1[v0, v1, v2, v3])
                                    T.writes(p1_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch": 2})
                                    p1_shared[v0, v1, v2, v3] = p1[v0, v1, v2, v3]
                            for ci_1, eps_3, nu_3, co_3, p_3, ci_2, eps_4, nu_4, co_4, p_4 in T.grid(T.int64(64), T.int64(1), T.int64(2), T.int64(1), T.int64(4), T.int64(4), T.int64(2), T.int64(1), T.int64(16), T.int64(1)):
                                with T.block("bgemm"):
                                    v_eps = T.axis.spatial(T.int64(4), eps_1_nu_1_co_1_p_1_fused // T.int64(4) * T.int64(2) + eps_3 * T.int64(2) + eps_4)
                                    v_nu = T.axis.spatial(T.int64(4), eps_2_nu_2_co_2_p_2_fused // T.int64(4) * T.int64(2) + nu_3 + nu_4)
                                    v_co = T.axis.spatial(T.int64(512), eps_0_nu_0_co_0_p_0_fused * T.int64(64) + eps_2_nu_2_co_2_p_2_fused % T.int64(4) * T.int64(16) + co_3 * T.int64(16) + co_4)
                                    v_p = T.axis.spatial(T.int64(16), eps_1_nu_1_co_1_p_1_fused % T.int64(4) * T.int64(4) + p_3 + p_4)
                                    v_ci = T.axis.reduce(T.int64(512), ci_0_fused * T.int64(256) + ci_1 * T.int64(4) + ci_2)
                                    T.reads(data_pack_shared[v_eps, v_nu, v_ci, v_p], p1_shared[v_eps, v_nu, v_ci, v_co])
                                    T.writes(bgemm_local[v_eps, v_nu, v_co, v_p])
                                    T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                    with T.init():
                                        bgemm_local[v_eps, v_nu, v_co, v_p] = T.float32(0)
                                    bgemm_local[v_eps, v_nu, v_co, v_p] = bgemm_local[v_eps, v_nu, v_co, v_p] + data_pack_shared[v_eps, v_nu, v_ci, v_p] * p1_shared[v_eps, v_nu, v_ci, v_co]
                        for ax0, ax1, ax2, ax3 in T.grid(T.int64(2), T.int64(2), T.int64(16), T.int64(4)):
                            with T.block("bgemm_local"):
                                v0 = T.axis.spatial(T.int64(4), eps_1_nu_1_co_1_p_1_fused // T.int64(4) * T.int64(2) + ax0)
                                v1 = T.axis.spatial(T.int64(4), eps_2_nu_2_co_2_p_2_fused // T.int64(4) * T.int64(2) + ax1)
                                v2 = T.axis.spatial(T.int64(512), eps_0_nu_0_co_0_p_0_fused * T.int64(64) + eps_2_nu_2_co_2_p_2_fused % T.int64(4) * T.int64(16) + ax2)
                                v3 = T.axis.spatial(T.int64(16), eps_1_nu_1_co_1_p_1_fused % T.int64(4) * T.int64(4) + ax3)
                                T.reads(bgemm_local[v0, v1, v2, v3])
                                T.writes(bgemm[v0, v1, v2, v3])
                                bgemm[v0, v1, v2, v3] = bgemm_local[v0, v1, v2, v3]
            for n_co_h_0_w_0_fused_0 in T.thread_binding(T.int64(8), thread="blockIdx.x"):
                for n_co_h_0_w_0_fused_1 in T.thread_binding(T.int64(1024), thread="threadIdx.x"):
                    for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                        for ax2 in T.unroll(T.int64(2)):
                            for ax3 in T.unroll(T.int64(2)):
                                for ax4 in T.unroll(T.int64(4)):
                                    for ax5 in T.unroll(T.int64(4)):
                                        with T.block("inverse"):
                                            v_co = T.axis.spatial(T.int64(512), (n_co_h_0_w_0_fused_0 * T.int64(1024) + n_co_h_0_w_0_fused_1) // T.int64(16) + ax0)
                                            v_p = T.axis.spatial(T.int64(16), (n_co_h_0_w_0_fused_0 * T.int64(1024) + n_co_h_0_w_0_fused_1) % T.int64(16) + ax1)
                                            v_vh, v_vw, v_r_a, v_r_b = T.axis.remap("SSRR", [ax2, ax3, ax4, ax5])
                                            T.reads(bgemm[v_r_a, v_r_b, v_co, v_p])
                                            T.writes(inverse_local[v_co, v_p, v_vh, v_vw])
                                            T.block_attr({"schedule_rule": "conv2d_nchw_winograd_inverse"})
                                            with T.init():
                                                inverse_local[v_co, v_p, v_vh, v_vw] = T.float32(0)
                                            inverse_local[v_co, v_p, v_vh, v_vw] = inverse_local[v_co, v_p, v_vh, v_vw] + bgemm[v_r_a, v_r_b, v_co, v_p] * T.Select(v_r_a % T.int64(4) == T.int64(3) and v_vh % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_vh % T.int64(2) == T.int64(0), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_vh % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_vh % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_vh % T.int64(2) == T.int64(1), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_vh % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_vh % T.int64(2) == T.int64(1), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_vh % T.int64(2) == T.int64(0), T.float32(1), T.float32(0))))))))) * T.Select(v_r_b % T.int64(4) == T.int64(3) and v_vw % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_vw % T.int64(2) == T.int64(0), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_vw % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_vw % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_vw % T.int64(2) == T.int64(1), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_vw % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_vw % T.int64(2) == T.int64(1), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_vw % T.int64(2) == T.int64(0), T.float32(1), T.float32(0)))))))))
                    for h_1, w_1 in T.grid(T.int64(2), T.int64(2)):
                        with T.block("conv2d_winograd"):
                            v_n = T.axis.spatial(T.int64(1), T.int64(0))
                            v_co = T.axis.spatial(T.int64(512), (n_co_h_0_w_0_fused_0 * T.int64(1024) + n_co_h_0_w_0_fused_1) // T.int64(16))
                            v_h = T.axis.spatial(T.int64(7), (n_co_h_0_w_0_fused_0 * T.int64(1024) + n_co_h_0_w_0_fused_1) % T.int64(16) // T.int64(4) * T.int64(2) + h_1)
                            v_w = T.axis.spatial(T.int64(7), (n_co_h_0_w_0_fused_0 * T.int64(1024) + n_co_h_0_w_0_fused_1) % T.int64(4) * T.int64(2) + w_1)
                            T.where((n_co_h_0_w_0_fused_0 * T.int64(1024) + n_co_h_0_w_0_fused_1) % T.int64(16) // T.int64(4) * T.int64(2) + h_1 < T.int64(7) and (n_co_h_0_w_0_fused_0 * T.int64(1024) + n_co_h_0_w_0_fused_1) % T.int64(4) * T.int64(2) + w_1 < T.int64(7))
                            T.reads(inverse_local[v_co, v_n * T.int64(16) + v_h // T.int64(2) * T.int64(4) + v_w // T.int64(2), v_h % T.int64(2), v_w % T.int64(2)], p2[v_n, v_co, v_h, v_w], p3[v_n, v_co, T.int64(0), T.int64(0)], p4[v_n, v_co, T.int64(0), T.int64(0)])
                            T.writes(T_relu[v_n, v_co, v_h, v_w])
                            T_relu[v_n, v_co, v_h, v_w] = T.max((inverse_local[v_co, v_n * T.int64(16) + v_h // T.int64(2) * T.int64(4) + v_w // T.int64(2), v_h % T.int64(2), v_w % T.int64(2)] + p2[v_n, v_co, v_h, v_w]) * p3[v_n, v_co, T.int64(0), T.int64(0)] + p4[v_n, v_co, T.int64(0), T.int64(0)], T.float32(0))
b0 = sch.get_block(name="data_pack", func_name="main")
b1 = sch.get_block(name="bgemm", func_name="main")
b2 = sch.get_block(name="inverse", func_name="main")
b3 = sch.get_block(name="conv2d_winograd", func_name="main")
b4 = sch.get_block(name="T_add", func_name="main")
b5 = sch.get_block(name="T_multiply", func_name="main")
b6 = sch.get_block(name="T_add_1", func_name="main")
b7 = sch.get_block(name="T_relu", func_name="main")
b8 = sch.get_block(name="root", func_name="main")
b9, b10 = sch.get_producers(block=b2)
sch.compute_inline(block=b10)
b11, = sch.get_consumers(block=b2)
l12, l13, l14, l15 = sch.get_loops(block=b11)
l16, l17 = sch.split(loop=l14, factors=[None, 2], preserve_unit_iters=True)
l18, l19 = sch.split(loop=l15, factors=[None, 2], preserve_unit_iters=True)
sch.reorder(l16, l18, l17, l19)
sch.compute_at(block=b2, loop=l18, preserve_unit_loops=True, index=-1)
sch.set_scope(block=b2, buffer_index=0, storage_scope="local")
l20, l21, l22, l23, l24, l25, l26, l27, l28, l29 = sch.get_loops(block=b2)
sch.unroll(loop=l26)
sch.unroll(loop=l27)
sch.unroll(loop=l28)
sch.unroll(loop=l29)
b30, b31 = sch.get_producers(block=b0)
sch.compute_inline(block=b31)
b32, = sch.get_producers(block=b30)
l33, l34, l35, l36, l37, l38 = sch.get_loops(block=b0)
sch.reorder(l35, l36, l33, l34, l37, l38)
sch.unroll(loop=l33)
sch.unroll(loop=l34)
sch.unroll(loop=l37)
sch.unroll(loop=l38)
l39 = sch.fuse(l35, l36, preserve_unit_iters=True)
v40 = sch.sample_categorical(candidates=[32, 64, 128, 256, 512, 1024], probs=[0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666], decision=0)
l41, l42 = sch.split(loop=l39, factors=[None, v40], preserve_unit_iters=True)
sch.bind(loop=l41, thread_axis="blockIdx.x")
sch.bind(loop=l42, thread_axis="threadIdx.x")
b43 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b43, loop=l42, preserve_unit_loops=True, index=-1)
sch.compute_at(block=b30, loop=l42, preserve_unit_loops=True, index=-1)
sch.set_scope(block=b30, buffer_index=0, storage_scope="local")
sch.compute_inline(block=b32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l44, l45, l46, l47, l48 = sch.get_loops(block=b1)
v49, v50, v51, v52, v53 = sch.sample_perfect_tile(loop=l44, n=5, max_innermost_factor=64, decision=[1, 2, 1, 1, 2])
l54, l55, l56, l57, l58 = sch.split(loop=l44, factors=[v49, v50, v51, v52, v53], preserve_unit_iters=True)
v59, v60, v61, v62, v63 = sch.sample_perfect_tile(loop=l45, n=5, max_innermost_factor=64, decision=[1, 1, 2, 2, 1])
l64, l65, l66, l67, l68 = sch.split(loop=l45, factors=[v59, v60, v61, v62, v63], preserve_unit_iters=True)
v69, v70, v71, v72, v73 = sch.sample_perfect_tile(loop=l46, n=5, max_innermost_factor=64, decision=[8, 1, 4, 1, 16])
l74, l75, l76, l77, l78 = sch.split(loop=l46, factors=[v69, v70, v71, v72, v73], preserve_unit_iters=True)
v79, v80, v81, v82, v83 = sch.sample_perfect_tile(loop=l47, n=5, max_innermost_factor=64, decision=[1, 4, 1, 4, 1])
l84, l85, l86, l87, l88 = sch.split(loop=l47, factors=[v79, v80, v81, v82, v83], preserve_unit_iters=True)
v89, v90, v91 = sch.sample_perfect_tile(loop=l48, n=3, max_innermost_factor=64, decision=[2, 64, 4])
l92, l93, l94 = sch.split(loop=l48, factors=[v89, v90, v91], preserve_unit_iters=True)
sch.reorder(l54, l64, l74, l84, l55, l65, l75, l85, l56, l66, l76, l86, l92, l93, l57, l67, l77, l87, l94, l58, l68, l78, l88)
l95 = sch.fuse(l54, l64, l74, l84, preserve_unit_iters=True)
sch.bind(loop=l95, thread_axis="blockIdx.x")
l96 = sch.fuse(l55, l65, l75, l85, preserve_unit_iters=True)
sch.bind(loop=l96, thread_axis="vthread.x")
l97 = sch.fuse(l56, l66, l76, l86, preserve_unit_iters=True)
sch.bind(loop=l97, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b98 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b98, loop=l97, preserve_unit_loops=True, index=-1)
b99 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b99, loop=l92, preserve_unit_loops=True, index=-1)
l100, l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b99)
l108 = sch.fuse(l104, l105, l106, l107, preserve_unit_iters=True)
v109 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b99, ann_key="meta_schedule.cooperative_fetch", ann_val=v109)
b110 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b110, loop=l92, preserve_unit_loops=True, index=-1)
l111, l112, l113, l114, l115, l116, l117, l118 = sch.get_loops(block=b110)
l119 = sch.fuse(l115, l116, l117, l118, preserve_unit_iters=True)
v120 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b110, ann_key="meta_schedule.cooperative_fetch", ann_val=v120)
l121 = sch.fuse(l92, preserve_unit_iters=True)
sch.annotate(block_or_loop=l121, ann_key="software_pipeline_stage", ann_val=[0, 0, 3])
sch.annotate(block_or_loop=l121, ann_key="software_pipeline_order", ann_val=[0, 1, 2])
sch.annotate(block_or_loop=l121, ann_key="software_pipeline_async_stages", ann_val=[0])
sch.reverse_compute_inline(block=b7)
sch.reverse_compute_inline(block=b6)
sch.reverse_compute_inline(block=b5)
sch.reverse_compute_inline(block=b4)
v122 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=1)
sch.annotate(block_or_loop=b8, ann_key="meta_schedule.unroll_explicit", ann_val=v122)
l123, l124, l125, l126, l127, l128 = sch.get_loops(block=b3)
l129 = sch.fuse(l123, l124, l125, l126, preserve_unit_iters=True)
v130 = sch.sample_categorical(candidates=[32, 64, 128, 256, 512, 1024], probs=[0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666], decision=5)
l131, l132 = sch.split(loop=l129, factors=[None, v130], preserve_unit_iters=True)
sch.bind(loop=l131, thread_axis="blockIdx.x")
sch.bind(loop=l132, thread_axis="threadIdx.x")
2023-11-11 00:12:52 [INFO] [evolutionary_search.cc:713] Generating candidates......
2023-11-11 00:12:52 [INFO] [evolutionary_search.cc:715] Picked top 0 candidate(s) from database
2023-11-11 00:13:00 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310fb03b28)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310eec3b08)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310dc5caf8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310f7cc2d8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x563110643e78)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310dc32028)]: 500 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310dc5cfb8)]: 0 failure(s)
2023-11-11 00:13:07 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310fb03b28)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310eec3b08)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310dc5caf8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310f7cc2d8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x563110643e78)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310dc32028)]: 1003 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310dc5cfb8)]: 0 failure(s)
2023-11-11 00:13:15 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310fb03b28)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310eec3b08)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310dc5caf8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310f7cc2d8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x563110643e78)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310dc32028)]: 1506 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310dc5cfb8)]: 0 failure(s)
2023-11-11 00:13:23 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310fb03b28)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310eec3b08)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310dc5caf8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310f7cc2d8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x563110643e78)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310dc32028)]: 2013 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310dc5cfb8)]: 0 failure(s)
2023-11-11 00:13:30 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310fb03b28)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310eec3b08)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310dc5caf8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310f7cc2d8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x563110643e78)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310dc32028)]: 2513 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310dc5cfb8)]: 0 failure(s)
2023-11-11 00:13:38 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310fb03b28)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310eec3b08)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310dc5caf8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310f7cc2d8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x563110643e78)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310dc32028)]: 3013 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310dc5cfb8)]: 0 failure(s)
2023-11-11 00:13:38 [INFO] [evolutionary_search.cc:723] Sampled 59 candidate(s)
2023-11-11 00:13:54 [INFO] [evolutionary_search.cc:621] Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310fb03b28)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310eec3b08)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310dc5caf8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310f7cc2d8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x563110643e78)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310dc32028)]: 119 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310dc5cfb8)]: 0 failure(s)
2023-11-11 00:14:09 [INFO] [evolutionary_search.cc:621] Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310fb03b28)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310eec3b08)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310dc5caf8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310f7cc2d8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x563110643e78)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310dc32028)]: 86 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310dc5cfb8)]: 0 failure(s)
2023-11-11 00:14:23 [INFO] [evolutionary_search.cc:621] Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310fb03b28)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310eec3b08)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310dc5caf8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310f7cc2d8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x563110643e78)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310dc32028)]: 97 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310dc5cfb8)]: 0 failure(s)
2023-11-11 00:14:38 [INFO] [evolutionary_search.cc:621] Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310fb03b28)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310eec3b08)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310dc5caf8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310f7cc2d8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x563110643e78)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310dc32028)]: 81 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310dc5cfb8)]: 0 failure(s)
2023-11-11 00:14:39 [INFO] [evolutionary_search.cc:649] Scores of the best 64 candidates:
[1 : 16]:	0.9969  0.9952  0.9945  0.9943  0.9939  0.9919  0.9917  0.9910  0.9895  0.9893  0.9881  0.9876  0.9872  0.9871  0.9862  0.9855
[17 : 32]:	0.9847  0.9844  0.9843  0.9833  0.9832  0.9831  0.9823  0.9811  0.9798  0.9778  0.9739  0.9733  0.9732  0.9724  0.9720  0.9719
[33 : 48]:	0.9717  0.9717  0.9716  0.9716  0.9712  0.9708  0.9707  0.9695  0.9695  0.9693  0.9679  0.9677  0.9667  0.9661  0.9657  0.9648
[49 : 64]:	0.9645  0.9645  0.9644  0.9632  0.9629  0.9628  0.9628  0.9622  0.9593  0.9590  0.9581  0.9576  0.9568  0.9568  0.9555  0.9550
2023-11-11 00:14:39 [INFO] [evolutionary_search.cc:727] Got 64 candidate(s) with evolutionary search
2023-11-11 00:14:39 [INFO] [evolutionary_search.cc:730] Sending 64 candidates(s) for measurement
2023-11-11 00:30:46 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #1: GFLOPs: 496.2297. Time: 286.5254 us. Best GFLOPs: 496.2297
2023-11-11 00:30:46 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #2: GFLOPs: 487.6200. Time: 291.5844 us. Best GFLOPs: 496.2297
2023-11-11 00:30:46 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #3: GFLOPs: 1950.1882. Time: 72.9070 us. Best GFLOPs: 1950.1882
2023-11-11 00:30:46 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #4: GFLOPs: 178.5652. Time: 796.2494 us. Best GFLOPs: 1950.1882
2023-11-11 00:30:46 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #5: GFLOPs: 213.9227. Time: 664.6437 us. Best GFLOPs: 1950.1882
2023-11-11 00:30:46 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #6: GFLOPs: 129.0849. Time: 1101.4639 us. Best GFLOPs: 1950.1882
2023-11-11 00:30:46 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #7: GFLOPs: 577.8946. Time: 246.0352 us. Best GFLOPs: 1950.1882
2023-11-11 00:30:46 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #8: GFLOPs: 140.9171. Time: 1008.9792 us. Best GFLOPs: 1950.1882
2023-11-11 00:30:46 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #9: GFLOPs: 1177.5741. Time: 120.7418 us. Best GFLOPs: 1950.1882
2023-11-11 00:30:46 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #10: GFLOPs: 938.8472. Time: 151.4436 us. Best GFLOPs: 1950.1882
2023-11-11 00:30:46 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #11: GFLOPs: 1507.1439. Time: 94.3390 us. Best GFLOPs: 1950.1882
2023-11-11 00:30:46 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #12: GFLOPs: 1649.4408. Time: 86.2004 us. Best GFLOPs: 1950.1882
2023-11-11 00:30:46 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #13: GFLOPs: 75.8283. Time: 1875.0578 us. Best GFLOPs: 1950.1882
2023-11-11 00:30:46 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #14: GFLOPs: 793.7016. Time: 179.1384 us. Best GFLOPs: 1950.1882
2023-11-11 00:30:46 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #15: GFLOPs: 678.4806. Time: 209.5600 us. Best GFLOPs: 1950.1882
2023-11-11 00:30:46 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #16: GFLOPs: 1515.1486. Time: 93.8406 us. Best GFLOPs: 1950.1882
2023-11-11 00:30:46 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #17: GFLOPs: 145.1633. Time: 979.4653 us. Best GFLOPs: 1950.1882
2023-11-11 00:30:46 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #18: GFLOPs: 98.2393. Time: 1447.3069 us. Best GFLOPs: 1950.1882
2023-11-11 00:30:46 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #19: GFLOPs: 1627.0300. Time: 87.3877 us. Best GFLOPs: 1950.1882
2023-11-11 00:30:46 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #20: GFLOPs: 382.6814. Time: 371.5425 us. Best GFLOPs: 1950.1882
2023-11-11 00:30:46 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #21: GFLOPs: 15.7388. Time: 9033.8987 us. Best GFLOPs: 1950.1882
2023-11-11 00:30:46 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #22: GFLOPs: 2238.6041. Time: 63.5139 us. Best GFLOPs: 2238.6041
2023-11-11 00:30:46 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #23: GFLOPs: 141.8197. Time: 1002.5574 us. Best GFLOPs: 2238.6041
2023-11-11 00:30:46 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #24: GFLOPs: 301.9818. Time: 470.8310 us. Best GFLOPs: 2238.6041
2023-11-11 00:30:46 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #25: GFLOPs: 610.9739. Time: 232.7143 us. Best GFLOPs: 2238.6041
2023-11-11 00:30:46 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #26: GFLOPs: 1512.8405. Time: 93.9837 us. Best GFLOPs: 2238.6041
2023-11-11 00:30:46 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #27: GFLOPs: 428.5005. Time: 331.8138 us. Best GFLOPs: 2238.6041
2023-11-11 00:30:46 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #28: GFLOPs: 736.8847. Time: 192.9507 us. Best GFLOPs: 2238.6041
2023-11-11 00:30:46 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #29: GFLOPs: 814.6032. Time: 174.5419 us. Best GFLOPs: 2238.6041
2023-11-11 00:30:46 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #30: GFLOPs: 308.4450. Time: 460.9652 us. Best GFLOPs: 2238.6041
2023-11-11 00:30:46 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #31: GFLOPs: 95.6267. Time: 1486.8480 us. Best GFLOPs: 2238.6041
2023-11-11 00:30:46 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #32: GFLOPs: 287.0887. Time: 495.2561 us. Best GFLOPs: 2238.6041
2023-11-11 00:30:46 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #33: GFLOPs: 164.6286. Time: 863.6557 us. Best GFLOPs: 2238.6041
2023-11-11 00:30:46 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #34: GFLOPs: 977.0559. Time: 145.5213 us. Best GFLOPs: 2238.6041
2023-11-11 00:30:46 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #35: GFLOPs: 459.3879. Time: 309.5040 us. Best GFLOPs: 2238.6041
2023-11-11 00:30:46 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #36: GFLOPs: 759.2999. Time: 187.2546 us. Best GFLOPs: 2238.6041
2023-11-11 00:30:46 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #37: GFLOPs: 64.2837. Time: 2211.7955 us. Best GFLOPs: 2238.6041
2023-11-11 00:30:46 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #38: GFLOPs: 676.5880. Time: 210.1462 us. Best GFLOPs: 2238.6041
2023-11-11 00:30:46 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #39: GFLOPs: 240.7375. Time: 590.6117 us. Best GFLOPs: 2238.6041
2023-11-11 00:30:46 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #40: GFLOPs: 795.0443. Time: 178.8358 us. Best GFLOPs: 2238.6041
2023-11-11 00:30:46 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #41: GFLOPs: 887.2515. Time: 160.2504 us. Best GFLOPs: 2238.6041
2023-11-11 00:30:46 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #42: GFLOPs: 300.1372. Time: 473.7246 us. Best GFLOPs: 2238.6041
2023-11-11 00:30:46 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #43: GFLOPs: 596.7297. Time: 238.2694 us. Best GFLOPs: 2238.6041
2023-11-11 00:30:46 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #44: GFLOPs: 1265.0912. Time: 112.3891 us. Best GFLOPs: 2238.6041
2023-11-11 00:30:46 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #45: GFLOPs: 661.2620. Time: 215.0168 us. Best GFLOPs: 2238.6041
2023-11-11 00:30:46 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #46: GFLOPs: 902.4622. Time: 157.5494 us. Best GFLOPs: 2238.6041
2023-11-11 00:30:46 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #47: GFLOPs: 233.3377. Time: 609.3418 us. Best GFLOPs: 2238.6041
2023-11-11 00:30:46 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #48: GFLOPs: 1987.9930. Time: 71.5206 us. Best GFLOPs: 2238.6041
2023-11-11 00:30:46 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #49: GFLOPs: 376.7931. Time: 377.3488 us. Best GFLOPs: 2238.6041
2023-11-11 00:30:46 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #50: GFLOPs: 702.8360. Time: 202.2981 us. Best GFLOPs: 2238.6041
2023-11-11 00:30:46 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #51: GFLOPs: 1735.2567. Time: 81.9374 us. Best GFLOPs: 2238.6041
2023-11-11 00:30:46 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #52: GFLOPs: 720.8553. Time: 197.2412 us. Best GFLOPs: 2238.6041
2023-11-11 00:30:46 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #53: GFLOPs: 2165.8616. Time: 65.6470 us. Best GFLOPs: 2238.6041
2023-11-11 00:30:46 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #54: GFLOPs: 748.4246. Time: 189.9756 us. Best GFLOPs: 2238.6041
2023-11-11 00:30:46 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #55: GFLOPs: 1200.1225. Time: 118.4732 us. Best GFLOPs: 2238.6041
2023-11-11 00:30:46 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #56: GFLOPs: 164.7886. Time: 862.8171 us. Best GFLOPs: 2238.6041
2023-11-11 00:30:46 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #57: GFLOPs: 14.0822. Time: 10096.6400 us. Best GFLOPs: 2238.6041
2023-11-11 00:30:46 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #58: GFLOPs: 1284.4473. Time: 110.6954 us. Best GFLOPs: 2238.6041
2023-11-11 00:30:46 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #59: GFLOPs: 801.3332. Time: 177.4323 us. Best GFLOPs: 2238.6041
2023-11-11 00:30:46 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #60: GFLOPs: 409.1033. Time: 347.5465 us. Best GFLOPs: 2238.6041
2023-11-11 00:30:46 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #61: GFLOPs: 739.6792. Time: 192.2217 us. Best GFLOPs: 2238.6041
2023-11-11 00:30:46 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #62: GFLOPs: 59.0438. Time: 2408.0823 us. Best GFLOPs: 2238.6041
2023-11-11 00:30:46 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #63: GFLOPs: 164.4588. Time: 864.5473 us. Best GFLOPs: 2238.6041
2023-11-11 00:30:46 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #64: GFLOPs: 362.6108. Time: 392.1075 us. Best GFLOPs: 2238.6041
2023-11-11 00:38:00 [INFO] [evolutionary_search.cc:713] Generating candidates......
2023-11-11 00:38:02 [INFO] [evolutionary_search.cc:715] Picked top 64 candidate(s) from database
2023-11-11 00:38:09 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310fb03b28)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310eec3b08)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310dc5caf8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310f7cc2d8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x563110643e78)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310dc32028)]: 437 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310dc5cfb8)]: 0 failure(s)
2023-11-11 00:38:16 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310fb03b28)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310eec3b08)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310dc5caf8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310f7cc2d8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x563110643e78)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310dc32028)]: 873 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310dc5cfb8)]: 0 failure(s)
2023-11-11 00:38:23 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310fb03b28)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310eec3b08)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310dc5caf8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310f7cc2d8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x563110643e78)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310dc32028)]: 1310 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310dc5cfb8)]: 0 failure(s)
2023-11-11 00:38:30 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310fb03b28)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310eec3b08)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310dc5caf8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310f7cc2d8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x563110643e78)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310dc32028)]: 1752 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310dc5cfb8)]: 0 failure(s)
2023-11-11 00:38:37 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310fb03b28)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310eec3b08)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310dc5caf8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310f7cc2d8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x563110643e78)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310dc32028)]: 2187 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310dc5cfb8)]: 0 failure(s)
2023-11-11 00:38:37 [INFO] [evolutionary_search.cc:723] Sampled 53 candidate(s)
2023-11-11 00:38:53 [INFO] [evolutionary_search.cc:621] Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310fb03b28)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310eec3b08)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310dc5caf8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310f7cc2d8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x563110643e78)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310dc32028)]: 100 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310dc5cfb8)]: 0 failure(s)
2023-11-11 00:39:12 [INFO] [evolutionary_search.cc:621] Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310fb03b28)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310eec3b08)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310dc5caf8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310f7cc2d8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x563110643e78)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310dc32028)]: 88 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310dc5cfb8)]: 0 failure(s)
2023-11-11 00:39:30 [INFO] [evolutionary_search.cc:621] Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310fb03b28)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310eec3b08)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310dc5caf8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310f7cc2d8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x563110643e78)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310dc32028)]: 72 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310dc5cfb8)]: 0 failure(s)
2023-11-11 00:39:49 [INFO] [evolutionary_search.cc:621] Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310fb03b28)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310eec3b08)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310dc5caf8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310f7cc2d8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x563110643e78)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310dc32028)]: 71 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310dc5cfb8)]: 0 failure(s)
2023-11-11 00:39:54 [INFO] [evolutionary_search.cc:649] Scores of the best 64 candidates:
[1 : 16]:	1.1384  1.1117  1.1098  1.0660  1.0411  1.0394  1.0348  1.0285  1.0278  1.0216  1.0176  1.0166  1.0152  0.9884  0.9881  0.9871
[17 : 32]:	0.9775  0.9759  0.9747  0.9737  0.9651  0.9641  0.9626  0.9608  0.9582  0.9573  0.9565  0.9561  0.9552  0.9527  0.9523  0.9501
[33 : 48]:	0.9486  0.9482  0.9448  0.9433  0.9417  0.9391  0.9385  0.9372  0.9349  0.9347  0.9343  0.9342  0.9316  0.9311  0.9273  0.9270
[49 : 64]:	0.9247  0.9212  0.9200  0.9193  0.9189  0.9176  0.9130  0.9129  0.9118  0.9110  0.9081  0.9074  0.9051  0.9040  0.9021  0.8991
2023-11-11 00:39:55 [INFO] [evolutionary_search.cc:727] Got 64 candidate(s) with evolutionary search
2023-11-11 00:39:55 [INFO] [evolutionary_search.cc:730] Sending 64 candidates(s) for measurement
2023-11-11 00:40:32 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #65: GFLOPs: 1748.0642. Time: 81.3371 us. Best GFLOPs: 2238.6041
2023-11-11 00:40:32 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #66: GFLOPs: 2020.1243. Time: 70.3830 us. Best GFLOPs: 2238.6041
2023-11-11 00:40:32 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #67: GFLOPs: 1393.1140. Time: 102.0608 us. Best GFLOPs: 2238.6041
2023-11-11 00:40:32 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #68: GFLOPs: 2020.7624. Time: 70.3608 us. Best GFLOPs: 2238.6041
2023-11-11 00:40:32 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #69: GFLOPs: 1832.0135. Time: 77.6099 us. Best GFLOPs: 2238.6041
2023-11-11 00:40:32 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #70: GFLOPs: 2459.2568. Time: 57.8152 us. Best GFLOPs: 2459.2568
2023-11-11 00:40:32 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #71: GFLOPs: 1831.6806. Time: 77.6240 us. Best GFLOPs: 2459.2568
2023-11-11 00:40:32 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #72: GFLOPs: 2524.8601. Time: 56.3130 us. Best GFLOPs: 2524.8601
2023-11-11 00:40:32 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #73: GFLOPs: 2050.3794. Time: 69.3444 us. Best GFLOPs: 2524.8601
2023-11-11 00:40:32 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #74: GFLOPs: 1873.2711. Time: 75.9006 us. Best GFLOPs: 2524.8601
2023-11-11 00:40:32 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #75: GFLOPs: 2217.7136. Time: 64.1122 us. Best GFLOPs: 2524.8601
2023-11-11 00:40:32 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #76: GFLOPs: 2261.1494. Time: 62.8806 us. Best GFLOPs: 2524.8601
2023-11-11 00:40:32 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #77: GFLOPs: 1454.1736. Time: 97.7754 us. Best GFLOPs: 2524.8601
2023-11-11 00:40:32 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #78: GFLOPs: 2515.4742. Time: 56.5231 us. Best GFLOPs: 2524.8601
2023-11-11 00:40:32 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #79: GFLOPs: 1828.0021. Time: 77.7802 us. Best GFLOPs: 2524.8601
2023-11-11 00:40:32 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #80: GFLOPs: 1899.8610. Time: 74.8383 us. Best GFLOPs: 2524.8601
2023-11-11 00:40:32 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #81: GFLOPs: 2553.1259. Time: 55.6895 us. Best GFLOPs: 2553.1259
2023-11-11 00:40:32 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #82: GFLOPs: 1872.8684. Time: 75.9169 us. Best GFLOPs: 2553.1259
2023-11-11 00:40:32 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #83: GFLOPs: 2534.9899. Time: 56.0880 us. Best GFLOPs: 2553.1259
2023-11-11 00:40:32 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #84: GFLOPs: 2221.1465. Time: 64.0131 us. Best GFLOPs: 2553.1259
2023-11-11 00:40:32 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #85: GFLOPs: 1494.0811. Time: 95.1638 us. Best GFLOPs: 2553.1259
2023-11-11 00:40:32 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #86: GFLOPs: 2471.8969. Time: 57.5196 us. Best GFLOPs: 2553.1259
2023-11-11 00:40:32 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #87: GFLOPs: 1934.8427. Time: 73.4852 us. Best GFLOPs: 2553.1259
2023-11-11 00:40:32 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #88: GFLOPs: 2229.9665. Time: 63.7599 us. Best GFLOPs: 2553.1259
2023-11-11 00:40:32 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #89: GFLOPs: 2130.1776. Time: 66.7467 us. Best GFLOPs: 2553.1259
2023-11-11 00:40:32 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #90: GFLOPs: 2114.1058. Time: 67.2542 us. Best GFLOPs: 2553.1259
2023-11-11 00:40:32 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #91: GFLOPs: 3605.5648. Time: 39.4342 us. Best GFLOPs: 3605.5648
2023-11-11 00:40:32 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #92: GFLOPs: 2227.9744. Time: 63.8169 us. Best GFLOPs: 3605.5648
2023-11-11 00:40:32 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #93: GFLOPs: 3516.5298. Time: 40.4326 us. Best GFLOPs: 3605.5648
2023-11-11 00:40:32 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #94: GFLOPs: 3400.3413. Time: 41.8142 us. Best GFLOPs: 3605.5648
2023-11-11 00:40:32 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #95: GFLOPs: 2248.1232. Time: 63.2449 us. Best GFLOPs: 3605.5648
2023-11-11 00:40:32 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #96: GFLOPs: 1673.3545. Time: 84.9685 us. Best GFLOPs: 3605.5648
2023-11-11 00:40:32 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #97: GFLOPs: 1929.6145. Time: 73.6844 us. Best GFLOPs: 3605.5648
2023-11-11 00:40:32 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #98: GFLOPs: 1638.9844. Time: 86.7503 us. Best GFLOPs: 3605.5648
2023-11-11 00:40:32 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #99: GFLOPs: 2518.0777. Time: 56.4647 us. Best GFLOPs: 3605.5648
2023-11-11 00:40:32 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #100: GFLOPs: 2233.9106. Time: 63.6473 us. Best GFLOPs: 3605.5648
2023-11-11 00:40:32 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #101: GFLOPs: 1840.1406. Time: 77.2671 us. Best GFLOPs: 3605.5648
2023-11-11 00:40:32 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #102: GFLOPs: 2075.5689. Time: 68.5029 us. Best GFLOPs: 3605.5648
2023-11-11 00:40:32 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #103: GFLOPs: 2143.5229. Time: 66.3312 us. Best GFLOPs: 3605.5648
2023-11-11 00:40:32 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #104: GFLOPs: 2346.8824. Time: 60.5835 us. Best GFLOPs: 3605.5648
2023-11-11 00:40:32 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #105: GFLOPs: 2529.2612. Time: 56.2150 us. Best GFLOPs: 3605.5648
2023-11-11 00:40:32 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #106: GFLOPs: 2528.2870. Time: 56.2367 us. Best GFLOPs: 3605.5648
2023-11-11 00:40:32 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #107: GFLOPs: 1491.6509. Time: 95.3188 us. Best GFLOPs: 3605.5648
2023-11-11 00:40:32 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #108: GFLOPs: 1898.4105. Time: 74.8955 us. Best GFLOPs: 3605.5648
2023-11-11 00:40:32 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #109: GFLOPs: 1403.5277. Time: 101.3036 us. Best GFLOPs: 3605.5648
2023-11-11 00:40:32 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #110: GFLOPs: 1556.2255. Time: 91.3636 us. Best GFLOPs: 3605.5648
2023-11-11 00:40:32 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #111: GFLOPs: 2079.1360. Time: 68.3853 us. Best GFLOPs: 3605.5648
2023-11-11 00:40:32 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #112: GFLOPs: 2462.2434. Time: 57.7451 us. Best GFLOPs: 3605.5648
2023-11-11 00:40:32 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #113: GFLOPs: 2380.4406. Time: 59.7294 us. Best GFLOPs: 3605.5648
2023-11-11 00:40:32 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #114: GFLOPs: 1953.1191. Time: 72.7976 us. Best GFLOPs: 3605.5648
2023-11-11 00:40:32 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #115: GFLOPs: 2121.5706. Time: 67.0175 us. Best GFLOPs: 3605.5648
2023-11-11 00:40:32 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #116: GFLOPs: 2040.7414. Time: 69.6719 us. Best GFLOPs: 3605.5648
2023-11-11 00:40:32 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #117: GFLOPs: 1926.7049. Time: 73.7956 us. Best GFLOPs: 3605.5648
2023-11-11 00:40:32 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #118: GFLOPs: 1992.9502. Time: 71.3427 us. Best GFLOPs: 3605.5648
2023-11-11 00:40:32 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #119: GFLOPs: 2509.4737. Time: 56.6583 us. Best GFLOPs: 3605.5648
2023-11-11 00:40:32 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #120: GFLOPs: 2488.9433. Time: 57.1256 us. Best GFLOPs: 3605.5648
2023-11-11 00:40:32 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #121: GFLOPs: 2006.9076. Time: 70.8465 us. Best GFLOPs: 3605.5648
2023-11-11 00:40:32 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #122: GFLOPs: 2147.6388. Time: 66.2041 us. Best GFLOPs: 3605.5648
2023-11-11 00:40:32 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #123: GFLOPs: 2509.6665. Time: 56.6539 us. Best GFLOPs: 3605.5648
2023-11-11 00:40:32 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #124: GFLOPs: 1911.7451. Time: 74.3731 us. Best GFLOPs: 3605.5648
2023-11-11 00:40:32 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #125: GFLOPs: 2486.6543. Time: 57.1782 us. Best GFLOPs: 3605.5648
2023-11-11 00:40:32 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #126: GFLOPs: 328.9302. Time: 432.2570 us. Best GFLOPs: 3605.5648
2023-11-11 00:40:32 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #127: GFLOPs: 1145.4570. Time: 124.1272 us. Best GFLOPs: 3605.5648
2023-11-11 00:40:32 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #128: GFLOPs: 589.0203. Time: 241.3880 us. Best GFLOPs: 3605.5648
2023-11-11 01:30:14 [INFO] [evolutionary_search.cc:713] Generating candidates......
2023-11-11 01:30:17 [INFO] [evolutionary_search.cc:715] Picked top 102 candidate(s) from database
2023-11-11 01:30:24 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310fb03b28)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310eec3b08)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310dc5caf8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310f7cc2d8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x563110643e78)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310dc32028)]: 401 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310dc5cfb8)]: 0 failure(s)
2023-11-11 01:30:29 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310fb03b28)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310eec3b08)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310dc5caf8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310f7cc2d8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x563110643e78)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310dc32028)]: 802 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310dc5cfb8)]: 0 failure(s)
2023-11-11 01:30:35 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310fb03b28)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310eec3b08)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310dc5caf8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310f7cc2d8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x563110643e78)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310dc32028)]: 1204 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310dc5cfb8)]: 0 failure(s)
2023-11-11 01:30:42 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310fb03b28)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310eec3b08)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310dc5caf8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310f7cc2d8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x563110643e78)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310dc32028)]: 1605 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310dc5cfb8)]: 0 failure(s)
2023-11-11 01:30:47 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310fb03b28)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310eec3b08)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310dc5caf8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310f7cc2d8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x563110643e78)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310dc32028)]: 2010 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310dc5cfb8)]: 0 failure(s)
2023-11-11 01:30:53 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310fb03b28)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310eec3b08)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310dc5caf8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310f7cc2d8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x563110643e78)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310dc32028)]: 2412 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310dc5cfb8)]: 0 failure(s)
2023-11-11 01:30:59 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310fb03b28)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310eec3b08)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310dc5caf8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310f7cc2d8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x563110643e78)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310dc32028)]: 2812 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310dc5cfb8)]: 0 failure(s)
2023-11-11 01:30:59 [INFO] [evolutionary_search.cc:723] Sampled 58 candidate(s)
2023-11-11 01:31:15 [INFO] [evolutionary_search.cc:621] Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310fb03b28)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310eec3b08)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310dc5caf8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310f7cc2d8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x563110643e78)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310dc32028)]: 84 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310dc5cfb8)]: 0 failure(s)
2023-11-11 01:31:34 [INFO] [evolutionary_search.cc:621] Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310fb03b28)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310eec3b08)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310dc5caf8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310f7cc2d8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x563110643e78)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310dc32028)]: 62 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310dc5cfb8)]: 0 failure(s)
2023-11-11 01:31:52 [INFO] [evolutionary_search.cc:621] Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310fb03b28)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310eec3b08)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310dc5caf8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310f7cc2d8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x563110643e78)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310dc32028)]: 69 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310dc5cfb8)]: 0 failure(s)
2023-11-11 01:32:12 [INFO] [evolutionary_search.cc:621] Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310fb03b28)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310eec3b08)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310dc5caf8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310f7cc2d8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x563110643e78)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310dc32028)]: 47 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310dc5cfb8)]: 0 failure(s)
2023-11-11 01:32:17 [INFO] [evolutionary_search.cc:649] Scores of the best 64 candidates:
[1 : 16]:	1.8811  1.0198  1.0086  0.9934  0.9898  0.9794  0.9787  0.9766  0.9754  0.9603  0.9549  0.9518  0.9501  0.9420  0.9418  0.9358
[17 : 32]:	0.9341  0.9312  0.9276  0.9260  0.9241  0.9228  0.9221  0.9207  0.9202  0.9154  0.9120  0.9083  0.9053  0.9042  0.9030  0.9012
[33 : 48]:	0.9003  0.8989  0.8987  0.8977  0.8868  0.8855  0.8825  0.8777  0.8742  0.8690  0.8689  0.8683  0.8632  0.8626  0.8605  0.8605
[49 : 64]:	0.8602  0.8592  0.8582  0.8575  0.8575  0.8495  0.8494  0.8482  0.8449  0.8439  0.8420  0.8398  0.8366  0.8345  0.8342  0.8337
2023-11-11 01:32:17 [INFO] [evolutionary_search.cc:727] Got 64 candidate(s) with evolutionary search
2023-11-11 01:32:17 [INFO] [evolutionary_search.cc:730] Sending 64 candidates(s) for measurement
2023-11-11 01:34:49 [INFO] [task_scheduler.cc:121] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #129: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/home2/xiachunwei/Software/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/home2/xiachunwei/Software/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/home2/xiachunwei/Software/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/home2/xiachunwei/Software/tvm/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/home2/xiachunwei/Software/tvm/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/home2/xiachunwei/Software/tvm/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/home2/xiachunwei/Software/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/home2/xiachunwei/Software/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  4: _ZN3tvm7runtime13PackedFun
  3: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  2: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  0: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  File "/home2/xiachunwei/Software/tvm/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(512), T.int64(7), T.int64(7)), "float32"), p1: T.Buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(512)), "float32"), p2: T.Buffer((T.int64(1), T.int64(512), T.int64(7), T.int64(7)), "float32"), p3: T.Buffer((T.int64(1), T.int64(512), T.int64(1), T.int64(1)), "float32"), p4: T.Buffer((T.int64(1), T.int64(512), T.int64(1), T.int64(1)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(512), T.int64(7), T.int64(7)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        input_tile_local = T.alloc_buffer((T.int64(512), T.int64(16), T.int64(4), T.int64(4)), scope="local")
        data_pack = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(16)))
        bgemm = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(16)))
        inverse_local = T.alloc_buffer((T.int64(512), T.int64(16), T.int64(2), T.int64(2)), scope="local")
        data_pack_local = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(16)), scope="local")
        bgemm_local = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(16)), scope="local")
        data_pack_shared = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(16)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(512)), scope="shared")
        for ci_p_fused_0 in T.thread_binding(T.int64(8), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 512, "pragma_unroll_explicit": 1}):
            for ci_p_fused_1 in T.thread_binding(T.int64(1024), thread="threadIdx.x"):
                for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(1), T.int64(4), T.int64(4)):
                    with T.block("input_tile"):
                        v_ci = T.axis.spatial(T.int64(512), ci_p_fused_0 * T.int64(64) + ci_p_fused_1 // T.int64(16) + ax0)
                        v_p = T.axis.spatial(T.int64(16), ci_p_fused_1 % T.int64(16) + ax1)
                        v_eps, v_nu = T.axis.remap("SS", [ax2, ax3])
                        T.reads(p0[v_p // T.int64(16), v_ci, v_p % T.int64(16) // T.int64(4) * T.int64(2) + v_eps - T.int64(1), v_p % T.int64(4) * T.int64(2) + v_nu - T.int64(1)])
                        T.writes(input_tile_local[v_ci, v_p, v_eps, v_nu])
                        T.block_attr({"schedule_rule": "None"})
                        input_tile_local[v_ci, v_p, v_eps, v_nu] = T.if_then_else(T.int64(1) <= v_p % T.int64(16) // T.int64(4) * T.int64(2) + v_eps and v_p % T.int64(16) // T.int64(4) * T.int64(2) + v_eps < T.int64(8) and T.int64(1) <= v_p % T.int64(4) * T.int64(2) + v_nu and v_p % T.int64(4) * T.int64(2) + v_nu < T.int64(8), p0[v_p // T.int64(16), v_ci, v_p % T.int64(16) // T.int64(4) * T.int64(2) + v_eps - T.int64(1), v_p % T.int64(4) * T.int64(2) + v_nu - T.int64(1)], T.float32(0))
                for eps in T.unroll(T.int64(4)):
                    for nu in T.unroll(T.int64(4)):
                        with T.block("data_pack_init"):
                            v_eps, v_nu = T.axis.remap("SS", [eps, nu])
                            v_ci = T.axis.spatial(T.int64(512), (ci_p_fused_0 * T.int64(1024) + ci_p_fused_1) // T.int64(16))
                            v_p = T.axis.spatial(T.int64(16), (ci_p_fused_0 * T.int64(1024) + ci_p_fused_1) % T.int64(16))
                            T.reads()
                            T.writes(data_pack_local[v_eps, v_nu, v_ci, v_p])
                            T.block_attr({"schedule_rule": "conv2d_nchw_winograd_data_pack"})
                            data_pack_local[v_eps, v_nu, v_ci, v_p] = T.float32(0)
                        for r_a in T.unroll(T.int64(4)):
                            for r_b in T.unroll(T.int64(4)):
                                with T.block("data_pack_update"):
                                    v_eps, v_nu = T.axis.remap("SS", [eps, nu])
                                    v_ci = T.axis.spatial(T.int64(512), (ci_p_fused_0 * T.int64(1024) + ci_p_fused_1) // T.int64(16))
                                    v_p = T.axis.spatial(T.int64(16), (ci_p_fused_0 * T.int64(1024) + ci_p_fused_1) % T.int64(16))
                                    v_r_a, v_r_b = T.axis.remap("RR", [r_a, r_b])
                                    T.reads(data_pack_local[v_eps, v_nu, v_ci, v_p], input_tile_local[v_ci, v_p, v_r_a, v_r_b])
                                    T.writes(data_pack_local[v_eps, v_nu, v_ci, v_p])
                                    T.block_attr({"schedule_rule": "conv2d_nchw_winograd_data_pack"})
                                    data_pack_local[v_eps, v_nu, v_ci, v_p] = data_pack_local[v_eps, v_nu, v_ci, v_p] + input_tile_local[v_ci, v_p, v_r_a, v_r_b] * T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(3), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(1), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(0), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(3), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(1), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(0), T.float32(1), T.float32(0))))))))))))))))) * T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(3), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(1), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(0), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(3), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(1), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(0), T.float32(1), T.float32(0)))))))))))))))))
                for ax0, ax1, ax2, ax3 in T.grid(T.int64(4), T.int64(4), T.int64(1), T.int64(1)):
                    with T.block("data_pack_local"):
                        v0, v1 = T.axis.remap("SS", [ax0, ax1])
                        v2 = T.axis.spatial(T.int64(512), ci_p_fused_0 * T.int64(64) + ci_p_fused_1 // T.int64(16) + ax2)
                        v3 = T.axis.spatial(T.int64(16), ci_p_fused_1 % T.int64(16) + ax3)
                        T.reads(data_pack_local[v0, v1, v2, v3])
                        T.writes(data_pack[v0, v1, v2, v3])
                        data_pack[v0, v1, v2, v3] = data_pack_local[v0, v1, v2, v3]
        for eps_0_nu_0_co_0_p_0_fused in T.thread_binding(T.int64(128), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 512, "pragma_unroll_explicit": 1}):
            for eps_1_nu_1_co_1_p_1_fused in T.thread_binding(T.int64(32), thread="vthread.x"):
                for eps_2_nu_2_co_2_p_2_fused in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                    for eps_3_init, nu_3_init, co_3_init, p_3_init, eps_4_init, nu_4_init, co_4_init, p_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("bgemm_init"):
                            v_eps = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(64) * T.int64(2) + eps_2_nu_2_co_2_p_2_fused // T.int64(16) + eps_3_init + eps_4_init)
                            v_nu = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused % T.int64(64) // T.int64(16) + nu_3_init + nu_4_init)
                            v_co = T.axis.spatial(T.int64(512), eps_0_nu_0_co_0_p_0_fused % T.int64(16) * T.int64(32) + eps_1_nu_1_co_1_p_1_fused // T.int64(16) * T.int64(16) + eps_2_nu_2_co_2_p_2_fused % T.int64(16) + co_3_init + co_4_init)
                            v_p = T.axis.spatial(T.int64(16), eps_1_nu_1_co_1_p_1_fused % T.int64(16) + p_3_init + p_4_init)
                            T.reads()
                            T.writes(bgemm_local[v_eps, v_nu, v_co, v_p])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            bgemm_local[v_eps, v_nu, v_co, v_p] = T.float32(0)
                    for ci_0 in range(T.int64(4)):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(128)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                                with T.block("data_pack_shared"):
                                    v0 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(64) * T.int64(2) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(32) + ax0_ax1_ax2_ax3_fused_1) // T.int64(2048))
                                    v1 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused % T.int64(64) // T.int64(16))
                                    v2 = T.axis.spatial(T.int64(512), ci_0 * T.int64(128) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(32) + ax0_ax1_ax2_ax3_fused_1) % T.int64(2048) // T.int64(16))
                                    v3 = T.axis.spatial(T.int64(16), (ax0_ax1_ax2_ax3_fused_0 * T.int64(32) + ax0_ax1_ax2_ax3_fused_1) % T.int64(16))
                                    T.reads(data_pack[v0, v1, v2, v3])
                                    T.writes(data_pack_shared[v0, v1, v2, v3])
                                    data_pack_shared[v0, v1, v2, v3] = data_pack[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(64)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("p1_shared"):
                                        v0 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(64) * T.int64(2) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) // T.int64(4096))
                                        v1 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused % T.int64(64) // T.int64(16))
                                        v2 = T.axis.spatial(T.int64(512), ci_0 * T.int64(128) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) % T.int64(4096) // T.int64(32))
                                        v3 = T.axis.spatial(T.int64(512), eps_0_nu_0_co_0_p_0_fused % T.int64(16) * T.int64(32) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) % T.int64(32))
                                        T.reads(p1[v0, v1, v2, v3])
                                        T.writes(p1_shared[v0, v1, v2, v3])
                                        p1_shared[v0, v1, v2, v3] = p1[v0, v1, v2, v3]
                        for ci_1, eps_3, nu_3, co_3, p_3, ci_2, eps_4, nu_4, co_4, p_4 in T.grid(T.int64(64), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                            with T.block("bgemm_update"):
                                v_eps = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(64) * T.int64(2) + eps_2_nu_2_co_2_p_2_fused // T.int64(16) + eps_3 + eps_4)
                                v_nu = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused % T.int64(64) // T.int64(16) + nu_3 + nu_4)
                                v_co = T.axis.spatial(T.int64(512), eps_0_nu_0_co_0_p_0_fused % T.int64(16) * T.int64(32) + eps_1_nu_1_co_1_p_1_fused // T.int64(16) * T.int64(16) + eps_2_nu_2_co_2_p_2_fused % T.int64(16) + co_3 + co_4)
                                v_p = T.axis.spatial(T.int64(16), eps_1_nu_1_co_1_p_1_fused % T.int64(16) + p_3 + p_4)
                                v_ci = T.axis.reduce(T.int64(512), ci_0 * T.int64(128) + ci_1 * T.int64(2) + ci_2)
                                T.reads(bgemm_local[v_eps, v_nu, v_co, v_p], data_pack_shared[v_eps, v_nu, v_ci, v_p], p1_shared[v_eps, v_nu, v_ci, v_co])
                                T.writes(bgemm_local[v_eps, v_nu, v_co, v_p])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                bgemm_local[v_eps, v_nu, v_co, v_p] = bgemm_local[v_eps, v_nu, v_co, v_p] + data_pack_shared[v_eps, v_nu, v_ci, v_p] * p1_shared[v_eps, v_nu, v_ci, v_co]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("bgemm_local"):
                            v0 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(64) * T.int64(2) + eps_2_nu_2_co_2_p_2_fused // T.int64(16) + ax0)
                            v1 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused % T.int64(64) // T.int64(16) + ax1)
                            v2 = T.axis.spatial(T.int64(512), eps_0_nu_0_co_0_p_0_fused % T.int64(16) * T.int64(32) + eps_1_nu_1_co_1_p_1_fused // T.int64(16) * T.int64(16) + eps_2_nu_2_co_2_p_2_fused % T.int64(16) + ax2)
                            v3 = T.axis.spatial(T.int64(16), eps_1_nu_1_co_1_p_1_fused % T.int64(16) + ax3)
                            T.reads(bgemm_local[v0, v1, v2, v3])
                            T.writes(bgemm[v0, v1, v2, v3])
                            bgemm[v0, v1, v2, v3] = bgemm_local[v0, v1, v2, v3]
        for n_co_h_0_w_0_fused_0 in T.thread_binding(T.int64(32), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 512, "pragma_unroll_explicit": 1}):
            for n_co_h_0_w_0_fused_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                    for ax2 in T.unroll(T.int64(2)):
                        for ax3 in T.unroll(T.int64(2)):
                            with T.block("inverse_init"):
                                v_co = T.axis.spatial(T.int64(512), (n_co_h_0_w_0_fused_0 * T.int64(256) + n_co_h_0_w_0_fused_1) // T.int64(16) + ax0)
                                v_p = T.axis.spatial(T.int64(16), (n_co_h_0_w_0_fused_0 * T.int64(256) + n_co_h_0_w_0_fused_1) % T.int64(16) + ax1)
                                v_vh, v_vw = T.axis.remap("SS", [ax2, ax3])
                                T.reads()
                                T.writes(inverse_local[v_co, v_p, v_vh, v_vw])
                                T.block_attr({"schedule_rule": "conv2d_nchw_winograd_inverse"})
                                inverse_local[v_co, v_p, v_vh, v_vw] = T.float32(0)
                            for ax4 in T.unroll(T.int64(4)):
                                for ax5 in T.unroll(T.int64(4)):
                                    with T.block("inverse_update"):
                                        v_co = T.axis.spatial(T.int64(512), (n_co_h_0_w_0_fused_0 * T.int64(256) + n_co_h_0_w_0_fused_1) // T.int64(16) + ax0)
                                        v_p = T.axis.spatial(T.int64(16), (n_co_h_0_w_0_fused_0 * T.int64(256) + n_co_h_0_w_0_fused_1) % T.int64(16) + ax1)
                                        v_vh, v_vw, v_r_a, v_r_b = T.axis.remap("SSRR", [ax2, ax3, ax4, ax5])
                                        T.reads(inverse_local[v_co, v_p, v_vh, v_vw], bgemm[v_r_a, v_r_b, v_co, v_p])
                                        T.writes(inverse_local[v_co, v_p, v_vh, v_vw])
                                        T.block_attr({"schedule_rule": "conv2d_nchw_winograd_inverse"})
                                        inverse_local[v_co, v_p, v_vh, v_vw] = inverse_local[v_co, v_p, v_vh, v_vw] + bgemm[v_r_a, v_r_b, v_co, v_p] * T.Select(v_r_a % T.int64(4) == T.int64(3) and v_vh % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_vh % T.int64(2) == T.int64(0), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_vh % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_vh % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_vh % T.int64(2) == T.int64(1), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_vh % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_vh % T.int64(2) == T.int64(1), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_vh % T.int64(2) == T.int64(0), T.float32(1), T.float32(0))))))))) * T.Select(v_r_b % T.int64(4) == T.int64(3) and v_vw % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_vw % T.int64(2) == T.int64(0), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_vw % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_vw % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_vw % T.int64(2) == T.int64(1), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_vw % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_vw % T.int64(2) == T.int64(1), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_vw % T.int64(2) == T.int64(0), T.float32(1), T.float32(0)))))))))
                for h_1, w_1 in T.grid(T.int64(2), T.int64(2)):
                    with T.block("conv2d_winograd"):
                        v_n = T.axis.spatial(T.int64(1), T.int64(0))
                        v_co = T.axis.spatial(T.int64(512), (n_co_h_0_w_0_fused_0 * T.int64(256) + n_co_h_0_w_0_fused_1) // T.int64(16))
                        v_h = T.axis.spatial(T.int64(7), (n_co_h_0_w_0_fused_0 * T.int64(256) + n_co_h_0_w_0_fused_1) % T.int64(16) // T.int64(4) * T.int64(2) + h_1)
                        v_w = T.axis.spatial(T.int64(7), (n_co_h_0_w_0_fused_0 * T.int64(256) + n_co_h_0_w_0_fused_1) % T.int64(4) * T.int64(2) + w_1)
                        T.where((n_co_h_0_w_0_fused_0 * T.int64(256) + n_co_h_0_w_0_fused_1) % T.int64(16) // T.int64(4) * T.int64(2) + h_1 < T.int64(7) and (n_co_h_0_w_0_fused_0 * T.int64(256) + n_co_h_0_w_0_fused_1) % T.int64(4) * T.int64(2) + w_1 < T.int64(7))
                        T.reads(inverse_local[v_co, v_n * T.int64(16) + v_h // T.int64(2) * T.int64(4) + v_w // T.int64(2), v_h % T.int64(2), v_w % T.int64(2)], p2[v_n, v_co, v_h, v_w], p3[v_n, v_co, T.int64(0), T.int64(0)], p4[v_n, v_co, T.int64(0), T.int64(0)])
                        T.writes(T_relu[v_n, v_co, v_h, v_w])
                        T_relu[v_n, v_co, v_h, v_w] = T.max((inverse_local[v_co, v_n * T.int64(16) + v_h // T.int64(2) * T.int64(4) + v_w // T.int64(2), v_h % T.int64(2), v_w % T.int64(2)] + p2[v_n, v_co, v_h, v_w]) * p3[v_n, v_co, T.int64(0), T.int64(0)] + p4[v_n, v_co, T.int64(0), T.int64(0)], T.float32(0))
b0 = sch.get_block(name="data_pack", func_name="main")
b1 = sch.get_block(name="bgemm", func_name="main")
b2 = sch.get_block(name="inverse", func_name="main")
b3 = sch.get_block(name="conv2d_winograd", func_name="main")
b4 = sch.get_block(name="T_add", func_name="main")
b5 = sch.get_block(name="T_multiply", func_name="main")
b6 = sch.get_block(name="T_add_1", func_name="main")
b7 = sch.get_block(name="T_relu", func_name="main")
b8 = sch.get_block(name="root", func_name="main")
b9, b10 = sch.get_producers(block=b2)
sch.compute_inline(block=b10)
b11, = sch.get_consumers(block=b2)
l12, l13, l14, l15 = sch.get_loops(block=b11)
l16, l17 = sch.split(loop=l14, factors=[None, 2], preserve_unit_iters=True)
l18, l19 = sch.split(loop=l15, factors=[None, 2], preserve_unit_iters=True)
sch.reorder(l16, l18, l17, l19)
sch.compute_at(block=b2, loop=l18, preserve_unit_loops=True, index=-1)
sch.set_scope(block=b2, buffer_index=0, storage_scope="local")
l20, l21, l22, l23, l24, l25, l26, l27, l28, l29 = sch.get_loops(block=b2)
sch.unroll(loop=l26)
sch.unroll(loop=l27)
sch.unroll(loop=l28)
sch.unroll(loop=l29)
b30, b31 = sch.get_producers(block=b0)
sch.compute_inline(block=b31)
b32, = sch.get_producers(block=b30)
l33, l34, l35, l36, l37, l38 = sch.get_loops(block=b0)
sch.reorder(l35, l36, l33, l34, l37, l38)
sch.unroll(loop=l33)
sch.unroll(loop=l34)
sch.unroll(loop=l37)
sch.unroll(loop=l38)
l39 = sch.fuse(l35, l36, preserve_unit_iters=True)
v40 = sch.sample_categorical(candidates=[32, 64, 128, 256, 512, 1024], probs=[0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666], decision=5)
l41, l42 = sch.split(loop=l39, factors=[None, v40], preserve_unit_iters=True)
sch.bind(loop=l41, thread_axis="blockIdx.x")
sch.bind(loop=l42, thread_axis="threadIdx.x")
b43 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b43, loop=l42, preserve_unit_loops=True, index=-1)
sch.compute_at(block=b30, loop=l42, preserve_unit_loops=True, index=-1)
sch.set_scope(block=b30, buffer_index=0, storage_scope="local")
sch.compute_inline(block=b32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l44, l45, l46, l47, l48 = sch.get_loops(block=b1)
v49, v50, v51, v52, v53 = sch.sample_perfect_tile(loop=l44, n=5, max_innermost_factor=64, decision=[2, 1, 2, 1, 1])
l54, l55, l56, l57, l58 = sch.split(loop=l44, factors=[v49, v50, v51, v52, v53], preserve_unit_iters=True)
v59, v60, v61, v62, v63 = sch.sample_perfect_tile(loop=l45, n=5, max_innermost_factor=64, decision=[4, 1, 1, 1, 1])
l64, l65, l66, l67, l68 = sch.split(loop=l45, factors=[v59, v60, v61, v62, v63], preserve_unit_iters=True)
v69, v70, v71, v72, v73 = sch.sample_perfect_tile(loop=l46, n=5, max_innermost_factor=64, decision=[16, 2, 16, 1, 1])
l74, l75, l76, l77, l78 = sch.split(loop=l46, factors=[v69, v70, v71, v72, v73], preserve_unit_iters=True)
v79, v80, v81, v82, v83 = sch.sample_perfect_tile(loop=l47, n=5, max_innermost_factor=64, decision=[1, 16, 1, 1, 1])
l84, l85, l86, l87, l88 = sch.split(loop=l47, factors=[v79, v80, v81, v82, v83], preserve_unit_iters=True)
v89, v90, v91 = sch.sample_perfect_tile(loop=l48, n=3, max_innermost_factor=64, decision=[4, 64, 2])
l92, l93, l94 = sch.split(loop=l48, factors=[v89, v90, v91], preserve_unit_iters=True)
sch.reorder(l54, l64, l74, l84, l55, l65, l75, l85, l56, l66, l76, l86, l92, l93, l57, l67, l77, l87, l94, l58, l68, l78, l88)
l95 = sch.fuse(l54, l64, l74, l84, preserve_unit_iters=True)
sch.bind(loop=l95, thread_axis="blockIdx.x")
l96 = sch.fuse(l55, l65, l75, l85, preserve_unit_iters=True)
sch.bind(loop=l96, thread_axis="vthread.x")
l97 = sch.fuse(l56, l66, l76, l86, preserve_unit_iters=True)
sch.bind(loop=l97, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b98 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b98, loop=l97, preserve_unit_loops=True, index=-1)
b99 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b99, loop=l92, preserve_unit_loops=True, index=-1)
l100, l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b99)
l108 = sch.fuse(l104, l105, l106, l107, preserve_unit_iters=True)
v109 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b99, ann_key="meta_schedule.cooperative_fetch", ann_val=v109)
b110 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b110, loop=l92, preserve_unit_loops=True, index=-1)
l111, l112, l113, l114, l115, l116, l117, l118 = sch.get_loops(block=b110)
l119 = sch.fuse(l115, l116, l117, l118, preserve_unit_iters=True)
v120 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b110, ann_key="meta_schedule.cooperative_fetch", ann_val=v120)
sch.reverse_compute_inline(block=b7)
sch.reverse_compute_inline(block=b6)
sch.reverse_compute_inline(block=b5)
sch.reverse_compute_inline(block=b4)
v121 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b8, ann_key="meta_schedule.unroll_explicit", ann_val=v121)
l122, l123, l124, l125, l126, l127 = sch.get_loops(block=b3)
l128 = sch.fuse(l122, l123, l124, l125, preserve_unit_iters=True)
v129 = sch.sample_categorical(candidates=[32, 64, 128, 256, 512, 1024], probs=[0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666], decision=3)
l130, l131 = sch.split(loop=l128, factors=[None, v129], preserve_unit_iters=True)
sch.bind(loop=l130, thread_axis="blockIdx.x")
sch.bind(loop=l131, thread_axis="threadIdx.x")
sch.enter_postproc()
sch.unannotate(block_or_loop=b99, ann_key="meta_schedule.cooperative_fetch")
l132, l133, l134, l135, l136 = sch.get_loops(block=b99)
l137, l138 = sch.split(loop=l136, factors=[None, 32], preserve_unit_iters=True)
sch.bind(loop=l138, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b110, ann_key="meta_schedule.cooperative_fetch")
l139, l140, l141, l142, l143 = sch.get_loops(block=b110)
l144, l145, l146 = sch.split(loop=l143, factors=[None, 32, 4], preserve_unit_iters=True)
sch.vectorize(loop=l146)
sch.bind(loop=l145, thread_axis="threadIdx.x")
b147 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b147, ann_key="meta_schedule.unroll_explicit")
b148, b149, b150, b151, b152, b153, b154, b155, b156 = sch.get_child_blocks(b147)
l157, l158, l159, l160, l161, l162 = sch.get_loops(block=b148)
l163, l164, l165, l166, l167, l168 = sch.get_loops(block=b149)
sch.annotate(block_or_loop=l163, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l163, ann_key="pragma_unroll_explicit", ann_val=1)
l169, l170, l171, l172, l173, l174 = sch.get_loops(block=b150)
l175, l176, l177, l178, l179, l180 = sch.get_loops(block=b151)
l181, l182, l183, l184, l185, l186, l187 = sch.get_loops(block=b152)
l188, l189, l190, l191, l192, l193, l194, l195, l196, l197, l198, l199, l200, l201 = sch.get_loops(block=b153)
sch.annotate(block_or_loop=l188, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l188, ann_key="pragma_unroll_explicit", ann_val=1)
l202, l203, l204, l205, l206, l207, l208 = sch.get_loops(block=b154)
l209, l210, l211, l212, l213, l214, l215, l216 = sch.get_loops(block=b155)
sch.annotate(block_or_loop=l209, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l209, ann_key="pragma_unroll_explicit", ann_val=1)
l217, l218, l219, l220 = sch.get_loops(block=b156)
b221 = sch.get_block(name="data_pack", func_name="main")
l222, l223, l224, l225, l226, l227 = sch.get_loops(block=b221)
b228 = sch.decompose_reduction(block=b221, loop=l226)
b229 = sch.get_block(name="bgemm", func_name="main")
l230, l231, l232, l233, l234, l235, l236, l237, l238, l239, l240, l241, l242, l243 = sch.get_loops(block=b229)
b244 = sch.decompose_reduction(block=b229, loop=l233)
b245 = sch.get_block(name="inverse", func_name="main")
l246, l247, l248, l249, l250, l251, l252, l253 = sch.get_loops(block=b245)
b254 = sch.decompose_reduction(block=b245, loop=l252)
2023-11-11 01:34:49 [INFO] [task_scheduler.cc:121] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #130: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/home2/xiachunwei/Software/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/home2/xiachunwei/Software/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/home2/xiachunwei/Software/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/home2/xiachunwei/Software/tvm/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/home2/xiachunwei/Software/tvm/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/home2/xiachunwei/Software/tvm/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/home2/xiachunwei/Software/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/home2/xiachunwei/Software/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  4: _ZN3tvm7runtime13PackedFun
  3: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  2: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  0: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  File "/home2/xiachunwei/Software/tvm/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(512), T.int64(7), T.int64(7)), "float32"), p1: T.Buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(512)), "float32"), p2: T.Buffer((T.int64(1), T.int64(512), T.int64(7), T.int64(7)), "float32"), p3: T.Buffer((T.int64(1), T.int64(512), T.int64(1), T.int64(1)), "float32"), p4: T.Buffer((T.int64(1), T.int64(512), T.int64(1), T.int64(1)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(512), T.int64(7), T.int64(7)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        input_tile_local = T.alloc_buffer((T.int64(512), T.int64(16), T.int64(4), T.int64(4)), scope="local")
        data_pack = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(16)))
        bgemm = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(16)))
        inverse_local = T.alloc_buffer((T.int64(512), T.int64(16), T.int64(2), T.int64(2)), scope="local")
        data_pack_local = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(16)), scope="local")
        bgemm_local = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(16)), scope="local")
        data_pack_shared = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(16)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(512)), scope="shared")
        for ci_p_fused_0 in T.thread_binding(T.int64(64), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 16, "pragma_unroll_explicit": 1}):
            for ci_p_fused_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(1), T.int64(4), T.int64(4)):
                    with T.block("input_tile"):
                        v_ci = T.axis.spatial(T.int64(512), ci_p_fused_0 * T.int64(8) + ci_p_fused_1 // T.int64(16) + ax0)
                        v_p = T.axis.spatial(T.int64(16), ci_p_fused_1 % T.int64(16) + ax1)
                        v_eps, v_nu = T.axis.remap("SS", [ax2, ax3])
                        T.reads(p0[v_p // T.int64(16), v_ci, v_p % T.int64(16) // T.int64(4) * T.int64(2) + v_eps - T.int64(1), v_p % T.int64(4) * T.int64(2) + v_nu - T.int64(1)])
                        T.writes(input_tile_local[v_ci, v_p, v_eps, v_nu])
                        T.block_attr({"schedule_rule": "None"})
                        input_tile_local[v_ci, v_p, v_eps, v_nu] = T.if_then_else(T.int64(1) <= v_p % T.int64(16) // T.int64(4) * T.int64(2) + v_eps and v_p % T.int64(16) // T.int64(4) * T.int64(2) + v_eps < T.int64(8) and T.int64(1) <= v_p % T.int64(4) * T.int64(2) + v_nu and v_p % T.int64(4) * T.int64(2) + v_nu < T.int64(8), p0[v_p // T.int64(16), v_ci, v_p % T.int64(16) // T.int64(4) * T.int64(2) + v_eps - T.int64(1), v_p % T.int64(4) * T.int64(2) + v_nu - T.int64(1)], T.float32(0))
                for eps in T.unroll(T.int64(4)):
                    for nu in T.unroll(T.int64(4)):
                        with T.block("data_pack_init"):
                            v_eps, v_nu = T.axis.remap("SS", [eps, nu])
                            v_ci = T.axis.spatial(T.int64(512), (ci_p_fused_0 * T.int64(128) + ci_p_fused_1) // T.int64(16))
                            v_p = T.axis.spatial(T.int64(16), (ci_p_fused_0 * T.int64(128) + ci_p_fused_1) % T.int64(16))
                            T.reads()
                            T.writes(data_pack_local[v_eps, v_nu, v_ci, v_p])
                            T.block_attr({"schedule_rule": "conv2d_nchw_winograd_data_pack"})
                            data_pack_local[v_eps, v_nu, v_ci, v_p] = T.float32(0)
                        for r_a in T.unroll(T.int64(4)):
                            for r_b in T.unroll(T.int64(4)):
                                with T.block("data_pack_update"):
                                    v_eps, v_nu = T.axis.remap("SS", [eps, nu])
                                    v_ci = T.axis.spatial(T.int64(512), (ci_p_fused_0 * T.int64(128) + ci_p_fused_1) // T.int64(16))
                                    v_p = T.axis.spatial(T.int64(16), (ci_p_fused_0 * T.int64(128) + ci_p_fused_1) % T.int64(16))
                                    v_r_a, v_r_b = T.axis.remap("RR", [r_a, r_b])
                                    T.reads(data_pack_local[v_eps, v_nu, v_ci, v_p], input_tile_local[v_ci, v_p, v_r_a, v_r_b])
                                    T.writes(data_pack_local[v_eps, v_nu, v_ci, v_p])
                                    T.block_attr({"schedule_rule": "conv2d_nchw_winograd_data_pack"})
                                    data_pack_local[v_eps, v_nu, v_ci, v_p] = data_pack_local[v_eps, v_nu, v_ci, v_p] + input_tile_local[v_ci, v_p, v_r_a, v_r_b] * T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(3), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(1), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(0), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(3), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(1), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(0), T.float32(1), T.float32(0))))))))))))))))) * T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(3), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(1), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(0), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(3), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(1), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(0), T.float32(1), T.float32(0)))))))))))))))))
                for ax0, ax1, ax2, ax3 in T.grid(T.int64(4), T.int64(4), T.int64(1), T.int64(1)):
                    with T.block("data_pack_local"):
                        v0, v1 = T.axis.remap("SS", [ax0, ax1])
                        v2 = T.axis.spatial(T.int64(512), ci_p_fused_0 * T.int64(8) + ci_p_fused_1 // T.int64(16) + ax2)
                        v3 = T.axis.spatial(T.int64(16), ci_p_fused_1 % T.int64(16) + ax3)
                        T.reads(data_pack_local[v0, v1, v2, v3])
                        T.writes(data_pack[v0, v1, v2, v3])
                        data_pack[v0, v1, v2, v3] = data_pack_local[v0, v1, v2, v3]
        for eps_0_nu_0_co_0_p_0_fused in T.thread_binding(T.int64(512), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 16, "pragma_unroll_explicit": 1}):
            for eps_1_nu_1_co_1_p_1_fused in T.thread_binding(T.int64(4), thread="vthread.x"):
                for eps_2_nu_2_co_2_p_2_fused in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                    for eps_3_init, nu_3_init, co_3_init, p_3_init, eps_4_init, nu_4_init, co_4_init, p_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("bgemm_init"):
                            v_eps = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(256) * T.int64(2) + eps_2_nu_2_co_2_p_2_fused // T.int64(32) + eps_3_init + eps_4_init)
                            v_nu = T.axis.spatial(T.int64(4), eps_1_nu_1_co_1_p_1_fused // T.int64(2) * T.int64(2) + eps_2_nu_2_co_2_p_2_fused % T.int64(32) // T.int64(16) + nu_3_init + nu_4_init)
                            v_co = T.axis.spatial(T.int64(512), eps_0_nu_0_co_0_p_0_fused % T.int64(256) // T.int64(2) * T.int64(4) + eps_1_nu_1_co_1_p_1_fused % T.int64(2) * T.int64(2) + eps_2_nu_2_co_2_p_2_fused % T.int64(16) // T.int64(8) + co_3_init + co_4_init)
                            v_p = T.axis.spatial(T.int64(16), eps_0_nu_0_co_0_p_0_fused % T.int64(2) * T.int64(8) + eps_2_nu_2_co_2_p_2_fused % T.int64(8) + p_3_init + p_4_init)
                            T.reads()
                            T.writes(bgemm_local[v_eps, v_nu, v_co, v_p])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            bgemm_local[v_eps, v_nu, v_co, v_p] = T.float32(0)
                    for ci_0 in range(T.int64(4)):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(64)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("data_pack_shared"):
                                        v0 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(256) * T.int64(2) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) // T.int64(4096))
                                        v1 = T.axis.spatial(T.int64(4), (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) % T.int64(4096) // T.int64(1024))
                                        v2 = T.axis.spatial(T.int64(512), ci_0 * T.int64(128) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) % T.int64(1024) // T.int64(8))
                                        v3 = T.axis.spatial(T.int64(16), eps_0_nu_0_co_0_p_0_fused % T.int64(2) * T.int64(8) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) % T.int64(8))
                                        T.reads(data_pack[v0, v1, v2, v3])
                                        T.writes(data_pack_shared[v0, v1, v2, v3])
                                        data_pack_shared[v0, v1, v2, v3] = data_pack[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(64)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                                with T.block("p1_shared"):
                                    v0 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(256) * T.int64(2) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused_1) // T.int64(2048))
                                    v1 = T.axis.spatial(T.int64(4), (ax0_ax1_ax2_ax3_fused_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused_1) % T.int64(2048) // T.int64(512))
                                    v2 = T.axis.spatial(T.int64(512), ci_0 * T.int64(128) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused_1) % T.int64(512) // T.int64(4))
                                    v3 = T.axis.spatial(T.int64(512), eps_0_nu_0_co_0_p_0_fused % T.int64(256) // T.int64(2) * T.int64(4) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused_1) % T.int64(4))
                                    T.reads(p1[v0, v1, v2, v3])
                                    T.writes(p1_shared[v0, v1, v2, v3])
                                    p1_shared[v0, v1, v2, v3] = p1[v0, v1, v2, v3]
                        for ci_1, eps_3, nu_3, co_3, p_3, ci_2, eps_4, nu_4, co_4, p_4 in T.grid(T.int64(4), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(32), T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                            with T.block("bgemm_update"):
                                v_eps = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(256) * T.int64(2) + eps_2_nu_2_co_2_p_2_fused // T.int64(32) + eps_3 + eps_4)
                                v_nu = T.axis.spatial(T.int64(4), eps_1_nu_1_co_1_p_1_fused // T.int64(2) * T.int64(2) + eps_2_nu_2_co_2_p_2_fused % T.int64(32) // T.int64(16) + nu_3 + nu_4)
                                v_co = T.axis.spatial(T.int64(512), eps_0_nu_0_co_0_p_0_fused % T.int64(256) // T.int64(2) * T.int64(4) + eps_1_nu_1_co_1_p_1_fused % T.int64(2) * T.int64(2) + eps_2_nu_2_co_2_p_2_fused % T.int64(16) // T.int64(8) + co_3 + co_4)
                                v_p = T.axis.spatial(T.int64(16), eps_0_nu_0_co_0_p_0_fused % T.int64(2) * T.int64(8) + eps_2_nu_2_co_2_p_2_fused % T.int64(8) + p_3 + p_4)
                                v_ci = T.axis.reduce(T.int64(512), ci_0 * T.int64(128) + ci_1 * T.int64(32) + ci_2)
                                T.reads(bgemm_local[v_eps, v_nu, v_co, v_p], data_pack_shared[v_eps, v_nu, v_ci, v_p], p1_shared[v_eps, v_nu, v_ci, v_co])
                                T.writes(bgemm_local[v_eps, v_nu, v_co, v_p])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                bgemm_local[v_eps, v_nu, v_co, v_p] = bgemm_local[v_eps, v_nu, v_co, v_p] + data_pack_shared[v_eps, v_nu, v_ci, v_p] * p1_shared[v_eps, v_nu, v_ci, v_co]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("bgemm_local"):
                            v0 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(256) * T.int64(2) + eps_2_nu_2_co_2_p_2_fused // T.int64(32) + ax0)
                            v1 = T.axis.spatial(T.int64(4), eps_1_nu_1_co_1_p_1_fused // T.int64(2) * T.int64(2) + eps_2_nu_2_co_2_p_2_fused % T.int64(32) // T.int64(16) + ax1)
                            v2 = T.axis.spatial(T.int64(512), eps_0_nu_0_co_0_p_0_fused % T.int64(256) // T.int64(2) * T.int64(4) + eps_1_nu_1_co_1_p_1_fused % T.int64(2) * T.int64(2) + eps_2_nu_2_co_2_p_2_fused % T.int64(16) // T.int64(8) + ax2)
                            v3 = T.axis.spatial(T.int64(16), eps_0_nu_0_co_0_p_0_fused % T.int64(2) * T.int64(8) + eps_2_nu_2_co_2_p_2_fused % T.int64(8) + ax3)
                            T.reads(bgemm_local[v0, v1, v2, v3])
                            T.writes(bgemm[v0, v1, v2, v3])
                            bgemm[v0, v1, v2, v3] = bgemm_local[v0, v1, v2, v3]
        for n_co_h_0_w_0_fused_0 in T.thread_binding(T.int64(256), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 16, "pragma_unroll_explicit": 1}):
            for n_co_h_0_w_0_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                    for ax2 in T.unroll(T.int64(2)):
                        for ax3 in T.unroll(T.int64(2)):
                            with T.block("inverse_init"):
                                v_co = T.axis.spatial(T.int64(512), (n_co_h_0_w_0_fused_0 * T.int64(32) + n_co_h_0_w_0_fused_1) // T.int64(16) + ax0)
                                v_p = T.axis.spatial(T.int64(16), (n_co_h_0_w_0_fused_0 * T.int64(32) + n_co_h_0_w_0_fused_1) % T.int64(16) + ax1)
                                v_vh, v_vw = T.axis.remap("SS", [ax2, ax3])
                                T.reads()
                                T.writes(inverse_local[v_co, v_p, v_vh, v_vw])
                                T.block_attr({"schedule_rule": "conv2d_nchw_winograd_inverse"})
                                inverse_local[v_co, v_p, v_vh, v_vw] = T.float32(0)
                            for ax4 in T.unroll(T.int64(4)):
                                for ax5 in T.unroll(T.int64(4)):
                                    with T.block("inverse_update"):
                                        v_co = T.axis.spatial(T.int64(512), (n_co_h_0_w_0_fused_0 * T.int64(32) + n_co_h_0_w_0_fused_1) // T.int64(16) + ax0)
                                        v_p = T.axis.spatial(T.int64(16), (n_co_h_0_w_0_fused_0 * T.int64(32) + n_co_h_0_w_0_fused_1) % T.int64(16) + ax1)
                                        v_vh, v_vw, v_r_a, v_r_b = T.axis.remap("SSRR", [ax2, ax3, ax4, ax5])
                                        T.reads(inverse_local[v_co, v_p, v_vh, v_vw], bgemm[v_r_a, v_r_b, v_co, v_p])
                                        T.writes(inverse_local[v_co, v_p, v_vh, v_vw])
                                        T.block_attr({"schedule_rule": "conv2d_nchw_winograd_inverse"})
                                        inverse_local[v_co, v_p, v_vh, v_vw] = inverse_local[v_co, v_p, v_vh, v_vw] + bgemm[v_r_a, v_r_b, v_co, v_p] * T.Select(v_r_a % T.int64(4) == T.int64(3) and v_vh % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_vh % T.int64(2) == T.int64(0), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_vh % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_vh % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_vh % T.int64(2) == T.int64(1), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_vh % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_vh % T.int64(2) == T.int64(1), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_vh % T.int64(2) == T.int64(0), T.float32(1), T.float32(0))))))))) * T.Select(v_r_b % T.int64(4) == T.int64(3) and v_vw % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_vw % T.int64(2) == T.int64(0), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_vw % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_vw % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_vw % T.int64(2) == T.int64(1), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_vw % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_vw % T.int64(2) == T.int64(1), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_vw % T.int64(2) == T.int64(0), T.float32(1), T.float32(0)))))))))
                for h_1, w_1 in T.grid(T.int64(2), T.int64(2)):
                    with T.block("conv2d_winograd"):
                        v_n = T.axis.spatial(T.int64(1), T.int64(0))
                        v_co = T.axis.spatial(T.int64(512), (n_co_h_0_w_0_fused_0 * T.int64(32) + n_co_h_0_w_0_fused_1) // T.int64(16))
                        v_h = T.axis.spatial(T.int64(7), (n_co_h_0_w_0_fused_0 * T.int64(32) + n_co_h_0_w_0_fused_1) % T.int64(16) // T.int64(4) * T.int64(2) + h_1)
                        v_w = T.axis.spatial(T.int64(7), (n_co_h_0_w_0_fused_0 * T.int64(32) + n_co_h_0_w_0_fused_1) % T.int64(4) * T.int64(2) + w_1)
                        T.where((n_co_h_0_w_0_fused_0 * T.int64(32) + n_co_h_0_w_0_fused_1) % T.int64(16) // T.int64(4) * T.int64(2) + h_1 < T.int64(7) and (n_co_h_0_w_0_fused_0 * T.int64(32) + n_co_h_0_w_0_fused_1) % T.int64(4) * T.int64(2) + w_1 < T.int64(7))
                        T.reads(inverse_local[v_co, v_n * T.int64(16) + v_h // T.int64(2) * T.int64(4) + v_w // T.int64(2), v_h % T.int64(2), v_w % T.int64(2)], p2[v_n, v_co, v_h, v_w], p3[v_n, v_co, T.int64(0), T.int64(0)], p4[v_n, v_co, T.int64(0), T.int64(0)])
                        T.writes(T_relu[v_n, v_co, v_h, v_w])
                        T_relu[v_n, v_co, v_h, v_w] = T.max((inverse_local[v_co, v_n * T.int64(16) + v_h // T.int64(2) * T.int64(4) + v_w // T.int64(2), v_h % T.int64(2), v_w % T.int64(2)] + p2[v_n, v_co, v_h, v_w]) * p3[v_n, v_co, T.int64(0), T.int64(0)] + p4[v_n, v_co, T.int64(0), T.int64(0)], T.float32(0))
b0 = sch.get_block(name="data_pack", func_name="main")
b1 = sch.get_block(name="bgemm", func_name="main")
b2 = sch.get_block(name="inverse", func_name="main")
b3 = sch.get_block(name="conv2d_winograd", func_name="main")
b4 = sch.get_block(name="T_add", func_name="main")
b5 = sch.get_block(name="T_multiply", func_name="main")
b6 = sch.get_block(name="T_add_1", func_name="main")
b7 = sch.get_block(name="T_relu", func_name="main")
b8 = sch.get_block(name="root", func_name="main")
b9, b10 = sch.get_producers(block=b2)
sch.compute_inline(block=b10)
b11, = sch.get_consumers(block=b2)
l12, l13, l14, l15 = sch.get_loops(block=b11)
l16, l17 = sch.split(loop=l14, factors=[None, 2], preserve_unit_iters=True)
l18, l19 = sch.split(loop=l15, factors=[None, 2], preserve_unit_iters=True)
sch.reorder(l16, l18, l17, l19)
sch.compute_at(block=b2, loop=l18, preserve_unit_loops=True, index=-1)
sch.set_scope(block=b2, buffer_index=0, storage_scope="local")
l20, l21, l22, l23, l24, l25, l26, l27, l28, l29 = sch.get_loops(block=b2)
sch.unroll(loop=l26)
sch.unroll(loop=l27)
sch.unroll(loop=l28)
sch.unroll(loop=l29)
b30, b31 = sch.get_producers(block=b0)
sch.compute_inline(block=b31)
b32, = sch.get_producers(block=b30)
l33, l34, l35, l36, l37, l38 = sch.get_loops(block=b0)
sch.reorder(l35, l36, l33, l34, l37, l38)
sch.unroll(loop=l33)
sch.unroll(loop=l34)
sch.unroll(loop=l37)
sch.unroll(loop=l38)
l39 = sch.fuse(l35, l36, preserve_unit_iters=True)
v40 = sch.sample_categorical(candidates=[32, 64, 128, 256, 512, 1024], probs=[0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666], decision=2)
l41, l42 = sch.split(loop=l39, factors=[None, v40], preserve_unit_iters=True)
sch.bind(loop=l41, thread_axis="blockIdx.x")
sch.bind(loop=l42, thread_axis="threadIdx.x")
b43 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b43, loop=l42, preserve_unit_loops=True, index=-1)
sch.compute_at(block=b30, loop=l42, preserve_unit_loops=True, index=-1)
sch.set_scope(block=b30, buffer_index=0, storage_scope="local")
sch.compute_inline(block=b32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l44, l45, l46, l47, l48 = sch.get_loops(block=b1)
v49, v50, v51, v52, v53 = sch.sample_perfect_tile(loop=l44, n=5, max_innermost_factor=64, decision=[2, 1, 2, 1, 1])
l54, l55, l56, l57, l58 = sch.split(loop=l44, factors=[v49, v50, v51, v52, v53], preserve_unit_iters=True)
v59, v60, v61, v62, v63 = sch.sample_perfect_tile(loop=l45, n=5, max_innermost_factor=64, decision=[1, 2, 2, 1, 1])
l64, l65, l66, l67, l68 = sch.split(loop=l45, factors=[v59, v60, v61, v62, v63], preserve_unit_iters=True)
v69, v70, v71, v72, v73 = sch.sample_perfect_tile(loop=l46, n=5, max_innermost_factor=64, decision=[128, 2, 2, 1, 1])
l74, l75, l76, l77, l78 = sch.split(loop=l46, factors=[v69, v70, v71, v72, v73], preserve_unit_iters=True)
v79, v80, v81, v82, v83 = sch.sample_perfect_tile(loop=l47, n=5, max_innermost_factor=64, decision=[2, 1, 8, 1, 1])
l84, l85, l86, l87, l88 = sch.split(loop=l47, factors=[v79, v80, v81, v82, v83], preserve_unit_iters=True)
v89, v90, v91 = sch.sample_perfect_tile(loop=l48, n=3, max_innermost_factor=64, decision=[4, 4, 32])
l92, l93, l94 = sch.split(loop=l48, factors=[v89, v90, v91], preserve_unit_iters=True)
sch.reorder(l54, l64, l74, l84, l55, l65, l75, l85, l56, l66, l76, l86, l92, l93, l57, l67, l77, l87, l94, l58, l68, l78, l88)
l95 = sch.fuse(l54, l64, l74, l84, preserve_unit_iters=True)
sch.bind(loop=l95, thread_axis="blockIdx.x")
l96 = sch.fuse(l55, l65, l75, l85, preserve_unit_iters=True)
sch.bind(loop=l96, thread_axis="vthread.x")
l97 = sch.fuse(l56, l66, l76, l86, preserve_unit_iters=True)
sch.bind(loop=l97, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b98 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b98, loop=l97, preserve_unit_loops=True, index=-1)
b99 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b99, loop=l92, preserve_unit_loops=True, index=-1)
l100, l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b99)
l108 = sch.fuse(l104, l105, l106, l107, preserve_unit_iters=True)
v109 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b99, ann_key="meta_schedule.cooperative_fetch", ann_val=v109)
b110 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b110, loop=l92, preserve_unit_loops=True, index=-1)
l111, l112, l113, l114, l115, l116, l117, l118 = sch.get_loops(block=b110)
l119 = sch.fuse(l115, l116, l117, l118, preserve_unit_iters=True)
v120 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b110, ann_key="meta_schedule.cooperative_fetch", ann_val=v120)
sch.reverse_compute_inline(block=b7)
sch.reverse_compute_inline(block=b6)
sch.reverse_compute_inline(block=b5)
sch.reverse_compute_inline(block=b4)
v121 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=1)
sch.annotate(block_or_loop=b8, ann_key="meta_schedule.unroll_explicit", ann_val=v121)
l122, l123, l124, l125, l126, l127 = sch.get_loops(block=b3)
l128 = sch.fuse(l122, l123, l124, l125, preserve_unit_iters=True)
v129 = sch.sample_categorical(candidates=[32, 64, 128, 256, 512, 1024], probs=[0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666], decision=0)
l130, l131 = sch.split(loop=l128, factors=[None, v129], preserve_unit_iters=True)
sch.bind(loop=l130, thread_axis="blockIdx.x")
sch.bind(loop=l131, thread_axis="threadIdx.x")
sch.enter_postproc()
sch.unannotate(block_or_loop=b99, ann_key="meta_schedule.cooperative_fetch")
l132, l133, l134, l135, l136 = sch.get_loops(block=b99)
l137, l138, l139 = sch.split(loop=l136, factors=[None, 64, 2], preserve_unit_iters=True)
sch.vectorize(loop=l139)
sch.bind(loop=l138, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b110, ann_key="meta_schedule.cooperative_fetch")
l140, l141, l142, l143, l144 = sch.get_loops(block=b110)
l145, l146 = sch.split(loop=l144, factors=[None, 64], preserve_unit_iters=True)
sch.bind(loop=l146, thread_axis="threadIdx.x")
b147 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b147, ann_key="meta_schedule.unroll_explicit")
b148, b149, b150, b151, b152, b153, b154, b155, b156 = sch.get_child_blocks(b147)
l157, l158, l159, l160, l161, l162 = sch.get_loops(block=b148)
l163, l164, l165, l166, l167, l168 = sch.get_loops(block=b149)
sch.annotate(block_or_loop=l163, ann_key="pragma_auto_unroll_max_step", ann_val=16)
sch.annotate(block_or_loop=l163, ann_key="pragma_unroll_explicit", ann_val=1)
l169, l170, l171, l172, l173, l174 = sch.get_loops(block=b150)
l175, l176, l177, l178, l179, l180, l181 = sch.get_loops(block=b151)
l182, l183, l184, l185, l186, l187 = sch.get_loops(block=b152)
l188, l189, l190, l191, l192, l193, l194, l195, l196, l197, l198, l199, l200, l201 = sch.get_loops(block=b153)
sch.annotate(block_or_loop=l188, ann_key="pragma_auto_unroll_max_step", ann_val=16)
sch.annotate(block_or_loop=l188, ann_key="pragma_unroll_explicit", ann_val=1)
l202, l203, l204, l205, l206, l207, l208 = sch.get_loops(block=b154)
l209, l210, l211, l212, l213, l214, l215, l216 = sch.get_loops(block=b155)
sch.annotate(block_or_loop=l209, ann_key="pragma_auto_unroll_max_step", ann_val=16)
sch.annotate(block_or_loop=l209, ann_key="pragma_unroll_explicit", ann_val=1)
l217, l218, l219, l220 = sch.get_loops(block=b156)
b221 = sch.get_block(name="data_pack", func_name="main")
l222, l223, l224, l225, l226, l227 = sch.get_loops(block=b221)
b228 = sch.decompose_reduction(block=b221, loop=l226)
b229 = sch.get_block(name="bgemm", func_name="main")
l230, l231, l232, l233, l234, l235, l236, l237, l238, l239, l240, l241, l242, l243 = sch.get_loops(block=b229)
b244 = sch.decompose_reduction(block=b229, loop=l233)
b245 = sch.get_block(name="inverse", func_name="main")
l246, l247, l248, l249, l250, l251, l252, l253 = sch.get_loops(block=b245)
b254 = sch.decompose_reduction(block=b245, loop=l252)
2023-11-11 01:34:49 [INFO] [task_scheduler.cc:121] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #131: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/home2/xiachunwei/Software/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/home2/xiachunwei/Software/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/home2/xiachunwei/Software/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/home2/xiachunwei/Software/tvm/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/home2/xiachunwei/Software/tvm/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/home2/xiachunwei/Software/tvm/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/home2/xiachunwei/Software/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/home2/xiachunwei/Software/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  4: _ZN3tvm7runtime13PackedFun
  3: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  2: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  0: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  File "/home2/xiachunwei/Software/tvm/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(512), T.int64(7), T.int64(7)), "float32"), p1: T.Buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(512)), "float32"), p2: T.Buffer((T.int64(1), T.int64(512), T.int64(7), T.int64(7)), "float32"), p3: T.Buffer((T.int64(1), T.int64(512), T.int64(1), T.int64(1)), "float32"), p4: T.Buffer((T.int64(1), T.int64(512), T.int64(1), T.int64(1)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(512), T.int64(7), T.int64(7)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        input_tile_local = T.alloc_buffer((T.int64(512), T.int64(16), T.int64(4), T.int64(4)), scope="local")
        data_pack = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(16)))
        bgemm = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(16)))
        inverse_local = T.alloc_buffer((T.int64(512), T.int64(16), T.int64(2), T.int64(2)), scope="local")
        data_pack_local = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(16)), scope="local")
        bgemm_local = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(16)), scope="local")
        data_pack_shared = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(16)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(512)), scope="shared")
        for ci_p_fused_0 in T.thread_binding(T.int64(64), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 512, "pragma_unroll_explicit": 1}):
            for ci_p_fused_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(1), T.int64(4), T.int64(4)):
                    with T.block("input_tile"):
                        v_ci = T.axis.spatial(T.int64(512), ci_p_fused_0 * T.int64(8) + ci_p_fused_1 // T.int64(16) + ax0)
                        v_p = T.axis.spatial(T.int64(16), ci_p_fused_1 % T.int64(16) + ax1)
                        v_eps, v_nu = T.axis.remap("SS", [ax2, ax3])
                        T.reads(p0[v_p // T.int64(16), v_ci, v_p % T.int64(16) // T.int64(4) * T.int64(2) + v_eps - T.int64(1), v_p % T.int64(4) * T.int64(2) + v_nu - T.int64(1)])
                        T.writes(input_tile_local[v_ci, v_p, v_eps, v_nu])
                        T.block_attr({"schedule_rule": "None"})
                        input_tile_local[v_ci, v_p, v_eps, v_nu] = T.if_then_else(T.int64(1) <= v_p % T.int64(16) // T.int64(4) * T.int64(2) + v_eps and v_p % T.int64(16) // T.int64(4) * T.int64(2) + v_eps < T.int64(8) and T.int64(1) <= v_p % T.int64(4) * T.int64(2) + v_nu and v_p % T.int64(4) * T.int64(2) + v_nu < T.int64(8), p0[v_p // T.int64(16), v_ci, v_p % T.int64(16) // T.int64(4) * T.int64(2) + v_eps - T.int64(1), v_p % T.int64(4) * T.int64(2) + v_nu - T.int64(1)], T.float32(0))
                for eps in T.unroll(T.int64(4)):
                    for nu in T.unroll(T.int64(4)):
                        with T.block("data_pack_init"):
                            v_eps, v_nu = T.axis.remap("SS", [eps, nu])
                            v_ci = T.axis.spatial(T.int64(512), (ci_p_fused_0 * T.int64(128) + ci_p_fused_1) // T.int64(16))
                            v_p = T.axis.spatial(T.int64(16), (ci_p_fused_0 * T.int64(128) + ci_p_fused_1) % T.int64(16))
                            T.reads()
                            T.writes(data_pack_local[v_eps, v_nu, v_ci, v_p])
                            T.block_attr({"schedule_rule": "conv2d_nchw_winograd_data_pack"})
                            data_pack_local[v_eps, v_nu, v_ci, v_p] = T.float32(0)
                        for r_a in T.unroll(T.int64(4)):
                            for r_b in T.unroll(T.int64(4)):
                                with T.block("data_pack_update"):
                                    v_eps, v_nu = T.axis.remap("SS", [eps, nu])
                                    v_ci = T.axis.spatial(T.int64(512), (ci_p_fused_0 * T.int64(128) + ci_p_fused_1) // T.int64(16))
                                    v_p = T.axis.spatial(T.int64(16), (ci_p_fused_0 * T.int64(128) + ci_p_fused_1) % T.int64(16))
                                    v_r_a, v_r_b = T.axis.remap("RR", [r_a, r_b])
                                    T.reads(data_pack_local[v_eps, v_nu, v_ci, v_p], input_tile_local[v_ci, v_p, v_r_a, v_r_b])
                                    T.writes(data_pack_local[v_eps, v_nu, v_ci, v_p])
                                    T.block_attr({"schedule_rule": "conv2d_nchw_winograd_data_pack"})
                                    data_pack_local[v_eps, v_nu, v_ci, v_p] = data_pack_local[v_eps, v_nu, v_ci, v_p] + input_tile_local[v_ci, v_p, v_r_a, v_r_b] * T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(3), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(1), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(0), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(3), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(1), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(0), T.float32(1), T.float32(0))))))))))))))))) * T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(3), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(1), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(0), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(3), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(1), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(0), T.float32(1), T.float32(0)))))))))))))))))
                for ax0, ax1, ax2, ax3 in T.grid(T.int64(4), T.int64(4), T.int64(1), T.int64(1)):
                    with T.block("data_pack_local"):
                        v0, v1 = T.axis.remap("SS", [ax0, ax1])
                        v2 = T.axis.spatial(T.int64(512), ci_p_fused_0 * T.int64(8) + ci_p_fused_1 // T.int64(16) + ax2)
                        v3 = T.axis.spatial(T.int64(16), ci_p_fused_1 % T.int64(16) + ax3)
                        T.reads(data_pack_local[v0, v1, v2, v3])
                        T.writes(data_pack[v0, v1, v2, v3])
                        data_pack[v0, v1, v2, v3] = data_pack_local[v0, v1, v2, v3]
        for eps_0_nu_0_co_0_p_0_fused in T.thread_binding(T.int64(128), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 512, "pragma_unroll_explicit": 1}):
            for eps_1_nu_1_co_1_p_1_fused in T.thread_binding(T.int64(4), thread="vthread.x"):
                for eps_2_nu_2_co_2_p_2_fused in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                    for eps_3_init, nu_3_init, co_3_init, p_3_init, eps_4_init, nu_4_init, co_4_init, p_4_init in T.grid(T.int64(1), T.int64(1), T.int64(2), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("bgemm_init"):
                            v_eps = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(32) + eps_3_init + eps_4_init)
                            v_nu = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused % T.int64(32) // T.int64(16) * T.int64(2) + eps_2_nu_2_co_2_p_2_fused // T.int64(64) + nu_3_init + nu_4_init)
                            v_co = T.axis.spatial(T.int64(512), eps_0_nu_0_co_0_p_0_fused % T.int64(16) * T.int64(32) + eps_1_nu_1_co_1_p_1_fused // T.int64(2) * T.int64(16) + eps_2_nu_2_co_2_p_2_fused % T.int64(64) // T.int64(8) * T.int64(2) + co_3_init + co_4_init)
                            v_p = T.axis.spatial(T.int64(16), eps_1_nu_1_co_1_p_1_fused % T.int64(2) * T.int64(8) + eps_2_nu_2_co_2_p_2_fused % T.int64(8) + p_3_init + p_4_init)
                            T.reads()
                            T.writes(bgemm_local[v_eps, v_nu, v_co, v_p])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            bgemm_local[v_eps, v_nu, v_co, v_p] = T.float32(0)
                    for ci_0 in range(T.int64(32)):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(4)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                                with T.block("data_pack_shared"):
                                    v0 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(32))
                                    v1 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused % T.int64(32) // T.int64(16) * T.int64(2) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1) // T.int64(256))
                                    v2 = T.axis.spatial(T.int64(512), ci_0 * T.int64(16) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1) % T.int64(256) // T.int64(16))
                                    v3 = T.axis.spatial(T.int64(16), (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1) % T.int64(16))
                                    T.reads(data_pack[v0, v1, v2, v3])
                                    T.writes(data_pack_shared[v0, v1, v2, v3])
                                    data_pack_shared[v0, v1, v2, v3] = data_pack[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(8)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                                with T.block("p1_shared"):
                                    v0 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(32))
                                    v1 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused % T.int64(32) // T.int64(16) * T.int64(2) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1) // T.int64(512))
                                    v2 = T.axis.spatial(T.int64(512), ci_0 * T.int64(16) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1) % T.int64(512) // T.int64(32))
                                    v3 = T.axis.spatial(T.int64(512), eps_0_nu_0_co_0_p_0_fused % T.int64(16) * T.int64(32) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1) % T.int64(32))
                                    T.reads(p1[v0, v1, v2, v3])
                                    T.writes(p1_shared[v0, v1, v2, v3])
                                    p1_shared[v0, v1, v2, v3] = p1[v0, v1, v2, v3]
                        for ci_1, eps_3, nu_3, co_3, p_3, ci_2, eps_4, nu_4, co_4, p_4 in T.grid(T.int64(2), T.int64(1), T.int64(1), T.int64(2), T.int64(1), T.int64(8), T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                            with T.block("bgemm_update"):
                                v_eps = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(32) + eps_3 + eps_4)
                                v_nu = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused % T.int64(32) // T.int64(16) * T.int64(2) + eps_2_nu_2_co_2_p_2_fused // T.int64(64) + nu_3 + nu_4)
                                v_co = T.axis.spatial(T.int64(512), eps_0_nu_0_co_0_p_0_fused % T.int64(16) * T.int64(32) + eps_1_nu_1_co_1_p_1_fused // T.int64(2) * T.int64(16) + eps_2_nu_2_co_2_p_2_fused % T.int64(64) // T.int64(8) * T.int64(2) + co_3 + co_4)
                                v_p = T.axis.spatial(T.int64(16), eps_1_nu_1_co_1_p_1_fused % T.int64(2) * T.int64(8) + eps_2_nu_2_co_2_p_2_fused % T.int64(8) + p_3 + p_4)
                                v_ci = T.axis.reduce(T.int64(512), ci_0 * T.int64(16) + ci_1 * T.int64(8) + ci_2)
                                T.reads(bgemm_local[v_eps, v_nu, v_co, v_p], data_pack_shared[v_eps, v_nu, v_ci, v_p], p1_shared[v_eps, v_nu, v_ci, v_co])
                                T.writes(bgemm_local[v_eps, v_nu, v_co, v_p])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                bgemm_local[v_eps, v_nu, v_co, v_p] = bgemm_local[v_eps, v_nu, v_co, v_p] + data_pack_shared[v_eps, v_nu, v_ci, v_p] * p1_shared[v_eps, v_nu, v_ci, v_co]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(1), T.int64(2), T.int64(1)):
                        with T.block("bgemm_local"):
                            v0 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(32) + ax0)
                            v1 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused % T.int64(32) // T.int64(16) * T.int64(2) + eps_2_nu_2_co_2_p_2_fused // T.int64(64) + ax1)
                            v2 = T.axis.spatial(T.int64(512), eps_0_nu_0_co_0_p_0_fused % T.int64(16) * T.int64(32) + eps_1_nu_1_co_1_p_1_fused // T.int64(2) * T.int64(16) + eps_2_nu_2_co_2_p_2_fused % T.int64(64) // T.int64(8) * T.int64(2) + ax2)
                            v3 = T.axis.spatial(T.int64(16), eps_1_nu_1_co_1_p_1_fused % T.int64(2) * T.int64(8) + eps_2_nu_2_co_2_p_2_fused % T.int64(8) + ax3)
                            T.reads(bgemm_local[v0, v1, v2, v3])
                            T.writes(bgemm[v0, v1, v2, v3])
                            bgemm[v0, v1, v2, v3] = bgemm_local[v0, v1, v2, v3]
        for n_co_h_0_w_0_fused_0 in T.thread_binding(T.int64(16), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 512, "pragma_unroll_explicit": 1}):
            for n_co_h_0_w_0_fused_1 in T.thread_binding(T.int64(512), thread="threadIdx.x"):
                for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                    for ax2 in T.unroll(T.int64(2)):
                        for ax3 in T.unroll(T.int64(2)):
                            with T.block("inverse_init"):
                                v_co = T.axis.spatial(T.int64(512), (n_co_h_0_w_0_fused_0 * T.int64(512) + n_co_h_0_w_0_fused_1) // T.int64(16) + ax0)
                                v_p = T.axis.spatial(T.int64(16), (n_co_h_0_w_0_fused_0 * T.int64(512) + n_co_h_0_w_0_fused_1) % T.int64(16) + ax1)
                                v_vh, v_vw = T.axis.remap("SS", [ax2, ax3])
                                T.reads()
                                T.writes(inverse_local[v_co, v_p, v_vh, v_vw])
                                T.block_attr({"schedule_rule": "conv2d_nchw_winograd_inverse"})
                                inverse_local[v_co, v_p, v_vh, v_vw] = T.float32(0)
                            for ax4 in T.unroll(T.int64(4)):
                                for ax5 in T.unroll(T.int64(4)):
                                    with T.block("inverse_update"):
                                        v_co = T.axis.spatial(T.int64(512), (n_co_h_0_w_0_fused_0 * T.int64(512) + n_co_h_0_w_0_fused_1) // T.int64(16) + ax0)
                                        v_p = T.axis.spatial(T.int64(16), (n_co_h_0_w_0_fused_0 * T.int64(512) + n_co_h_0_w_0_fused_1) % T.int64(16) + ax1)
                                        v_vh, v_vw, v_r_a, v_r_b = T.axis.remap("SSRR", [ax2, ax3, ax4, ax5])
                                        T.reads(inverse_local[v_co, v_p, v_vh, v_vw], bgemm[v_r_a, v_r_b, v_co, v_p])
                                        T.writes(inverse_local[v_co, v_p, v_vh, v_vw])
                                        T.block_attr({"schedule_rule": "conv2d_nchw_winograd_inverse"})
                                        inverse_local[v_co, v_p, v_vh, v_vw] = inverse_local[v_co, v_p, v_vh, v_vw] + bgemm[v_r_a, v_r_b, v_co, v_p] * T.Select(v_r_a % T.int64(4) == T.int64(3) and v_vh % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_vh % T.int64(2) == T.int64(0), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_vh % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_vh % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_vh % T.int64(2) == T.int64(1), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_vh % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_vh % T.int64(2) == T.int64(1), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_vh % T.int64(2) == T.int64(0), T.float32(1), T.float32(0))))))))) * T.Select(v_r_b % T.int64(4) == T.int64(3) and v_vw % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_vw % T.int64(2) == T.int64(0), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_vw % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_vw % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_vw % T.int64(2) == T.int64(1), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_vw % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_vw % T.int64(2) == T.int64(1), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_vw % T.int64(2) == T.int64(0), T.float32(1), T.float32(0)))))))))
                for h_1, w_1 in T.grid(T.int64(2), T.int64(2)):
                    with T.block("conv2d_winograd"):
                        v_n = T.axis.spatial(T.int64(1), T.int64(0))
                        v_co = T.axis.spatial(T.int64(512), (n_co_h_0_w_0_fused_0 * T.int64(512) + n_co_h_0_w_0_fused_1) // T.int64(16))
                        v_h = T.axis.spatial(T.int64(7), (n_co_h_0_w_0_fused_0 * T.int64(512) + n_co_h_0_w_0_fused_1) % T.int64(16) // T.int64(4) * T.int64(2) + h_1)
                        v_w = T.axis.spatial(T.int64(7), (n_co_h_0_w_0_fused_0 * T.int64(512) + n_co_h_0_w_0_fused_1) % T.int64(4) * T.int64(2) + w_1)
                        T.where((n_co_h_0_w_0_fused_0 * T.int64(512) + n_co_h_0_w_0_fused_1) % T.int64(16) // T.int64(4) * T.int64(2) + h_1 < T.int64(7) and (n_co_h_0_w_0_fused_0 * T.int64(512) + n_co_h_0_w_0_fused_1) % T.int64(4) * T.int64(2) + w_1 < T.int64(7))
                        T.reads(inverse_local[v_co, v_n * T.int64(16) + v_h // T.int64(2) * T.int64(4) + v_w // T.int64(2), v_h % T.int64(2), v_w % T.int64(2)], p2[v_n, v_co, v_h, v_w], p3[v_n, v_co, T.int64(0), T.int64(0)], p4[v_n, v_co, T.int64(0), T.int64(0)])
                        T.writes(T_relu[v_n, v_co, v_h, v_w])
                        T_relu[v_n, v_co, v_h, v_w] = T.max((inverse_local[v_co, v_n * T.int64(16) + v_h // T.int64(2) * T.int64(4) + v_w // T.int64(2), v_h % T.int64(2), v_w % T.int64(2)] + p2[v_n, v_co, v_h, v_w]) * p3[v_n, v_co, T.int64(0), T.int64(0)] + p4[v_n, v_co, T.int64(0), T.int64(0)], T.float32(0))
b0 = sch.get_block(name="data_pack", func_name="main")
b1 = sch.get_block(name="bgemm", func_name="main")
b2 = sch.get_block(name="inverse", func_name="main")
b3 = sch.get_block(name="conv2d_winograd", func_name="main")
b4 = sch.get_block(name="T_add", func_name="main")
b5 = sch.get_block(name="T_multiply", func_name="main")
b6 = sch.get_block(name="T_add_1", func_name="main")
b7 = sch.get_block(name="T_relu", func_name="main")
b8 = sch.get_block(name="root", func_name="main")
b9, b10 = sch.get_producers(block=b2)
sch.compute_inline(block=b10)
b11, = sch.get_consumers(block=b2)
l12, l13, l14, l15 = sch.get_loops(block=b11)
l16, l17 = sch.split(loop=l14, factors=[None, 2], preserve_unit_iters=True)
l18, l19 = sch.split(loop=l15, factors=[None, 2], preserve_unit_iters=True)
sch.reorder(l16, l18, l17, l19)
sch.compute_at(block=b2, loop=l18, preserve_unit_loops=True, index=-1)
sch.set_scope(block=b2, buffer_index=0, storage_scope="local")
l20, l21, l22, l23, l24, l25, l26, l27, l28, l29 = sch.get_loops(block=b2)
sch.unroll(loop=l26)
sch.unroll(loop=l27)
sch.unroll(loop=l28)
sch.unroll(loop=l29)
b30, b31 = sch.get_producers(block=b0)
sch.compute_inline(block=b31)
b32, = sch.get_producers(block=b30)
l33, l34, l35, l36, l37, l38 = sch.get_loops(block=b0)
sch.reorder(l35, l36, l33, l34, l37, l38)
sch.unroll(loop=l33)
sch.unroll(loop=l34)
sch.unroll(loop=l37)
sch.unroll(loop=l38)
l39 = sch.fuse(l35, l36, preserve_unit_iters=True)
v40 = sch.sample_categorical(candidates=[32, 64, 128, 256, 512, 1024], probs=[0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666], decision=2)
l41, l42 = sch.split(loop=l39, factors=[None, v40], preserve_unit_iters=True)
sch.bind(loop=l41, thread_axis="blockIdx.x")
sch.bind(loop=l42, thread_axis="threadIdx.x")
b43 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b43, loop=l42, preserve_unit_loops=True, index=-1)
sch.compute_at(block=b30, loop=l42, preserve_unit_loops=True, index=-1)
sch.set_scope(block=b30, buffer_index=0, storage_scope="local")
sch.compute_inline(block=b32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l44, l45, l46, l47, l48 = sch.get_loops(block=b1)
v49, v50, v51, v52, v53 = sch.sample_perfect_tile(loop=l44, n=5, max_innermost_factor=64, decision=[4, 1, 1, 1, 1])
l54, l55, l56, l57, l58 = sch.split(loop=l44, factors=[v49, v50, v51, v52, v53], preserve_unit_iters=True)
v59, v60, v61, v62, v63 = sch.sample_perfect_tile(loop=l45, n=5, max_innermost_factor=64, decision=[2, 1, 2, 1, 1])
l64, l65, l66, l67, l68 = sch.split(loop=l45, factors=[v59, v60, v61, v62, v63], preserve_unit_iters=True)
v69, v70, v71, v72, v73 = sch.sample_perfect_tile(loop=l46, n=5, max_innermost_factor=64, decision=[16, 2, 8, 2, 1])
l74, l75, l76, l77, l78 = sch.split(loop=l46, factors=[v69, v70, v71, v72, v73], preserve_unit_iters=True)
v79, v80, v81, v82, v83 = sch.sample_perfect_tile(loop=l47, n=5, max_innermost_factor=64, decision=[1, 2, 8, 1, 1])
l84, l85, l86, l87, l88 = sch.split(loop=l47, factors=[v79, v80, v81, v82, v83], preserve_unit_iters=True)
v89, v90, v91 = sch.sample_perfect_tile(loop=l48, n=3, max_innermost_factor=64, decision=[32, 2, 8])
l92, l93, l94 = sch.split(loop=l48, factors=[v89, v90, v91], preserve_unit_iters=True)
sch.reorder(l54, l64, l74, l84, l55, l65, l75, l85, l56, l66, l76, l86, l92, l93, l57, l67, l77, l87, l94, l58, l68, l78, l88)
l95 = sch.fuse(l54, l64, l74, l84, preserve_unit_iters=True)
sch.bind(loop=l95, thread_axis="blockIdx.x")
l96 = sch.fuse(l55, l65, l75, l85, preserve_unit_iters=True)
sch.bind(loop=l96, thread_axis="vthread.x")
l97 = sch.fuse(l56, l66, l76, l86, preserve_unit_iters=True)
sch.bind(loop=l97, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b98 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b98, loop=l97, preserve_unit_loops=True, index=-1)
b99 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b99, loop=l92, preserve_unit_loops=True, index=-1)
l100, l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b99)
l108 = sch.fuse(l104, l105, l106, l107, preserve_unit_iters=True)
v109 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b99, ann_key="meta_schedule.cooperative_fetch", ann_val=v109)
b110 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b110, loop=l92, preserve_unit_loops=True, index=-1)
l111, l112, l113, l114, l115, l116, l117, l118 = sch.get_loops(block=b110)
l119 = sch.fuse(l115, l116, l117, l118, preserve_unit_iters=True)
v120 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b110, ann_key="meta_schedule.cooperative_fetch", ann_val=v120)
sch.reverse_compute_inline(block=b7)
sch.reverse_compute_inline(block=b6)
sch.reverse_compute_inline(block=b5)
sch.reverse_compute_inline(block=b4)
v121 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b8, ann_key="meta_schedule.unroll_explicit", ann_val=v121)
l122, l123, l124, l125, l126, l127 = sch.get_loops(block=b3)
l128 = sch.fuse(l122, l123, l124, l125, preserve_unit_iters=True)
v129 = sch.sample_categorical(candidates=[32, 64, 128, 256, 512, 1024], probs=[0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666], decision=4)
l130, l131 = sch.split(loop=l128, factors=[None, v129], preserve_unit_iters=True)
sch.bind(loop=l130, thread_axis="blockIdx.x")
sch.bind(loop=l131, thread_axis="threadIdx.x")
sch.enter_postproc()
sch.unannotate(block_or_loop=b99, ann_key="meta_schedule.cooperative_fetch")
l132, l133, l134, l135, l136 = sch.get_loops(block=b99)
l137, l138 = sch.split(loop=l136, factors=[None, 128], preserve_unit_iters=True)
sch.bind(loop=l138, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b110, ann_key="meta_schedule.cooperative_fetch")
l139, l140, l141, l142, l143 = sch.get_loops(block=b110)
l144, l145 = sch.split(loop=l143, factors=[None, 128], preserve_unit_iters=True)
sch.bind(loop=l145, thread_axis="threadIdx.x")
b146 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b146, ann_key="meta_schedule.unroll_explicit")
b147, b148, b149, b150, b151, b152, b153, b154, b155 = sch.get_child_blocks(b146)
l156, l157, l158, l159, l160, l161 = sch.get_loops(block=b147)
l162, l163, l164, l165, l166, l167 = sch.get_loops(block=b148)
sch.annotate(block_or_loop=l162, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l162, ann_key="pragma_unroll_explicit", ann_val=1)
l168, l169, l170, l171, l172, l173 = sch.get_loops(block=b149)
l174, l175, l176, l177, l178, l179 = sch.get_loops(block=b150)
l180, l181, l182, l183, l184, l185 = sch.get_loops(block=b151)
l186, l187, l188, l189, l190, l191, l192, l193, l194, l195, l196, l197, l198, l199 = sch.get_loops(block=b152)
sch.annotate(block_or_loop=l186, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l186, ann_key="pragma_unroll_explicit", ann_val=1)
l200, l201, l202, l203, l204, l205, l206 = sch.get_loops(block=b153)
l207, l208, l209, l210, l211, l212, l213, l214 = sch.get_loops(block=b154)
sch.annotate(block_or_loop=l207, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l207, ann_key="pragma_unroll_explicit", ann_val=1)
l215, l216, l217, l218 = sch.get_loops(block=b155)
b219 = sch.get_block(name="data_pack", func_name="main")
l220, l221, l222, l223, l224, l225 = sch.get_loops(block=b219)
b226 = sch.decompose_reduction(block=b219, loop=l224)
b227 = sch.get_block(name="bgemm", func_name="main")
l228, l229, l230, l231, l232, l233, l234, l235, l236, l237, l238, l239, l240, l241 = sch.get_loops(block=b227)
b242 = sch.decompose_reduction(block=b227, loop=l231)
b243 = sch.get_block(name="inverse", func_name="main")
l244, l245, l246, l247, l248, l249, l250, l251 = sch.get_loops(block=b243)
b252 = sch.decompose_reduction(block=b243, loop=l250)
2023-11-11 01:34:49 [INFO] [task_scheduler.cc:121] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #132: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/home2/xiachunwei/Software/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/home2/xiachunwei/Software/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/home2/xiachunwei/Software/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/home2/xiachunwei/Software/tvm/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/home2/xiachunwei/Software/tvm/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/home2/xiachunwei/Software/tvm/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/home2/xiachunwei/Software/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/home2/xiachunwei/Software/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  4: _ZN3tvm7runtime13PackedFun
  3: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  2: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  0: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  File "/home2/xiachunwei/Software/tvm/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(512), T.int64(7), T.int64(7)), "float32"), p1: T.Buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(512)), "float32"), p2: T.Buffer((T.int64(1), T.int64(512), T.int64(7), T.int64(7)), "float32"), p3: T.Buffer((T.int64(1), T.int64(512), T.int64(1), T.int64(1)), "float32"), p4: T.Buffer((T.int64(1), T.int64(512), T.int64(1), T.int64(1)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(512), T.int64(7), T.int64(7)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        input_tile_local = T.alloc_buffer((T.int64(512), T.int64(16), T.int64(4), T.int64(4)), scope="local")
        data_pack = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(16)))
        bgemm = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(16)))
        inverse_local = T.alloc_buffer((T.int64(512), T.int64(16), T.int64(2), T.int64(2)), scope="local")
        data_pack_local = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(16)), scope="local")
        bgemm_local = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(16)), scope="local")
        data_pack_shared = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(16)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(512)), scope="shared")
        for ci_p_fused_0 in T.thread_binding(T.int64(256), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 16, "pragma_unroll_explicit": 1}):
            for ci_p_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(1), T.int64(4), T.int64(4)):
                    with T.block("input_tile"):
                        v_ci = T.axis.spatial(T.int64(512), ci_p_fused_0 * T.int64(2) + ci_p_fused_1 // T.int64(16) + ax0)
                        v_p = T.axis.spatial(T.int64(16), ci_p_fused_1 % T.int64(16) + ax1)
                        v_eps, v_nu = T.axis.remap("SS", [ax2, ax3])
                        T.reads(p0[v_p // T.int64(16), v_ci, v_p % T.int64(16) // T.int64(4) * T.int64(2) + v_eps - T.int64(1), v_p % T.int64(4) * T.int64(2) + v_nu - T.int64(1)])
                        T.writes(input_tile_local[v_ci, v_p, v_eps, v_nu])
                        T.block_attr({"schedule_rule": "None"})
                        input_tile_local[v_ci, v_p, v_eps, v_nu] = T.if_then_else(T.int64(1) <= v_p % T.int64(16) // T.int64(4) * T.int64(2) + v_eps and v_p % T.int64(16) // T.int64(4) * T.int64(2) + v_eps < T.int64(8) and T.int64(1) <= v_p % T.int64(4) * T.int64(2) + v_nu and v_p % T.int64(4) * T.int64(2) + v_nu < T.int64(8), p0[v_p // T.int64(16), v_ci, v_p % T.int64(16) // T.int64(4) * T.int64(2) + v_eps - T.int64(1), v_p % T.int64(4) * T.int64(2) + v_nu - T.int64(1)], T.float32(0))
                for eps in T.unroll(T.int64(4)):
                    for nu in T.unroll(T.int64(4)):
                        with T.block("data_pack_init"):
                            v_eps, v_nu = T.axis.remap("SS", [eps, nu])
                            v_ci = T.axis.spatial(T.int64(512), (ci_p_fused_0 * T.int64(32) + ci_p_fused_1) // T.int64(16))
                            v_p = T.axis.spatial(T.int64(16), (ci_p_fused_0 * T.int64(32) + ci_p_fused_1) % T.int64(16))
                            T.reads()
                            T.writes(data_pack_local[v_eps, v_nu, v_ci, v_p])
                            T.block_attr({"schedule_rule": "conv2d_nchw_winograd_data_pack"})
                            data_pack_local[v_eps, v_nu, v_ci, v_p] = T.float32(0)
                        for r_a in T.unroll(T.int64(4)):
                            for r_b in T.unroll(T.int64(4)):
                                with T.block("data_pack_update"):
                                    v_eps, v_nu = T.axis.remap("SS", [eps, nu])
                                    v_ci = T.axis.spatial(T.int64(512), (ci_p_fused_0 * T.int64(32) + ci_p_fused_1) // T.int64(16))
                                    v_p = T.axis.spatial(T.int64(16), (ci_p_fused_0 * T.int64(32) + ci_p_fused_1) % T.int64(16))
                                    v_r_a, v_r_b = T.axis.remap("RR", [r_a, r_b])
                                    T.reads(data_pack_local[v_eps, v_nu, v_ci, v_p], input_tile_local[v_ci, v_p, v_r_a, v_r_b])
                                    T.writes(data_pack_local[v_eps, v_nu, v_ci, v_p])
                                    T.block_attr({"schedule_rule": "conv2d_nchw_winograd_data_pack"})
                                    data_pack_local[v_eps, v_nu, v_ci, v_p] = data_pack_local[v_eps, v_nu, v_ci, v_p] + input_tile_local[v_ci, v_p, v_r_a, v_r_b] * T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(3), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(1), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(0), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(3), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(1), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(0), T.float32(1), T.float32(0))))))))))))))))) * T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(3), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(1), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(0), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(3), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(1), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(0), T.float32(1), T.float32(0)))))))))))))))))
                for ax0, ax1, ax2, ax3 in T.grid(T.int64(4), T.int64(4), T.int64(1), T.int64(1)):
                    with T.block("data_pack_local"):
                        v0, v1 = T.axis.remap("SS", [ax0, ax1])
                        v2 = T.axis.spatial(T.int64(512), ci_p_fused_0 * T.int64(2) + ci_p_fused_1 // T.int64(16) + ax2)
                        v3 = T.axis.spatial(T.int64(16), ci_p_fused_1 % T.int64(16) + ax3)
                        T.reads(data_pack_local[v0, v1, v2, v3])
                        T.writes(data_pack[v0, v1, v2, v3])
                        data_pack[v0, v1, v2, v3] = data_pack_local[v0, v1, v2, v3]
        for eps_0_nu_0_co_0_p_0_fused in T.thread_binding(T.int64(256), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 16, "pragma_unroll_explicit": 1}):
            for eps_1_nu_1_co_1_p_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for eps_2_nu_2_co_2_p_2_fused in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                    for eps_3_init, nu_3_init, co_3_init, p_3_init, eps_4_init, nu_4_init, co_4_init, p_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(2)):
                        with T.block("bgemm_init"):
                            v_eps = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(64) + eps_3_init + eps_4_init)
                            v_nu = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused % T.int64(64) // T.int64(32) * T.int64(2) + eps_2_nu_2_co_2_p_2_fused // T.int64(64) + nu_3_init + nu_4_init)
                            v_co = T.axis.spatial(T.int64(512), eps_0_nu_0_co_0_p_0_fused % T.int64(32) * T.int64(16) + eps_2_nu_2_co_2_p_2_fused % T.int64(64) // T.int64(8) * T.int64(2) + co_3_init * T.int64(2) + co_4_init)
                            v_p = T.axis.spatial(T.int64(16), eps_2_nu_2_co_2_p_2_fused % T.int64(8) * T.int64(2) + p_3_init * T.int64(2) + p_4_init)
                            T.reads()
                            T.writes(bgemm_local[v_eps, v_nu, v_co, v_p])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            bgemm_local[v_eps, v_nu, v_co, v_p] = T.float32(0)
                    for ci_0 in range(T.int64(32)):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(4)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                                with T.block("data_pack_shared"):
                                    v0 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(64))
                                    v1 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused % T.int64(64) // T.int64(32) * T.int64(2) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1) // T.int64(256))
                                    v2 = T.axis.spatial(T.int64(512), ci_0 * T.int64(16) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1) % T.int64(256) // T.int64(16))
                                    v3 = T.axis.spatial(T.int64(16), (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1) % T.int64(16))
                                    T.reads(data_pack[v0, v1, v2, v3])
                                    T.writes(data_pack_shared[v0, v1, v2, v3])
                                    data_pack_shared[v0, v1, v2, v3] = data_pack[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(1)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("p1_shared"):
                                        v0 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(64))
                                        v1 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused % T.int64(64) // T.int64(32) * T.int64(2) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(512) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) // T.int64(256))
                                        v2 = T.axis.spatial(T.int64(512), ci_0 * T.int64(16) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(512) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) % T.int64(256) // T.int64(16))
                                        v3 = T.axis.spatial(T.int64(512), eps_0_nu_0_co_0_p_0_fused % T.int64(32) * T.int64(16) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(512) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) % T.int64(16))
                                        T.reads(p1[v0, v1, v2, v3])
                                        T.writes(p1_shared[v0, v1, v2, v3])
                                        p1_shared[v0, v1, v2, v3] = p1[v0, v1, v2, v3]
                        for ci_1, eps_3, nu_3, co_3, p_3, ci_2, eps_4, nu_4, co_4, p_4 in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(16), T.int64(1), T.int64(1), T.int64(2), T.int64(2)):
                            with T.block("bgemm_update"):
                                v_eps = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(64) + eps_3 + eps_4)
                                v_nu = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused % T.int64(64) // T.int64(32) * T.int64(2) + eps_2_nu_2_co_2_p_2_fused // T.int64(64) + nu_3 + nu_4)
                                v_co = T.axis.spatial(T.int64(512), eps_0_nu_0_co_0_p_0_fused % T.int64(32) * T.int64(16) + eps_2_nu_2_co_2_p_2_fused % T.int64(64) // T.int64(8) * T.int64(2) + co_3 * T.int64(2) + co_4)
                                v_p = T.axis.spatial(T.int64(16), eps_2_nu_2_co_2_p_2_fused % T.int64(8) * T.int64(2) + p_3 * T.int64(2) + p_4)
                                v_ci = T.axis.reduce(T.int64(512), ci_0 * T.int64(16) + ci_1 * T.int64(16) + ci_2)
                                T.reads(bgemm_local[v_eps, v_nu, v_co, v_p], data_pack_shared[v_eps, v_nu, v_ci, v_p], p1_shared[v_eps, v_nu, v_ci, v_co])
                                T.writes(bgemm_local[v_eps, v_nu, v_co, v_p])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                bgemm_local[v_eps, v_nu, v_co, v_p] = bgemm_local[v_eps, v_nu, v_co, v_p] + data_pack_shared[v_eps, v_nu, v_ci, v_p] * p1_shared[v_eps, v_nu, v_ci, v_co]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(1), T.int64(2), T.int64(2)):
                        with T.block("bgemm_local"):
                            v0 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(64) + ax0)
                            v1 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused % T.int64(64) // T.int64(32) * T.int64(2) + eps_2_nu_2_co_2_p_2_fused // T.int64(64) + ax1)
                            v2 = T.axis.spatial(T.int64(512), eps_0_nu_0_co_0_p_0_fused % T.int64(32) * T.int64(16) + eps_2_nu_2_co_2_p_2_fused % T.int64(64) // T.int64(8) * T.int64(2) + ax2)
                            v3 = T.axis.spatial(T.int64(16), eps_2_nu_2_co_2_p_2_fused % T.int64(8) * T.int64(2) + ax3)
                            T.reads(bgemm_local[v0, v1, v2, v3])
                            T.writes(bgemm[v0, v1, v2, v3])
                            bgemm[v0, v1, v2, v3] = bgemm_local[v0, v1, v2, v3]
        for n_co_h_0_w_0_fused_0 in T.thread_binding(T.int64(32), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 16, "pragma_unroll_explicit": 1}):
            for n_co_h_0_w_0_fused_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                    for ax2 in T.unroll(T.int64(2)):
                        for ax3 in T.unroll(T.int64(2)):
                            with T.block("inverse_init"):
                                v_co = T.axis.spatial(T.int64(512), (n_co_h_0_w_0_fused_0 * T.int64(256) + n_co_h_0_w_0_fused_1) // T.int64(16) + ax0)
                                v_p = T.axis.spatial(T.int64(16), (n_co_h_0_w_0_fused_0 * T.int64(256) + n_co_h_0_w_0_fused_1) % T.int64(16) + ax1)
                                v_vh, v_vw = T.axis.remap("SS", [ax2, ax3])
                                T.reads()
                                T.writes(inverse_local[v_co, v_p, v_vh, v_vw])
                                T.block_attr({"schedule_rule": "conv2d_nchw_winograd_inverse"})
                                inverse_local[v_co, v_p, v_vh, v_vw] = T.float32(0)
                            for ax4 in T.unroll(T.int64(4)):
                                for ax5 in T.unroll(T.int64(4)):
                                    with T.block("inverse_update"):
                                        v_co = T.axis.spatial(T.int64(512), (n_co_h_0_w_0_fused_0 * T.int64(256) + n_co_h_0_w_0_fused_1) // T.int64(16) + ax0)
                                        v_p = T.axis.spatial(T.int64(16), (n_co_h_0_w_0_fused_0 * T.int64(256) + n_co_h_0_w_0_fused_1) % T.int64(16) + ax1)
                                        v_vh, v_vw, v_r_a, v_r_b = T.axis.remap("SSRR", [ax2, ax3, ax4, ax5])
                                        T.reads(inverse_local[v_co, v_p, v_vh, v_vw], bgemm[v_r_a, v_r_b, v_co, v_p])
                                        T.writes(inverse_local[v_co, v_p, v_vh, v_vw])
                                        T.block_attr({"schedule_rule": "conv2d_nchw_winograd_inverse"})
                                        inverse_local[v_co, v_p, v_vh, v_vw] = inverse_local[v_co, v_p, v_vh, v_vw] + bgemm[v_r_a, v_r_b, v_co, v_p] * T.Select(v_r_a % T.int64(4) == T.int64(3) and v_vh % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_vh % T.int64(2) == T.int64(0), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_vh % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_vh % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_vh % T.int64(2) == T.int64(1), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_vh % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_vh % T.int64(2) == T.int64(1), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_vh % T.int64(2) == T.int64(0), T.float32(1), T.float32(0))))))))) * T.Select(v_r_b % T.int64(4) == T.int64(3) and v_vw % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_vw % T.int64(2) == T.int64(0), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_vw % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_vw % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_vw % T.int64(2) == T.int64(1), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_vw % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_vw % T.int64(2) == T.int64(1), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_vw % T.int64(2) == T.int64(0), T.float32(1), T.float32(0)))))))))
                for h_1, w_1 in T.grid(T.int64(2), T.int64(2)):
                    with T.block("conv2d_winograd"):
                        v_n = T.axis.spatial(T.int64(1), T.int64(0))
                        v_co = T.axis.spatial(T.int64(512), (n_co_h_0_w_0_fused_0 * T.int64(256) + n_co_h_0_w_0_fused_1) // T.int64(16))
                        v_h = T.axis.spatial(T.int64(7), (n_co_h_0_w_0_fused_0 * T.int64(256) + n_co_h_0_w_0_fused_1) % T.int64(16) // T.int64(4) * T.int64(2) + h_1)
                        v_w = T.axis.spatial(T.int64(7), (n_co_h_0_w_0_fused_0 * T.int64(256) + n_co_h_0_w_0_fused_1) % T.int64(4) * T.int64(2) + w_1)
                        T.where((n_co_h_0_w_0_fused_0 * T.int64(256) + n_co_h_0_w_0_fused_1) % T.int64(16) // T.int64(4) * T.int64(2) + h_1 < T.int64(7) and (n_co_h_0_w_0_fused_0 * T.int64(256) + n_co_h_0_w_0_fused_1) % T.int64(4) * T.int64(2) + w_1 < T.int64(7))
                        T.reads(inverse_local[v_co, v_n * T.int64(16) + v_h // T.int64(2) * T.int64(4) + v_w // T.int64(2), v_h % T.int64(2), v_w % T.int64(2)], p2[v_n, v_co, v_h, v_w], p3[v_n, v_co, T.int64(0), T.int64(0)], p4[v_n, v_co, T.int64(0), T.int64(0)])
                        T.writes(T_relu[v_n, v_co, v_h, v_w])
                        T_relu[v_n, v_co, v_h, v_w] = T.max((inverse_local[v_co, v_n * T.int64(16) + v_h // T.int64(2) * T.int64(4) + v_w // T.int64(2), v_h % T.int64(2), v_w % T.int64(2)] + p2[v_n, v_co, v_h, v_w]) * p3[v_n, v_co, T.int64(0), T.int64(0)] + p4[v_n, v_co, T.int64(0), T.int64(0)], T.float32(0))
b0 = sch.get_block(name="data_pack", func_name="main")
b1 = sch.get_block(name="bgemm", func_name="main")
b2 = sch.get_block(name="inverse", func_name="main")
b3 = sch.get_block(name="conv2d_winograd", func_name="main")
b4 = sch.get_block(name="T_add", func_name="main")
b5 = sch.get_block(name="T_multiply", func_name="main")
b6 = sch.get_block(name="T_add_1", func_name="main")
b7 = sch.get_block(name="T_relu", func_name="main")
b8 = sch.get_block(name="root", func_name="main")
b9, b10 = sch.get_producers(block=b2)
sch.compute_inline(block=b10)
b11, = sch.get_consumers(block=b2)
l12, l13, l14, l15 = sch.get_loops(block=b11)
l16, l17 = sch.split(loop=l14, factors=[None, 2], preserve_unit_iters=True)
l18, l19 = sch.split(loop=l15, factors=[None, 2], preserve_unit_iters=True)
sch.reorder(l16, l18, l17, l19)
sch.compute_at(block=b2, loop=l18, preserve_unit_loops=True, index=-1)
sch.set_scope(block=b2, buffer_index=0, storage_scope="local")
l20, l21, l22, l23, l24, l25, l26, l27, l28, l29 = sch.get_loops(block=b2)
sch.unroll(loop=l26)
sch.unroll(loop=l27)
sch.unroll(loop=l28)
sch.unroll(loop=l29)
b30, b31 = sch.get_producers(block=b0)
sch.compute_inline(block=b31)
b32, = sch.get_producers(block=b30)
l33, l34, l35, l36, l37, l38 = sch.get_loops(block=b0)
sch.reorder(l35, l36, l33, l34, l37, l38)
sch.unroll(loop=l33)
sch.unroll(loop=l34)
sch.unroll(loop=l37)
sch.unroll(loop=l38)
l39 = sch.fuse(l35, l36, preserve_unit_iters=True)
v40 = sch.sample_categorical(candidates=[32, 64, 128, 256, 512, 1024], probs=[0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666], decision=0)
l41, l42 = sch.split(loop=l39, factors=[None, v40], preserve_unit_iters=True)
sch.bind(loop=l41, thread_axis="blockIdx.x")
sch.bind(loop=l42, thread_axis="threadIdx.x")
b43 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b43, loop=l42, preserve_unit_loops=True, index=-1)
sch.compute_at(block=b30, loop=l42, preserve_unit_loops=True, index=-1)
sch.set_scope(block=b30, buffer_index=0, storage_scope="local")
sch.compute_inline(block=b32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l44, l45, l46, l47, l48 = sch.get_loops(block=b1)
v49, v50, v51, v52, v53 = sch.sample_perfect_tile(loop=l44, n=5, max_innermost_factor=64, decision=[4, 1, 1, 1, 1])
l54, l55, l56, l57, l58 = sch.split(loop=l44, factors=[v49, v50, v51, v52, v53], preserve_unit_iters=True)
v59, v60, v61, v62, v63 = sch.sample_perfect_tile(loop=l45, n=5, max_innermost_factor=64, decision=[2, 1, 2, 1, 1])
l64, l65, l66, l67, l68 = sch.split(loop=l45, factors=[v59, v60, v61, v62, v63], preserve_unit_iters=True)
v69, v70, v71, v72, v73 = sch.sample_perfect_tile(loop=l46, n=5, max_innermost_factor=64, decision=[32, 1, 8, 1, 2])
l74, l75, l76, l77, l78 = sch.split(loop=l46, factors=[v69, v70, v71, v72, v73], preserve_unit_iters=True)
v79, v80, v81, v82, v83 = sch.sample_perfect_tile(loop=l47, n=5, max_innermost_factor=64, decision=[1, 1, 8, 1, 2])
l84, l85, l86, l87, l88 = sch.split(loop=l47, factors=[v79, v80, v81, v82, v83], preserve_unit_iters=True)
v89, v90, v91 = sch.sample_perfect_tile(loop=l48, n=3, max_innermost_factor=64, decision=[32, 1, 16])
l92, l93, l94 = sch.split(loop=l48, factors=[v89, v90, v91], preserve_unit_iters=True)
sch.reorder(l54, l64, l74, l84, l55, l65, l75, l85, l56, l66, l76, l86, l92, l93, l57, l67, l77, l87, l94, l58, l68, l78, l88)
l95 = sch.fuse(l54, l64, l74, l84, preserve_unit_iters=True)
sch.bind(loop=l95, thread_axis="blockIdx.x")
l96 = sch.fuse(l55, l65, l75, l85, preserve_unit_iters=True)
sch.bind(loop=l96, thread_axis="vthread.x")
l97 = sch.fuse(l56, l66, l76, l86, preserve_unit_iters=True)
sch.bind(loop=l97, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b98 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b98, loop=l97, preserve_unit_loops=True, index=-1)
b99 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b99, loop=l92, preserve_unit_loops=True, index=-1)
l100, l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b99)
l108 = sch.fuse(l104, l105, l106, l107, preserve_unit_iters=True)
v109 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b99, ann_key="meta_schedule.cooperative_fetch", ann_val=v109)
b110 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b110, loop=l92, preserve_unit_loops=True, index=-1)
l111, l112, l113, l114, l115, l116, l117, l118 = sch.get_loops(block=b110)
l119 = sch.fuse(l115, l116, l117, l118, preserve_unit_iters=True)
v120 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b110, ann_key="meta_schedule.cooperative_fetch", ann_val=v120)
sch.reverse_compute_inline(block=b7)
sch.reverse_compute_inline(block=b6)
sch.reverse_compute_inline(block=b5)
sch.reverse_compute_inline(block=b4)
v121 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=1)
sch.annotate(block_or_loop=b8, ann_key="meta_schedule.unroll_explicit", ann_val=v121)
l122, l123, l124, l125, l126, l127 = sch.get_loops(block=b3)
l128 = sch.fuse(l122, l123, l124, l125, preserve_unit_iters=True)
v129 = sch.sample_categorical(candidates=[32, 64, 128, 256, 512, 1024], probs=[0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666], decision=3)
l130, l131 = sch.split(loop=l128, factors=[None, v129], preserve_unit_iters=True)
sch.bind(loop=l130, thread_axis="blockIdx.x")
sch.bind(loop=l131, thread_axis="threadIdx.x")
sch.enter_postproc()
sch.unannotate(block_or_loop=b99, ann_key="meta_schedule.cooperative_fetch")
l132, l133, l134, l135, l136 = sch.get_loops(block=b99)
l137, l138 = sch.split(loop=l136, factors=[None, 128], preserve_unit_iters=True)
sch.bind(loop=l138, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b110, ann_key="meta_schedule.cooperative_fetch")
l139, l140, l141, l142, l143 = sch.get_loops(block=b110)
l144, l145, l146 = sch.split(loop=l143, factors=[None, 128, 4], preserve_unit_iters=True)
sch.vectorize(loop=l146)
sch.bind(loop=l145, thread_axis="threadIdx.x")
b147 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b147, ann_key="meta_schedule.unroll_explicit")
b148, b149, b150, b151, b152, b153, b154, b155, b156 = sch.get_child_blocks(b147)
l157, l158, l159, l160, l161, l162 = sch.get_loops(block=b148)
l163, l164, l165, l166, l167, l168 = sch.get_loops(block=b149)
sch.annotate(block_or_loop=l163, ann_key="pragma_auto_unroll_max_step", ann_val=16)
sch.annotate(block_or_loop=l163, ann_key="pragma_unroll_explicit", ann_val=1)
l169, l170, l171, l172, l173, l174 = sch.get_loops(block=b150)
l175, l176, l177, l178, l179, l180 = sch.get_loops(block=b151)
l181, l182, l183, l184, l185, l186, l187 = sch.get_loops(block=b152)
l188, l189, l190, l191, l192, l193, l194, l195, l196, l197, l198, l199, l200, l201 = sch.get_loops(block=b153)
sch.annotate(block_or_loop=l188, ann_key="pragma_auto_unroll_max_step", ann_val=16)
sch.annotate(block_or_loop=l188, ann_key="pragma_unroll_explicit", ann_val=1)
l202, l203, l204, l205, l206, l207, l208 = sch.get_loops(block=b154)
l209, l210, l211, l212, l213, l214, l215, l216 = sch.get_loops(block=b155)
sch.annotate(block_or_loop=l209, ann_key="pragma_auto_unroll_max_step", ann_val=16)
sch.annotate(block_or_loop=l209, ann_key="pragma_unroll_explicit", ann_val=1)
l217, l218, l219, l220 = sch.get_loops(block=b156)
b221 = sch.get_block(name="data_pack", func_name="main")
l222, l223, l224, l225, l226, l227 = sch.get_loops(block=b221)
b228 = sch.decompose_reduction(block=b221, loop=l226)
b229 = sch.get_block(name="bgemm", func_name="main")
l230, l231, l232, l233, l234, l235, l236, l237, l238, l239, l240, l241, l242, l243 = sch.get_loops(block=b229)
b244 = sch.decompose_reduction(block=b229, loop=l233)
b245 = sch.get_block(name="inverse", func_name="main")
l246, l247, l248, l249, l250, l251, l252, l253 = sch.get_loops(block=b245)
b254 = sch.decompose_reduction(block=b245, loop=l252)
2023-11-11 01:34:49 [INFO] [task_scheduler.cc:121] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #133: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/home2/xiachunwei/Software/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/home2/xiachunwei/Software/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/home2/xiachunwei/Software/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/home2/xiachunwei/Software/tvm/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/home2/xiachunwei/Software/tvm/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/home2/xiachunwei/Software/tvm/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/home2/xiachunwei/Software/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/home2/xiachunwei/Software/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  4: _ZN3tvm7runtime13PackedFun
  3: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  2: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  0: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  File "/home2/xiachunwei/Software/tvm/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(512), T.int64(7), T.int64(7)), "float32"), p1: T.Buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(512)), "float32"), p2: T.Buffer((T.int64(1), T.int64(512), T.int64(7), T.int64(7)), "float32"), p3: T.Buffer((T.int64(1), T.int64(512), T.int64(1), T.int64(1)), "float32"), p4: T.Buffer((T.int64(1), T.int64(512), T.int64(1), T.int64(1)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(512), T.int64(7), T.int64(7)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        input_tile_local = T.alloc_buffer((T.int64(512), T.int64(16), T.int64(4), T.int64(4)), scope="local")
        data_pack = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(16)))
        bgemm = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(16)))
        inverse_local = T.alloc_buffer((T.int64(512), T.int64(16), T.int64(2), T.int64(2)), scope="local")
        data_pack_local = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(16)), scope="local")
        bgemm_local = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(16)), scope="local")
        data_pack_shared = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(16)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(512)), scope="shared")
        for ci_p_fused_0 in T.thread_binding(T.int64(64), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 512, "pragma_unroll_explicit": 1}):
            for ci_p_fused_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(1), T.int64(4), T.int64(4)):
                    with T.block("input_tile"):
                        v_ci = T.axis.spatial(T.int64(512), ci_p_fused_0 * T.int64(8) + ci_p_fused_1 // T.int64(16) + ax0)
                        v_p = T.axis.spatial(T.int64(16), ci_p_fused_1 % T.int64(16) + ax1)
                        v_eps, v_nu = T.axis.remap("SS", [ax2, ax3])
                        T.reads(p0[v_p // T.int64(16), v_ci, v_p % T.int64(16) // T.int64(4) * T.int64(2) + v_eps - T.int64(1), v_p % T.int64(4) * T.int64(2) + v_nu - T.int64(1)])
                        T.writes(input_tile_local[v_ci, v_p, v_eps, v_nu])
                        T.block_attr({"schedule_rule": "None"})
                        input_tile_local[v_ci, v_p, v_eps, v_nu] = T.if_then_else(T.int64(1) <= v_p % T.int64(16) // T.int64(4) * T.int64(2) + v_eps and v_p % T.int64(16) // T.int64(4) * T.int64(2) + v_eps < T.int64(8) and T.int64(1) <= v_p % T.int64(4) * T.int64(2) + v_nu and v_p % T.int64(4) * T.int64(2) + v_nu < T.int64(8), p0[v_p // T.int64(16), v_ci, v_p % T.int64(16) // T.int64(4) * T.int64(2) + v_eps - T.int64(1), v_p % T.int64(4) * T.int64(2) + v_nu - T.int64(1)], T.float32(0))
                for eps in T.unroll(T.int64(4)):
                    for nu in T.unroll(T.int64(4)):
                        with T.block("data_pack_init"):
                            v_eps, v_nu = T.axis.remap("SS", [eps, nu])
                            v_ci = T.axis.spatial(T.int64(512), (ci_p_fused_0 * T.int64(128) + ci_p_fused_1) // T.int64(16))
                            v_p = T.axis.spatial(T.int64(16), (ci_p_fused_0 * T.int64(128) + ci_p_fused_1) % T.int64(16))
                            T.reads()
                            T.writes(data_pack_local[v_eps, v_nu, v_ci, v_p])
                            T.block_attr({"schedule_rule": "conv2d_nchw_winograd_data_pack"})
                            data_pack_local[v_eps, v_nu, v_ci, v_p] = T.float32(0)
                        for r_a in T.unroll(T.int64(4)):
                            for r_b in T.unroll(T.int64(4)):
                                with T.block("data_pack_update"):
                                    v_eps, v_nu = T.axis.remap("SS", [eps, nu])
                                    v_ci = T.axis.spatial(T.int64(512), (ci_p_fused_0 * T.int64(128) + ci_p_fused_1) // T.int64(16))
                                    v_p = T.axis.spatial(T.int64(16), (ci_p_fused_0 * T.int64(128) + ci_p_fused_1) % T.int64(16))
                                    v_r_a, v_r_b = T.axis.remap("RR", [r_a, r_b])
                                    T.reads(data_pack_local[v_eps, v_nu, v_ci, v_p], input_tile_local[v_ci, v_p, v_r_a, v_r_b])
                                    T.writes(data_pack_local[v_eps, v_nu, v_ci, v_p])
                                    T.block_attr({"schedule_rule": "conv2d_nchw_winograd_data_pack"})
                                    data_pack_local[v_eps, v_nu, v_ci, v_p] = data_pack_local[v_eps, v_nu, v_ci, v_p] + input_tile_local[v_ci, v_p, v_r_a, v_r_b] * T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(3), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(1), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(0), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(3), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(1), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(0), T.float32(1), T.float32(0))))))))))))))))) * T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(3), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(1), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(0), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(3), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(1), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(0), T.float32(1), T.float32(0)))))))))))))))))
                for ax0, ax1, ax2, ax3 in T.grid(T.int64(4), T.int64(4), T.int64(1), T.int64(1)):
                    with T.block("data_pack_local"):
                        v0, v1 = T.axis.remap("SS", [ax0, ax1])
                        v2 = T.axis.spatial(T.int64(512), ci_p_fused_0 * T.int64(8) + ci_p_fused_1 // T.int64(16) + ax2)
                        v3 = T.axis.spatial(T.int64(16), ci_p_fused_1 % T.int64(16) + ax3)
                        T.reads(data_pack_local[v0, v1, v2, v3])
                        T.writes(data_pack[v0, v1, v2, v3])
                        data_pack[v0, v1, v2, v3] = data_pack_local[v0, v1, v2, v3]
        for eps_0_nu_0_co_0_p_0_fused in T.thread_binding(T.int64(128), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 512, "pragma_unroll_explicit": 1}):
            for eps_1_nu_1_co_1_p_1_fused in T.thread_binding(T.int64(4), thread="vthread.x"):
                for eps_2_nu_2_co_2_p_2_fused in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                    for eps_3_init, nu_3_init, co_3_init, p_3_init, eps_4_init, nu_4_init, co_4_init, p_4_init in T.grid(T.int64(1), T.int64(1), T.int64(2), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("bgemm_init"):
                            v_eps = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(32) + eps_3_init + eps_4_init)
                            v_nu = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused % T.int64(32) // T.int64(16) * T.int64(2) + eps_2_nu_2_co_2_p_2_fused // T.int64(64) + nu_3_init + nu_4_init)
                            v_co = T.axis.spatial(T.int64(512), eps_0_nu_0_co_0_p_0_fused % T.int64(16) * T.int64(32) + eps_1_nu_1_co_1_p_1_fused // T.int64(2) * T.int64(16) + eps_2_nu_2_co_2_p_2_fused % T.int64(64) // T.int64(8) * T.int64(2) + co_3_init + co_4_init)
                            v_p = T.axis.spatial(T.int64(16), eps_1_nu_1_co_1_p_1_fused % T.int64(2) * T.int64(8) + eps_2_nu_2_co_2_p_2_fused % T.int64(8) + p_3_init + p_4_init)
                            T.reads()
                            T.writes(bgemm_local[v_eps, v_nu, v_co, v_p])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            bgemm_local[v_eps, v_nu, v_co, v_p] = T.float32(0)
                    for ci_0 in range(T.int64(32)):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(4)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                                with T.block("data_pack_shared"):
                                    v0 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(32))
                                    v1 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused % T.int64(32) // T.int64(16) * T.int64(2) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1) // T.int64(256))
                                    v2 = T.axis.spatial(T.int64(512), ci_0 * T.int64(16) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1) % T.int64(256) // T.int64(16))
                                    v3 = T.axis.spatial(T.int64(16), (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1) % T.int64(16))
                                    T.reads(data_pack[v0, v1, v2, v3])
                                    T.writes(data_pack_shared[v0, v1, v2, v3])
                                    data_pack_shared[v0, v1, v2, v3] = data_pack[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(2)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("p1_shared"):
                                        v0 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(32))
                                        v1 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused % T.int64(32) // T.int64(16) * T.int64(2) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(512) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) // T.int64(512))
                                        v2 = T.axis.spatial(T.int64(512), ci_0 * T.int64(16) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(512) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) % T.int64(512) // T.int64(32))
                                        v3 = T.axis.spatial(T.int64(512), eps_0_nu_0_co_0_p_0_fused % T.int64(16) * T.int64(32) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(512) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) % T.int64(32))
                                        T.reads(p1[v0, v1, v2, v3])
                                        T.writes(p1_shared[v0, v1, v2, v3])
                                        p1_shared[v0, v1, v2, v3] = p1[v0, v1, v2, v3]
                        for ci_1, eps_3, nu_3, co_3, p_3, ci_2, eps_4, nu_4, co_4, p_4 in T.grid(T.int64(2), T.int64(1), T.int64(1), T.int64(2), T.int64(1), T.int64(8), T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                            with T.block("bgemm_update"):
                                v_eps = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(32) + eps_3 + eps_4)
                                v_nu = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused % T.int64(32) // T.int64(16) * T.int64(2) + eps_2_nu_2_co_2_p_2_fused // T.int64(64) + nu_3 + nu_4)
                                v_co = T.axis.spatial(T.int64(512), eps_0_nu_0_co_0_p_0_fused % T.int64(16) * T.int64(32) + eps_1_nu_1_co_1_p_1_fused // T.int64(2) * T.int64(16) + eps_2_nu_2_co_2_p_2_fused % T.int64(64) // T.int64(8) * T.int64(2) + co_3 + co_4)
                                v_p = T.axis.spatial(T.int64(16), eps_1_nu_1_co_1_p_1_fused % T.int64(2) * T.int64(8) + eps_2_nu_2_co_2_p_2_fused % T.int64(8) + p_3 + p_4)
                                v_ci = T.axis.reduce(T.int64(512), ci_0 * T.int64(16) + ci_1 * T.int64(8) + ci_2)
                                T.reads(bgemm_local[v_eps, v_nu, v_co, v_p], data_pack_shared[v_eps, v_nu, v_ci, v_p], p1_shared[v_eps, v_nu, v_ci, v_co])
                                T.writes(bgemm_local[v_eps, v_nu, v_co, v_p])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                bgemm_local[v_eps, v_nu, v_co, v_p] = bgemm_local[v_eps, v_nu, v_co, v_p] + data_pack_shared[v_eps, v_nu, v_ci, v_p] * p1_shared[v_eps, v_nu, v_ci, v_co]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(1), T.int64(2), T.int64(1)):
                        with T.block("bgemm_local"):
                            v0 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(32) + ax0)
                            v1 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused % T.int64(32) // T.int64(16) * T.int64(2) + eps_2_nu_2_co_2_p_2_fused // T.int64(64) + ax1)
                            v2 = T.axis.spatial(T.int64(512), eps_0_nu_0_co_0_p_0_fused % T.int64(16) * T.int64(32) + eps_1_nu_1_co_1_p_1_fused // T.int64(2) * T.int64(16) + eps_2_nu_2_co_2_p_2_fused % T.int64(64) // T.int64(8) * T.int64(2) + ax2)
                            v3 = T.axis.spatial(T.int64(16), eps_1_nu_1_co_1_p_1_fused % T.int64(2) * T.int64(8) + eps_2_nu_2_co_2_p_2_fused % T.int64(8) + ax3)
                            T.reads(bgemm_local[v0, v1, v2, v3])
                            T.writes(bgemm[v0, v1, v2, v3])
                            bgemm[v0, v1, v2, v3] = bgemm_local[v0, v1, v2, v3]
        for n_co_h_0_w_0_fused_0 in T.thread_binding(T.int64(16), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 512, "pragma_unroll_explicit": 1}):
            for n_co_h_0_w_0_fused_1 in T.thread_binding(T.int64(512), thread="threadIdx.x"):
                for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                    for ax2 in T.unroll(T.int64(2)):
                        for ax3 in T.unroll(T.int64(2)):
                            with T.block("inverse_init"):
                                v_co = T.axis.spatial(T.int64(512), (n_co_h_0_w_0_fused_0 * T.int64(512) + n_co_h_0_w_0_fused_1) // T.int64(16) + ax0)
                                v_p = T.axis.spatial(T.int64(16), (n_co_h_0_w_0_fused_0 * T.int64(512) + n_co_h_0_w_0_fused_1) % T.int64(16) + ax1)
                                v_vh, v_vw = T.axis.remap("SS", [ax2, ax3])
                                T.reads()
                                T.writes(inverse_local[v_co, v_p, v_vh, v_vw])
                                T.block_attr({"schedule_rule": "conv2d_nchw_winograd_inverse"})
                                inverse_local[v_co, v_p, v_vh, v_vw] = T.float32(0)
                            for ax4 in T.unroll(T.int64(4)):
                                for ax5 in T.unroll(T.int64(4)):
                                    with T.block("inverse_update"):
                                        v_co = T.axis.spatial(T.int64(512), (n_co_h_0_w_0_fused_0 * T.int64(512) + n_co_h_0_w_0_fused_1) // T.int64(16) + ax0)
                                        v_p = T.axis.spatial(T.int64(16), (n_co_h_0_w_0_fused_0 * T.int64(512) + n_co_h_0_w_0_fused_1) % T.int64(16) + ax1)
                                        v_vh, v_vw, v_r_a, v_r_b = T.axis.remap("SSRR", [ax2, ax3, ax4, ax5])
                                        T.reads(inverse_local[v_co, v_p, v_vh, v_vw], bgemm[v_r_a, v_r_b, v_co, v_p])
                                        T.writes(inverse_local[v_co, v_p, v_vh, v_vw])
                                        T.block_attr({"schedule_rule": "conv2d_nchw_winograd_inverse"})
                                        inverse_local[v_co, v_p, v_vh, v_vw] = inverse_local[v_co, v_p, v_vh, v_vw] + bgemm[v_r_a, v_r_b, v_co, v_p] * T.Select(v_r_a % T.int64(4) == T.int64(3) and v_vh % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_vh % T.int64(2) == T.int64(0), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_vh % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_vh % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_vh % T.int64(2) == T.int64(1), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_vh % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_vh % T.int64(2) == T.int64(1), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_vh % T.int64(2) == T.int64(0), T.float32(1), T.float32(0))))))))) * T.Select(v_r_b % T.int64(4) == T.int64(3) and v_vw % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_vw % T.int64(2) == T.int64(0), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_vw % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_vw % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_vw % T.int64(2) == T.int64(1), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_vw % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_vw % T.int64(2) == T.int64(1), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_vw % T.int64(2) == T.int64(0), T.float32(1), T.float32(0)))))))))
                for h_1, w_1 in T.grid(T.int64(2), T.int64(2)):
                    with T.block("conv2d_winograd"):
                        v_n = T.axis.spatial(T.int64(1), T.int64(0))
                        v_co = T.axis.spatial(T.int64(512), (n_co_h_0_w_0_fused_0 * T.int64(512) + n_co_h_0_w_0_fused_1) // T.int64(16))
                        v_h = T.axis.spatial(T.int64(7), (n_co_h_0_w_0_fused_0 * T.int64(512) + n_co_h_0_w_0_fused_1) % T.int64(16) // T.int64(4) * T.int64(2) + h_1)
                        v_w = T.axis.spatial(T.int64(7), (n_co_h_0_w_0_fused_0 * T.int64(512) + n_co_h_0_w_0_fused_1) % T.int64(4) * T.int64(2) + w_1)
                        T.where((n_co_h_0_w_0_fused_0 * T.int64(512) + n_co_h_0_w_0_fused_1) % T.int64(16) // T.int64(4) * T.int64(2) + h_1 < T.int64(7) and (n_co_h_0_w_0_fused_0 * T.int64(512) + n_co_h_0_w_0_fused_1) % T.int64(4) * T.int64(2) + w_1 < T.int64(7))
                        T.reads(inverse_local[v_co, v_n * T.int64(16) + v_h // T.int64(2) * T.int64(4) + v_w // T.int64(2), v_h % T.int64(2), v_w % T.int64(2)], p2[v_n, v_co, v_h, v_w], p3[v_n, v_co, T.int64(0), T.int64(0)], p4[v_n, v_co, T.int64(0), T.int64(0)])
                        T.writes(T_relu[v_n, v_co, v_h, v_w])
                        T_relu[v_n, v_co, v_h, v_w] = T.max((inverse_local[v_co, v_n * T.int64(16) + v_h // T.int64(2) * T.int64(4) + v_w // T.int64(2), v_h % T.int64(2), v_w % T.int64(2)] + p2[v_n, v_co, v_h, v_w]) * p3[v_n, v_co, T.int64(0), T.int64(0)] + p4[v_n, v_co, T.int64(0), T.int64(0)], T.float32(0))
b0 = sch.get_block(name="data_pack", func_name="main")
b1 = sch.get_block(name="bgemm", func_name="main")
b2 = sch.get_block(name="inverse", func_name="main")
b3 = sch.get_block(name="conv2d_winograd", func_name="main")
b4 = sch.get_block(name="T_add", func_name="main")
b5 = sch.get_block(name="T_multiply", func_name="main")
b6 = sch.get_block(name="T_add_1", func_name="main")
b7 = sch.get_block(name="T_relu", func_name="main")
b8 = sch.get_block(name="root", func_name="main")
b9, b10 = sch.get_producers(block=b2)
sch.compute_inline(block=b10)
b11, = sch.get_consumers(block=b2)
l12, l13, l14, l15 = sch.get_loops(block=b11)
l16, l17 = sch.split(loop=l14, factors=[None, 2], preserve_unit_iters=True)
l18, l19 = sch.split(loop=l15, factors=[None, 2], preserve_unit_iters=True)
sch.reorder(l16, l18, l17, l19)
sch.compute_at(block=b2, loop=l18, preserve_unit_loops=True, index=-1)
sch.set_scope(block=b2, buffer_index=0, storage_scope="local")
l20, l21, l22, l23, l24, l25, l26, l27, l28, l29 = sch.get_loops(block=b2)
sch.unroll(loop=l26)
sch.unroll(loop=l27)
sch.unroll(loop=l28)
sch.unroll(loop=l29)
b30, b31 = sch.get_producers(block=b0)
sch.compute_inline(block=b31)
b32, = sch.get_producers(block=b30)
l33, l34, l35, l36, l37, l38 = sch.get_loops(block=b0)
sch.reorder(l35, l36, l33, l34, l37, l38)
sch.unroll(loop=l33)
sch.unroll(loop=l34)
sch.unroll(loop=l37)
sch.unroll(loop=l38)
l39 = sch.fuse(l35, l36, preserve_unit_iters=True)
v40 = sch.sample_categorical(candidates=[32, 64, 128, 256, 512, 1024], probs=[0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666], decision=2)
l41, l42 = sch.split(loop=l39, factors=[None, v40], preserve_unit_iters=True)
sch.bind(loop=l41, thread_axis="blockIdx.x")
sch.bind(loop=l42, thread_axis="threadIdx.x")
b43 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b43, loop=l42, preserve_unit_loops=True, index=-1)
sch.compute_at(block=b30, loop=l42, preserve_unit_loops=True, index=-1)
sch.set_scope(block=b30, buffer_index=0, storage_scope="local")
sch.compute_inline(block=b32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l44, l45, l46, l47, l48 = sch.get_loops(block=b1)
v49, v50, v51, v52, v53 = sch.sample_perfect_tile(loop=l44, n=5, max_innermost_factor=64, decision=[4, 1, 1, 1, 1])
l54, l55, l56, l57, l58 = sch.split(loop=l44, factors=[v49, v50, v51, v52, v53], preserve_unit_iters=True)
v59, v60, v61, v62, v63 = sch.sample_perfect_tile(loop=l45, n=5, max_innermost_factor=64, decision=[2, 1, 2, 1, 1])
l64, l65, l66, l67, l68 = sch.split(loop=l45, factors=[v59, v60, v61, v62, v63], preserve_unit_iters=True)
v69, v70, v71, v72, v73 = sch.sample_perfect_tile(loop=l46, n=5, max_innermost_factor=64, decision=[16, 2, 8, 2, 1])
l74, l75, l76, l77, l78 = sch.split(loop=l46, factors=[v69, v70, v71, v72, v73], preserve_unit_iters=True)
v79, v80, v81, v82, v83 = sch.sample_perfect_tile(loop=l47, n=5, max_innermost_factor=64, decision=[1, 2, 8, 1, 1])
l84, l85, l86, l87, l88 = sch.split(loop=l47, factors=[v79, v80, v81, v82, v83], preserve_unit_iters=True)
v89, v90, v91 = sch.sample_perfect_tile(loop=l48, n=3, max_innermost_factor=64, decision=[32, 2, 8])
l92, l93, l94 = sch.split(loop=l48, factors=[v89, v90, v91], preserve_unit_iters=True)
sch.reorder(l54, l64, l74, l84, l55, l65, l75, l85, l56, l66, l76, l86, l92, l93, l57, l67, l77, l87, l94, l58, l68, l78, l88)
l95 = sch.fuse(l54, l64, l74, l84, preserve_unit_iters=True)
sch.bind(loop=l95, thread_axis="blockIdx.x")
l96 = sch.fuse(l55, l65, l75, l85, preserve_unit_iters=True)
sch.bind(loop=l96, thread_axis="vthread.x")
l97 = sch.fuse(l56, l66, l76, l86, preserve_unit_iters=True)
sch.bind(loop=l97, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b98 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b98, loop=l97, preserve_unit_loops=True, index=-1)
b99 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b99, loop=l92, preserve_unit_loops=True, index=-1)
l100, l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b99)
l108 = sch.fuse(l104, l105, l106, l107, preserve_unit_iters=True)
v109 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b99, ann_key="meta_schedule.cooperative_fetch", ann_val=v109)
b110 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b110, loop=l92, preserve_unit_loops=True, index=-1)
l111, l112, l113, l114, l115, l116, l117, l118 = sch.get_loops(block=b110)
l119 = sch.fuse(l115, l116, l117, l118, preserve_unit_iters=True)
v120 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b110, ann_key="meta_schedule.cooperative_fetch", ann_val=v120)
sch.reverse_compute_inline(block=b7)
sch.reverse_compute_inline(block=b6)
sch.reverse_compute_inline(block=b5)
sch.reverse_compute_inline(block=b4)
v121 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b8, ann_key="meta_schedule.unroll_explicit", ann_val=v121)
l122, l123, l124, l125, l126, l127 = sch.get_loops(block=b3)
l128 = sch.fuse(l122, l123, l124, l125, preserve_unit_iters=True)
v129 = sch.sample_categorical(candidates=[32, 64, 128, 256, 512, 1024], probs=[0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666], decision=4)
l130, l131 = sch.split(loop=l128, factors=[None, v129], preserve_unit_iters=True)
sch.bind(loop=l130, thread_axis="blockIdx.x")
sch.bind(loop=l131, thread_axis="threadIdx.x")
sch.enter_postproc()
sch.unannotate(block_or_loop=b99, ann_key="meta_schedule.cooperative_fetch")
l132, l133, l134, l135, l136 = sch.get_loops(block=b99)
l137, l138 = sch.split(loop=l136, factors=[None, 128], preserve_unit_iters=True)
sch.bind(loop=l138, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b110, ann_key="meta_schedule.cooperative_fetch")
l139, l140, l141, l142, l143 = sch.get_loops(block=b110)
l144, l145, l146 = sch.split(loop=l143, factors=[None, 128, 4], preserve_unit_iters=True)
sch.vectorize(loop=l146)
sch.bind(loop=l145, thread_axis="threadIdx.x")
b147 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b147, ann_key="meta_schedule.unroll_explicit")
b148, b149, b150, b151, b152, b153, b154, b155, b156 = sch.get_child_blocks(b147)
l157, l158, l159, l160, l161, l162 = sch.get_loops(block=b148)
l163, l164, l165, l166, l167, l168 = sch.get_loops(block=b149)
sch.annotate(block_or_loop=l163, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l163, ann_key="pragma_unroll_explicit", ann_val=1)
l169, l170, l171, l172, l173, l174 = sch.get_loops(block=b150)
l175, l176, l177, l178, l179, l180 = sch.get_loops(block=b151)
l181, l182, l183, l184, l185, l186, l187 = sch.get_loops(block=b152)
l188, l189, l190, l191, l192, l193, l194, l195, l196, l197, l198, l199, l200, l201 = sch.get_loops(block=b153)
sch.annotate(block_or_loop=l188, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l188, ann_key="pragma_unroll_explicit", ann_val=1)
l202, l203, l204, l205, l206, l207, l208 = sch.get_loops(block=b154)
l209, l210, l211, l212, l213, l214, l215, l216 = sch.get_loops(block=b155)
sch.annotate(block_or_loop=l209, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l209, ann_key="pragma_unroll_explicit", ann_val=1)
l217, l218, l219, l220 = sch.get_loops(block=b156)
b221 = sch.get_block(name="data_pack", func_name="main")
l222, l223, l224, l225, l226, l227 = sch.get_loops(block=b221)
b228 = sch.decompose_reduction(block=b221, loop=l226)
b229 = sch.get_block(name="bgemm", func_name="main")
l230, l231, l232, l233, l234, l235, l236, l237, l238, l239, l240, l241, l242, l243 = sch.get_loops(block=b229)
b244 = sch.decompose_reduction(block=b229, loop=l233)
b245 = sch.get_block(name="inverse", func_name="main")
l246, l247, l248, l249, l250, l251, l252, l253 = sch.get_loops(block=b245)
b254 = sch.decompose_reduction(block=b245, loop=l252)
2023-11-11 01:34:49 [INFO] [task_scheduler.cc:121] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #134: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/home2/xiachunwei/Software/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/home2/xiachunwei/Software/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/home2/xiachunwei/Software/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/home2/xiachunwei/Software/tvm/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/home2/xiachunwei/Software/tvm/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/home2/xiachunwei/Software/tvm/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/home2/xiachunwei/Software/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/home2/xiachunwei/Software/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  4: _ZN3tvm7runtime13PackedFun
  3: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  2: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  0: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  File "/home2/xiachunwei/Software/tvm/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(512), T.int64(7), T.int64(7)), "float32"), p1: T.Buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(512)), "float32"), p2: T.Buffer((T.int64(1), T.int64(512), T.int64(7), T.int64(7)), "float32"), p3: T.Buffer((T.int64(1), T.int64(512), T.int64(1), T.int64(1)), "float32"), p4: T.Buffer((T.int64(1), T.int64(512), T.int64(1), T.int64(1)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(512), T.int64(7), T.int64(7)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        input_tile_local = T.alloc_buffer((T.int64(512), T.int64(16), T.int64(4), T.int64(4)), scope="local")
        data_pack = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(16)))
        bgemm = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(16)))
        inverse_local = T.alloc_buffer((T.int64(512), T.int64(16), T.int64(2), T.int64(2)), scope="local")
        data_pack_local = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(16)), scope="local")
        bgemm_local = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(16)), scope="local")
        data_pack_shared = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(16)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(512)), scope="shared")
        for ci_p_fused_0 in T.thread_binding(T.int64(256), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 512, "pragma_unroll_explicit": 1}):
            for ci_p_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(1), T.int64(4), T.int64(4)):
                    with T.block("input_tile"):
                        v_ci = T.axis.spatial(T.int64(512), ci_p_fused_0 * T.int64(2) + ci_p_fused_1 // T.int64(16) + ax0)
                        v_p = T.axis.spatial(T.int64(16), ci_p_fused_1 % T.int64(16) + ax1)
                        v_eps, v_nu = T.axis.remap("SS", [ax2, ax3])
                        T.reads(p0[v_p // T.int64(16), v_ci, v_p % T.int64(16) // T.int64(4) * T.int64(2) + v_eps - T.int64(1), v_p % T.int64(4) * T.int64(2) + v_nu - T.int64(1)])
                        T.writes(input_tile_local[v_ci, v_p, v_eps, v_nu])
                        T.block_attr({"schedule_rule": "None"})
                        input_tile_local[v_ci, v_p, v_eps, v_nu] = T.if_then_else(T.int64(1) <= v_p % T.int64(16) // T.int64(4) * T.int64(2) + v_eps and v_p % T.int64(16) // T.int64(4) * T.int64(2) + v_eps < T.int64(8) and T.int64(1) <= v_p % T.int64(4) * T.int64(2) + v_nu and v_p % T.int64(4) * T.int64(2) + v_nu < T.int64(8), p0[v_p // T.int64(16), v_ci, v_p % T.int64(16) // T.int64(4) * T.int64(2) + v_eps - T.int64(1), v_p % T.int64(4) * T.int64(2) + v_nu - T.int64(1)], T.float32(0))
                for eps in T.unroll(T.int64(4)):
                    for nu in T.unroll(T.int64(4)):
                        with T.block("data_pack_init"):
                            v_eps, v_nu = T.axis.remap("SS", [eps, nu])
                            v_ci = T.axis.spatial(T.int64(512), (ci_p_fused_0 * T.int64(32) + ci_p_fused_1) // T.int64(16))
                            v_p = T.axis.spatial(T.int64(16), (ci_p_fused_0 * T.int64(32) + ci_p_fused_1) % T.int64(16))
                            T.reads()
                            T.writes(data_pack_local[v_eps, v_nu, v_ci, v_p])
                            T.block_attr({"schedule_rule": "conv2d_nchw_winograd_data_pack"})
                            data_pack_local[v_eps, v_nu, v_ci, v_p] = T.float32(0)
                        for r_a in T.unroll(T.int64(4)):
                            for r_b in T.unroll(T.int64(4)):
                                with T.block("data_pack_update"):
                                    v_eps, v_nu = T.axis.remap("SS", [eps, nu])
                                    v_ci = T.axis.spatial(T.int64(512), (ci_p_fused_0 * T.int64(32) + ci_p_fused_1) // T.int64(16))
                                    v_p = T.axis.spatial(T.int64(16), (ci_p_fused_0 * T.int64(32) + ci_p_fused_1) % T.int64(16))
                                    v_r_a, v_r_b = T.axis.remap("RR", [r_a, r_b])
                                    T.reads(data_pack_local[v_eps, v_nu, v_ci, v_p], input_tile_local[v_ci, v_p, v_r_a, v_r_b])
                                    T.writes(data_pack_local[v_eps, v_nu, v_ci, v_p])
                                    T.block_attr({"schedule_rule": "conv2d_nchw_winograd_data_pack"})
                                    data_pack_local[v_eps, v_nu, v_ci, v_p] = data_pack_local[v_eps, v_nu, v_ci, v_p] + input_tile_local[v_ci, v_p, v_r_a, v_r_b] * T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(3), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(1), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(0), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(3), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(1), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(0), T.float32(1), T.float32(0))))))))))))))))) * T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(3), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(1), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(0), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(3), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(1), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(0), T.float32(1), T.float32(0)))))))))))))))))
                for ax0, ax1, ax2, ax3 in T.grid(T.int64(4), T.int64(4), T.int64(1), T.int64(1)):
                    with T.block("data_pack_local"):
                        v0, v1 = T.axis.remap("SS", [ax0, ax1])
                        v2 = T.axis.spatial(T.int64(512), ci_p_fused_0 * T.int64(2) + ci_p_fused_1 // T.int64(16) + ax2)
                        v3 = T.axis.spatial(T.int64(16), ci_p_fused_1 % T.int64(16) + ax3)
                        T.reads(data_pack_local[v0, v1, v2, v3])
                        T.writes(data_pack[v0, v1, v2, v3])
                        data_pack[v0, v1, v2, v3] = data_pack_local[v0, v1, v2, v3]
        for eps_0_nu_0_co_0_p_0_fused in T.thread_binding(T.int64(128), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 512, "pragma_unroll_explicit": 1}):
            for eps_1_nu_1_co_1_p_1_fused in T.thread_binding(T.int64(4), thread="vthread.x"):
                for eps_2_nu_2_co_2_p_2_fused in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                    for eps_3_init, nu_3_init, co_3_init, p_3_init, eps_4_init, nu_4_init, co_4_init, p_4_init in T.grid(T.int64(1), T.int64(2), T.int64(2), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("bgemm_init"):
                            v_eps = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(32) + eps_3_init + eps_4_init)
                            v_nu = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused % T.int64(32) // T.int64(16) * T.int64(2) + nu_3_init + nu_4_init)
                            v_co = T.axis.spatial(T.int64(512), eps_0_nu_0_co_0_p_0_fused % T.int64(16) * T.int64(32) + eps_1_nu_1_co_1_p_1_fused // T.int64(2) * T.int64(16) + eps_2_nu_2_co_2_p_2_fused // T.int64(8) * T.int64(2) + co_3_init + co_4_init)
                            v_p = T.axis.spatial(T.int64(16), eps_1_nu_1_co_1_p_1_fused % T.int64(2) * T.int64(8) + eps_2_nu_2_co_2_p_2_fused % T.int64(8) + p_3_init + p_4_init)
                            T.reads()
                            T.writes(bgemm_local[v_eps, v_nu, v_co, v_p])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            bgemm_local[v_eps, v_nu, v_co, v_p] = T.float32(0)
                    for ci_0 in range(T.int64(32)):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(8)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                                with T.block("data_pack_shared"):
                                    v0 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(32))
                                    v1 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused % T.int64(32) // T.int64(16) * T.int64(2) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused_1) // T.int64(256))
                                    v2 = T.axis.spatial(T.int64(512), ci_0 * T.int64(16) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused_1) % T.int64(256) // T.int64(16))
                                    v3 = T.axis.spatial(T.int64(16), (ax0_ax1_ax2_ax3_fused_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused_1) % T.int64(16))
                                    T.reads(data_pack[v0, v1, v2, v3])
                                    T.writes(data_pack_shared[v0, v1, v2, v3])
                                    data_pack_shared[v0, v1, v2, v3] = data_pack[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(16)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                                with T.block("p1_shared"):
                                    v0 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(32))
                                    v1 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused % T.int64(32) // T.int64(16) * T.int64(2) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused_1) // T.int64(512))
                                    v2 = T.axis.spatial(T.int64(512), ci_0 * T.int64(16) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused_1) % T.int64(512) // T.int64(32))
                                    v3 = T.axis.spatial(T.int64(512), eps_0_nu_0_co_0_p_0_fused % T.int64(16) * T.int64(32) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused_1) % T.int64(32))
                                    T.reads(p1[v0, v1, v2, v3])
                                    T.writes(p1_shared[v0, v1, v2, v3])
                                    p1_shared[v0, v1, v2, v3] = p1[v0, v1, v2, v3]
                        for ci_1, eps_3, nu_3, co_3, p_3, ci_2, eps_4, nu_4, co_4, p_4 in T.grid(T.int64(2), T.int64(1), T.int64(2), T.int64(2), T.int64(1), T.int64(8), T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                            with T.block("bgemm_update"):
                                v_eps = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(32) + eps_3 + eps_4)
                                v_nu = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused % T.int64(32) // T.int64(16) * T.int64(2) + nu_3 + nu_4)
                                v_co = T.axis.spatial(T.int64(512), eps_0_nu_0_co_0_p_0_fused % T.int64(16) * T.int64(32) + eps_1_nu_1_co_1_p_1_fused // T.int64(2) * T.int64(16) + eps_2_nu_2_co_2_p_2_fused // T.int64(8) * T.int64(2) + co_3 + co_4)
                                v_p = T.axis.spatial(T.int64(16), eps_1_nu_1_co_1_p_1_fused % T.int64(2) * T.int64(8) + eps_2_nu_2_co_2_p_2_fused % T.int64(8) + p_3 + p_4)
                                v_ci = T.axis.reduce(T.int64(512), ci_0 * T.int64(16) + ci_1 * T.int64(8) + ci_2)
                                T.reads(bgemm_local[v_eps, v_nu, v_co, v_p], data_pack_shared[v_eps, v_nu, v_ci, v_p], p1_shared[v_eps, v_nu, v_ci, v_co])
                                T.writes(bgemm_local[v_eps, v_nu, v_co, v_p])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                bgemm_local[v_eps, v_nu, v_co, v_p] = bgemm_local[v_eps, v_nu, v_co, v_p] + data_pack_shared[v_eps, v_nu, v_ci, v_p] * p1_shared[v_eps, v_nu, v_ci, v_co]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(2), T.int64(2), T.int64(1)):
                        with T.block("bgemm_local"):
                            v0 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(32) + ax0)
                            v1 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused % T.int64(32) // T.int64(16) * T.int64(2) + ax1)
                            v2 = T.axis.spatial(T.int64(512), eps_0_nu_0_co_0_p_0_fused % T.int64(16) * T.int64(32) + eps_1_nu_1_co_1_p_1_fused // T.int64(2) * T.int64(16) + eps_2_nu_2_co_2_p_2_fused // T.int64(8) * T.int64(2) + ax2)
                            v3 = T.axis.spatial(T.int64(16), eps_1_nu_1_co_1_p_1_fused % T.int64(2) * T.int64(8) + eps_2_nu_2_co_2_p_2_fused % T.int64(8) + ax3)
                            T.reads(bgemm_local[v0, v1, v2, v3])
                            T.writes(bgemm[v0, v1, v2, v3])
                            bgemm[v0, v1, v2, v3] = bgemm_local[v0, v1, v2, v3]
        for n_co_h_0_w_0_fused_0 in T.thread_binding(T.int64(256), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 512, "pragma_unroll_explicit": 1}):
            for n_co_h_0_w_0_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                    for ax2 in T.unroll(T.int64(2)):
                        for ax3 in T.unroll(T.int64(2)):
                            with T.block("inverse_init"):
                                v_co = T.axis.spatial(T.int64(512), (n_co_h_0_w_0_fused_0 * T.int64(32) + n_co_h_0_w_0_fused_1) // T.int64(16) + ax0)
                                v_p = T.axis.spatial(T.int64(16), (n_co_h_0_w_0_fused_0 * T.int64(32) + n_co_h_0_w_0_fused_1) % T.int64(16) + ax1)
                                v_vh, v_vw = T.axis.remap("SS", [ax2, ax3])
                                T.reads()
                                T.writes(inverse_local[v_co, v_p, v_vh, v_vw])
                                T.block_attr({"schedule_rule": "conv2d_nchw_winograd_inverse"})
                                inverse_local[v_co, v_p, v_vh, v_vw] = T.float32(0)
                            for ax4 in T.unroll(T.int64(4)):
                                for ax5 in T.unroll(T.int64(4)):
                                    with T.block("inverse_update"):
                                        v_co = T.axis.spatial(T.int64(512), (n_co_h_0_w_0_fused_0 * T.int64(32) + n_co_h_0_w_0_fused_1) // T.int64(16) + ax0)
                                        v_p = T.axis.spatial(T.int64(16), (n_co_h_0_w_0_fused_0 * T.int64(32) + n_co_h_0_w_0_fused_1) % T.int64(16) + ax1)
                                        v_vh, v_vw, v_r_a, v_r_b = T.axis.remap("SSRR", [ax2, ax3, ax4, ax5])
                                        T.reads(inverse_local[v_co, v_p, v_vh, v_vw], bgemm[v_r_a, v_r_b, v_co, v_p])
                                        T.writes(inverse_local[v_co, v_p, v_vh, v_vw])
                                        T.block_attr({"schedule_rule": "conv2d_nchw_winograd_inverse"})
                                        inverse_local[v_co, v_p, v_vh, v_vw] = inverse_local[v_co, v_p, v_vh, v_vw] + bgemm[v_r_a, v_r_b, v_co, v_p] * T.Select(v_r_a % T.int64(4) == T.int64(3) and v_vh % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_vh % T.int64(2) == T.int64(0), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_vh % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_vh % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_vh % T.int64(2) == T.int64(1), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_vh % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_vh % T.int64(2) == T.int64(1), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_vh % T.int64(2) == T.int64(0), T.float32(1), T.float32(0))))))))) * T.Select(v_r_b % T.int64(4) == T.int64(3) and v_vw % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_vw % T.int64(2) == T.int64(0), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_vw % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_vw % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_vw % T.int64(2) == T.int64(1), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_vw % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_vw % T.int64(2) == T.int64(1), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_vw % T.int64(2) == T.int64(0), T.float32(1), T.float32(0)))))))))
                for h_1, w_1 in T.grid(T.int64(2), T.int64(2)):
                    with T.block("conv2d_winograd"):
                        v_n = T.axis.spatial(T.int64(1), T.int64(0))
                        v_co = T.axis.spatial(T.int64(512), (n_co_h_0_w_0_fused_0 * T.int64(32) + n_co_h_0_w_0_fused_1) // T.int64(16))
                        v_h = T.axis.spatial(T.int64(7), (n_co_h_0_w_0_fused_0 * T.int64(32) + n_co_h_0_w_0_fused_1) % T.int64(16) // T.int64(4) * T.int64(2) + h_1)
                        v_w = T.axis.spatial(T.int64(7), (n_co_h_0_w_0_fused_0 * T.int64(32) + n_co_h_0_w_0_fused_1) % T.int64(4) * T.int64(2) + w_1)
                        T.where((n_co_h_0_w_0_fused_0 * T.int64(32) + n_co_h_0_w_0_fused_1) % T.int64(16) // T.int64(4) * T.int64(2) + h_1 < T.int64(7) and (n_co_h_0_w_0_fused_0 * T.int64(32) + n_co_h_0_w_0_fused_1) % T.int64(4) * T.int64(2) + w_1 < T.int64(7))
                        T.reads(inverse_local[v_co, v_n * T.int64(16) + v_h // T.int64(2) * T.int64(4) + v_w // T.int64(2), v_h % T.int64(2), v_w % T.int64(2)], p2[v_n, v_co, v_h, v_w], p3[v_n, v_co, T.int64(0), T.int64(0)], p4[v_n, v_co, T.int64(0), T.int64(0)])
                        T.writes(T_relu[v_n, v_co, v_h, v_w])
                        T_relu[v_n, v_co, v_h, v_w] = T.max((inverse_local[v_co, v_n * T.int64(16) + v_h // T.int64(2) * T.int64(4) + v_w // T.int64(2), v_h % T.int64(2), v_w % T.int64(2)] + p2[v_n, v_co, v_h, v_w]) * p3[v_n, v_co, T.int64(0), T.int64(0)] + p4[v_n, v_co, T.int64(0), T.int64(0)], T.float32(0))
b0 = sch.get_block(name="data_pack", func_name="main")
b1 = sch.get_block(name="bgemm", func_name="main")
b2 = sch.get_block(name="inverse", func_name="main")
b3 = sch.get_block(name="conv2d_winograd", func_name="main")
b4 = sch.get_block(name="T_add", func_name="main")
b5 = sch.get_block(name="T_multiply", func_name="main")
b6 = sch.get_block(name="T_add_1", func_name="main")
b7 = sch.get_block(name="T_relu", func_name="main")
b8 = sch.get_block(name="root", func_name="main")
b9, b10 = sch.get_producers(block=b2)
sch.compute_inline(block=b10)
b11, = sch.get_consumers(block=b2)
l12, l13, l14, l15 = sch.get_loops(block=b11)
l16, l17 = sch.split(loop=l14, factors=[None, 2], preserve_unit_iters=True)
l18, l19 = sch.split(loop=l15, factors=[None, 2], preserve_unit_iters=True)
sch.reorder(l16, l18, l17, l19)
sch.compute_at(block=b2, loop=l18, preserve_unit_loops=True, index=-1)
sch.set_scope(block=b2, buffer_index=0, storage_scope="local")
l20, l21, l22, l23, l24, l25, l26, l27, l28, l29 = sch.get_loops(block=b2)
sch.unroll(loop=l26)
sch.unroll(loop=l27)
sch.unroll(loop=l28)
sch.unroll(loop=l29)
b30, b31 = sch.get_producers(block=b0)
sch.compute_inline(block=b31)
b32, = sch.get_producers(block=b30)
l33, l34, l35, l36, l37, l38 = sch.get_loops(block=b0)
sch.reorder(l35, l36, l33, l34, l37, l38)
sch.unroll(loop=l33)
sch.unroll(loop=l34)
sch.unroll(loop=l37)
sch.unroll(loop=l38)
l39 = sch.fuse(l35, l36, preserve_unit_iters=True)
v40 = sch.sample_categorical(candidates=[32, 64, 128, 256, 512, 1024], probs=[0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666], decision=0)
l41, l42 = sch.split(loop=l39, factors=[None, v40], preserve_unit_iters=True)
sch.bind(loop=l41, thread_axis="blockIdx.x")
sch.bind(loop=l42, thread_axis="threadIdx.x")
b43 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b43, loop=l42, preserve_unit_loops=True, index=-1)
sch.compute_at(block=b30, loop=l42, preserve_unit_loops=True, index=-1)
sch.set_scope(block=b30, buffer_index=0, storage_scope="local")
sch.compute_inline(block=b32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l44, l45, l46, l47, l48 = sch.get_loops(block=b1)
v49, v50, v51, v52, v53 = sch.sample_perfect_tile(loop=l44, n=5, max_innermost_factor=64, decision=[4, 1, 1, 1, 1])
l54, l55, l56, l57, l58 = sch.split(loop=l44, factors=[v49, v50, v51, v52, v53], preserve_unit_iters=True)
v59, v60, v61, v62, v63 = sch.sample_perfect_tile(loop=l45, n=5, max_innermost_factor=64, decision=[2, 1, 1, 2, 1])
l64, l65, l66, l67, l68 = sch.split(loop=l45, factors=[v59, v60, v61, v62, v63], preserve_unit_iters=True)
v69, v70, v71, v72, v73 = sch.sample_perfect_tile(loop=l46, n=5, max_innermost_factor=64, decision=[16, 2, 8, 2, 1])
l74, l75, l76, l77, l78 = sch.split(loop=l46, factors=[v69, v70, v71, v72, v73], preserve_unit_iters=True)
v79, v80, v81, v82, v83 = sch.sample_perfect_tile(loop=l47, n=5, max_innermost_factor=64, decision=[1, 2, 8, 1, 1])
l84, l85, l86, l87, l88 = sch.split(loop=l47, factors=[v79, v80, v81, v82, v83], preserve_unit_iters=True)
v89, v90, v91 = sch.sample_perfect_tile(loop=l48, n=3, max_innermost_factor=64, decision=[32, 2, 8])
l92, l93, l94 = sch.split(loop=l48, factors=[v89, v90, v91], preserve_unit_iters=True)
sch.reorder(l54, l64, l74, l84, l55, l65, l75, l85, l56, l66, l76, l86, l92, l93, l57, l67, l77, l87, l94, l58, l68, l78, l88)
l95 = sch.fuse(l54, l64, l74, l84, preserve_unit_iters=True)
sch.bind(loop=l95, thread_axis="blockIdx.x")
l96 = sch.fuse(l55, l65, l75, l85, preserve_unit_iters=True)
sch.bind(loop=l96, thread_axis="vthread.x")
l97 = sch.fuse(l56, l66, l76, l86, preserve_unit_iters=True)
sch.bind(loop=l97, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b98 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b98, loop=l97, preserve_unit_loops=True, index=-1)
b99 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b99, loop=l92, preserve_unit_loops=True, index=-1)
l100, l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b99)
l108 = sch.fuse(l104, l105, l106, l107, preserve_unit_iters=True)
v109 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b99, ann_key="meta_schedule.cooperative_fetch", ann_val=v109)
b110 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b110, loop=l92, preserve_unit_loops=True, index=-1)
l111, l112, l113, l114, l115, l116, l117, l118 = sch.get_loops(block=b110)
l119 = sch.fuse(l115, l116, l117, l118, preserve_unit_iters=True)
v120 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b110, ann_key="meta_schedule.cooperative_fetch", ann_val=v120)
sch.reverse_compute_inline(block=b7)
sch.reverse_compute_inline(block=b6)
sch.reverse_compute_inline(block=b5)
sch.reverse_compute_inline(block=b4)
v121 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b8, ann_key="meta_schedule.unroll_explicit", ann_val=v121)
l122, l123, l124, l125, l126, l127 = sch.get_loops(block=b3)
l128 = sch.fuse(l122, l123, l124, l125, preserve_unit_iters=True)
v129 = sch.sample_categorical(candidates=[32, 64, 128, 256, 512, 1024], probs=[0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666], decision=0)
l130, l131 = sch.split(loop=l128, factors=[None, v129], preserve_unit_iters=True)
sch.bind(loop=l130, thread_axis="blockIdx.x")
sch.bind(loop=l131, thread_axis="threadIdx.x")
sch.enter_postproc()
sch.unannotate(block_or_loop=b99, ann_key="meta_schedule.cooperative_fetch")
l132, l133, l134, l135, l136 = sch.get_loops(block=b99)
l137, l138 = sch.split(loop=l136, factors=[None, 64], preserve_unit_iters=True)
sch.bind(loop=l138, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b110, ann_key="meta_schedule.cooperative_fetch")
l139, l140, l141, l142, l143 = sch.get_loops(block=b110)
l144, l145 = sch.split(loop=l143, factors=[None, 64], preserve_unit_iters=True)
sch.bind(loop=l145, thread_axis="threadIdx.x")
b146 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b146, ann_key="meta_schedule.unroll_explicit")
b147, b148, b149, b150, b151, b152, b153, b154, b155 = sch.get_child_blocks(b146)
l156, l157, l158, l159, l160, l161 = sch.get_loops(block=b147)
l162, l163, l164, l165, l166, l167 = sch.get_loops(block=b148)
sch.annotate(block_or_loop=l162, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l162, ann_key="pragma_unroll_explicit", ann_val=1)
l168, l169, l170, l171, l172, l173 = sch.get_loops(block=b149)
l174, l175, l176, l177, l178, l179 = sch.get_loops(block=b150)
l180, l181, l182, l183, l184, l185 = sch.get_loops(block=b151)
l186, l187, l188, l189, l190, l191, l192, l193, l194, l195, l196, l197, l198, l199 = sch.get_loops(block=b152)
sch.annotate(block_or_loop=l186, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l186, ann_key="pragma_unroll_explicit", ann_val=1)
l200, l201, l202, l203, l204, l205, l206 = sch.get_loops(block=b153)
l207, l208, l209, l210, l211, l212, l213, l214 = sch.get_loops(block=b154)
sch.annotate(block_or_loop=l207, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l207, ann_key="pragma_unroll_explicit", ann_val=1)
l215, l216, l217, l218 = sch.get_loops(block=b155)
b219 = sch.get_block(name="data_pack", func_name="main")
l220, l221, l222, l223, l224, l225 = sch.get_loops(block=b219)
b226 = sch.decompose_reduction(block=b219, loop=l224)
b227 = sch.get_block(name="bgemm", func_name="main")
l228, l229, l230, l231, l232, l233, l234, l235, l236, l237, l238, l239, l240, l241 = sch.get_loops(block=b227)
b242 = sch.decompose_reduction(block=b227, loop=l231)
b243 = sch.get_block(name="inverse", func_name="main")
l244, l245, l246, l247, l248, l249, l250, l251 = sch.get_loops(block=b243)
b252 = sch.decompose_reduction(block=b243, loop=l250)
2023-11-11 01:34:49 [INFO] [task_scheduler.cc:121] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #135: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/home2/xiachunwei/Software/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/home2/xiachunwei/Software/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/home2/xiachunwei/Software/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/home2/xiachunwei/Software/tvm/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/home2/xiachunwei/Software/tvm/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/home2/xiachunwei/Software/tvm/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/home2/xiachunwei/Software/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/home2/xiachunwei/Software/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  4: _ZN3tvm7runtime13PackedFun
  3: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  2: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  0: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  File "/home2/xiachunwei/Software/tvm/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(512), T.int64(7), T.int64(7)), "float32"), p1: T.Buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(512)), "float32"), p2: T.Buffer((T.int64(1), T.int64(512), T.int64(7), T.int64(7)), "float32"), p3: T.Buffer((T.int64(1), T.int64(512), T.int64(1), T.int64(1)), "float32"), p4: T.Buffer((T.int64(1), T.int64(512), T.int64(1), T.int64(1)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(512), T.int64(7), T.int64(7)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        input_tile_local = T.alloc_buffer((T.int64(512), T.int64(16), T.int64(4), T.int64(4)), scope="local")
        data_pack = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(16)))
        bgemm = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(16)))
        inverse_local = T.alloc_buffer((T.int64(512), T.int64(16), T.int64(2), T.int64(2)), scope="local")
        data_pack_local = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(16)), scope="local")
        bgemm_local = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(16)), scope="local")
        data_pack_shared = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(16)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(512)), scope="shared")
        for ci_p_fused_0 in T.thread_binding(T.int64(256), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 16, "pragma_unroll_explicit": 1}):
            for ci_p_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(1), T.int64(4), T.int64(4)):
                    with T.block("input_tile"):
                        v_ci = T.axis.spatial(T.int64(512), ci_p_fused_0 * T.int64(2) + ci_p_fused_1 // T.int64(16) + ax0)
                        v_p = T.axis.spatial(T.int64(16), ci_p_fused_1 % T.int64(16) + ax1)
                        v_eps, v_nu = T.axis.remap("SS", [ax2, ax3])
                        T.reads(p0[v_p // T.int64(16), v_ci, v_p % T.int64(16) // T.int64(4) * T.int64(2) + v_eps - T.int64(1), v_p % T.int64(4) * T.int64(2) + v_nu - T.int64(1)])
                        T.writes(input_tile_local[v_ci, v_p, v_eps, v_nu])
                        T.block_attr({"schedule_rule": "None"})
                        input_tile_local[v_ci, v_p, v_eps, v_nu] = T.if_then_else(T.int64(1) <= v_p % T.int64(16) // T.int64(4) * T.int64(2) + v_eps and v_p % T.int64(16) // T.int64(4) * T.int64(2) + v_eps < T.int64(8) and T.int64(1) <= v_p % T.int64(4) * T.int64(2) + v_nu and v_p % T.int64(4) * T.int64(2) + v_nu < T.int64(8), p0[v_p // T.int64(16), v_ci, v_p % T.int64(16) // T.int64(4) * T.int64(2) + v_eps - T.int64(1), v_p % T.int64(4) * T.int64(2) + v_nu - T.int64(1)], T.float32(0))
                for eps in T.unroll(T.int64(4)):
                    for nu in T.unroll(T.int64(4)):
                        with T.block("data_pack_init"):
                            v_eps, v_nu = T.axis.remap("SS", [eps, nu])
                            v_ci = T.axis.spatial(T.int64(512), (ci_p_fused_0 * T.int64(32) + ci_p_fused_1) // T.int64(16))
                            v_p = T.axis.spatial(T.int64(16), (ci_p_fused_0 * T.int64(32) + ci_p_fused_1) % T.int64(16))
                            T.reads()
                            T.writes(data_pack_local[v_eps, v_nu, v_ci, v_p])
                            T.block_attr({"schedule_rule": "conv2d_nchw_winograd_data_pack"})
                            data_pack_local[v_eps, v_nu, v_ci, v_p] = T.float32(0)
                        for r_a in T.unroll(T.int64(4)):
                            for r_b in T.unroll(T.int64(4)):
                                with T.block("data_pack_update"):
                                    v_eps, v_nu = T.axis.remap("SS", [eps, nu])
                                    v_ci = T.axis.spatial(T.int64(512), (ci_p_fused_0 * T.int64(32) + ci_p_fused_1) // T.int64(16))
                                    v_p = T.axis.spatial(T.int64(16), (ci_p_fused_0 * T.int64(32) + ci_p_fused_1) % T.int64(16))
                                    v_r_a, v_r_b = T.axis.remap("RR", [r_a, r_b])
                                    T.reads(data_pack_local[v_eps, v_nu, v_ci, v_p], input_tile_local[v_ci, v_p, v_r_a, v_r_b])
                                    T.writes(data_pack_local[v_eps, v_nu, v_ci, v_p])
                                    T.block_attr({"schedule_rule": "conv2d_nchw_winograd_data_pack"})
                                    data_pack_local[v_eps, v_nu, v_ci, v_p] = data_pack_local[v_eps, v_nu, v_ci, v_p] + input_tile_local[v_ci, v_p, v_r_a, v_r_b] * T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(3), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(1), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(0), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(3), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(1), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(0), T.float32(1), T.float32(0))))))))))))))))) * T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(3), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(1), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(0), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(3), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(1), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(0), T.float32(1), T.float32(0)))))))))))))))))
                for ax0, ax1, ax2, ax3 in T.grid(T.int64(4), T.int64(4), T.int64(1), T.int64(1)):
                    with T.block("data_pack_local"):
                        v0, v1 = T.axis.remap("SS", [ax0, ax1])
                        v2 = T.axis.spatial(T.int64(512), ci_p_fused_0 * T.int64(2) + ci_p_fused_1 // T.int64(16) + ax2)
                        v3 = T.axis.spatial(T.int64(16), ci_p_fused_1 % T.int64(16) + ax3)
                        T.reads(data_pack_local[v0, v1, v2, v3])
                        T.writes(data_pack[v0, v1, v2, v3])
                        data_pack[v0, v1, v2, v3] = data_pack_local[v0, v1, v2, v3]
        for eps_0_nu_0_co_0_p_0_fused in T.thread_binding(T.int64(128), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 16, "pragma_unroll_explicit": 1}):
            for eps_1_nu_1_co_1_p_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for eps_2_nu_2_co_2_p_2_fused in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                    for eps_3_init, nu_3_init, co_3_init, p_3_init, eps_4_init, nu_4_init, co_4_init, p_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(2)):
                        with T.block("bgemm_init"):
                            v_eps = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(64) * T.int64(2) + eps_2_nu_2_co_2_p_2_fused // T.int64(128) + eps_3_init + eps_4_init)
                            v_nu = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused % T.int64(64) // T.int64(32) * T.int64(2) + eps_2_nu_2_co_2_p_2_fused % T.int64(128) // T.int64(64) + nu_3_init + nu_4_init)
                            v_co = T.axis.spatial(T.int64(512), eps_0_nu_0_co_0_p_0_fused % T.int64(32) * T.int64(16) + eps_2_nu_2_co_2_p_2_fused % T.int64(64) // T.int64(8) * T.int64(2) + co_3_init * T.int64(2) + co_4_init)
                            v_p = T.axis.spatial(T.int64(16), eps_2_nu_2_co_2_p_2_fused % T.int64(8) * T.int64(2) + p_3_init * T.int64(2) + p_4_init)
                            T.reads()
                            T.writes(bgemm_local[v_eps, v_nu, v_co, v_p])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            bgemm_local[v_eps, v_nu, v_co, v_p] = T.float32(0)
                    for ci_0 in range(T.int64(32)):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(4)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                                with T.block("data_pack_shared"):
                                    v0 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(64) * T.int64(2) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1) // T.int64(512))
                                    v1 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused % T.int64(64) // T.int64(32) * T.int64(2) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1) % T.int64(512) // T.int64(256))
                                    v2 = T.axis.spatial(T.int64(512), ci_0 * T.int64(16) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1) % T.int64(256) // T.int64(16))
                                    v3 = T.axis.spatial(T.int64(16), (ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1) % T.int64(16))
                                    T.reads(data_pack[v0, v1, v2, v3])
                                    T.writes(data_pack_shared[v0, v1, v2, v3])
                                    data_pack_shared[v0, v1, v2, v3] = data_pack[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(1)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("p1_shared"):
                                        v0 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(64) * T.int64(2) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(1024) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) // T.int64(512))
                                        v1 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused % T.int64(64) // T.int64(32) * T.int64(2) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(1024) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) % T.int64(512) // T.int64(256))
                                        v2 = T.axis.spatial(T.int64(512), ci_0 * T.int64(16) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(1024) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) % T.int64(256) // T.int64(16))
                                        v3 = T.axis.spatial(T.int64(512), eps_0_nu_0_co_0_p_0_fused % T.int64(32) * T.int64(16) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(1024) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) % T.int64(16))
                                        T.reads(p1[v0, v1, v2, v3])
                                        T.writes(p1_shared[v0, v1, v2, v3])
                                        p1_shared[v0, v1, v2, v3] = p1[v0, v1, v2, v3]
                        for ci_1, eps_3, nu_3, co_3, p_3, ci_2, eps_4, nu_4, co_4, p_4 in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(16), T.int64(1), T.int64(1), T.int64(2), T.int64(2)):
                            with T.block("bgemm_update"):
                                v_eps = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(64) * T.int64(2) + eps_2_nu_2_co_2_p_2_fused // T.int64(128) + eps_3 + eps_4)
                                v_nu = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused % T.int64(64) // T.int64(32) * T.int64(2) + eps_2_nu_2_co_2_p_2_fused % T.int64(128) // T.int64(64) + nu_3 + nu_4)
                                v_co = T.axis.spatial(T.int64(512), eps_0_nu_0_co_0_p_0_fused % T.int64(32) * T.int64(16) + eps_2_nu_2_co_2_p_2_fused % T.int64(64) // T.int64(8) * T.int64(2) + co_3 * T.int64(2) + co_4)
                                v_p = T.axis.spatial(T.int64(16), eps_2_nu_2_co_2_p_2_fused % T.int64(8) * T.int64(2) + p_3 * T.int64(2) + p_4)
                                v_ci = T.axis.reduce(T.int64(512), ci_0 * T.int64(16) + ci_1 * T.int64(16) + ci_2)
                                T.reads(bgemm_local[v_eps, v_nu, v_co, v_p], data_pack_shared[v_eps, v_nu, v_ci, v_p], p1_shared[v_eps, v_nu, v_ci, v_co])
                                T.writes(bgemm_local[v_eps, v_nu, v_co, v_p])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                bgemm_local[v_eps, v_nu, v_co, v_p] = bgemm_local[v_eps, v_nu, v_co, v_p] + data_pack_shared[v_eps, v_nu, v_ci, v_p] * p1_shared[v_eps, v_nu, v_ci, v_co]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(1), T.int64(2), T.int64(2)):
                        with T.block("bgemm_local"):
                            v0 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(64) * T.int64(2) + eps_2_nu_2_co_2_p_2_fused // T.int64(128) + ax0)
                            v1 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused % T.int64(64) // T.int64(32) * T.int64(2) + eps_2_nu_2_co_2_p_2_fused % T.int64(128) // T.int64(64) + ax1)
                            v2 = T.axis.spatial(T.int64(512), eps_0_nu_0_co_0_p_0_fused % T.int64(32) * T.int64(16) + eps_2_nu_2_co_2_p_2_fused % T.int64(64) // T.int64(8) * T.int64(2) + ax2)
                            v3 = T.axis.spatial(T.int64(16), eps_2_nu_2_co_2_p_2_fused % T.int64(8) * T.int64(2) + ax3)
                            T.reads(bgemm_local[v0, v1, v2, v3])
                            T.writes(bgemm[v0, v1, v2, v3])
                            bgemm[v0, v1, v2, v3] = bgemm_local[v0, v1, v2, v3]
        for n_co_h_0_w_0_fused_0 in T.thread_binding(T.int64(32), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 16, "pragma_unroll_explicit": 1}):
            for n_co_h_0_w_0_fused_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                    for ax2 in T.unroll(T.int64(2)):
                        for ax3 in T.unroll(T.int64(2)):
                            with T.block("inverse_init"):
                                v_co = T.axis.spatial(T.int64(512), (n_co_h_0_w_0_fused_0 * T.int64(256) + n_co_h_0_w_0_fused_1) // T.int64(16) + ax0)
                                v_p = T.axis.spatial(T.int64(16), (n_co_h_0_w_0_fused_0 * T.int64(256) + n_co_h_0_w_0_fused_1) % T.int64(16) + ax1)
                                v_vh, v_vw = T.axis.remap("SS", [ax2, ax3])
                                T.reads()
                                T.writes(inverse_local[v_co, v_p, v_vh, v_vw])
                                T.block_attr({"schedule_rule": "conv2d_nchw_winograd_inverse"})
                                inverse_local[v_co, v_p, v_vh, v_vw] = T.float32(0)
                            for ax4 in T.unroll(T.int64(4)):
                                for ax5 in T.unroll(T.int64(4)):
                                    with T.block("inverse_update"):
                                        v_co = T.axis.spatial(T.int64(512), (n_co_h_0_w_0_fused_0 * T.int64(256) + n_co_h_0_w_0_fused_1) // T.int64(16) + ax0)
                                        v_p = T.axis.spatial(T.int64(16), (n_co_h_0_w_0_fused_0 * T.int64(256) + n_co_h_0_w_0_fused_1) % T.int64(16) + ax1)
                                        v_vh, v_vw, v_r_a, v_r_b = T.axis.remap("SSRR", [ax2, ax3, ax4, ax5])
                                        T.reads(inverse_local[v_co, v_p, v_vh, v_vw], bgemm[v_r_a, v_r_b, v_co, v_p])
                                        T.writes(inverse_local[v_co, v_p, v_vh, v_vw])
                                        T.block_attr({"schedule_rule": "conv2d_nchw_winograd_inverse"})
                                        inverse_local[v_co, v_p, v_vh, v_vw] = inverse_local[v_co, v_p, v_vh, v_vw] + bgemm[v_r_a, v_r_b, v_co, v_p] * T.Select(v_r_a % T.int64(4) == T.int64(3) and v_vh % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_vh % T.int64(2) == T.int64(0), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_vh % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_vh % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_vh % T.int64(2) == T.int64(1), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_vh % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_vh % T.int64(2) == T.int64(1), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_vh % T.int64(2) == T.int64(0), T.float32(1), T.float32(0))))))))) * T.Select(v_r_b % T.int64(4) == T.int64(3) and v_vw % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_vw % T.int64(2) == T.int64(0), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_vw % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_vw % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_vw % T.int64(2) == T.int64(1), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_vw % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_vw % T.int64(2) == T.int64(1), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_vw % T.int64(2) == T.int64(0), T.float32(1), T.float32(0)))))))))
                for h_1, w_1 in T.grid(T.int64(2), T.int64(2)):
                    with T.block("conv2d_winograd"):
                        v_n = T.axis.spatial(T.int64(1), T.int64(0))
                        v_co = T.axis.spatial(T.int64(512), (n_co_h_0_w_0_fused_0 * T.int64(256) + n_co_h_0_w_0_fused_1) // T.int64(16))
                        v_h = T.axis.spatial(T.int64(7), (n_co_h_0_w_0_fused_0 * T.int64(256) + n_co_h_0_w_0_fused_1) % T.int64(16) // T.int64(4) * T.int64(2) + h_1)
                        v_w = T.axis.spatial(T.int64(7), (n_co_h_0_w_0_fused_0 * T.int64(256) + n_co_h_0_w_0_fused_1) % T.int64(4) * T.int64(2) + w_1)
                        T.where((n_co_h_0_w_0_fused_0 * T.int64(256) + n_co_h_0_w_0_fused_1) % T.int64(16) // T.int64(4) * T.int64(2) + h_1 < T.int64(7) and (n_co_h_0_w_0_fused_0 * T.int64(256) + n_co_h_0_w_0_fused_1) % T.int64(4) * T.int64(2) + w_1 < T.int64(7))
                        T.reads(inverse_local[v_co, v_n * T.int64(16) + v_h // T.int64(2) * T.int64(4) + v_w // T.int64(2), v_h % T.int64(2), v_w % T.int64(2)], p2[v_n, v_co, v_h, v_w], p3[v_n, v_co, T.int64(0), T.int64(0)], p4[v_n, v_co, T.int64(0), T.int64(0)])
                        T.writes(T_relu[v_n, v_co, v_h, v_w])
                        T_relu[v_n, v_co, v_h, v_w] = T.max((inverse_local[v_co, v_n * T.int64(16) + v_h // T.int64(2) * T.int64(4) + v_w // T.int64(2), v_h % T.int64(2), v_w % T.int64(2)] + p2[v_n, v_co, v_h, v_w]) * p3[v_n, v_co, T.int64(0), T.int64(0)] + p4[v_n, v_co, T.int64(0), T.int64(0)], T.float32(0))
b0 = sch.get_block(name="data_pack", func_name="main")
b1 = sch.get_block(name="bgemm", func_name="main")
b2 = sch.get_block(name="inverse", func_name="main")
b3 = sch.get_block(name="conv2d_winograd", func_name="main")
b4 = sch.get_block(name="T_add", func_name="main")
b5 = sch.get_block(name="T_multiply", func_name="main")
b6 = sch.get_block(name="T_add_1", func_name="main")
b7 = sch.get_block(name="T_relu", func_name="main")
b8 = sch.get_block(name="root", func_name="main")
b9, b10 = sch.get_producers(block=b2)
sch.compute_inline(block=b10)
b11, = sch.get_consumers(block=b2)
l12, l13, l14, l15 = sch.get_loops(block=b11)
l16, l17 = sch.split(loop=l14, factors=[None, 2], preserve_unit_iters=True)
l18, l19 = sch.split(loop=l15, factors=[None, 2], preserve_unit_iters=True)
sch.reorder(l16, l18, l17, l19)
sch.compute_at(block=b2, loop=l18, preserve_unit_loops=True, index=-1)
sch.set_scope(block=b2, buffer_index=0, storage_scope="local")
l20, l21, l22, l23, l24, l25, l26, l27, l28, l29 = sch.get_loops(block=b2)
sch.unroll(loop=l26)
sch.unroll(loop=l27)
sch.unroll(loop=l28)
sch.unroll(loop=l29)
b30, b31 = sch.get_producers(block=b0)
sch.compute_inline(block=b31)
b32, = sch.get_producers(block=b30)
l33, l34, l35, l36, l37, l38 = sch.get_loops(block=b0)
sch.reorder(l35, l36, l33, l34, l37, l38)
sch.unroll(loop=l33)
sch.unroll(loop=l34)
sch.unroll(loop=l37)
sch.unroll(loop=l38)
l39 = sch.fuse(l35, l36, preserve_unit_iters=True)
v40 = sch.sample_categorical(candidates=[32, 64, 128, 256, 512, 1024], probs=[0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666], decision=0)
l41, l42 = sch.split(loop=l39, factors=[None, v40], preserve_unit_iters=True)
sch.bind(loop=l41, thread_axis="blockIdx.x")
sch.bind(loop=l42, thread_axis="threadIdx.x")
b43 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b43, loop=l42, preserve_unit_loops=True, index=-1)
sch.compute_at(block=b30, loop=l42, preserve_unit_loops=True, index=-1)
sch.set_scope(block=b30, buffer_index=0, storage_scope="local")
sch.compute_inline(block=b32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l44, l45, l46, l47, l48 = sch.get_loops(block=b1)
v49, v50, v51, v52, v53 = sch.sample_perfect_tile(loop=l44, n=5, max_innermost_factor=64, decision=[2, 1, 2, 1, 1])
l54, l55, l56, l57, l58 = sch.split(loop=l44, factors=[v49, v50, v51, v52, v53], preserve_unit_iters=True)
v59, v60, v61, v62, v63 = sch.sample_perfect_tile(loop=l45, n=5, max_innermost_factor=64, decision=[2, 1, 2, 1, 1])
l64, l65, l66, l67, l68 = sch.split(loop=l45, factors=[v59, v60, v61, v62, v63], preserve_unit_iters=True)
v69, v70, v71, v72, v73 = sch.sample_perfect_tile(loop=l46, n=5, max_innermost_factor=64, decision=[32, 1, 8, 1, 2])
l74, l75, l76, l77, l78 = sch.split(loop=l46, factors=[v69, v70, v71, v72, v73], preserve_unit_iters=True)
v79, v80, v81, v82, v83 = sch.sample_perfect_tile(loop=l47, n=5, max_innermost_factor=64, decision=[1, 1, 8, 1, 2])
l84, l85, l86, l87, l88 = sch.split(loop=l47, factors=[v79, v80, v81, v82, v83], preserve_unit_iters=True)
v89, v90, v91 = sch.sample_perfect_tile(loop=l48, n=3, max_innermost_factor=64, decision=[32, 1, 16])
l92, l93, l94 = sch.split(loop=l48, factors=[v89, v90, v91], preserve_unit_iters=True)
sch.reorder(l54, l64, l74, l84, l55, l65, l75, l85, l56, l66, l76, l86, l92, l93, l57, l67, l77, l87, l94, l58, l68, l78, l88)
l95 = sch.fuse(l54, l64, l74, l84, preserve_unit_iters=True)
sch.bind(loop=l95, thread_axis="blockIdx.x")
l96 = sch.fuse(l55, l65, l75, l85, preserve_unit_iters=True)
sch.bind(loop=l96, thread_axis="vthread.x")
l97 = sch.fuse(l56, l66, l76, l86, preserve_unit_iters=True)
sch.bind(loop=l97, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b98 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b98, loop=l97, preserve_unit_loops=True, index=-1)
b99 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b99, loop=l92, preserve_unit_loops=True, index=-1)
l100, l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b99)
l108 = sch.fuse(l104, l105, l106, l107, preserve_unit_iters=True)
v109 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b99, ann_key="meta_schedule.cooperative_fetch", ann_val=v109)
b110 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b110, loop=l92, preserve_unit_loops=True, index=-1)
l111, l112, l113, l114, l115, l116, l117, l118 = sch.get_loops(block=b110)
l119 = sch.fuse(l115, l116, l117, l118, preserve_unit_iters=True)
v120 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b110, ann_key="meta_schedule.cooperative_fetch", ann_val=v120)
sch.reverse_compute_inline(block=b7)
sch.reverse_compute_inline(block=b6)
sch.reverse_compute_inline(block=b5)
sch.reverse_compute_inline(block=b4)
v121 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=1)
sch.annotate(block_or_loop=b8, ann_key="meta_schedule.unroll_explicit", ann_val=v121)
l122, l123, l124, l125, l126, l127 = sch.get_loops(block=b3)
l128 = sch.fuse(l122, l123, l124, l125, preserve_unit_iters=True)
v129 = sch.sample_categorical(candidates=[32, 64, 128, 256, 512, 1024], probs=[0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666], decision=3)
l130, l131 = sch.split(loop=l128, factors=[None, v129], preserve_unit_iters=True)
sch.bind(loop=l130, thread_axis="blockIdx.x")
sch.bind(loop=l131, thread_axis="threadIdx.x")
sch.enter_postproc()
sch.unannotate(block_or_loop=b99, ann_key="meta_schedule.cooperative_fetch")
l132, l133, l134, l135, l136 = sch.get_loops(block=b99)
l137, l138 = sch.split(loop=l136, factors=[None, 256], preserve_unit_iters=True)
sch.bind(loop=l138, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b110, ann_key="meta_schedule.cooperative_fetch")
l139, l140, l141, l142, l143 = sch.get_loops(block=b110)
l144, l145, l146 = sch.split(loop=l143, factors=[None, 256, 4], preserve_unit_iters=True)
sch.vectorize(loop=l146)
sch.bind(loop=l145, thread_axis="threadIdx.x")
b147 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b147, ann_key="meta_schedule.unroll_explicit")
b148, b149, b150, b151, b152, b153, b154, b155, b156 = sch.get_child_blocks(b147)
l157, l158, l159, l160, l161, l162 = sch.get_loops(block=b148)
l163, l164, l165, l166, l167, l168 = sch.get_loops(block=b149)
sch.annotate(block_or_loop=l163, ann_key="pragma_auto_unroll_max_step", ann_val=16)
sch.annotate(block_or_loop=l163, ann_key="pragma_unroll_explicit", ann_val=1)
l169, l170, l171, l172, l173, l174 = sch.get_loops(block=b150)
l175, l176, l177, l178, l179, l180 = sch.get_loops(block=b151)
l181, l182, l183, l184, l185, l186, l187 = sch.get_loops(block=b152)
l188, l189, l190, l191, l192, l193, l194, l195, l196, l197, l198, l199, l200, l201 = sch.get_loops(block=b153)
sch.annotate(block_or_loop=l188, ann_key="pragma_auto_unroll_max_step", ann_val=16)
sch.annotate(block_or_loop=l188, ann_key="pragma_unroll_explicit", ann_val=1)
l202, l203, l204, l205, l206, l207, l208 = sch.get_loops(block=b154)
l209, l210, l211, l212, l213, l214, l215, l216 = sch.get_loops(block=b155)
sch.annotate(block_or_loop=l209, ann_key="pragma_auto_unroll_max_step", ann_val=16)
sch.annotate(block_or_loop=l209, ann_key="pragma_unroll_explicit", ann_val=1)
l217, l218, l219, l220 = sch.get_loops(block=b156)
b221 = sch.get_block(name="data_pack", func_name="main")
l222, l223, l224, l225, l226, l227 = sch.get_loops(block=b221)
b228 = sch.decompose_reduction(block=b221, loop=l226)
b229 = sch.get_block(name="bgemm", func_name="main")
l230, l231, l232, l233, l234, l235, l236, l237, l238, l239, l240, l241, l242, l243 = sch.get_loops(block=b229)
b244 = sch.decompose_reduction(block=b229, loop=l233)
b245 = sch.get_block(name="inverse", func_name="main")
l246, l247, l248, l249, l250, l251, l252, l253 = sch.get_loops(block=b245)
b254 = sch.decompose_reduction(block=b245, loop=l252)
2023-11-11 01:34:49 [INFO] [task_scheduler.cc:121] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #136: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/home2/xiachunwei/Software/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/home2/xiachunwei/Software/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/home2/xiachunwei/Software/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/home2/xiachunwei/Software/tvm/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/home2/xiachunwei/Software/tvm/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/home2/xiachunwei/Software/tvm/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/home2/xiachunwei/Software/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/home2/xiachunwei/Software/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  4: _ZN3tvm7runtime13PackedFun
  3: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  2: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  0: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  File "/home2/xiachunwei/Software/tvm/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(512), T.int64(7), T.int64(7)), "float32"), p1: T.Buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(512)), "float32"), p2: T.Buffer((T.int64(1), T.int64(512), T.int64(7), T.int64(7)), "float32"), p3: T.Buffer((T.int64(1), T.int64(512), T.int64(1), T.int64(1)), "float32"), p4: T.Buffer((T.int64(1), T.int64(512), T.int64(1), T.int64(1)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(512), T.int64(7), T.int64(7)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        input_tile_local = T.alloc_buffer((T.int64(512), T.int64(16), T.int64(4), T.int64(4)), scope="local")
        data_pack = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(16)))
        bgemm = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(16)))
        inverse_local = T.alloc_buffer((T.int64(512), T.int64(16), T.int64(2), T.int64(2)), scope="local")
        data_pack_local = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(16)), scope="local")
        bgemm_local = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(16)), scope="local")
        data_pack_shared = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(16)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(512)), scope="shared")
        for ci_p_fused_0 in T.thread_binding(T.int64(128), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 512, "pragma_unroll_explicit": 1}):
            for ci_p_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(1), T.int64(4), T.int64(4)):
                    with T.block("input_tile"):
                        v_ci = T.axis.spatial(T.int64(512), ci_p_fused_0 * T.int64(4) + ci_p_fused_1 // T.int64(16) + ax0)
                        v_p = T.axis.spatial(T.int64(16), ci_p_fused_1 % T.int64(16) + ax1)
                        v_eps, v_nu = T.axis.remap("SS", [ax2, ax3])
                        T.reads(p0[v_p // T.int64(16), v_ci, v_p % T.int64(16) // T.int64(4) * T.int64(2) + v_eps - T.int64(1), v_p % T.int64(4) * T.int64(2) + v_nu - T.int64(1)])
                        T.writes(input_tile_local[v_ci, v_p, v_eps, v_nu])
                        T.block_attr({"schedule_rule": "None"})
                        input_tile_local[v_ci, v_p, v_eps, v_nu] = T.if_then_else(T.int64(1) <= v_p % T.int64(16) // T.int64(4) * T.int64(2) + v_eps and v_p % T.int64(16) // T.int64(4) * T.int64(2) + v_eps < T.int64(8) and T.int64(1) <= v_p % T.int64(4) * T.int64(2) + v_nu and v_p % T.int64(4) * T.int64(2) + v_nu < T.int64(8), p0[v_p // T.int64(16), v_ci, v_p % T.int64(16) // T.int64(4) * T.int64(2) + v_eps - T.int64(1), v_p % T.int64(4) * T.int64(2) + v_nu - T.int64(1)], T.float32(0))
                for eps in T.unroll(T.int64(4)):
                    for nu in T.unroll(T.int64(4)):
                        with T.block("data_pack_init"):
                            v_eps, v_nu = T.axis.remap("SS", [eps, nu])
                            v_ci = T.axis.spatial(T.int64(512), (ci_p_fused_0 * T.int64(64) + ci_p_fused_1) // T.int64(16))
                            v_p = T.axis.spatial(T.int64(16), (ci_p_fused_0 * T.int64(64) + ci_p_fused_1) % T.int64(16))
                            T.reads()
                            T.writes(data_pack_local[v_eps, v_nu, v_ci, v_p])
                            T.block_attr({"schedule_rule": "conv2d_nchw_winograd_data_pack"})
                            data_pack_local[v_eps, v_nu, v_ci, v_p] = T.float32(0)
                        for r_a in T.unroll(T.int64(4)):
                            for r_b in T.unroll(T.int64(4)):
                                with T.block("data_pack_update"):
                                    v_eps, v_nu = T.axis.remap("SS", [eps, nu])
                                    v_ci = T.axis.spatial(T.int64(512), (ci_p_fused_0 * T.int64(64) + ci_p_fused_1) // T.int64(16))
                                    v_p = T.axis.spatial(T.int64(16), (ci_p_fused_0 * T.int64(64) + ci_p_fused_1) % T.int64(16))
                                    v_r_a, v_r_b = T.axis.remap("RR", [r_a, r_b])
                                    T.reads(data_pack_local[v_eps, v_nu, v_ci, v_p], input_tile_local[v_ci, v_p, v_r_a, v_r_b])
                                    T.writes(data_pack_local[v_eps, v_nu, v_ci, v_p])
                                    T.block_attr({"schedule_rule": "conv2d_nchw_winograd_data_pack"})
                                    data_pack_local[v_eps, v_nu, v_ci, v_p] = data_pack_local[v_eps, v_nu, v_ci, v_p] + input_tile_local[v_ci, v_p, v_r_a, v_r_b] * T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(3), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(1), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(0), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(3), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(1), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(0), T.float32(1), T.float32(0))))))))))))))))) * T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(3), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(1), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(0), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(3), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(1), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(0), T.float32(1), T.float32(0)))))))))))))))))
                for ax0, ax1, ax2, ax3 in T.grid(T.int64(4), T.int64(4), T.int64(1), T.int64(1)):
                    with T.block("data_pack_local"):
                        v0, v1 = T.axis.remap("SS", [ax0, ax1])
                        v2 = T.axis.spatial(T.int64(512), ci_p_fused_0 * T.int64(4) + ci_p_fused_1 // T.int64(16) + ax2)
                        v3 = T.axis.spatial(T.int64(16), ci_p_fused_1 % T.int64(16) + ax3)
                        T.reads(data_pack_local[v0, v1, v2, v3])
                        T.writes(data_pack[v0, v1, v2, v3])
                        data_pack[v0, v1, v2, v3] = data_pack_local[v0, v1, v2, v3]
        for eps_0_nu_0_co_0_p_0_fused in T.thread_binding(T.int64(2048), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 512, "pragma_unroll_explicit": 1}):
            for eps_1_nu_1_co_1_p_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for eps_2_nu_2_co_2_p_2_fused in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                    for eps_3_init, nu_3_init, co_3_init, p_3_init, eps_4_init, nu_4_init, co_4_init, p_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(1)):
                        with T.block("bgemm_init"):
                            v_eps = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(1024) * T.int64(2) + eps_2_nu_2_co_2_p_2_fused // T.int64(16) + eps_3_init + eps_4_init)
                            v_nu = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused % T.int64(1024) // T.int64(256) + nu_3_init + nu_4_init)
                            v_co = T.axis.spatial(T.int64(512), eps_0_nu_0_co_0_p_0_fused % T.int64(256) // T.int64(8) * T.int64(16) + eps_2_nu_2_co_2_p_2_fused % T.int64(16) // T.int64(2) * T.int64(2) + co_3_init * T.int64(2) + co_4_init)
                            v_p = T.axis.spatial(T.int64(16), eps_0_nu_0_co_0_p_0_fused % T.int64(8) * T.int64(2) + eps_2_nu_2_co_2_p_2_fused % T.int64(2) + p_3_init + p_4_init)
                            T.reads()
                            T.writes(bgemm_local[v_eps, v_nu, v_co, v_p])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            bgemm_local[v_eps, v_nu, v_co, v_p] = T.float32(0)
                    for ci_0_fused in T.serial(T.int64(32), annotations={"software_pipeline_async_stages": [0], "software_pipeline_order": [0, 1, 2], "software_pipeline_stage": [0, 0, 3]}):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(1)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("data_pack_shared"):
                                        v0 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(1024) * T.int64(2) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) // T.int64(32))
                                        v1 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused % T.int64(1024) // T.int64(256))
                                        v2 = T.axis.spatial(T.int64(512), ci_0_fused * T.int64(16) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) % T.int64(32) // T.int64(2))
                                        v3 = T.axis.spatial(T.int64(16), eps_0_nu_0_co_0_p_0_fused % T.int64(8) * T.int64(2) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) % T.int64(2))
                                        T.reads(data_pack[v0, v1, v2, v3])
                                        T.writes(data_pack_shared[v0, v1, v2, v3])
                                        data_pack_shared[v0, v1, v2, v3] = data_pack[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(16)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                                with T.block("p1_shared"):
                                    v0 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(1024) * T.int64(2) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(32) + ax0_ax1_ax2_ax3_fused_1) // T.int64(256))
                                    v1 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused % T.int64(1024) // T.int64(256))
                                    v2 = T.axis.spatial(T.int64(512), ci_0_fused * T.int64(16) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(32) + ax0_ax1_ax2_ax3_fused_1) % T.int64(256) // T.int64(16))
                                    v3 = T.axis.spatial(T.int64(512), eps_0_nu_0_co_0_p_0_fused % T.int64(256) // T.int64(8) * T.int64(16) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(32) + ax0_ax1_ax2_ax3_fused_1) % T.int64(16))
                                    T.reads(p1[v0, v1, v2, v3])
                                    T.writes(p1_shared[v0, v1, v2, v3])
                                    p1_shared[v0, v1, v2, v3] = p1[v0, v1, v2, v3]
                        for ci_1, eps_3, nu_3, co_3, p_3, ci_2, eps_4, nu_4, co_4, p_4 in T.grid(T.int64(16), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(1)):
                            with T.block("bgemm_update"):
                                v_eps = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(1024) * T.int64(2) + eps_2_nu_2_co_2_p_2_fused // T.int64(16) + eps_3 + eps_4)
                                v_nu = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused % T.int64(1024) // T.int64(256) + nu_3 + nu_4)
                                v_co = T.axis.spatial(T.int64(512), eps_0_nu_0_co_0_p_0_fused % T.int64(256) // T.int64(8) * T.int64(16) + eps_2_nu_2_co_2_p_2_fused % T.int64(16) // T.int64(2) * T.int64(2) + co_3 * T.int64(2) + co_4)
                                v_p = T.axis.spatial(T.int64(16), eps_0_nu_0_co_0_p_0_fused % T.int64(8) * T.int64(2) + eps_2_nu_2_co_2_p_2_fused % T.int64(2) + p_3 + p_4)
                                v_ci = T.axis.reduce(T.int64(512), ci_0_fused * T.int64(16) + ci_1 + ci_2)
                                T.reads(bgemm_local[v_eps, v_nu, v_co, v_p], data_pack_shared[v_eps, v_nu, v_ci, v_p], p1_shared[v_eps, v_nu, v_ci, v_co])
                                T.writes(bgemm_local[v_eps, v_nu, v_co, v_p])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                bgemm_local[v_eps, v_nu, v_co, v_p] = bgemm_local[v_eps, v_nu, v_co, v_p] + data_pack_shared[v_eps, v_nu, v_ci, v_p] * p1_shared[v_eps, v_nu, v_ci, v_co]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(1), T.int64(2), T.int64(1)):
                        with T.block("bgemm_local"):
                            v0 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(1024) * T.int64(2) + eps_2_nu_2_co_2_p_2_fused // T.int64(16) + ax0)
                            v1 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused % T.int64(1024) // T.int64(256) + ax1)
                            v2 = T.axis.spatial(T.int64(512), eps_0_nu_0_co_0_p_0_fused % T.int64(256) // T.int64(8) * T.int64(16) + eps_2_nu_2_co_2_p_2_fused % T.int64(16) // T.int64(2) * T.int64(2) + ax2)
                            v3 = T.axis.spatial(T.int64(16), eps_0_nu_0_co_0_p_0_fused % T.int64(8) * T.int64(2) + eps_2_nu_2_co_2_p_2_fused % T.int64(2) + ax3)
                            T.reads(bgemm_local[v0, v1, v2, v3])
                            T.writes(bgemm[v0, v1, v2, v3])
                            bgemm[v0, v1, v2, v3] = bgemm_local[v0, v1, v2, v3]
        for n_co_h_0_w_0_fused_0 in T.thread_binding(T.int64(8), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 512, "pragma_unroll_explicit": 1}):
            for n_co_h_0_w_0_fused_1 in T.thread_binding(T.int64(1024), thread="threadIdx.x"):
                for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                    for ax2 in T.unroll(T.int64(2)):
                        for ax3 in T.unroll(T.int64(2)):
                            with T.block("inverse_init"):
                                v_co = T.axis.spatial(T.int64(512), (n_co_h_0_w_0_fused_0 * T.int64(1024) + n_co_h_0_w_0_fused_1) // T.int64(16) + ax0)
                                v_p = T.axis.spatial(T.int64(16), (n_co_h_0_w_0_fused_0 * T.int64(1024) + n_co_h_0_w_0_fused_1) % T.int64(16) + ax1)
                                v_vh, v_vw = T.axis.remap("SS", [ax2, ax3])
                                T.reads()
                                T.writes(inverse_local[v_co, v_p, v_vh, v_vw])
                                T.block_attr({"schedule_rule": "conv2d_nchw_winograd_inverse"})
                                inverse_local[v_co, v_p, v_vh, v_vw] = T.float32(0)
                            for ax4 in T.unroll(T.int64(4)):
                                for ax5 in T.unroll(T.int64(4)):
                                    with T.block("inverse_update"):
                                        v_co = T.axis.spatial(T.int64(512), (n_co_h_0_w_0_fused_0 * T.int64(1024) + n_co_h_0_w_0_fused_1) // T.int64(16) + ax0)
                                        v_p = T.axis.spatial(T.int64(16), (n_co_h_0_w_0_fused_0 * T.int64(1024) + n_co_h_0_w_0_fused_1) % T.int64(16) + ax1)
                                        v_vh, v_vw, v_r_a, v_r_b = T.axis.remap("SSRR", [ax2, ax3, ax4, ax5])
                                        T.reads(inverse_local[v_co, v_p, v_vh, v_vw], bgemm[v_r_a, v_r_b, v_co, v_p])
                                        T.writes(inverse_local[v_co, v_p, v_vh, v_vw])
                                        T.block_attr({"schedule_rule": "conv2d_nchw_winograd_inverse"})
                                        inverse_local[v_co, v_p, v_vh, v_vw] = inverse_local[v_co, v_p, v_vh, v_vw] + bgemm[v_r_a, v_r_b, v_co, v_p] * T.Select(v_r_a % T.int64(4) == T.int64(3) and v_vh % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_vh % T.int64(2) == T.int64(0), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_vh % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_vh % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_vh % T.int64(2) == T.int64(1), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_vh % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_vh % T.int64(2) == T.int64(1), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_vh % T.int64(2) == T.int64(0), T.float32(1), T.float32(0))))))))) * T.Select(v_r_b % T.int64(4) == T.int64(3) and v_vw % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_vw % T.int64(2) == T.int64(0), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_vw % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_vw % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_vw % T.int64(2) == T.int64(1), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_vw % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_vw % T.int64(2) == T.int64(1), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_vw % T.int64(2) == T.int64(0), T.float32(1), T.float32(0)))))))))
                for h_1, w_1 in T.grid(T.int64(2), T.int64(2)):
                    with T.block("conv2d_winograd"):
                        v_n = T.axis.spatial(T.int64(1), T.int64(0))
                        v_co = T.axis.spatial(T.int64(512), (n_co_h_0_w_0_fused_0 * T.int64(1024) + n_co_h_0_w_0_fused_1) // T.int64(16))
                        v_h = T.axis.spatial(T.int64(7), (n_co_h_0_w_0_fused_0 * T.int64(1024) + n_co_h_0_w_0_fused_1) % T.int64(16) // T.int64(4) * T.int64(2) + h_1)
                        v_w = T.axis.spatial(T.int64(7), (n_co_h_0_w_0_fused_0 * T.int64(1024) + n_co_h_0_w_0_fused_1) % T.int64(4) * T.int64(2) + w_1)
                        T.where((n_co_h_0_w_0_fused_0 * T.int64(1024) + n_co_h_0_w_0_fused_1) % T.int64(16) // T.int64(4) * T.int64(2) + h_1 < T.int64(7) and (n_co_h_0_w_0_fused_0 * T.int64(1024) + n_co_h_0_w_0_fused_1) % T.int64(4) * T.int64(2) + w_1 < T.int64(7))
                        T.reads(inverse_local[v_co, v_n * T.int64(16) + v_h // T.int64(2) * T.int64(4) + v_w // T.int64(2), v_h % T.int64(2), v_w % T.int64(2)], p2[v_n, v_co, v_h, v_w], p3[v_n, v_co, T.int64(0), T.int64(0)], p4[v_n, v_co, T.int64(0), T.int64(0)])
                        T.writes(T_relu[v_n, v_co, v_h, v_w])
                        T_relu[v_n, v_co, v_h, v_w] = T.max((inverse_local[v_co, v_n * T.int64(16) + v_h // T.int64(2) * T.int64(4) + v_w // T.int64(2), v_h % T.int64(2), v_w % T.int64(2)] + p2[v_n, v_co, v_h, v_w]) * p3[v_n, v_co, T.int64(0), T.int64(0)] + p4[v_n, v_co, T.int64(0), T.int64(0)], T.float32(0))
b0 = sch.get_block(name="data_pack", func_name="main")
b1 = sch.get_block(name="bgemm", func_name="main")
b2 = sch.get_block(name="inverse", func_name="main")
b3 = sch.get_block(name="conv2d_winograd", func_name="main")
b4 = sch.get_block(name="T_add", func_name="main")
b5 = sch.get_block(name="T_multiply", func_name="main")
b6 = sch.get_block(name="T_add_1", func_name="main")
b7 = sch.get_block(name="T_relu", func_name="main")
b8 = sch.get_block(name="root", func_name="main")
b9, b10 = sch.get_producers(block=b2)
sch.compute_inline(block=b10)
b11, = sch.get_consumers(block=b2)
l12, l13, l14, l15 = sch.get_loops(block=b11)
l16, l17 = sch.split(loop=l14, factors=[None, 2], preserve_unit_iters=True)
l18, l19 = sch.split(loop=l15, factors=[None, 2], preserve_unit_iters=True)
sch.reorder(l16, l18, l17, l19)
sch.compute_at(block=b2, loop=l18, preserve_unit_loops=True, index=-1)
sch.set_scope(block=b2, buffer_index=0, storage_scope="local")
l20, l21, l22, l23, l24, l25, l26, l27, l28, l29 = sch.get_loops(block=b2)
sch.unroll(loop=l26)
sch.unroll(loop=l27)
sch.unroll(loop=l28)
sch.unroll(loop=l29)
b30, b31 = sch.get_producers(block=b0)
sch.compute_inline(block=b31)
b32, = sch.get_producers(block=b30)
l33, l34, l35, l36, l37, l38 = sch.get_loops(block=b0)
sch.reorder(l35, l36, l33, l34, l37, l38)
sch.unroll(loop=l33)
sch.unroll(loop=l34)
sch.unroll(loop=l37)
sch.unroll(loop=l38)
l39 = sch.fuse(l35, l36, preserve_unit_iters=True)
v40 = sch.sample_categorical(candidates=[32, 64, 128, 256, 512, 1024], probs=[0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666], decision=1)
l41, l42 = sch.split(loop=l39, factors=[None, v40], preserve_unit_iters=True)
sch.bind(loop=l41, thread_axis="blockIdx.x")
sch.bind(loop=l42, thread_axis="threadIdx.x")
b43 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b43, loop=l42, preserve_unit_loops=True, index=-1)
sch.compute_at(block=b30, loop=l42, preserve_unit_loops=True, index=-1)
sch.set_scope(block=b30, buffer_index=0, storage_scope="local")
sch.compute_inline(block=b32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l44, l45, l46, l47, l48 = sch.get_loops(block=b1)
v49, v50, v51, v52, v53 = sch.sample_perfect_tile(loop=l44, n=5, max_innermost_factor=64, decision=[2, 1, 2, 1, 1])
l54, l55, l56, l57, l58 = sch.split(loop=l44, factors=[v49, v50, v51, v52, v53], preserve_unit_iters=True)
v59, v60, v61, v62, v63 = sch.sample_perfect_tile(loop=l45, n=5, max_innermost_factor=64, decision=[4, 1, 1, 1, 1])
l64, l65, l66, l67, l68 = sch.split(loop=l45, factors=[v59, v60, v61, v62, v63], preserve_unit_iters=True)
v69, v70, v71, v72, v73 = sch.sample_perfect_tile(loop=l46, n=5, max_innermost_factor=64, decision=[32, 1, 8, 1, 2])
l74, l75, l76, l77, l78 = sch.split(loop=l46, factors=[v69, v70, v71, v72, v73], preserve_unit_iters=True)
v79, v80, v81, v82, v83 = sch.sample_perfect_tile(loop=l47, n=5, max_innermost_factor=64, decision=[8, 1, 2, 1, 1])
l84, l85, l86, l87, l88 = sch.split(loop=l47, factors=[v79, v80, v81, v82, v83], preserve_unit_iters=True)
v89, v90, v91 = sch.sample_perfect_tile(loop=l48, n=3, max_innermost_factor=64, decision=[32, 16, 1])
l92, l93, l94 = sch.split(loop=l48, factors=[v89, v90, v91], preserve_unit_iters=True)
sch.reorder(l54, l64, l74, l84, l55, l65, l75, l85, l56, l66, l76, l86, l92, l93, l57, l67, l77, l87, l94, l58, l68, l78, l88)
l95 = sch.fuse(l54, l64, l74, l84, preserve_unit_iters=True)
sch.bind(loop=l95, thread_axis="blockIdx.x")
l96 = sch.fuse(l55, l65, l75, l85, preserve_unit_iters=True)
sch.bind(loop=l96, thread_axis="vthread.x")
l97 = sch.fuse(l56, l66, l76, l86, preserve_unit_iters=True)
sch.bind(loop=l97, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b98 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b98, loop=l97, preserve_unit_loops=True, index=-1)
b99 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b99, loop=l92, preserve_unit_loops=True, index=-1)
l100, l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b99)
l108 = sch.fuse(l104, l105, l106, l107, preserve_unit_iters=True)
v109 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b99, ann_key="meta_schedule.cooperative_fetch", ann_val=v109)
b110 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b110, loop=l92, preserve_unit_loops=True, index=-1)
l111, l112, l113, l114, l115, l116, l117, l118 = sch.get_loops(block=b110)
l119 = sch.fuse(l115, l116, l117, l118, preserve_unit_iters=True)
v120 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b110, ann_key="meta_schedule.cooperative_fetch", ann_val=v120)
l121 = sch.fuse(l92, preserve_unit_iters=True)
sch.annotate(block_or_loop=l121, ann_key="software_pipeline_stage", ann_val=[0, 0, 3])
sch.annotate(block_or_loop=l121, ann_key="software_pipeline_order", ann_val=[0, 1, 2])
sch.annotate(block_or_loop=l121, ann_key="software_pipeline_async_stages", ann_val=[0])
sch.reverse_compute_inline(block=b7)
sch.reverse_compute_inline(block=b6)
sch.reverse_compute_inline(block=b5)
sch.reverse_compute_inline(block=b4)
v122 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b8, ann_key="meta_schedule.unroll_explicit", ann_val=v122)
l123, l124, l125, l126, l127, l128 = sch.get_loops(block=b3)
l129 = sch.fuse(l123, l124, l125, l126, preserve_unit_iters=True)
v130 = sch.sample_categorical(candidates=[32, 64, 128, 256, 512, 1024], probs=[0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666], decision=5)
l131, l132 = sch.split(loop=l129, factors=[None, v130], preserve_unit_iters=True)
sch.bind(loop=l131, thread_axis="blockIdx.x")
sch.bind(loop=l132, thread_axis="threadIdx.x")
sch.enter_postproc()
sch.unannotate(block_or_loop=b99, ann_key="meta_schedule.cooperative_fetch")
l133, l134, l135, l136, l137 = sch.get_loops(block=b99)
l138, l139, l140 = sch.split(loop=l137, factors=[None, 32, 2], preserve_unit_iters=True)
sch.vectorize(loop=l140)
sch.bind(loop=l139, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b110, ann_key="meta_schedule.cooperative_fetch")
l141, l142, l143, l144, l145 = sch.get_loops(block=b110)
l146, l147 = sch.split(loop=l145, factors=[None, 32], preserve_unit_iters=True)
sch.bind(loop=l147, thread_axis="threadIdx.x")
b148 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b148, ann_key="meta_schedule.unroll_explicit")
b149, b150, b151, b152, b153, b154, b155, b156, b157 = sch.get_child_blocks(b148)
l158, l159, l160, l161, l162, l163 = sch.get_loops(block=b149)
l164, l165, l166, l167, l168, l169 = sch.get_loops(block=b150)
sch.annotate(block_or_loop=l164, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l164, ann_key="pragma_unroll_explicit", ann_val=1)
l170, l171, l172, l173, l174, l175 = sch.get_loops(block=b151)
l176, l177, l178, l179, l180, l181, l182 = sch.get_loops(block=b152)
l183, l184, l185, l186, l187, l188 = sch.get_loops(block=b153)
l189, l190, l191, l192, l193, l194, l195, l196, l197, l198, l199, l200, l201, l202 = sch.get_loops(block=b154)
sch.annotate(block_or_loop=l189, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l189, ann_key="pragma_unroll_explicit", ann_val=1)
l203, l204, l205, l206, l207, l208, l209 = sch.get_loops(block=b155)
l210, l211, l212, l213, l214, l215, l216, l217 = sch.get_loops(block=b156)
sch.annotate(block_or_loop=l210, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l210, ann_key="pragma_unroll_explicit", ann_val=1)
l218, l219, l220, l221 = sch.get_loops(block=b157)
b222 = sch.get_block(name="data_pack", func_name="main")
l223, l224, l225, l226, l227, l228 = sch.get_loops(block=b222)
b229 = sch.decompose_reduction(block=b222, loop=l227)
b230 = sch.get_block(name="bgemm", func_name="main")
l231, l232, l233, l234, l235, l236, l237, l238, l239, l240, l241, l242, l243, l244 = sch.get_loops(block=b230)
b245 = sch.decompose_reduction(block=b230, loop=l234)
b246 = sch.get_block(name="inverse", func_name="main")
l247, l248, l249, l250, l251, l252, l253, l254 = sch.get_loops(block=b246)
b255 = sch.decompose_reduction(block=b246, loop=l253)
2023-11-11 01:34:49 [INFO] [task_scheduler.cc:121] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #137: Error in running:
LocalRunner: An exception occurred
Subprocess terminated
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(512), T.int64(7), T.int64(7)), "float32"), p1: T.Buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(512)), "float32"), p2: T.Buffer((T.int64(1), T.int64(512), T.int64(7), T.int64(7)), "float32"), p3: T.Buffer((T.int64(1), T.int64(512), T.int64(1), T.int64(1)), "float32"), p4: T.Buffer((T.int64(1), T.int64(512), T.int64(1), T.int64(1)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(512), T.int64(7), T.int64(7)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        input_tile_local = T.alloc_buffer((T.int64(512), T.int64(16), T.int64(4), T.int64(4)), scope="local")
        data_pack = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(16)))
        bgemm = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(16)))
        inverse_local = T.alloc_buffer((T.int64(512), T.int64(16), T.int64(2), T.int64(2)), scope="local")
        data_pack_local = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(16)), scope="local")
        bgemm_local = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(16)), scope="local")
        data_pack_shared = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(16)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(512)), scope="shared")
        for ci_p_fused_0 in T.thread_binding(T.int64(64), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 512, "pragma_unroll_explicit": 1}):
            for ci_p_fused_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(1), T.int64(4), T.int64(4)):
                    with T.block("input_tile"):
                        v_ci = T.axis.spatial(T.int64(512), ci_p_fused_0 * T.int64(8) + ci_p_fused_1 // T.int64(16) + ax0)
                        v_p = T.axis.spatial(T.int64(16), ci_p_fused_1 % T.int64(16) + ax1)
                        v_eps, v_nu = T.axis.remap("SS", [ax2, ax3])
                        T.reads(p0[v_p // T.int64(16), v_ci, v_p % T.int64(16) // T.int64(4) * T.int64(2) + v_eps - T.int64(1), v_p % T.int64(4) * T.int64(2) + v_nu - T.int64(1)])
                        T.writes(input_tile_local[v_ci, v_p, v_eps, v_nu])
                        T.block_attr({"schedule_rule": "None"})
                        input_tile_local[v_ci, v_p, v_eps, v_nu] = T.if_then_else(T.int64(1) <= v_p % T.int64(16) // T.int64(4) * T.int64(2) + v_eps and v_p % T.int64(16) // T.int64(4) * T.int64(2) + v_eps < T.int64(8) and T.int64(1) <= v_p % T.int64(4) * T.int64(2) + v_nu and v_p % T.int64(4) * T.int64(2) + v_nu < T.int64(8), p0[v_p // T.int64(16), v_ci, v_p % T.int64(16) // T.int64(4) * T.int64(2) + v_eps - T.int64(1), v_p % T.int64(4) * T.int64(2) + v_nu - T.int64(1)], T.float32(0))
                for eps in T.unroll(T.int64(4)):
                    for nu in T.unroll(T.int64(4)):
                        with T.block("data_pack_init"):
                            v_eps, v_nu = T.axis.remap("SS", [eps, nu])
                            v_ci = T.axis.spatial(T.int64(512), (ci_p_fused_0 * T.int64(128) + ci_p_fused_1) // T.int64(16))
                            v_p = T.axis.spatial(T.int64(16), (ci_p_fused_0 * T.int64(128) + ci_p_fused_1) % T.int64(16))
                            T.reads()
                            T.writes(data_pack_local[v_eps, v_nu, v_ci, v_p])
                            T.block_attr({"schedule_rule": "conv2d_nchw_winograd_data_pack"})
                            data_pack_local[v_eps, v_nu, v_ci, v_p] = T.float32(0)
                        for r_a in T.unroll(T.int64(4)):
                            for r_b in T.unroll(T.int64(4)):
                                with T.block("data_pack_update"):
                                    v_eps, v_nu = T.axis.remap("SS", [eps, nu])
                                    v_ci = T.axis.spatial(T.int64(512), (ci_p_fused_0 * T.int64(128) + ci_p_fused_1) // T.int64(16))
                                    v_p = T.axis.spatial(T.int64(16), (ci_p_fused_0 * T.int64(128) + ci_p_fused_1) % T.int64(16))
                                    v_r_a, v_r_b = T.axis.remap("RR", [r_a, r_b])
                                    T.reads(data_pack_local[v_eps, v_nu, v_ci, v_p], input_tile_local[v_ci, v_p, v_r_a, v_r_b])
                                    T.writes(data_pack_local[v_eps, v_nu, v_ci, v_p])
                                    T.block_attr({"schedule_rule": "conv2d_nchw_winograd_data_pack"})
                                    data_pack_local[v_eps, v_nu, v_ci, v_p] = data_pack_local[v_eps, v_nu, v_ci, v_p] + input_tile_local[v_ci, v_p, v_r_a, v_r_b] * T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(3), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(1), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(0), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(3), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(1), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(0), T.float32(1), T.float32(0))))))))))))))))) * T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(3), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(1), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(0), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(3), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(1), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(0), T.float32(1), T.float32(0)))))))))))))))))
                for ax0, ax1, ax2, ax3 in T.grid(T.int64(4), T.int64(4), T.int64(1), T.int64(1)):
                    with T.block("data_pack_local"):
                        v0, v1 = T.axis.remap("SS", [ax0, ax1])
                        v2 = T.axis.spatial(T.int64(512), ci_p_fused_0 * T.int64(8) + ci_p_fused_1 // T.int64(16) + ax2)
                        v3 = T.axis.spatial(T.int64(16), ci_p_fused_1 % T.int64(16) + ax3)
                        T.reads(data_pack_local[v0, v1, v2, v3])
                        T.writes(data_pack[v0, v1, v2, v3])
                        data_pack[v0, v1, v2, v3] = data_pack_local[v0, v1, v2, v3]
        for eps_0_nu_0_co_0_p_0_fused in T.thread_binding(T.int64(256), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 512, "pragma_unroll_explicit": 1}):
            for eps_1_nu_1_co_1_p_1_fused in T.thread_binding(T.int64(4), thread="vthread.x"):
                for eps_2_nu_2_co_2_p_2_fused in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                    for eps_3_init, nu_3_init, co_3_init, p_3_init, eps_4_init, nu_4_init, co_4_init, p_4_init in T.grid(T.int64(1), T.int64(1), T.int64(2), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("bgemm_init"):
                            v_eps = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(64) + eps_3_init + eps_4_init)
                            v_nu = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused % T.int64(64) // T.int64(16) + nu_3_init + nu_4_init)
                            v_co = T.axis.spatial(T.int64(512), eps_0_nu_0_co_0_p_0_fused % T.int64(16) * T.int64(32) + eps_1_nu_1_co_1_p_1_fused // T.int64(2) * T.int64(16) + eps_2_nu_2_co_2_p_2_fused // T.int64(8) * T.int64(2) + co_3_init + co_4_init)
                            v_p = T.axis.spatial(T.int64(16), eps_1_nu_1_co_1_p_1_fused % T.int64(2) * T.int64(8) + eps_2_nu_2_co_2_p_2_fused % T.int64(8) + p_3_init + p_4_init)
                            T.reads()
                            T.writes(bgemm_local[v_eps, v_nu, v_co, v_p])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            bgemm_local[v_eps, v_nu, v_co, v_p] = T.float32(0)
                    for ci_0 in range(T.int64(16)):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(2)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("data_pack_shared"):
                                        v0 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(64))
                                        v1 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused % T.int64(64) // T.int64(16))
                                        v2 = T.axis.spatial(T.int64(512), ci_0 * T.int64(32) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) // T.int64(16))
                                        v3 = T.axis.spatial(T.int64(16), (ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) % T.int64(16))
                                        T.reads(data_pack[v0, v1, v2, v3])
                                        T.writes(data_pack_shared[v0, v1, v2, v3])
                                        data_pack_shared[v0, v1, v2, v3] = data_pack[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(8)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("p1_shared"):
                                        v0 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(64))
                                        v1 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused % T.int64(64) // T.int64(16))
                                        v2 = T.axis.spatial(T.int64(512), ci_0 * T.int64(32) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) // T.int64(32))
                                        v3 = T.axis.spatial(T.int64(512), eps_0_nu_0_co_0_p_0_fused % T.int64(16) * T.int64(32) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) % T.int64(32))
                                        T.reads(p1[v0, v1, v2, v3])
                                        T.writes(p1_shared[v0, v1, v2, v3])
                                        p1_shared[v0, v1, v2, v3] = p1[v0, v1, v2, v3]
                        for ci_1, eps_3, nu_3, co_3, p_3, ci_2, eps_4, nu_4, co_4, p_4 in T.grid(T.int64(2), T.int64(1), T.int64(1), T.int64(2), T.int64(1), T.int64(16), T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                            with T.block("bgemm_update"):
                                v_eps = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(64) + eps_3 + eps_4)
                                v_nu = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused % T.int64(64) // T.int64(16) + nu_3 + nu_4)
                                v_co = T.axis.spatial(T.int64(512), eps_0_nu_0_co_0_p_0_fused % T.int64(16) * T.int64(32) + eps_1_nu_1_co_1_p_1_fused // T.int64(2) * T.int64(16) + eps_2_nu_2_co_2_p_2_fused // T.int64(8) * T.int64(2) + co_3 + co_4)
                                v_p = T.axis.spatial(T.int64(16), eps_1_nu_1_co_1_p_1_fused % T.int64(2) * T.int64(8) + eps_2_nu_2_co_2_p_2_fused % T.int64(8) + p_3 + p_4)
                                v_ci = T.axis.reduce(T.int64(512), ci_0 * T.int64(32) + ci_1 * T.int64(16) + ci_2)
                                T.reads(bgemm_local[v_eps, v_nu, v_co, v_p], data_pack_shared[v_eps, v_nu, v_ci, v_p], p1_shared[v_eps, v_nu, v_ci, v_co])
                                T.writes(bgemm_local[v_eps, v_nu, v_co, v_p])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                bgemm_local[v_eps, v_nu, v_co, v_p] = bgemm_local[v_eps, v_nu, v_co, v_p] + data_pack_shared[v_eps, v_nu, v_ci, v_p] * p1_shared[v_eps, v_nu, v_ci, v_co]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(1), T.int64(2), T.int64(1)):
                        with T.block("bgemm_local"):
                            v0 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(64) + ax0)
                            v1 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused % T.int64(64) // T.int64(16) + ax1)
                            v2 = T.axis.spatial(T.int64(512), eps_0_nu_0_co_0_p_0_fused % T.int64(16) * T.int64(32) + eps_1_nu_1_co_1_p_1_fused // T.int64(2) * T.int64(16) + eps_2_nu_2_co_2_p_2_fused // T.int64(8) * T.int64(2) + ax2)
                            v3 = T.axis.spatial(T.int64(16), eps_1_nu_1_co_1_p_1_fused % T.int64(2) * T.int64(8) + eps_2_nu_2_co_2_p_2_fused % T.int64(8) + ax3)
                            T.reads(bgemm_local[v0, v1, v2, v3])
                            T.writes(bgemm[v0, v1, v2, v3])
                            bgemm[v0, v1, v2, v3] = bgemm_local[v0, v1, v2, v3]
        for n_co_h_0_w_0_fused_0 in T.thread_binding(T.int64(256), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 512, "pragma_unroll_explicit": 1}):
            for n_co_h_0_w_0_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                    for ax2 in T.unroll(T.int64(2)):
                        for ax3 in T.unroll(T.int64(2)):
                            with T.block("inverse_init"):
                                v_co = T.axis.spatial(T.int64(512), (n_co_h_0_w_0_fused_0 * T.int64(32) + n_co_h_0_w_0_fused_1) // T.int64(16) + ax0)
                                v_p = T.axis.spatial(T.int64(16), (n_co_h_0_w_0_fused_0 * T.int64(32) + n_co_h_0_w_0_fused_1) % T.int64(16) + ax1)
                                v_vh, v_vw = T.axis.remap("SS", [ax2, ax3])
                                T.reads()
                                T.writes(inverse_local[v_co, v_p, v_vh, v_vw])
                                T.block_attr({"schedule_rule": "conv2d_nchw_winograd_inverse"})
                                inverse_local[v_co, v_p, v_vh, v_vw] = T.float32(0)
                            for ax4 in T.unroll(T.int64(4)):
                                for ax5 in T.unroll(T.int64(4)):
                                    with T.block("inverse_update"):
                                        v_co = T.axis.spatial(T.int64(512), (n_co_h_0_w_0_fused_0 * T.int64(32) + n_co_h_0_w_0_fused_1) // T.int64(16) + ax0)
                                        v_p = T.axis.spatial(T.int64(16), (n_co_h_0_w_0_fused_0 * T.int64(32) + n_co_h_0_w_0_fused_1) % T.int64(16) + ax1)
                                        v_vh, v_vw, v_r_a, v_r_b = T.axis.remap("SSRR", [ax2, ax3, ax4, ax5])
                                        T.reads(inverse_local[v_co, v_p, v_vh, v_vw], bgemm[v_r_a, v_r_b, v_co, v_p])
                                        T.writes(inverse_local[v_co, v_p, v_vh, v_vw])
                                        T.block_attr({"schedule_rule": "conv2d_nchw_winograd_inverse"})
                                        inverse_local[v_co, v_p, v_vh, v_vw] = inverse_local[v_co, v_p, v_vh, v_vw] + bgemm[v_r_a, v_r_b, v_co, v_p] * T.Select(v_r_a % T.int64(4) == T.int64(3) and v_vh % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_vh % T.int64(2) == T.int64(0), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_vh % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_vh % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_vh % T.int64(2) == T.int64(1), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_vh % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_vh % T.int64(2) == T.int64(1), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_vh % T.int64(2) == T.int64(0), T.float32(1), T.float32(0))))))))) * T.Select(v_r_b % T.int64(4) == T.int64(3) and v_vw % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_vw % T.int64(2) == T.int64(0), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_vw % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_vw % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_vw % T.int64(2) == T.int64(1), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_vw % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_vw % T.int64(2) == T.int64(1), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_vw % T.int64(2) == T.int64(0), T.float32(1), T.float32(0)))))))))
                for h_1, w_1 in T.grid(T.int64(2), T.int64(2)):
                    with T.block("conv2d_winograd"):
                        v_n = T.axis.spatial(T.int64(1), T.int64(0))
                        v_co = T.axis.spatial(T.int64(512), (n_co_h_0_w_0_fused_0 * T.int64(32) + n_co_h_0_w_0_fused_1) // T.int64(16))
                        v_h = T.axis.spatial(T.int64(7), (n_co_h_0_w_0_fused_0 * T.int64(32) + n_co_h_0_w_0_fused_1) % T.int64(16) // T.int64(4) * T.int64(2) + h_1)
                        v_w = T.axis.spatial(T.int64(7), (n_co_h_0_w_0_fused_0 * T.int64(32) + n_co_h_0_w_0_fused_1) % T.int64(4) * T.int64(2) + w_1)
                        T.where((n_co_h_0_w_0_fused_0 * T.int64(32) + n_co_h_0_w_0_fused_1) % T.int64(16) // T.int64(4) * T.int64(2) + h_1 < T.int64(7) and (n_co_h_0_w_0_fused_0 * T.int64(32) + n_co_h_0_w_0_fused_1) % T.int64(4) * T.int64(2) + w_1 < T.int64(7))
                        T.reads(inverse_local[v_co, v_n * T.int64(16) + v_h // T.int64(2) * T.int64(4) + v_w // T.int64(2), v_h % T.int64(2), v_w % T.int64(2)], p2[v_n, v_co, v_h, v_w], p3[v_n, v_co, T.int64(0), T.int64(0)], p4[v_n, v_co, T.int64(0), T.int64(0)])
                        T.writes(T_relu[v_n, v_co, v_h, v_w])
                        T_relu[v_n, v_co, v_h, v_w] = T.max((inverse_local[v_co, v_n * T.int64(16) + v_h // T.int64(2) * T.int64(4) + v_w // T.int64(2), v_h % T.int64(2), v_w % T.int64(2)] + p2[v_n, v_co, v_h, v_w]) * p3[v_n, v_co, T.int64(0), T.int64(0)] + p4[v_n, v_co, T.int64(0), T.int64(0)], T.float32(0))
b0 = sch.get_block(name="data_pack", func_name="main")
b1 = sch.get_block(name="bgemm", func_name="main")
b2 = sch.get_block(name="inverse", func_name="main")
b3 = sch.get_block(name="conv2d_winograd", func_name="main")
b4 = sch.get_block(name="T_add", func_name="main")
b5 = sch.get_block(name="T_multiply", func_name="main")
b6 = sch.get_block(name="T_add_1", func_name="main")
b7 = sch.get_block(name="T_relu", func_name="main")
b8 = sch.get_block(name="root", func_name="main")
b9, b10 = sch.get_producers(block=b2)
sch.compute_inline(block=b10)
b11, = sch.get_consumers(block=b2)
l12, l13, l14, l15 = sch.get_loops(block=b11)
l16, l17 = sch.split(loop=l14, factors=[None, 2], preserve_unit_iters=True)
l18, l19 = sch.split(loop=l15, factors=[None, 2], preserve_unit_iters=True)
sch.reorder(l16, l18, l17, l19)
sch.compute_at(block=b2, loop=l18, preserve_unit_loops=True, index=-1)
sch.set_scope(block=b2, buffer_index=0, storage_scope="local")
l20, l21, l22, l23, l24, l25, l26, l27, l28, l29 = sch.get_loops(block=b2)
sch.unroll(loop=l26)
sch.unroll(loop=l27)
sch.unroll(loop=l28)
sch.unroll(loop=l29)
b30, b31 = sch.get_producers(block=b0)
sch.compute_inline(block=b31)
b32, = sch.get_producers(block=b30)
l33, l34, l35, l36, l37, l38 = sch.get_loops(block=b0)
sch.reorder(l35, l36, l33, l34, l37, l38)
sch.unroll(loop=l33)
sch.unroll(loop=l34)
sch.unroll(loop=l37)
sch.unroll(loop=l38)
l39 = sch.fuse(l35, l36, preserve_unit_iters=True)
v40 = sch.sample_categorical(candidates=[32, 64, 128, 256, 512, 1024], probs=[0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666], decision=2)
l41, l42 = sch.split(loop=l39, factors=[None, v40], preserve_unit_iters=True)
sch.bind(loop=l41, thread_axis="blockIdx.x")
sch.bind(loop=l42, thread_axis="threadIdx.x")
b43 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b43, loop=l42, preserve_unit_loops=True, index=-1)
sch.compute_at(block=b30, loop=l42, preserve_unit_loops=True, index=-1)
sch.set_scope(block=b30, buffer_index=0, storage_scope="local")
sch.compute_inline(block=b32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l44, l45, l46, l47, l48 = sch.get_loops(block=b1)
v49, v50, v51, v52, v53 = sch.sample_perfect_tile(loop=l44, n=5, max_innermost_factor=64, decision=[4, 1, 1, 1, 1])
l54, l55, l56, l57, l58 = sch.split(loop=l44, factors=[v49, v50, v51, v52, v53], preserve_unit_iters=True)
v59, v60, v61, v62, v63 = sch.sample_perfect_tile(loop=l45, n=5, max_innermost_factor=64, decision=[4, 1, 1, 1, 1])
l64, l65, l66, l67, l68 = sch.split(loop=l45, factors=[v59, v60, v61, v62, v63], preserve_unit_iters=True)
v69, v70, v71, v72, v73 = sch.sample_perfect_tile(loop=l46, n=5, max_innermost_factor=64, decision=[16, 2, 8, 2, 1])
l74, l75, l76, l77, l78 = sch.split(loop=l46, factors=[v69, v70, v71, v72, v73], preserve_unit_iters=True)
v79, v80, v81, v82, v83 = sch.sample_perfect_tile(loop=l47, n=5, max_innermost_factor=64, decision=[1, 2, 8, 1, 1])
l84, l85, l86, l87, l88 = sch.split(loop=l47, factors=[v79, v80, v81, v82, v83], preserve_unit_iters=True)
v89, v90, v91 = sch.sample_perfect_tile(loop=l48, n=3, max_innermost_factor=64, decision=[16, 2, 16])
l92, l93, l94 = sch.split(loop=l48, factors=[v89, v90, v91], preserve_unit_iters=True)
sch.reorder(l54, l64, l74, l84, l55, l65, l75, l85, l56, l66, l76, l86, l92, l93, l57, l67, l77, l87, l94, l58, l68, l78, l88)
l95 = sch.fuse(l54, l64, l74, l84, preserve_unit_iters=True)
sch.bind(loop=l95, thread_axis="blockIdx.x")
l96 = sch.fuse(l55, l65, l75, l85, preserve_unit_iters=True)
sch.bind(loop=l96, thread_axis="vthread.x")
l97 = sch.fuse(l56, l66, l76, l86, preserve_unit_iters=True)
sch.bind(loop=l97, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b98 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b98, loop=l97, preserve_unit_loops=True, index=-1)
b99 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b99, loop=l92, preserve_unit_loops=True, index=-1)
l100, l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b99)
l108 = sch.fuse(l104, l105, l106, l107, preserve_unit_iters=True)
v109 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b99, ann_key="meta_schedule.cooperative_fetch", ann_val=v109)
b110 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b110, loop=l92, preserve_unit_loops=True, index=-1)
l111, l112, l113, l114, l115, l116, l117, l118 = sch.get_loops(block=b110)
l119 = sch.fuse(l115, l116, l117, l118, preserve_unit_iters=True)
v120 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b110, ann_key="meta_schedule.cooperative_fetch", ann_val=v120)
sch.reverse_compute_inline(block=b7)
sch.reverse_compute_inline(block=b6)
sch.reverse_compute_inline(block=b5)
sch.reverse_compute_inline(block=b4)
v121 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b8, ann_key="meta_schedule.unroll_explicit", ann_val=v121)
l122, l123, l124, l125, l126, l127 = sch.get_loops(block=b3)
l128 = sch.fuse(l122, l123, l124, l125, preserve_unit_iters=True)
v129 = sch.sample_categorical(candidates=[32, 64, 128, 256, 512, 1024], probs=[0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666], decision=0)
l130, l131 = sch.split(loop=l128, factors=[None, v129], preserve_unit_iters=True)
sch.bind(loop=l130, thread_axis="blockIdx.x")
sch.bind(loop=l131, thread_axis="threadIdx.x")
sch.enter_postproc()
sch.unannotate(block_or_loop=b99, ann_key="meta_schedule.cooperative_fetch")
l132, l133, l134, l135, l136 = sch.get_loops(block=b99)
l137, l138, l139 = sch.split(loop=l136, factors=[None, 64, 4], preserve_unit_iters=True)
sch.vectorize(loop=l139)
sch.bind(loop=l138, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b110, ann_key="meta_schedule.cooperative_fetch")
l140, l141, l142, l143, l144 = sch.get_loops(block=b110)
l145, l146, l147 = sch.split(loop=l144, factors=[None, 64, 2], preserve_unit_iters=True)
sch.vectorize(loop=l147)
sch.bind(loop=l146, thread_axis="threadIdx.x")
b148 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b148, ann_key="meta_schedule.unroll_explicit")
b149, b150, b151, b152, b153, b154, b155, b156, b157 = sch.get_child_blocks(b148)
l158, l159, l160, l161, l162, l163 = sch.get_loops(block=b149)
l164, l165, l166, l167, l168, l169 = sch.get_loops(block=b150)
sch.annotate(block_or_loop=l164, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l164, ann_key="pragma_unroll_explicit", ann_val=1)
l170, l171, l172, l173, l174, l175 = sch.get_loops(block=b151)
l176, l177, l178, l179, l180, l181, l182 = sch.get_loops(block=b152)
l183, l184, l185, l186, l187, l188, l189 = sch.get_loops(block=b153)
l190, l191, l192, l193, l194, l195, l196, l197, l198, l199, l200, l201, l202, l203 = sch.get_loops(block=b154)
sch.annotate(block_or_loop=l190, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l190, ann_key="pragma_unroll_explicit", ann_val=1)
l204, l205, l206, l207, l208, l209, l210 = sch.get_loops(block=b155)
l211, l212, l213, l214, l215, l216, l217, l218 = sch.get_loops(block=b156)
sch.annotate(block_or_loop=l211, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l211, ann_key="pragma_unroll_explicit", ann_val=1)
l219, l220, l221, l222 = sch.get_loops(block=b157)
b223 = sch.get_block(name="data_pack", func_name="main")
l224, l225, l226, l227, l228, l229 = sch.get_loops(block=b223)
b230 = sch.decompose_reduction(block=b223, loop=l228)
b231 = sch.get_block(name="bgemm", func_name="main")
l232, l233, l234, l235, l236, l237, l238, l239, l240, l241, l242, l243, l244, l245 = sch.get_loops(block=b231)
b246 = sch.decompose_reduction(block=b231, loop=l235)
b247 = sch.get_block(name="inverse", func_name="main")
l248, l249, l250, l251, l252, l253, l254, l255 = sch.get_loops(block=b247)
b256 = sch.decompose_reduction(block=b247, loop=l254)
2023-11-11 01:34:49 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #138: GFLOPs: 3495.4801. Time: 40.6761 us. Best GFLOPs: 3605.5648
2023-11-11 01:34:49 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #139: GFLOPs: 3362.8188. Time: 42.2807 us. Best GFLOPs: 3605.5648
2023-11-11 01:34:49 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #140: GFLOPs: 3601.9827. Time: 39.4734 us. Best GFLOPs: 3605.5648
2023-11-11 01:34:49 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #141: GFLOPs: 2232.8561. Time: 63.6774 us. Best GFLOPs: 3605.5648
2023-11-11 01:34:49 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #142: GFLOPs: 2240.8126. Time: 63.4513 us. Best GFLOPs: 3605.5648
2023-11-11 01:34:49 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #143: GFLOPs: 3501.9856. Time: 40.6005 us. Best GFLOPs: 3605.5648
2023-11-11 01:34:49 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #144: GFLOPs: 2468.7987. Time: 57.5917 us. Best GFLOPs: 3605.5648
2023-11-11 01:34:49 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #145: GFLOPs: 2296.2616. Time: 61.9191 us. Best GFLOPs: 3605.5648
2023-11-11 01:34:49 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #146: GFLOPs: 3469.4350. Time: 40.9814 us. Best GFLOPs: 3605.5648
2023-11-11 01:34:49 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #147: GFLOPs: 3120.2660. Time: 45.5674 us. Best GFLOPs: 3605.5648
2023-11-11 01:34:49 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #148: GFLOPs: 2294.7959. Time: 61.9586 us. Best GFLOPs: 3605.5648
2023-11-11 01:34:49 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #149: GFLOPs: 3536.3625. Time: 40.2058 us. Best GFLOPs: 3605.5648
2023-11-11 01:34:49 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #150: GFLOPs: 2006.1409. Time: 70.8736 us. Best GFLOPs: 3605.5648
2023-11-11 01:34:49 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #151: GFLOPs: 3409.6264. Time: 41.7003 us. Best GFLOPs: 3605.5648
2023-11-11 01:34:49 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #152: GFLOPs: 3382.8422. Time: 42.0305 us. Best GFLOPs: 3605.5648
2023-11-11 01:34:49 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #153: GFLOPs: 2349.9636. Time: 60.5041 us. Best GFLOPs: 3605.5648
2023-11-11 01:34:49 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #154: GFLOPs: 3483.3455. Time: 40.8178 us. Best GFLOPs: 3605.5648
2023-11-11 01:34:49 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #155: GFLOPs: 2360.9099. Time: 60.2236 us. Best GFLOPs: 3605.5648
2023-11-11 01:34:49 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #156: GFLOPs: 2320.3194. Time: 61.2771 us. Best GFLOPs: 3605.5648
2023-11-11 01:34:49 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #157: GFLOPs: 2419.7911. Time: 58.7581 us. Best GFLOPs: 3605.5648
2023-11-11 01:34:49 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #158: GFLOPs: 2336.0540. Time: 60.8643 us. Best GFLOPs: 3605.5648
2023-11-11 01:34:49 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #159: GFLOPs: 3495.4568. Time: 40.6763 us. Best GFLOPs: 3605.5648
2023-11-11 01:34:49 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #160: GFLOPs: 3955.1346. Time: 35.9488 us. Best GFLOPs: 3955.1346
2023-11-11 01:34:49 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #161: GFLOPs: 2320.3857. Time: 61.2753 us. Best GFLOPs: 3955.1346
2023-11-11 01:34:49 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #162: GFLOPs: 3575.8655. Time: 39.7617 us. Best GFLOPs: 3955.1346
2023-11-11 01:34:49 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #163: GFLOPs: 3005.9334. Time: 47.3006 us. Best GFLOPs: 3955.1346
2023-11-11 01:34:49 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #164: GFLOPs: 3392.8605. Time: 41.9064 us. Best GFLOPs: 3955.1346
2023-11-11 01:34:49 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #165: GFLOPs: 3350.1448. Time: 42.4407 us. Best GFLOPs: 3955.1346
2023-11-11 01:34:49 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #166: GFLOPs: 1841.6690. Time: 77.2030 us. Best GFLOPs: 3955.1346
2023-11-11 01:34:49 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #167: GFLOPs: 2344.7972. Time: 60.6374 us. Best GFLOPs: 3955.1346
2023-11-11 01:34:49 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #168: GFLOPs: 4254.8018. Time: 33.4169 us. Best GFLOPs: 4254.8018
2023-11-11 01:34:49 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #169: GFLOPs: 2888.9255. Time: 49.2164 us. Best GFLOPs: 4254.8018
2023-11-11 01:34:49 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #170: GFLOPs: 2109.8306. Time: 67.3904 us. Best GFLOPs: 4254.8018
2023-11-11 01:34:49 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #171: GFLOPs: 2953.1599. Time: 48.1459 us. Best GFLOPs: 4254.8018
2023-11-11 01:34:49 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #172: GFLOPs: 2938.8621. Time: 48.3801 us. Best GFLOPs: 4254.8018
2023-11-11 01:34:49 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #173: GFLOPs: 3360.7413. Time: 42.3069 us. Best GFLOPs: 4254.8018
2023-11-11 01:34:49 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #174: GFLOPs: 2251.1213. Time: 63.1607 us. Best GFLOPs: 4254.8018
2023-11-11 01:34:49 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #175: GFLOPs: 2326.8641. Time: 61.1047 us. Best GFLOPs: 4254.8018
2023-11-11 01:34:49 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #176: GFLOPs: 2323.0223. Time: 61.2058 us. Best GFLOPs: 4254.8018
2023-11-11 01:34:49 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #177: GFLOPs: 2577.3481. Time: 55.1662 us. Best GFLOPs: 4254.8018
2023-11-11 01:34:49 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #178: GFLOPs: 2370.3765. Time: 59.9830 us. Best GFLOPs: 4254.8018
2023-11-11 01:34:49 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #179: GFLOPs: 3032.3575. Time: 46.8884 us. Best GFLOPs: 4254.8018
2023-11-11 01:34:49 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #180: GFLOPs: 2236.7400. Time: 63.5668 us. Best GFLOPs: 4254.8018
2023-11-11 01:34:49 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #181: GFLOPs: 2236.5647. Time: 63.5718 us. Best GFLOPs: 4254.8018
2023-11-11 01:34:49 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #182: GFLOPs: 2236.6963. Time: 63.5680 us. Best GFLOPs: 4254.8018
2023-11-11 01:34:49 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #183: GFLOPs: 2626.8580. Time: 54.1264 us. Best GFLOPs: 4254.8018
2023-11-11 01:34:49 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #184: GFLOPs: 2344.7517. Time: 60.6386 us. Best GFLOPs: 4254.8018
2023-11-11 01:34:49 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #185: GFLOPs: 4200.4562. Time: 33.8493 us. Best GFLOPs: 4254.8018
2023-11-11 01:34:49 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #186: GFLOPs: 2577.2809. Time: 55.1676 us. Best GFLOPs: 4254.8018
2023-11-11 01:34:49 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #187: GFLOPs: 4212.6437. Time: 33.7513 us. Best GFLOPs: 4254.8018
2023-11-11 01:34:49 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #188: GFLOPs: 2014.4496. Time: 70.5813 us. Best GFLOPs: 4254.8018
2023-11-11 01:34:49 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #189: GFLOPs: 3646.2623. Time: 38.9940 us. Best GFLOPs: 4254.8018
2023-11-11 01:34:49 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #190: GFLOPs: 1080.2060. Time: 131.6253 us. Best GFLOPs: 4254.8018
2023-11-11 01:34:49 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #191: GFLOPs: 1126.7090. Time: 126.1927 us. Best GFLOPs: 4254.8018
2023-11-11 01:34:49 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #192: GFLOPs: 801.0424. Time: 177.4967 us. Best GFLOPs: 4254.8018
2023-11-11 02:24:45 [INFO] [evolutionary_search.cc:713] Generating candidates......
2023-11-11 02:24:47 [INFO] [evolutionary_search.cc:715] Picked top 102 candidate(s) from database
2023-11-11 02:24:54 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310fb03b28)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310eec3b08)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310dc5caf8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310f7cc2d8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x563110643e78)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310dc32028)]: 402 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310dc5cfb8)]: 0 failure(s)
2023-11-11 02:25:00 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310fb03b28)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310eec3b08)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310dc5caf8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310f7cc2d8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x563110643e78)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310dc32028)]: 804 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310dc5cfb8)]: 0 failure(s)
2023-11-11 02:25:07 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310fb03b28)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310eec3b08)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310dc5caf8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310f7cc2d8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x563110643e78)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310dc32028)]: 1207 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310dc5cfb8)]: 0 failure(s)
2023-11-11 02:25:13 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310fb03b28)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310eec3b08)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310dc5caf8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310f7cc2d8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x563110643e78)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310dc32028)]: 1607 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310dc5cfb8)]: 0 failure(s)
2023-11-11 02:25:19 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310fb03b28)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310eec3b08)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310dc5caf8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310f7cc2d8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x563110643e78)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310dc32028)]: 2008 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310dc5cfb8)]: 0 failure(s)
2023-11-11 02:25:26 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310fb03b28)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310eec3b08)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310dc5caf8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310f7cc2d8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x563110643e78)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310dc32028)]: 2410 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310dc5cfb8)]: 0 failure(s)
2023-11-11 02:25:26 [INFO] [evolutionary_search.cc:723] Sampled 50 candidate(s)
2023-11-11 02:25:41 [INFO] [evolutionary_search.cc:621] Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310fb03b28)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310eec3b08)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310dc5caf8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310f7cc2d8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x563110643e78)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310dc32028)]: 60 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310dc5cfb8)]: 0 failure(s)
2023-11-11 02:26:00 [INFO] [evolutionary_search.cc:621] Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310fb03b28)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310eec3b08)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310dc5caf8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310f7cc2d8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x563110643e78)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310dc32028)]: 70 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310dc5cfb8)]: 0 failure(s)
2023-11-11 02:26:18 [INFO] [evolutionary_search.cc:621] Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310fb03b28)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310eec3b08)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310dc5caf8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310f7cc2d8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x563110643e78)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310dc32028)]: 54 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310dc5cfb8)]: 0 failure(s)
2023-11-11 02:26:37 [INFO] [evolutionary_search.cc:621] Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310fb03b28)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310eec3b08)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310dc5caf8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310f7cc2d8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x563110643e78)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310dc32028)]: 44 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310dc5cfb8)]: 0 failure(s)
2023-11-11 02:26:42 [INFO] [evolutionary_search.cc:649] Scores of the best 64 candidates:
[1 : 16]:	0.9833  0.9831  0.9718  0.9639  0.9609  0.9598  0.9596  0.9594  0.9592  0.9562  0.9496  0.9459  0.9438  0.9404  0.9393  0.9389
[17 : 32]:	0.9387  0.9386  0.9377  0.9359  0.9347  0.9317  0.9315  0.9285  0.9202  0.9197  0.9192  0.9182  0.9173  0.9125  0.9112  0.9097
[33 : 48]:	0.9096  0.9040  0.8999  0.8979  0.8971  0.8952  0.8952  0.8933  0.8922  0.8918  0.8898  0.8882  0.8869  0.8775  0.8760  0.8740
[49 : 64]:	0.8739  0.8736  0.8685  0.8684  0.8682  0.8679  0.8672  0.8670  0.8621  0.8616  0.8608  0.8605  0.8598  0.8595  0.8594  0.8589
2023-11-11 02:26:42 [INFO] [evolutionary_search.cc:727] Got 64 candidate(s) with evolutionary search
2023-11-11 02:26:42 [INFO] [evolutionary_search.cc:730] Sending 64 candidates(s) for measurement
2023-11-11 02:29:36 [INFO] [task_scheduler.cc:121] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #193: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/home2/xiachunwei/Software/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/home2/xiachunwei/Software/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/home2/xiachunwei/Software/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/home2/xiachunwei/Software/tvm/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/home2/xiachunwei/Software/tvm/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/home2/xiachunwei/Software/tvm/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/home2/xiachunwei/Software/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/home2/xiachunwei/Software/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  4: _ZN3tvm7runtime13PackedFun
  3: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  2: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  0: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  File "/home2/xiachunwei/Software/tvm/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(512), T.int64(7), T.int64(7)), "float32"), p1: T.Buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(512)), "float32"), p2: T.Buffer((T.int64(1), T.int64(512), T.int64(7), T.int64(7)), "float32"), p3: T.Buffer((T.int64(1), T.int64(512), T.int64(1), T.int64(1)), "float32"), p4: T.Buffer((T.int64(1), T.int64(512), T.int64(1), T.int64(1)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(512), T.int64(7), T.int64(7)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        input_tile_local = T.alloc_buffer((T.int64(512), T.int64(16), T.int64(4), T.int64(4)), scope="local")
        data_pack = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(16)))
        bgemm = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(16)))
        inverse_local = T.alloc_buffer((T.int64(512), T.int64(16), T.int64(2), T.int64(2)), scope="local")
        data_pack_local = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(16)), scope="local")
        bgemm_local = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(16)), scope="local")
        data_pack_shared = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(16)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(512)), scope="shared")
        for ci_p_fused_0 in T.thread_binding(T.int64(64), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 1024, "pragma_unroll_explicit": 1}):
            for ci_p_fused_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(1), T.int64(4), T.int64(4)):
                    with T.block("input_tile"):
                        v_ci = T.axis.spatial(T.int64(512), ci_p_fused_0 * T.int64(8) + ci_p_fused_1 // T.int64(16) + ax0)
                        v_p = T.axis.spatial(T.int64(16), ci_p_fused_1 % T.int64(16) + ax1)
                        v_eps, v_nu = T.axis.remap("SS", [ax2, ax3])
                        T.reads(p0[v_p // T.int64(16), v_ci, v_p % T.int64(16) // T.int64(4) * T.int64(2) + v_eps - T.int64(1), v_p % T.int64(4) * T.int64(2) + v_nu - T.int64(1)])
                        T.writes(input_tile_local[v_ci, v_p, v_eps, v_nu])
                        T.block_attr({"schedule_rule": "None"})
                        input_tile_local[v_ci, v_p, v_eps, v_nu] = T.if_then_else(T.int64(1) <= v_p % T.int64(16) // T.int64(4) * T.int64(2) + v_eps and v_p % T.int64(16) // T.int64(4) * T.int64(2) + v_eps < T.int64(8) and T.int64(1) <= v_p % T.int64(4) * T.int64(2) + v_nu and v_p % T.int64(4) * T.int64(2) + v_nu < T.int64(8), p0[v_p // T.int64(16), v_ci, v_p % T.int64(16) // T.int64(4) * T.int64(2) + v_eps - T.int64(1), v_p % T.int64(4) * T.int64(2) + v_nu - T.int64(1)], T.float32(0))
                for eps in T.unroll(T.int64(4)):
                    for nu in T.unroll(T.int64(4)):
                        with T.block("data_pack_init"):
                            v_eps, v_nu = T.axis.remap("SS", [eps, nu])
                            v_ci = T.axis.spatial(T.int64(512), (ci_p_fused_0 * T.int64(128) + ci_p_fused_1) // T.int64(16))
                            v_p = T.axis.spatial(T.int64(16), (ci_p_fused_0 * T.int64(128) + ci_p_fused_1) % T.int64(16))
                            T.reads()
                            T.writes(data_pack_local[v_eps, v_nu, v_ci, v_p])
                            T.block_attr({"schedule_rule": "conv2d_nchw_winograd_data_pack"})
                            data_pack_local[v_eps, v_nu, v_ci, v_p] = T.float32(0)
                        for r_a in T.unroll(T.int64(4)):
                            for r_b in T.unroll(T.int64(4)):
                                with T.block("data_pack_update"):
                                    v_eps, v_nu = T.axis.remap("SS", [eps, nu])
                                    v_ci = T.axis.spatial(T.int64(512), (ci_p_fused_0 * T.int64(128) + ci_p_fused_1) // T.int64(16))
                                    v_p = T.axis.spatial(T.int64(16), (ci_p_fused_0 * T.int64(128) + ci_p_fused_1) % T.int64(16))
                                    v_r_a, v_r_b = T.axis.remap("RR", [r_a, r_b])
                                    T.reads(data_pack_local[v_eps, v_nu, v_ci, v_p], input_tile_local[v_ci, v_p, v_r_a, v_r_b])
                                    T.writes(data_pack_local[v_eps, v_nu, v_ci, v_p])
                                    T.block_attr({"schedule_rule": "conv2d_nchw_winograd_data_pack"})
                                    data_pack_local[v_eps, v_nu, v_ci, v_p] = data_pack_local[v_eps, v_nu, v_ci, v_p] + input_tile_local[v_ci, v_p, v_r_a, v_r_b] * T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(3), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(1), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(0), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(3), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(1), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(0), T.float32(1), T.float32(0))))))))))))))))) * T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(3), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(1), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(0), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(3), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(1), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(0), T.float32(1), T.float32(0)))))))))))))))))
                for ax0, ax1, ax2, ax3 in T.grid(T.int64(4), T.int64(4), T.int64(1), T.int64(1)):
                    with T.block("data_pack_local"):
                        v0, v1 = T.axis.remap("SS", [ax0, ax1])
                        v2 = T.axis.spatial(T.int64(512), ci_p_fused_0 * T.int64(8) + ci_p_fused_1 // T.int64(16) + ax2)
                        v3 = T.axis.spatial(T.int64(16), ci_p_fused_1 % T.int64(16) + ax3)
                        T.reads(data_pack_local[v0, v1, v2, v3])
                        T.writes(data_pack[v0, v1, v2, v3])
                        data_pack[v0, v1, v2, v3] = data_pack_local[v0, v1, v2, v3]
        for eps_0_nu_0_co_0_p_0_fused in T.thread_binding(T.int64(64), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 1024, "pragma_unroll_explicit": 1}):
            for eps_1_nu_1_co_1_p_1_fused in T.thread_binding(T.int64(8), thread="vthread.x"):
                for eps_2_nu_2_co_2_p_2_fused in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                    for eps_3_init, nu_3_init, co_3_init, p_3_init, eps_4_init, nu_4_init, co_4_init, p_4_init in T.grid(T.int64(1), T.int64(1), T.int64(2), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("bgemm_init"):
                            v_eps = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(32) * T.int64(2) + eps_2_nu_2_co_2_p_2_fused // T.int64(64) + eps_3_init + eps_4_init)
                            v_nu = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused % T.int64(32) // T.int64(16) * T.int64(2) + eps_1_nu_1_co_1_p_1_fused // T.int64(4) + nu_3_init + nu_4_init)
                            v_co = T.axis.spatial(T.int64(512), eps_0_nu_0_co_0_p_0_fused % T.int64(16) * T.int64(32) + eps_1_nu_1_co_1_p_1_fused % T.int64(4) // T.int64(2) * T.int64(16) + eps_2_nu_2_co_2_p_2_fused % T.int64(64) // T.int64(8) * T.int64(2) + co_3_init + co_4_init)
                            v_p = T.axis.spatial(T.int64(16), eps_1_nu_1_co_1_p_1_fused % T.int64(2) * T.int64(8) + eps_2_nu_2_co_2_p_2_fused % T.int64(8) + p_3_init + p_4_init)
                            T.reads()
                            T.writes(bgemm_local[v_eps, v_nu, v_co, v_p])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            bgemm_local[v_eps, v_nu, v_co, v_p] = T.float32(0)
                    for ci_0 in range(T.int64(8)):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(8)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("data_pack_shared"):
                                        v0 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(32) * T.int64(2) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(512) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) // T.int64(2048))
                                        v1 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused % T.int64(32) // T.int64(16) * T.int64(2) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(512) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) % T.int64(2048) // T.int64(1024))
                                        v2 = T.axis.spatial(T.int64(512), ci_0 * T.int64(64) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(512) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) % T.int64(1024) // T.int64(16))
                                        v3 = T.axis.spatial(T.int64(16), (ax0_ax1_ax2_ax3_fused_0 * T.int64(512) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) % T.int64(16))
                                        T.reads(data_pack[v0, v1, v2, v3])
                                        T.writes(data_pack_shared[v0, v1, v2, v3])
                                        data_pack_shared[v0, v1, v2, v3] = data_pack[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(32)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("p1_shared"):
                                        v0 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(32) * T.int64(2) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) // T.int64(4096))
                                        v1 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused % T.int64(32) // T.int64(16) * T.int64(2) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) % T.int64(4096) // T.int64(2048))
                                        v2 = T.axis.spatial(T.int64(512), ci_0 * T.int64(64) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) % T.int64(2048) // T.int64(32))
                                        v3 = T.axis.spatial(T.int64(512), eps_0_nu_0_co_0_p_0_fused % T.int64(16) * T.int64(32) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) % T.int64(32))
                                        T.reads(p1[v0, v1, v2, v3])
                                        T.writes(p1_shared[v0, v1, v2, v3])
                                        p1_shared[v0, v1, v2, v3] = p1[v0, v1, v2, v3]
                        for ci_1, eps_3, nu_3, co_3, p_3, ci_2, eps_4, nu_4, co_4, p_4 in T.grid(T.int64(2), T.int64(1), T.int64(1), T.int64(2), T.int64(1), T.int64(32), T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                            with T.block("bgemm_update"):
                                v_eps = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(32) * T.int64(2) + eps_2_nu_2_co_2_p_2_fused // T.int64(64) + eps_3 + eps_4)
                                v_nu = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused % T.int64(32) // T.int64(16) * T.int64(2) + eps_1_nu_1_co_1_p_1_fused // T.int64(4) + nu_3 + nu_4)
                                v_co = T.axis.spatial(T.int64(512), eps_0_nu_0_co_0_p_0_fused % T.int64(16) * T.int64(32) + eps_1_nu_1_co_1_p_1_fused % T.int64(4) // T.int64(2) * T.int64(16) + eps_2_nu_2_co_2_p_2_fused % T.int64(64) // T.int64(8) * T.int64(2) + co_3 + co_4)
                                v_p = T.axis.spatial(T.int64(16), eps_1_nu_1_co_1_p_1_fused % T.int64(2) * T.int64(8) + eps_2_nu_2_co_2_p_2_fused % T.int64(8) + p_3 + p_4)
                                v_ci = T.axis.reduce(T.int64(512), ci_0 * T.int64(64) + ci_1 * T.int64(32) + ci_2)
                                T.reads(bgemm_local[v_eps, v_nu, v_co, v_p], data_pack_shared[v_eps, v_nu, v_ci, v_p], p1_shared[v_eps, v_nu, v_ci, v_co])
                                T.writes(bgemm_local[v_eps, v_nu, v_co, v_p])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                bgemm_local[v_eps, v_nu, v_co, v_p] = bgemm_local[v_eps, v_nu, v_co, v_p] + data_pack_shared[v_eps, v_nu, v_ci, v_p] * p1_shared[v_eps, v_nu, v_ci, v_co]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(1), T.int64(2), T.int64(1)):
                        with T.block("bgemm_local"):
                            v0 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(32) * T.int64(2) + eps_2_nu_2_co_2_p_2_fused // T.int64(64) + ax0)
                            v1 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused % T.int64(32) // T.int64(16) * T.int64(2) + eps_1_nu_1_co_1_p_1_fused // T.int64(4) + ax1)
                            v2 = T.axis.spatial(T.int64(512), eps_0_nu_0_co_0_p_0_fused % T.int64(16) * T.int64(32) + eps_1_nu_1_co_1_p_1_fused % T.int64(4) // T.int64(2) * T.int64(16) + eps_2_nu_2_co_2_p_2_fused % T.int64(64) // T.int64(8) * T.int64(2) + ax2)
                            v3 = T.axis.spatial(T.int64(16), eps_1_nu_1_co_1_p_1_fused % T.int64(2) * T.int64(8) + eps_2_nu_2_co_2_p_2_fused % T.int64(8) + ax3)
                            T.reads(bgemm_local[v0, v1, v2, v3])
                            T.writes(bgemm[v0, v1, v2, v3])
                            bgemm[v0, v1, v2, v3] = bgemm_local[v0, v1, v2, v3]
        for n_co_h_0_w_0_fused_0 in T.thread_binding(T.int64(256), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 1024, "pragma_unroll_explicit": 1}):
            for n_co_h_0_w_0_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                    for ax2 in T.unroll(T.int64(2)):
                        for ax3 in T.unroll(T.int64(2)):
                            with T.block("inverse_init"):
                                v_co = T.axis.spatial(T.int64(512), (n_co_h_0_w_0_fused_0 * T.int64(32) + n_co_h_0_w_0_fused_1) // T.int64(16) + ax0)
                                v_p = T.axis.spatial(T.int64(16), (n_co_h_0_w_0_fused_0 * T.int64(32) + n_co_h_0_w_0_fused_1) % T.int64(16) + ax1)
                                v_vh, v_vw = T.axis.remap("SS", [ax2, ax3])
                                T.reads()
                                T.writes(inverse_local[v_co, v_p, v_vh, v_vw])
                                T.block_attr({"schedule_rule": "conv2d_nchw_winograd_inverse"})
                                inverse_local[v_co, v_p, v_vh, v_vw] = T.float32(0)
                            for ax4 in T.unroll(T.int64(4)):
                                for ax5 in T.unroll(T.int64(4)):
                                    with T.block("inverse_update"):
                                        v_co = T.axis.spatial(T.int64(512), (n_co_h_0_w_0_fused_0 * T.int64(32) + n_co_h_0_w_0_fused_1) // T.int64(16) + ax0)
                                        v_p = T.axis.spatial(T.int64(16), (n_co_h_0_w_0_fused_0 * T.int64(32) + n_co_h_0_w_0_fused_1) % T.int64(16) + ax1)
                                        v_vh, v_vw, v_r_a, v_r_b = T.axis.remap("SSRR", [ax2, ax3, ax4, ax5])
                                        T.reads(inverse_local[v_co, v_p, v_vh, v_vw], bgemm[v_r_a, v_r_b, v_co, v_p])
                                        T.writes(inverse_local[v_co, v_p, v_vh, v_vw])
                                        T.block_attr({"schedule_rule": "conv2d_nchw_winograd_inverse"})
                                        inverse_local[v_co, v_p, v_vh, v_vw] = inverse_local[v_co, v_p, v_vh, v_vw] + bgemm[v_r_a, v_r_b, v_co, v_p] * T.Select(v_r_a % T.int64(4) == T.int64(3) and v_vh % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_vh % T.int64(2) == T.int64(0), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_vh % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_vh % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_vh % T.int64(2) == T.int64(1), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_vh % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_vh % T.int64(2) == T.int64(1), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_vh % T.int64(2) == T.int64(0), T.float32(1), T.float32(0))))))))) * T.Select(v_r_b % T.int64(4) == T.int64(3) and v_vw % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_vw % T.int64(2) == T.int64(0), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_vw % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_vw % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_vw % T.int64(2) == T.int64(1), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_vw % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_vw % T.int64(2) == T.int64(1), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_vw % T.int64(2) == T.int64(0), T.float32(1), T.float32(0)))))))))
                for h_1, w_1 in T.grid(T.int64(2), T.int64(2)):
                    with T.block("conv2d_winograd"):
                        v_n = T.axis.spatial(T.int64(1), T.int64(0))
                        v_co = T.axis.spatial(T.int64(512), (n_co_h_0_w_0_fused_0 * T.int64(32) + n_co_h_0_w_0_fused_1) // T.int64(16))
                        v_h = T.axis.spatial(T.int64(7), (n_co_h_0_w_0_fused_0 * T.int64(32) + n_co_h_0_w_0_fused_1) % T.int64(16) // T.int64(4) * T.int64(2) + h_1)
                        v_w = T.axis.spatial(T.int64(7), (n_co_h_0_w_0_fused_0 * T.int64(32) + n_co_h_0_w_0_fused_1) % T.int64(4) * T.int64(2) + w_1)
                        T.where((n_co_h_0_w_0_fused_0 * T.int64(32) + n_co_h_0_w_0_fused_1) % T.int64(16) // T.int64(4) * T.int64(2) + h_1 < T.int64(7) and (n_co_h_0_w_0_fused_0 * T.int64(32) + n_co_h_0_w_0_fused_1) % T.int64(4) * T.int64(2) + w_1 < T.int64(7))
                        T.reads(inverse_local[v_co, v_n * T.int64(16) + v_h // T.int64(2) * T.int64(4) + v_w // T.int64(2), v_h % T.int64(2), v_w % T.int64(2)], p2[v_n, v_co, v_h, v_w], p3[v_n, v_co, T.int64(0), T.int64(0)], p4[v_n, v_co, T.int64(0), T.int64(0)])
                        T.writes(T_relu[v_n, v_co, v_h, v_w])
                        T_relu[v_n, v_co, v_h, v_w] = T.max((inverse_local[v_co, v_n * T.int64(16) + v_h // T.int64(2) * T.int64(4) + v_w // T.int64(2), v_h % T.int64(2), v_w % T.int64(2)] + p2[v_n, v_co, v_h, v_w]) * p3[v_n, v_co, T.int64(0), T.int64(0)] + p4[v_n, v_co, T.int64(0), T.int64(0)], T.float32(0))
b0 = sch.get_block(name="data_pack", func_name="main")
b1 = sch.get_block(name="bgemm", func_name="main")
b2 = sch.get_block(name="inverse", func_name="main")
b3 = sch.get_block(name="conv2d_winograd", func_name="main")
b4 = sch.get_block(name="T_add", func_name="main")
b5 = sch.get_block(name="T_multiply", func_name="main")
b6 = sch.get_block(name="T_add_1", func_name="main")
b7 = sch.get_block(name="T_relu", func_name="main")
b8 = sch.get_block(name="root", func_name="main")
b9, b10 = sch.get_producers(block=b2)
sch.compute_inline(block=b10)
b11, = sch.get_consumers(block=b2)
l12, l13, l14, l15 = sch.get_loops(block=b11)
l16, l17 = sch.split(loop=l14, factors=[None, 2], preserve_unit_iters=True)
l18, l19 = sch.split(loop=l15, factors=[None, 2], preserve_unit_iters=True)
sch.reorder(l16, l18, l17, l19)
sch.compute_at(block=b2, loop=l18, preserve_unit_loops=True, index=-1)
sch.set_scope(block=b2, buffer_index=0, storage_scope="local")
l20, l21, l22, l23, l24, l25, l26, l27, l28, l29 = sch.get_loops(block=b2)
sch.unroll(loop=l26)
sch.unroll(loop=l27)
sch.unroll(loop=l28)
sch.unroll(loop=l29)
b30, b31 = sch.get_producers(block=b0)
sch.compute_inline(block=b31)
b32, = sch.get_producers(block=b30)
l33, l34, l35, l36, l37, l38 = sch.get_loops(block=b0)
sch.reorder(l35, l36, l33, l34, l37, l38)
sch.unroll(loop=l33)
sch.unroll(loop=l34)
sch.unroll(loop=l37)
sch.unroll(loop=l38)
l39 = sch.fuse(l35, l36, preserve_unit_iters=True)
v40 = sch.sample_categorical(candidates=[32, 64, 128, 256, 512, 1024], probs=[0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666], decision=2)
l41, l42 = sch.split(loop=l39, factors=[None, v40], preserve_unit_iters=True)
sch.bind(loop=l41, thread_axis="blockIdx.x")
sch.bind(loop=l42, thread_axis="threadIdx.x")
b43 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b43, loop=l42, preserve_unit_loops=True, index=-1)
sch.compute_at(block=b30, loop=l42, preserve_unit_loops=True, index=-1)
sch.set_scope(block=b30, buffer_index=0, storage_scope="local")
sch.compute_inline(block=b32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l44, l45, l46, l47, l48 = sch.get_loops(block=b1)
v49, v50, v51, v52, v53 = sch.sample_perfect_tile(loop=l44, n=5, max_innermost_factor=64, decision=[2, 1, 2, 1, 1])
l54, l55, l56, l57, l58 = sch.split(loop=l44, factors=[v49, v50, v51, v52, v53], preserve_unit_iters=True)
v59, v60, v61, v62, v63 = sch.sample_perfect_tile(loop=l45, n=5, max_innermost_factor=64, decision=[2, 2, 1, 1, 1])
l64, l65, l66, l67, l68 = sch.split(loop=l45, factors=[v59, v60, v61, v62, v63], preserve_unit_iters=True)
v69, v70, v71, v72, v73 = sch.sample_perfect_tile(loop=l46, n=5, max_innermost_factor=64, decision=[16, 2, 8, 2, 1])
l74, l75, l76, l77, l78 = sch.split(loop=l46, factors=[v69, v70, v71, v72, v73], preserve_unit_iters=True)
v79, v80, v81, v82, v83 = sch.sample_perfect_tile(loop=l47, n=5, max_innermost_factor=64, decision=[1, 2, 8, 1, 1])
l84, l85, l86, l87, l88 = sch.split(loop=l47, factors=[v79, v80, v81, v82, v83], preserve_unit_iters=True)
v89, v90, v91 = sch.sample_perfect_tile(loop=l48, n=3, max_innermost_factor=64, decision=[8, 2, 32])
l92, l93, l94 = sch.split(loop=l48, factors=[v89, v90, v91], preserve_unit_iters=True)
sch.reorder(l54, l64, l74, l84, l55, l65, l75, l85, l56, l66, l76, l86, l92, l93, l57, l67, l77, l87, l94, l58, l68, l78, l88)
l95 = sch.fuse(l54, l64, l74, l84, preserve_unit_iters=True)
sch.bind(loop=l95, thread_axis="blockIdx.x")
l96 = sch.fuse(l55, l65, l75, l85, preserve_unit_iters=True)
sch.bind(loop=l96, thread_axis="vthread.x")
l97 = sch.fuse(l56, l66, l76, l86, preserve_unit_iters=True)
sch.bind(loop=l97, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b98 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b98, loop=l97, preserve_unit_loops=True, index=-1)
b99 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b99, loop=l92, preserve_unit_loops=True, index=-1)
l100, l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b99)
l108 = sch.fuse(l104, l105, l106, l107, preserve_unit_iters=True)
v109 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b99, ann_key="meta_schedule.cooperative_fetch", ann_val=v109)
b110 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b110, loop=l92, preserve_unit_loops=True, index=-1)
l111, l112, l113, l114, l115, l116, l117, l118 = sch.get_loops(block=b110)
l119 = sch.fuse(l115, l116, l117, l118, preserve_unit_iters=True)
v120 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b110, ann_key="meta_schedule.cooperative_fetch", ann_val=v120)
sch.reverse_compute_inline(block=b7)
sch.reverse_compute_inline(block=b6)
sch.reverse_compute_inline(block=b5)
sch.reverse_compute_inline(block=b4)
v121 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b8, ann_key="meta_schedule.unroll_explicit", ann_val=v121)
l122, l123, l124, l125, l126, l127 = sch.get_loops(block=b3)
l128 = sch.fuse(l122, l123, l124, l125, preserve_unit_iters=True)
v129 = sch.sample_categorical(candidates=[32, 64, 128, 256, 512, 1024], probs=[0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666], decision=0)
l130, l131 = sch.split(loop=l128, factors=[None, v129], preserve_unit_iters=True)
sch.bind(loop=l130, thread_axis="blockIdx.x")
sch.bind(loop=l131, thread_axis="threadIdx.x")
sch.enter_postproc()
sch.unannotate(block_or_loop=b99, ann_key="meta_schedule.cooperative_fetch")
l132, l133, l134, l135, l136 = sch.get_loops(block=b99)
l137, l138, l139 = sch.split(loop=l136, factors=[None, 128, 4], preserve_unit_iters=True)
sch.vectorize(loop=l139)
sch.bind(loop=l138, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b110, ann_key="meta_schedule.cooperative_fetch")
l140, l141, l142, l143, l144 = sch.get_loops(block=b110)
l145, l146, l147 = sch.split(loop=l144, factors=[None, 128, 2], preserve_unit_iters=True)
sch.vectorize(loop=l147)
sch.bind(loop=l146, thread_axis="threadIdx.x")
b148 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b148, ann_key="meta_schedule.unroll_explicit")
b149, b150, b151, b152, b153, b154, b155, b156, b157 = sch.get_child_blocks(b148)
l158, l159, l160, l161, l162, l163 = sch.get_loops(block=b149)
l164, l165, l166, l167, l168, l169 = sch.get_loops(block=b150)
sch.annotate(block_or_loop=l164, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l164, ann_key="pragma_unroll_explicit", ann_val=1)
l170, l171, l172, l173, l174, l175 = sch.get_loops(block=b151)
l176, l177, l178, l179, l180, l181, l182 = sch.get_loops(block=b152)
l183, l184, l185, l186, l187, l188, l189 = sch.get_loops(block=b153)
l190, l191, l192, l193, l194, l195, l196, l197, l198, l199, l200, l201, l202, l203 = sch.get_loops(block=b154)
sch.annotate(block_or_loop=l190, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l190, ann_key="pragma_unroll_explicit", ann_val=1)
l204, l205, l206, l207, l208, l209, l210 = sch.get_loops(block=b155)
l211, l212, l213, l214, l215, l216, l217, l218 = sch.get_loops(block=b156)
sch.annotate(block_or_loop=l211, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l211, ann_key="pragma_unroll_explicit", ann_val=1)
l219, l220, l221, l222 = sch.get_loops(block=b157)
b223 = sch.get_block(name="data_pack", func_name="main")
l224, l225, l226, l227, l228, l229 = sch.get_loops(block=b223)
b230 = sch.decompose_reduction(block=b223, loop=l228)
b231 = sch.get_block(name="bgemm", func_name="main")
l232, l233, l234, l235, l236, l237, l238, l239, l240, l241, l242, l243, l244, l245 = sch.get_loops(block=b231)
b246 = sch.decompose_reduction(block=b231, loop=l235)
b247 = sch.get_block(name="inverse", func_name="main")
l248, l249, l250, l251, l252, l253, l254, l255 = sch.get_loops(block=b247)
b256 = sch.decompose_reduction(block=b247, loop=l254)
2023-11-11 02:29:36 [INFO] [task_scheduler.cc:121] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #194: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/home2/xiachunwei/Software/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/home2/xiachunwei/Software/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/home2/xiachunwei/Software/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/home2/xiachunwei/Software/tvm/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/home2/xiachunwei/Software/tvm/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/home2/xiachunwei/Software/tvm/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/home2/xiachunwei/Software/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/home2/xiachunwei/Software/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  4: _ZN3tvm7runtime13PackedFun
  3: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  2: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  0: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  File "/home2/xiachunwei/Software/tvm/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(512), T.int64(7), T.int64(7)), "float32"), p1: T.Buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(512)), "float32"), p2: T.Buffer((T.int64(1), T.int64(512), T.int64(7), T.int64(7)), "float32"), p3: T.Buffer((T.int64(1), T.int64(512), T.int64(1), T.int64(1)), "float32"), p4: T.Buffer((T.int64(1), T.int64(512), T.int64(1), T.int64(1)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(512), T.int64(7), T.int64(7)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        input_tile_local = T.alloc_buffer((T.int64(512), T.int64(16), T.int64(4), T.int64(4)), scope="local")
        data_pack = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(16)))
        bgemm = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(16)))
        inverse_local = T.alloc_buffer((T.int64(512), T.int64(16), T.int64(2), T.int64(2)), scope="local")
        data_pack_local = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(16)), scope="local")
        bgemm_local = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(16)), scope="local")
        data_pack_shared = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(16)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(512)), scope="shared")
        for ci_p_fused_0 in T.thread_binding(T.int64(64), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 1024, "pragma_unroll_explicit": 1}):
            for ci_p_fused_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(1), T.int64(4), T.int64(4)):
                    with T.block("input_tile"):
                        v_ci = T.axis.spatial(T.int64(512), ci_p_fused_0 * T.int64(8) + ci_p_fused_1 // T.int64(16) + ax0)
                        v_p = T.axis.spatial(T.int64(16), ci_p_fused_1 % T.int64(16) + ax1)
                        v_eps, v_nu = T.axis.remap("SS", [ax2, ax3])
                        T.reads(p0[v_p // T.int64(16), v_ci, v_p % T.int64(16) // T.int64(4) * T.int64(2) + v_eps - T.int64(1), v_p % T.int64(4) * T.int64(2) + v_nu - T.int64(1)])
                        T.writes(input_tile_local[v_ci, v_p, v_eps, v_nu])
                        T.block_attr({"schedule_rule": "None"})
                        input_tile_local[v_ci, v_p, v_eps, v_nu] = T.if_then_else(T.int64(1) <= v_p % T.int64(16) // T.int64(4) * T.int64(2) + v_eps and v_p % T.int64(16) // T.int64(4) * T.int64(2) + v_eps < T.int64(8) and T.int64(1) <= v_p % T.int64(4) * T.int64(2) + v_nu and v_p % T.int64(4) * T.int64(2) + v_nu < T.int64(8), p0[v_p // T.int64(16), v_ci, v_p % T.int64(16) // T.int64(4) * T.int64(2) + v_eps - T.int64(1), v_p % T.int64(4) * T.int64(2) + v_nu - T.int64(1)], T.float32(0))
                for eps in T.unroll(T.int64(4)):
                    for nu in T.unroll(T.int64(4)):
                        with T.block("data_pack_init"):
                            v_eps, v_nu = T.axis.remap("SS", [eps, nu])
                            v_ci = T.axis.spatial(T.int64(512), (ci_p_fused_0 * T.int64(128) + ci_p_fused_1) // T.int64(16))
                            v_p = T.axis.spatial(T.int64(16), (ci_p_fused_0 * T.int64(128) + ci_p_fused_1) % T.int64(16))
                            T.reads()
                            T.writes(data_pack_local[v_eps, v_nu, v_ci, v_p])
                            T.block_attr({"schedule_rule": "conv2d_nchw_winograd_data_pack"})
                            data_pack_local[v_eps, v_nu, v_ci, v_p] = T.float32(0)
                        for r_a in T.unroll(T.int64(4)):
                            for r_b in T.unroll(T.int64(4)):
                                with T.block("data_pack_update"):
                                    v_eps, v_nu = T.axis.remap("SS", [eps, nu])
                                    v_ci = T.axis.spatial(T.int64(512), (ci_p_fused_0 * T.int64(128) + ci_p_fused_1) // T.int64(16))
                                    v_p = T.axis.spatial(T.int64(16), (ci_p_fused_0 * T.int64(128) + ci_p_fused_1) % T.int64(16))
                                    v_r_a, v_r_b = T.axis.remap("RR", [r_a, r_b])
                                    T.reads(data_pack_local[v_eps, v_nu, v_ci, v_p], input_tile_local[v_ci, v_p, v_r_a, v_r_b])
                                    T.writes(data_pack_local[v_eps, v_nu, v_ci, v_p])
                                    T.block_attr({"schedule_rule": "conv2d_nchw_winograd_data_pack"})
                                    data_pack_local[v_eps, v_nu, v_ci, v_p] = data_pack_local[v_eps, v_nu, v_ci, v_p] + input_tile_local[v_ci, v_p, v_r_a, v_r_b] * T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(3), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(1), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(0), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(3), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(1), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(0), T.float32(1), T.float32(0))))))))))))))))) * T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(3), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(1), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(0), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(3), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(1), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(0), T.float32(1), T.float32(0)))))))))))))))))
                for ax0, ax1, ax2, ax3 in T.grid(T.int64(4), T.int64(4), T.int64(1), T.int64(1)):
                    with T.block("data_pack_local"):
                        v0, v1 = T.axis.remap("SS", [ax0, ax1])
                        v2 = T.axis.spatial(T.int64(512), ci_p_fused_0 * T.int64(8) + ci_p_fused_1 // T.int64(16) + ax2)
                        v3 = T.axis.spatial(T.int64(16), ci_p_fused_1 % T.int64(16) + ax3)
                        T.reads(data_pack_local[v0, v1, v2, v3])
                        T.writes(data_pack[v0, v1, v2, v3])
                        data_pack[v0, v1, v2, v3] = data_pack_local[v0, v1, v2, v3]
        for eps_0_nu_0_co_0_p_0_fused in T.thread_binding(T.int64(64), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 1024, "pragma_unroll_explicit": 1}):
            for eps_1_nu_1_co_1_p_1_fused in T.thread_binding(T.int64(8), thread="vthread.x"):
                for eps_2_nu_2_co_2_p_2_fused in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                    for eps_3_init, nu_3_init, co_3_init, p_3_init, eps_4_init, nu_4_init, co_4_init, p_4_init in T.grid(T.int64(1), T.int64(1), T.int64(2), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("bgemm_init"):
                            v_eps = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(32) * T.int64(2) + eps_1_nu_1_co_1_p_1_fused // T.int64(4) + eps_3_init + eps_4_init)
                            v_nu = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused % T.int64(32) // T.int64(16) * T.int64(2) + eps_2_nu_2_co_2_p_2_fused // T.int64(64) + nu_3_init + nu_4_init)
                            v_co = T.axis.spatial(T.int64(512), eps_0_nu_0_co_0_p_0_fused % T.int64(16) * T.int64(32) + eps_1_nu_1_co_1_p_1_fused % T.int64(4) // T.int64(2) * T.int64(16) + eps_2_nu_2_co_2_p_2_fused % T.int64(64) // T.int64(8) * T.int64(2) + co_3_init + co_4_init)
                            v_p = T.axis.spatial(T.int64(16), eps_1_nu_1_co_1_p_1_fused % T.int64(2) * T.int64(8) + eps_2_nu_2_co_2_p_2_fused % T.int64(8) + p_3_init + p_4_init)
                            T.reads()
                            T.writes(bgemm_local[v_eps, v_nu, v_co, v_p])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            bgemm_local[v_eps, v_nu, v_co, v_p] = T.float32(0)
                    for ci_0 in range(T.int64(8)):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(32)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                                with T.block("data_pack_shared"):
                                    v0 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(32) * T.int64(2) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1) // T.int64(2048))
                                    v1 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused % T.int64(32) // T.int64(16) * T.int64(2) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1) % T.int64(2048) // T.int64(1024))
                                    v2 = T.axis.spatial(T.int64(512), ci_0 * T.int64(64) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1) % T.int64(1024) // T.int64(16))
                                    v3 = T.axis.spatial(T.int64(16), (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1) % T.int64(16))
                                    T.reads(data_pack[v0, v1, v2, v3])
                                    T.writes(data_pack_shared[v0, v1, v2, v3])
                                    data_pack_shared[v0, v1, v2, v3] = data_pack[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(32)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("p1_shared"):
                                        v0 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(32) * T.int64(2) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) // T.int64(4096))
                                        v1 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused % T.int64(32) // T.int64(16) * T.int64(2) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) % T.int64(4096) // T.int64(2048))
                                        v2 = T.axis.spatial(T.int64(512), ci_0 * T.int64(64) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) % T.int64(2048) // T.int64(32))
                                        v3 = T.axis.spatial(T.int64(512), eps_0_nu_0_co_0_p_0_fused % T.int64(16) * T.int64(32) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) % T.int64(32))
                                        T.reads(p1[v0, v1, v2, v3])
                                        T.writes(p1_shared[v0, v1, v2, v3])
                                        p1_shared[v0, v1, v2, v3] = p1[v0, v1, v2, v3]
                        for ci_1, eps_3, nu_3, co_3, p_3, ci_2, eps_4, nu_4, co_4, p_4 in T.grid(T.int64(2), T.int64(1), T.int64(1), T.int64(2), T.int64(1), T.int64(32), T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                            with T.block("bgemm_update"):
                                v_eps = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(32) * T.int64(2) + eps_1_nu_1_co_1_p_1_fused // T.int64(4) + eps_3 + eps_4)
                                v_nu = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused % T.int64(32) // T.int64(16) * T.int64(2) + eps_2_nu_2_co_2_p_2_fused // T.int64(64) + nu_3 + nu_4)
                                v_co = T.axis.spatial(T.int64(512), eps_0_nu_0_co_0_p_0_fused % T.int64(16) * T.int64(32) + eps_1_nu_1_co_1_p_1_fused % T.int64(4) // T.int64(2) * T.int64(16) + eps_2_nu_2_co_2_p_2_fused % T.int64(64) // T.int64(8) * T.int64(2) + co_3 + co_4)
                                v_p = T.axis.spatial(T.int64(16), eps_1_nu_1_co_1_p_1_fused % T.int64(2) * T.int64(8) + eps_2_nu_2_co_2_p_2_fused % T.int64(8) + p_3 + p_4)
                                v_ci = T.axis.reduce(T.int64(512), ci_0 * T.int64(64) + ci_1 * T.int64(32) + ci_2)
                                T.reads(bgemm_local[v_eps, v_nu, v_co, v_p], data_pack_shared[v_eps, v_nu, v_ci, v_p], p1_shared[v_eps, v_nu, v_ci, v_co])
                                T.writes(bgemm_local[v_eps, v_nu, v_co, v_p])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                bgemm_local[v_eps, v_nu, v_co, v_p] = bgemm_local[v_eps, v_nu, v_co, v_p] + data_pack_shared[v_eps, v_nu, v_ci, v_p] * p1_shared[v_eps, v_nu, v_ci, v_co]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(1), T.int64(2), T.int64(1)):
                        with T.block("bgemm_local"):
                            v0 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(32) * T.int64(2) + eps_1_nu_1_co_1_p_1_fused // T.int64(4) + ax0)
                            v1 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused % T.int64(32) // T.int64(16) * T.int64(2) + eps_2_nu_2_co_2_p_2_fused // T.int64(64) + ax1)
                            v2 = T.axis.spatial(T.int64(512), eps_0_nu_0_co_0_p_0_fused % T.int64(16) * T.int64(32) + eps_1_nu_1_co_1_p_1_fused % T.int64(4) // T.int64(2) * T.int64(16) + eps_2_nu_2_co_2_p_2_fused % T.int64(64) // T.int64(8) * T.int64(2) + ax2)
                            v3 = T.axis.spatial(T.int64(16), eps_1_nu_1_co_1_p_1_fused % T.int64(2) * T.int64(8) + eps_2_nu_2_co_2_p_2_fused % T.int64(8) + ax3)
                            T.reads(bgemm_local[v0, v1, v2, v3])
                            T.writes(bgemm[v0, v1, v2, v3])
                            bgemm[v0, v1, v2, v3] = bgemm_local[v0, v1, v2, v3]
        for n_co_h_0_w_0_fused_0 in T.thread_binding(T.int64(256), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 1024, "pragma_unroll_explicit": 1}):
            for n_co_h_0_w_0_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                    for ax2 in T.unroll(T.int64(2)):
                        for ax3 in T.unroll(T.int64(2)):
                            with T.block("inverse_init"):
                                v_co = T.axis.spatial(T.int64(512), (n_co_h_0_w_0_fused_0 * T.int64(32) + n_co_h_0_w_0_fused_1) // T.int64(16) + ax0)
                                v_p = T.axis.spatial(T.int64(16), (n_co_h_0_w_0_fused_0 * T.int64(32) + n_co_h_0_w_0_fused_1) % T.int64(16) + ax1)
                                v_vh, v_vw = T.axis.remap("SS", [ax2, ax3])
                                T.reads()
                                T.writes(inverse_local[v_co, v_p, v_vh, v_vw])
                                T.block_attr({"schedule_rule": "conv2d_nchw_winograd_inverse"})
                                inverse_local[v_co, v_p, v_vh, v_vw] = T.float32(0)
                            for ax4 in T.unroll(T.int64(4)):
                                for ax5 in T.unroll(T.int64(4)):
                                    with T.block("inverse_update"):
                                        v_co = T.axis.spatial(T.int64(512), (n_co_h_0_w_0_fused_0 * T.int64(32) + n_co_h_0_w_0_fused_1) // T.int64(16) + ax0)
                                        v_p = T.axis.spatial(T.int64(16), (n_co_h_0_w_0_fused_0 * T.int64(32) + n_co_h_0_w_0_fused_1) % T.int64(16) + ax1)
                                        v_vh, v_vw, v_r_a, v_r_b = T.axis.remap("SSRR", [ax2, ax3, ax4, ax5])
                                        T.reads(inverse_local[v_co, v_p, v_vh, v_vw], bgemm[v_r_a, v_r_b, v_co, v_p])
                                        T.writes(inverse_local[v_co, v_p, v_vh, v_vw])
                                        T.block_attr({"schedule_rule": "conv2d_nchw_winograd_inverse"})
                                        inverse_local[v_co, v_p, v_vh, v_vw] = inverse_local[v_co, v_p, v_vh, v_vw] + bgemm[v_r_a, v_r_b, v_co, v_p] * T.Select(v_r_a % T.int64(4) == T.int64(3) and v_vh % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_vh % T.int64(2) == T.int64(0), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_vh % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_vh % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_vh % T.int64(2) == T.int64(1), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_vh % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_vh % T.int64(2) == T.int64(1), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_vh % T.int64(2) == T.int64(0), T.float32(1), T.float32(0))))))))) * T.Select(v_r_b % T.int64(4) == T.int64(3) and v_vw % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_vw % T.int64(2) == T.int64(0), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_vw % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_vw % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_vw % T.int64(2) == T.int64(1), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_vw % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_vw % T.int64(2) == T.int64(1), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_vw % T.int64(2) == T.int64(0), T.float32(1), T.float32(0)))))))))
                for h_1, w_1 in T.grid(T.int64(2), T.int64(2)):
                    with T.block("conv2d_winograd"):
                        v_n = T.axis.spatial(T.int64(1), T.int64(0))
                        v_co = T.axis.spatial(T.int64(512), (n_co_h_0_w_0_fused_0 * T.int64(32) + n_co_h_0_w_0_fused_1) // T.int64(16))
                        v_h = T.axis.spatial(T.int64(7), (n_co_h_0_w_0_fused_0 * T.int64(32) + n_co_h_0_w_0_fused_1) % T.int64(16) // T.int64(4) * T.int64(2) + h_1)
                        v_w = T.axis.spatial(T.int64(7), (n_co_h_0_w_0_fused_0 * T.int64(32) + n_co_h_0_w_0_fused_1) % T.int64(4) * T.int64(2) + w_1)
                        T.where((n_co_h_0_w_0_fused_0 * T.int64(32) + n_co_h_0_w_0_fused_1) % T.int64(16) // T.int64(4) * T.int64(2) + h_1 < T.int64(7) and (n_co_h_0_w_0_fused_0 * T.int64(32) + n_co_h_0_w_0_fused_1) % T.int64(4) * T.int64(2) + w_1 < T.int64(7))
                        T.reads(inverse_local[v_co, v_n * T.int64(16) + v_h // T.int64(2) * T.int64(4) + v_w // T.int64(2), v_h % T.int64(2), v_w % T.int64(2)], p2[v_n, v_co, v_h, v_w], p3[v_n, v_co, T.int64(0), T.int64(0)], p4[v_n, v_co, T.int64(0), T.int64(0)])
                        T.writes(T_relu[v_n, v_co, v_h, v_w])
                        T_relu[v_n, v_co, v_h, v_w] = T.max((inverse_local[v_co, v_n * T.int64(16) + v_h // T.int64(2) * T.int64(4) + v_w // T.int64(2), v_h % T.int64(2), v_w % T.int64(2)] + p2[v_n, v_co, v_h, v_w]) * p3[v_n, v_co, T.int64(0), T.int64(0)] + p4[v_n, v_co, T.int64(0), T.int64(0)], T.float32(0))
b0 = sch.get_block(name="data_pack", func_name="main")
b1 = sch.get_block(name="bgemm", func_name="main")
b2 = sch.get_block(name="inverse", func_name="main")
b3 = sch.get_block(name="conv2d_winograd", func_name="main")
b4 = sch.get_block(name="T_add", func_name="main")
b5 = sch.get_block(name="T_multiply", func_name="main")
b6 = sch.get_block(name="T_add_1", func_name="main")
b7 = sch.get_block(name="T_relu", func_name="main")
b8 = sch.get_block(name="root", func_name="main")
b9, b10 = sch.get_producers(block=b2)
sch.compute_inline(block=b10)
b11, = sch.get_consumers(block=b2)
l12, l13, l14, l15 = sch.get_loops(block=b11)
l16, l17 = sch.split(loop=l14, factors=[None, 2], preserve_unit_iters=True)
l18, l19 = sch.split(loop=l15, factors=[None, 2], preserve_unit_iters=True)
sch.reorder(l16, l18, l17, l19)
sch.compute_at(block=b2, loop=l18, preserve_unit_loops=True, index=-1)
sch.set_scope(block=b2, buffer_index=0, storage_scope="local")
l20, l21, l22, l23, l24, l25, l26, l27, l28, l29 = sch.get_loops(block=b2)
sch.unroll(loop=l26)
sch.unroll(loop=l27)
sch.unroll(loop=l28)
sch.unroll(loop=l29)
b30, b31 = sch.get_producers(block=b0)
sch.compute_inline(block=b31)
b32, = sch.get_producers(block=b30)
l33, l34, l35, l36, l37, l38 = sch.get_loops(block=b0)
sch.reorder(l35, l36, l33, l34, l37, l38)
sch.unroll(loop=l33)
sch.unroll(loop=l34)
sch.unroll(loop=l37)
sch.unroll(loop=l38)
l39 = sch.fuse(l35, l36, preserve_unit_iters=True)
v40 = sch.sample_categorical(candidates=[32, 64, 128, 256, 512, 1024], probs=[0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666], decision=2)
l41, l42 = sch.split(loop=l39, factors=[None, v40], preserve_unit_iters=True)
sch.bind(loop=l41, thread_axis="blockIdx.x")
sch.bind(loop=l42, thread_axis="threadIdx.x")
b43 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b43, loop=l42, preserve_unit_loops=True, index=-1)
sch.compute_at(block=b30, loop=l42, preserve_unit_loops=True, index=-1)
sch.set_scope(block=b30, buffer_index=0, storage_scope="local")
sch.compute_inline(block=b32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l44, l45, l46, l47, l48 = sch.get_loops(block=b1)
v49, v50, v51, v52, v53 = sch.sample_perfect_tile(loop=l44, n=5, max_innermost_factor=64, decision=[2, 2, 1, 1, 1])
l54, l55, l56, l57, l58 = sch.split(loop=l44, factors=[v49, v50, v51, v52, v53], preserve_unit_iters=True)
v59, v60, v61, v62, v63 = sch.sample_perfect_tile(loop=l45, n=5, max_innermost_factor=64, decision=[2, 1, 2, 1, 1])
l64, l65, l66, l67, l68 = sch.split(loop=l45, factors=[v59, v60, v61, v62, v63], preserve_unit_iters=True)
v69, v70, v71, v72, v73 = sch.sample_perfect_tile(loop=l46, n=5, max_innermost_factor=64, decision=[16, 2, 8, 2, 1])
l74, l75, l76, l77, l78 = sch.split(loop=l46, factors=[v69, v70, v71, v72, v73], preserve_unit_iters=True)
v79, v80, v81, v82, v83 = sch.sample_perfect_tile(loop=l47, n=5, max_innermost_factor=64, decision=[1, 2, 8, 1, 1])
l84, l85, l86, l87, l88 = sch.split(loop=l47, factors=[v79, v80, v81, v82, v83], preserve_unit_iters=True)
v89, v90, v91 = sch.sample_perfect_tile(loop=l48, n=3, max_innermost_factor=64, decision=[8, 2, 32])
l92, l93, l94 = sch.split(loop=l48, factors=[v89, v90, v91], preserve_unit_iters=True)
sch.reorder(l54, l64, l74, l84, l55, l65, l75, l85, l56, l66, l76, l86, l92, l93, l57, l67, l77, l87, l94, l58, l68, l78, l88)
l95 = sch.fuse(l54, l64, l74, l84, preserve_unit_iters=True)
sch.bind(loop=l95, thread_axis="blockIdx.x")
l96 = sch.fuse(l55, l65, l75, l85, preserve_unit_iters=True)
sch.bind(loop=l96, thread_axis="vthread.x")
l97 = sch.fuse(l56, l66, l76, l86, preserve_unit_iters=True)
sch.bind(loop=l97, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b98 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b98, loop=l97, preserve_unit_loops=True, index=-1)
b99 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b99, loop=l92, preserve_unit_loops=True, index=-1)
l100, l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b99)
l108 = sch.fuse(l104, l105, l106, l107, preserve_unit_iters=True)
v109 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b99, ann_key="meta_schedule.cooperative_fetch", ann_val=v109)
b110 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b110, loop=l92, preserve_unit_loops=True, index=-1)
l111, l112, l113, l114, l115, l116, l117, l118 = sch.get_loops(block=b110)
l119 = sch.fuse(l115, l116, l117, l118, preserve_unit_iters=True)
v120 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b110, ann_key="meta_schedule.cooperative_fetch", ann_val=v120)
sch.reverse_compute_inline(block=b7)
sch.reverse_compute_inline(block=b6)
sch.reverse_compute_inline(block=b5)
sch.reverse_compute_inline(block=b4)
v121 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b8, ann_key="meta_schedule.unroll_explicit", ann_val=v121)
l122, l123, l124, l125, l126, l127 = sch.get_loops(block=b3)
l128 = sch.fuse(l122, l123, l124, l125, preserve_unit_iters=True)
v129 = sch.sample_categorical(candidates=[32, 64, 128, 256, 512, 1024], probs=[0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666], decision=0)
l130, l131 = sch.split(loop=l128, factors=[None, v129], preserve_unit_iters=True)
sch.bind(loop=l130, thread_axis="blockIdx.x")
sch.bind(loop=l131, thread_axis="threadIdx.x")
sch.enter_postproc()
sch.unannotate(block_or_loop=b99, ann_key="meta_schedule.cooperative_fetch")
l132, l133, l134, l135, l136 = sch.get_loops(block=b99)
l137, l138 = sch.split(loop=l136, factors=[None, 128], preserve_unit_iters=True)
sch.bind(loop=l138, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b110, ann_key="meta_schedule.cooperative_fetch")
l139, l140, l141, l142, l143 = sch.get_loops(block=b110)
l144, l145, l146 = sch.split(loop=l143, factors=[None, 128, 2], preserve_unit_iters=True)
sch.vectorize(loop=l146)
sch.bind(loop=l145, thread_axis="threadIdx.x")
b147 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b147, ann_key="meta_schedule.unroll_explicit")
b148, b149, b150, b151, b152, b153, b154, b155, b156 = sch.get_child_blocks(b147)
l157, l158, l159, l160, l161, l162 = sch.get_loops(block=b148)
l163, l164, l165, l166, l167, l168 = sch.get_loops(block=b149)
sch.annotate(block_or_loop=l163, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l163, ann_key="pragma_unroll_explicit", ann_val=1)
l169, l170, l171, l172, l173, l174 = sch.get_loops(block=b150)
l175, l176, l177, l178, l179, l180 = sch.get_loops(block=b151)
l181, l182, l183, l184, l185, l186, l187 = sch.get_loops(block=b152)
l188, l189, l190, l191, l192, l193, l194, l195, l196, l197, l198, l199, l200, l201 = sch.get_loops(block=b153)
sch.annotate(block_or_loop=l188, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l188, ann_key="pragma_unroll_explicit", ann_val=1)
l202, l203, l204, l205, l206, l207, l208 = sch.get_loops(block=b154)
l209, l210, l211, l212, l213, l214, l215, l216 = sch.get_loops(block=b155)
sch.annotate(block_or_loop=l209, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l209, ann_key="pragma_unroll_explicit", ann_val=1)
l217, l218, l219, l220 = sch.get_loops(block=b156)
b221 = sch.get_block(name="data_pack", func_name="main")
l222, l223, l224, l225, l226, l227 = sch.get_loops(block=b221)
b228 = sch.decompose_reduction(block=b221, loop=l226)
b229 = sch.get_block(name="bgemm", func_name="main")
l230, l231, l232, l233, l234, l235, l236, l237, l238, l239, l240, l241, l242, l243 = sch.get_loops(block=b229)
b244 = sch.decompose_reduction(block=b229, loop=l233)
b245 = sch.get_block(name="inverse", func_name="main")
l246, l247, l248, l249, l250, l251, l252, l253 = sch.get_loops(block=b245)
b254 = sch.decompose_reduction(block=b245, loop=l252)
2023-11-11 02:29:36 [INFO] [task_scheduler.cc:121] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #195: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/home2/xiachunwei/Software/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/home2/xiachunwei/Software/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/home2/xiachunwei/Software/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/home2/xiachunwei/Software/tvm/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/home2/xiachunwei/Software/tvm/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/home2/xiachunwei/Software/tvm/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/home2/xiachunwei/Software/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/home2/xiachunwei/Software/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  4: _ZN3tvm7runtime13PackedFun
  3: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  2: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  0: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  File "/home2/xiachunwei/Software/tvm/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(512), T.int64(7), T.int64(7)), "float32"), p1: T.Buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(512)), "float32"), p2: T.Buffer((T.int64(1), T.int64(512), T.int64(7), T.int64(7)), "float32"), p3: T.Buffer((T.int64(1), T.int64(512), T.int64(1), T.int64(1)), "float32"), p4: T.Buffer((T.int64(1), T.int64(512), T.int64(1), T.int64(1)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(512), T.int64(7), T.int64(7)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        input_tile_local = T.alloc_buffer((T.int64(512), T.int64(16), T.int64(4), T.int64(4)), scope="local")
        data_pack = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(16)))
        bgemm = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(16)))
        inverse_local = T.alloc_buffer((T.int64(512), T.int64(16), T.int64(2), T.int64(2)), scope="local")
        data_pack_local = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(16)), scope="local")
        bgemm_local = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(16)), scope="local")
        data_pack_shared = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(16)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(512)), scope="shared")
        for ci_p_fused_0 in T.thread_binding(T.int64(64), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 512, "pragma_unroll_explicit": 1}):
            for ci_p_fused_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(1), T.int64(4), T.int64(4)):
                    with T.block("input_tile"):
                        v_ci = T.axis.spatial(T.int64(512), ci_p_fused_0 * T.int64(8) + ci_p_fused_1 // T.int64(16) + ax0)
                        v_p = T.axis.spatial(T.int64(16), ci_p_fused_1 % T.int64(16) + ax1)
                        v_eps, v_nu = T.axis.remap("SS", [ax2, ax3])
                        T.reads(p0[v_p // T.int64(16), v_ci, v_p % T.int64(16) // T.int64(4) * T.int64(2) + v_eps - T.int64(1), v_p % T.int64(4) * T.int64(2) + v_nu - T.int64(1)])
                        T.writes(input_tile_local[v_ci, v_p, v_eps, v_nu])
                        T.block_attr({"schedule_rule": "None"})
                        input_tile_local[v_ci, v_p, v_eps, v_nu] = T.if_then_else(T.int64(1) <= v_p % T.int64(16) // T.int64(4) * T.int64(2) + v_eps and v_p % T.int64(16) // T.int64(4) * T.int64(2) + v_eps < T.int64(8) and T.int64(1) <= v_p % T.int64(4) * T.int64(2) + v_nu and v_p % T.int64(4) * T.int64(2) + v_nu < T.int64(8), p0[v_p // T.int64(16), v_ci, v_p % T.int64(16) // T.int64(4) * T.int64(2) + v_eps - T.int64(1), v_p % T.int64(4) * T.int64(2) + v_nu - T.int64(1)], T.float32(0))
                for eps in T.unroll(T.int64(4)):
                    for nu in T.unroll(T.int64(4)):
                        with T.block("data_pack_init"):
                            v_eps, v_nu = T.axis.remap("SS", [eps, nu])
                            v_ci = T.axis.spatial(T.int64(512), (ci_p_fused_0 * T.int64(128) + ci_p_fused_1) // T.int64(16))
                            v_p = T.axis.spatial(T.int64(16), (ci_p_fused_0 * T.int64(128) + ci_p_fused_1) % T.int64(16))
                            T.reads()
                            T.writes(data_pack_local[v_eps, v_nu, v_ci, v_p])
                            T.block_attr({"schedule_rule": "conv2d_nchw_winograd_data_pack"})
                            data_pack_local[v_eps, v_nu, v_ci, v_p] = T.float32(0)
                        for r_a in T.unroll(T.int64(4)):
                            for r_b in T.unroll(T.int64(4)):
                                with T.block("data_pack_update"):
                                    v_eps, v_nu = T.axis.remap("SS", [eps, nu])
                                    v_ci = T.axis.spatial(T.int64(512), (ci_p_fused_0 * T.int64(128) + ci_p_fused_1) // T.int64(16))
                                    v_p = T.axis.spatial(T.int64(16), (ci_p_fused_0 * T.int64(128) + ci_p_fused_1) % T.int64(16))
                                    v_r_a, v_r_b = T.axis.remap("RR", [r_a, r_b])
                                    T.reads(data_pack_local[v_eps, v_nu, v_ci, v_p], input_tile_local[v_ci, v_p, v_r_a, v_r_b])
                                    T.writes(data_pack_local[v_eps, v_nu, v_ci, v_p])
                                    T.block_attr({"schedule_rule": "conv2d_nchw_winograd_data_pack"})
                                    data_pack_local[v_eps, v_nu, v_ci, v_p] = data_pack_local[v_eps, v_nu, v_ci, v_p] + input_tile_local[v_ci, v_p, v_r_a, v_r_b] * T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(3), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(1), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(0), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(3), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(1), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(0), T.float32(1), T.float32(0))))))))))))))))) * T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(3), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(1), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(0), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(3), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(1), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(0), T.float32(1), T.float32(0)))))))))))))))))
                for ax0, ax1, ax2, ax3 in T.grid(T.int64(4), T.int64(4), T.int64(1), T.int64(1)):
                    with T.block("data_pack_local"):
                        v0, v1 = T.axis.remap("SS", [ax0, ax1])
                        v2 = T.axis.spatial(T.int64(512), ci_p_fused_0 * T.int64(8) + ci_p_fused_1 // T.int64(16) + ax2)
                        v3 = T.axis.spatial(T.int64(16), ci_p_fused_1 % T.int64(16) + ax3)
                        T.reads(data_pack_local[v0, v1, v2, v3])
                        T.writes(data_pack[v0, v1, v2, v3])
                        data_pack[v0, v1, v2, v3] = data_pack_local[v0, v1, v2, v3]
        for eps_0_nu_0_co_0_p_0_fused in T.thread_binding(T.int64(256), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 512, "pragma_unroll_explicit": 1}):
            for eps_1_nu_1_co_1_p_1_fused in T.thread_binding(T.int64(2), thread="vthread.x"):
                for eps_2_nu_2_co_2_p_2_fused in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                    for eps_3_init, nu_3_init, co_3_init, p_3_init, eps_4_init, nu_4_init, co_4_init, p_4_init in T.grid(T.int64(1), T.int64(1), T.int64(2), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("bgemm_init"):
                            v_eps = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(64) + eps_3_init + eps_4_init)
                            v_nu = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused % T.int64(64) // T.int64(32) * T.int64(2) + eps_2_nu_2_co_2_p_2_fused // T.int64(64) + nu_3_init + nu_4_init)
                            v_co = T.axis.spatial(T.int64(512), eps_0_nu_0_co_0_p_0_fused % T.int64(32) // T.int64(2) * T.int64(32) + eps_1_nu_1_co_1_p_1_fused * T.int64(16) + eps_2_nu_2_co_2_p_2_fused % T.int64(64) // T.int64(8) * T.int64(2) + co_3_init + co_4_init)
                            v_p = T.axis.spatial(T.int64(16), eps_0_nu_0_co_0_p_0_fused % T.int64(2) * T.int64(8) + eps_2_nu_2_co_2_p_2_fused % T.int64(8) + p_3_init + p_4_init)
                            T.reads()
                            T.writes(bgemm_local[v_eps, v_nu, v_co, v_p])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            bgemm_local[v_eps, v_nu, v_co, v_p] = T.float32(0)
                    for ci_0 in range(T.int64(8)):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(8)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                                with T.block("data_pack_shared"):
                                    v0 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(64))
                                    v1 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused % T.int64(64) // T.int64(32) * T.int64(2) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1) // T.int64(512))
                                    v2 = T.axis.spatial(T.int64(512), ci_0 * T.int64(64) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1) % T.int64(512) // T.int64(8))
                                    v3 = T.axis.spatial(T.int64(16), eps_0_nu_0_co_0_p_0_fused % T.int64(2) * T.int64(8) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1) % T.int64(8))
                                    T.reads(data_pack[v0, v1, v2, v3])
                                    T.writes(data_pack_shared[v0, v1, v2, v3])
                                    data_pack_shared[v0, v1, v2, v3] = data_pack[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(32)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                                with T.block("p1_shared"):
                                    v0 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(64))
                                    v1 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused % T.int64(64) // T.int64(32) * T.int64(2) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1) // T.int64(2048))
                                    v2 = T.axis.spatial(T.int64(512), ci_0 * T.int64(64) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1) % T.int64(2048) // T.int64(32))
                                    v3 = T.axis.spatial(T.int64(512), eps_0_nu_0_co_0_p_0_fused % T.int64(32) // T.int64(2) * T.int64(32) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1) % T.int64(32))
                                    T.reads(p1[v0, v1, v2, v3])
                                    T.writes(p1_shared[v0, v1, v2, v3])
                                    p1_shared[v0, v1, v2, v3] = p1[v0, v1, v2, v3]
                        for ci_1, eps_3, nu_3, co_3, p_3, ci_2, eps_4, nu_4, co_4, p_4 in T.grid(T.int64(4), T.int64(1), T.int64(1), T.int64(2), T.int64(1), T.int64(16), T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                            with T.block("bgemm_update"):
                                v_eps = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(64) + eps_3 + eps_4)
                                v_nu = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused % T.int64(64) // T.int64(32) * T.int64(2) + eps_2_nu_2_co_2_p_2_fused // T.int64(64) + nu_3 + nu_4)
                                v_co = T.axis.spatial(T.int64(512), eps_0_nu_0_co_0_p_0_fused % T.int64(32) // T.int64(2) * T.int64(32) + eps_1_nu_1_co_1_p_1_fused * T.int64(16) + eps_2_nu_2_co_2_p_2_fused % T.int64(64) // T.int64(8) * T.int64(2) + co_3 + co_4)
                                v_p = T.axis.spatial(T.int64(16), eps_0_nu_0_co_0_p_0_fused % T.int64(2) * T.int64(8) + eps_2_nu_2_co_2_p_2_fused % T.int64(8) + p_3 + p_4)
                                v_ci = T.axis.reduce(T.int64(512), ci_0 * T.int64(64) + ci_1 * T.int64(16) + ci_2)
                                T.reads(bgemm_local[v_eps, v_nu, v_co, v_p], data_pack_shared[v_eps, v_nu, v_ci, v_p], p1_shared[v_eps, v_nu, v_ci, v_co])
                                T.writes(bgemm_local[v_eps, v_nu, v_co, v_p])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                bgemm_local[v_eps, v_nu, v_co, v_p] = bgemm_local[v_eps, v_nu, v_co, v_p] + data_pack_shared[v_eps, v_nu, v_ci, v_p] * p1_shared[v_eps, v_nu, v_ci, v_co]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(1), T.int64(2), T.int64(1)):
                        with T.block("bgemm_local"):
                            v0 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(64) + ax0)
                            v1 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused % T.int64(64) // T.int64(32) * T.int64(2) + eps_2_nu_2_co_2_p_2_fused // T.int64(64) + ax1)
                            v2 = T.axis.spatial(T.int64(512), eps_0_nu_0_co_0_p_0_fused % T.int64(32) // T.int64(2) * T.int64(32) + eps_1_nu_1_co_1_p_1_fused * T.int64(16) + eps_2_nu_2_co_2_p_2_fused % T.int64(64) // T.int64(8) * T.int64(2) + ax2)
                            v3 = T.axis.spatial(T.int64(16), eps_0_nu_0_co_0_p_0_fused % T.int64(2) * T.int64(8) + eps_2_nu_2_co_2_p_2_fused % T.int64(8) + ax3)
                            T.reads(bgemm_local[v0, v1, v2, v3])
                            T.writes(bgemm[v0, v1, v2, v3])
                            bgemm[v0, v1, v2, v3] = bgemm_local[v0, v1, v2, v3]
        for n_co_h_0_w_0_fused_0 in T.thread_binding(T.int64(64), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 512, "pragma_unroll_explicit": 1}):
            for n_co_h_0_w_0_fused_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                    for ax2 in T.unroll(T.int64(2)):
                        for ax3 in T.unroll(T.int64(2)):
                            with T.block("inverse_init"):
                                v_co = T.axis.spatial(T.int64(512), (n_co_h_0_w_0_fused_0 * T.int64(128) + n_co_h_0_w_0_fused_1) // T.int64(16) + ax0)
                                v_p = T.axis.spatial(T.int64(16), (n_co_h_0_w_0_fused_0 * T.int64(128) + n_co_h_0_w_0_fused_1) % T.int64(16) + ax1)
                                v_vh, v_vw = T.axis.remap("SS", [ax2, ax3])
                                T.reads()
                                T.writes(inverse_local[v_co, v_p, v_vh, v_vw])
                                T.block_attr({"schedule_rule": "conv2d_nchw_winograd_inverse"})
                                inverse_local[v_co, v_p, v_vh, v_vw] = T.float32(0)
                            for ax4 in T.unroll(T.int64(4)):
                                for ax5 in T.unroll(T.int64(4)):
                                    with T.block("inverse_update"):
                                        v_co = T.axis.spatial(T.int64(512), (n_co_h_0_w_0_fused_0 * T.int64(128) + n_co_h_0_w_0_fused_1) // T.int64(16) + ax0)
                                        v_p = T.axis.spatial(T.int64(16), (n_co_h_0_w_0_fused_0 * T.int64(128) + n_co_h_0_w_0_fused_1) % T.int64(16) + ax1)
                                        v_vh, v_vw, v_r_a, v_r_b = T.axis.remap("SSRR", [ax2, ax3, ax4, ax5])
                                        T.reads(inverse_local[v_co, v_p, v_vh, v_vw], bgemm[v_r_a, v_r_b, v_co, v_p])
                                        T.writes(inverse_local[v_co, v_p, v_vh, v_vw])
                                        T.block_attr({"schedule_rule": "conv2d_nchw_winograd_inverse"})
                                        inverse_local[v_co, v_p, v_vh, v_vw] = inverse_local[v_co, v_p, v_vh, v_vw] + bgemm[v_r_a, v_r_b, v_co, v_p] * T.Select(v_r_a % T.int64(4) == T.int64(3) and v_vh % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_vh % T.int64(2) == T.int64(0), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_vh % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_vh % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_vh % T.int64(2) == T.int64(1), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_vh % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_vh % T.int64(2) == T.int64(1), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_vh % T.int64(2) == T.int64(0), T.float32(1), T.float32(0))))))))) * T.Select(v_r_b % T.int64(4) == T.int64(3) and v_vw % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_vw % T.int64(2) == T.int64(0), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_vw % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_vw % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_vw % T.int64(2) == T.int64(1), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_vw % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_vw % T.int64(2) == T.int64(1), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_vw % T.int64(2) == T.int64(0), T.float32(1), T.float32(0)))))))))
                for h_1, w_1 in T.grid(T.int64(2), T.int64(2)):
                    with T.block("conv2d_winograd"):
                        v_n = T.axis.spatial(T.int64(1), T.int64(0))
                        v_co = T.axis.spatial(T.int64(512), (n_co_h_0_w_0_fused_0 * T.int64(128) + n_co_h_0_w_0_fused_1) // T.int64(16))
                        v_h = T.axis.spatial(T.int64(7), (n_co_h_0_w_0_fused_0 * T.int64(128) + n_co_h_0_w_0_fused_1) % T.int64(16) // T.int64(4) * T.int64(2) + h_1)
                        v_w = T.axis.spatial(T.int64(7), (n_co_h_0_w_0_fused_0 * T.int64(128) + n_co_h_0_w_0_fused_1) % T.int64(4) * T.int64(2) + w_1)
                        T.where((n_co_h_0_w_0_fused_0 * T.int64(128) + n_co_h_0_w_0_fused_1) % T.int64(16) // T.int64(4) * T.int64(2) + h_1 < T.int64(7) and (n_co_h_0_w_0_fused_0 * T.int64(128) + n_co_h_0_w_0_fused_1) % T.int64(4) * T.int64(2) + w_1 < T.int64(7))
                        T.reads(inverse_local[v_co, v_n * T.int64(16) + v_h // T.int64(2) * T.int64(4) + v_w // T.int64(2), v_h % T.int64(2), v_w % T.int64(2)], p2[v_n, v_co, v_h, v_w], p3[v_n, v_co, T.int64(0), T.int64(0)], p4[v_n, v_co, T.int64(0), T.int64(0)])
                        T.writes(T_relu[v_n, v_co, v_h, v_w])
                        T_relu[v_n, v_co, v_h, v_w] = T.max((inverse_local[v_co, v_n * T.int64(16) + v_h // T.int64(2) * T.int64(4) + v_w // T.int64(2), v_h % T.int64(2), v_w % T.int64(2)] + p2[v_n, v_co, v_h, v_w]) * p3[v_n, v_co, T.int64(0), T.int64(0)] + p4[v_n, v_co, T.int64(0), T.int64(0)], T.float32(0))
b0 = sch.get_block(name="data_pack", func_name="main")
b1 = sch.get_block(name="bgemm", func_name="main")
b2 = sch.get_block(name="inverse", func_name="main")
b3 = sch.get_block(name="conv2d_winograd", func_name="main")
b4 = sch.get_block(name="T_add", func_name="main")
b5 = sch.get_block(name="T_multiply", func_name="main")
b6 = sch.get_block(name="T_add_1", func_name="main")
b7 = sch.get_block(name="T_relu", func_name="main")
b8 = sch.get_block(name="root", func_name="main")
b9, b10 = sch.get_producers(block=b2)
sch.compute_inline(block=b10)
b11, = sch.get_consumers(block=b2)
l12, l13, l14, l15 = sch.get_loops(block=b11)
l16, l17 = sch.split(loop=l14, factors=[None, 2], preserve_unit_iters=True)
l18, l19 = sch.split(loop=l15, factors=[None, 2], preserve_unit_iters=True)
sch.reorder(l16, l18, l17, l19)
sch.compute_at(block=b2, loop=l18, preserve_unit_loops=True, index=-1)
sch.set_scope(block=b2, buffer_index=0, storage_scope="local")
l20, l21, l22, l23, l24, l25, l26, l27, l28, l29 = sch.get_loops(block=b2)
sch.unroll(loop=l26)
sch.unroll(loop=l27)
sch.unroll(loop=l28)
sch.unroll(loop=l29)
b30, b31 = sch.get_producers(block=b0)
sch.compute_inline(block=b31)
b32, = sch.get_producers(block=b30)
l33, l34, l35, l36, l37, l38 = sch.get_loops(block=b0)
sch.reorder(l35, l36, l33, l34, l37, l38)
sch.unroll(loop=l33)
sch.unroll(loop=l34)
sch.unroll(loop=l37)
sch.unroll(loop=l38)
l39 = sch.fuse(l35, l36, preserve_unit_iters=True)
v40 = sch.sample_categorical(candidates=[32, 64, 128, 256, 512, 1024], probs=[0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666], decision=2)
l41, l42 = sch.split(loop=l39, factors=[None, v40], preserve_unit_iters=True)
sch.bind(loop=l41, thread_axis="blockIdx.x")
sch.bind(loop=l42, thread_axis="threadIdx.x")
b43 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b43, loop=l42, preserve_unit_loops=True, index=-1)
sch.compute_at(block=b30, loop=l42, preserve_unit_loops=True, index=-1)
sch.set_scope(block=b30, buffer_index=0, storage_scope="local")
sch.compute_inline(block=b32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l44, l45, l46, l47, l48 = sch.get_loops(block=b1)
v49, v50, v51, v52, v53 = sch.sample_perfect_tile(loop=l44, n=5, max_innermost_factor=64, decision=[4, 1, 1, 1, 1])
l54, l55, l56, l57, l58 = sch.split(loop=l44, factors=[v49, v50, v51, v52, v53], preserve_unit_iters=True)
v59, v60, v61, v62, v63 = sch.sample_perfect_tile(loop=l45, n=5, max_innermost_factor=64, decision=[2, 1, 2, 1, 1])
l64, l65, l66, l67, l68 = sch.split(loop=l45, factors=[v59, v60, v61, v62, v63], preserve_unit_iters=True)
v69, v70, v71, v72, v73 = sch.sample_perfect_tile(loop=l46, n=5, max_innermost_factor=64, decision=[16, 2, 8, 2, 1])
l74, l75, l76, l77, l78 = sch.split(loop=l46, factors=[v69, v70, v71, v72, v73], preserve_unit_iters=True)
v79, v80, v81, v82, v83 = sch.sample_perfect_tile(loop=l47, n=5, max_innermost_factor=64, decision=[2, 1, 8, 1, 1])
l84, l85, l86, l87, l88 = sch.split(loop=l47, factors=[v79, v80, v81, v82, v83], preserve_unit_iters=True)
v89, v90, v91 = sch.sample_perfect_tile(loop=l48, n=3, max_innermost_factor=64, decision=[8, 4, 16])
l92, l93, l94 = sch.split(loop=l48, factors=[v89, v90, v91], preserve_unit_iters=True)
sch.reorder(l54, l64, l74, l84, l55, l65, l75, l85, l56, l66, l76, l86, l92, l93, l57, l67, l77, l87, l94, l58, l68, l78, l88)
l95 = sch.fuse(l54, l64, l74, l84, preserve_unit_iters=True)
sch.bind(loop=l95, thread_axis="blockIdx.x")
l96 = sch.fuse(l55, l65, l75, l85, preserve_unit_iters=True)
sch.bind(loop=l96, thread_axis="vthread.x")
l97 = sch.fuse(l56, l66, l76, l86, preserve_unit_iters=True)
sch.bind(loop=l97, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b98 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b98, loop=l97, preserve_unit_loops=True, index=-1)
b99 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b99, loop=l92, preserve_unit_loops=True, index=-1)
l100, l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b99)
l108 = sch.fuse(l104, l105, l106, l107, preserve_unit_iters=True)
v109 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b99, ann_key="meta_schedule.cooperative_fetch", ann_val=v109)
b110 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b110, loop=l92, preserve_unit_loops=True, index=-1)
l111, l112, l113, l114, l115, l116, l117, l118 = sch.get_loops(block=b110)
l119 = sch.fuse(l115, l116, l117, l118, preserve_unit_iters=True)
v120 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b110, ann_key="meta_schedule.cooperative_fetch", ann_val=v120)
sch.reverse_compute_inline(block=b7)
sch.reverse_compute_inline(block=b6)
sch.reverse_compute_inline(block=b5)
sch.reverse_compute_inline(block=b4)
v121 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b8, ann_key="meta_schedule.unroll_explicit", ann_val=v121)
l122, l123, l124, l125, l126, l127 = sch.get_loops(block=b3)
l128 = sch.fuse(l122, l123, l124, l125, preserve_unit_iters=True)
v129 = sch.sample_categorical(candidates=[32, 64, 128, 256, 512, 1024], probs=[0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666], decision=2)
l130, l131 = sch.split(loop=l128, factors=[None, v129], preserve_unit_iters=True)
sch.bind(loop=l130, thread_axis="blockIdx.x")
sch.bind(loop=l131, thread_axis="threadIdx.x")
sch.enter_postproc()
sch.unannotate(block_or_loop=b99, ann_key="meta_schedule.cooperative_fetch")
l132, l133, l134, l135, l136 = sch.get_loops(block=b99)
l137, l138 = sch.split(loop=l136, factors=[None, 128], preserve_unit_iters=True)
sch.bind(loop=l138, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b110, ann_key="meta_schedule.cooperative_fetch")
l139, l140, l141, l142, l143 = sch.get_loops(block=b110)
l144, l145 = sch.split(loop=l143, factors=[None, 128], preserve_unit_iters=True)
sch.bind(loop=l145, thread_axis="threadIdx.x")
b146 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b146, ann_key="meta_schedule.unroll_explicit")
b147, b148, b149, b150, b151, b152, b153, b154, b155 = sch.get_child_blocks(b146)
l156, l157, l158, l159, l160, l161 = sch.get_loops(block=b147)
l162, l163, l164, l165, l166, l167 = sch.get_loops(block=b148)
sch.annotate(block_or_loop=l162, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l162, ann_key="pragma_unroll_explicit", ann_val=1)
l168, l169, l170, l171, l172, l173 = sch.get_loops(block=b149)
l174, l175, l176, l177, l178, l179 = sch.get_loops(block=b150)
l180, l181, l182, l183, l184, l185 = sch.get_loops(block=b151)
l186, l187, l188, l189, l190, l191, l192, l193, l194, l195, l196, l197, l198, l199 = sch.get_loops(block=b152)
sch.annotate(block_or_loop=l186, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l186, ann_key="pragma_unroll_explicit", ann_val=1)
l200, l201, l202, l203, l204, l205, l206 = sch.get_loops(block=b153)
l207, l208, l209, l210, l211, l212, l213, l214 = sch.get_loops(block=b154)
sch.annotate(block_or_loop=l207, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l207, ann_key="pragma_unroll_explicit", ann_val=1)
l215, l216, l217, l218 = sch.get_loops(block=b155)
b219 = sch.get_block(name="data_pack", func_name="main")
l220, l221, l222, l223, l224, l225 = sch.get_loops(block=b219)
b226 = sch.decompose_reduction(block=b219, loop=l224)
b227 = sch.get_block(name="bgemm", func_name="main")
l228, l229, l230, l231, l232, l233, l234, l235, l236, l237, l238, l239, l240, l241 = sch.get_loops(block=b227)
b242 = sch.decompose_reduction(block=b227, loop=l231)
b243 = sch.get_block(name="inverse", func_name="main")
l244, l245, l246, l247, l248, l249, l250, l251 = sch.get_loops(block=b243)
b252 = sch.decompose_reduction(block=b243, loop=l250)
2023-11-11 02:29:36 [INFO] [task_scheduler.cc:121] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #196: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/home2/xiachunwei/Software/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/home2/xiachunwei/Software/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/home2/xiachunwei/Software/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/home2/xiachunwei/Software/tvm/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/home2/xiachunwei/Software/tvm/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/home2/xiachunwei/Software/tvm/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/home2/xiachunwei/Software/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/home2/xiachunwei/Software/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  4: _ZN3tvm7runtime13PackedFun
  3: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  2: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  0: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  File "/home2/xiachunwei/Software/tvm/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(512), T.int64(7), T.int64(7)), "float32"), p1: T.Buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(512)), "float32"), p2: T.Buffer((T.int64(1), T.int64(512), T.int64(7), T.int64(7)), "float32"), p3: T.Buffer((T.int64(1), T.int64(512), T.int64(1), T.int64(1)), "float32"), p4: T.Buffer((T.int64(1), T.int64(512), T.int64(1), T.int64(1)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(512), T.int64(7), T.int64(7)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        input_tile_local = T.alloc_buffer((T.int64(512), T.int64(16), T.int64(4), T.int64(4)), scope="local")
        data_pack = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(16)))
        bgemm = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(16)))
        inverse_local = T.alloc_buffer((T.int64(512), T.int64(16), T.int64(2), T.int64(2)), scope="local")
        data_pack_local = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(16)), scope="local")
        bgemm_local = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(16)), scope="local")
        data_pack_shared = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(16)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(512)), scope="shared")
        for ci_p_fused_0 in T.thread_binding(T.int64(64), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 1024, "pragma_unroll_explicit": 1}):
            for ci_p_fused_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(1), T.int64(4), T.int64(4)):
                    with T.block("input_tile"):
                        v_ci = T.axis.spatial(T.int64(512), ci_p_fused_0 * T.int64(8) + ci_p_fused_1 // T.int64(16) + ax0)
                        v_p = T.axis.spatial(T.int64(16), ci_p_fused_1 % T.int64(16) + ax1)
                        v_eps, v_nu = T.axis.remap("SS", [ax2, ax3])
                        T.reads(p0[v_p // T.int64(16), v_ci, v_p % T.int64(16) // T.int64(4) * T.int64(2) + v_eps - T.int64(1), v_p % T.int64(4) * T.int64(2) + v_nu - T.int64(1)])
                        T.writes(input_tile_local[v_ci, v_p, v_eps, v_nu])
                        T.block_attr({"schedule_rule": "None"})
                        input_tile_local[v_ci, v_p, v_eps, v_nu] = T.if_then_else(T.int64(1) <= v_p % T.int64(16) // T.int64(4) * T.int64(2) + v_eps and v_p % T.int64(16) // T.int64(4) * T.int64(2) + v_eps < T.int64(8) and T.int64(1) <= v_p % T.int64(4) * T.int64(2) + v_nu and v_p % T.int64(4) * T.int64(2) + v_nu < T.int64(8), p0[v_p // T.int64(16), v_ci, v_p % T.int64(16) // T.int64(4) * T.int64(2) + v_eps - T.int64(1), v_p % T.int64(4) * T.int64(2) + v_nu - T.int64(1)], T.float32(0))
                for eps in T.unroll(T.int64(4)):
                    for nu in T.unroll(T.int64(4)):
                        with T.block("data_pack_init"):
                            v_eps, v_nu = T.axis.remap("SS", [eps, nu])
                            v_ci = T.axis.spatial(T.int64(512), (ci_p_fused_0 * T.int64(128) + ci_p_fused_1) // T.int64(16))
                            v_p = T.axis.spatial(T.int64(16), (ci_p_fused_0 * T.int64(128) + ci_p_fused_1) % T.int64(16))
                            T.reads()
                            T.writes(data_pack_local[v_eps, v_nu, v_ci, v_p])
                            T.block_attr({"schedule_rule": "conv2d_nchw_winograd_data_pack"})
                            data_pack_local[v_eps, v_nu, v_ci, v_p] = T.float32(0)
                        for r_a in T.unroll(T.int64(4)):
                            for r_b in T.unroll(T.int64(4)):
                                with T.block("data_pack_update"):
                                    v_eps, v_nu = T.axis.remap("SS", [eps, nu])
                                    v_ci = T.axis.spatial(T.int64(512), (ci_p_fused_0 * T.int64(128) + ci_p_fused_1) // T.int64(16))
                                    v_p = T.axis.spatial(T.int64(16), (ci_p_fused_0 * T.int64(128) + ci_p_fused_1) % T.int64(16))
                                    v_r_a, v_r_b = T.axis.remap("RR", [r_a, r_b])
                                    T.reads(data_pack_local[v_eps, v_nu, v_ci, v_p], input_tile_local[v_ci, v_p, v_r_a, v_r_b])
                                    T.writes(data_pack_local[v_eps, v_nu, v_ci, v_p])
                                    T.block_attr({"schedule_rule": "conv2d_nchw_winograd_data_pack"})
                                    data_pack_local[v_eps, v_nu, v_ci, v_p] = data_pack_local[v_eps, v_nu, v_ci, v_p] + input_tile_local[v_ci, v_p, v_r_a, v_r_b] * T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(3), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(1), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(0), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(3), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(1), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(0), T.float32(1), T.float32(0))))))))))))))))) * T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(3), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(1), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(0), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(3), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(1), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(0), T.float32(1), T.float32(0)))))))))))))))))
                for ax0, ax1, ax2, ax3 in T.grid(T.int64(4), T.int64(4), T.int64(1), T.int64(1)):
                    with T.block("data_pack_local"):
                        v0, v1 = T.axis.remap("SS", [ax0, ax1])
                        v2 = T.axis.spatial(T.int64(512), ci_p_fused_0 * T.int64(8) + ci_p_fused_1 // T.int64(16) + ax2)
                        v3 = T.axis.spatial(T.int64(16), ci_p_fused_1 % T.int64(16) + ax3)
                        T.reads(data_pack_local[v0, v1, v2, v3])
                        T.writes(data_pack[v0, v1, v2, v3])
                        data_pack[v0, v1, v2, v3] = data_pack_local[v0, v1, v2, v3]
        for eps_0_nu_0_co_0_p_0_fused in T.thread_binding(T.int64(128), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 1024, "pragma_unroll_explicit": 1}):
            for eps_1_nu_1_co_1_p_1_fused in T.thread_binding(T.int64(4), thread="vthread.x"):
                for eps_2_nu_2_co_2_p_2_fused in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                    for eps_3_init, nu_3_init, co_3_init, p_3_init, eps_4_init, nu_4_init, co_4_init, p_4_init in T.grid(T.int64(1), T.int64(1), T.int64(2), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("bgemm_init"):
                            v_eps = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(32) + eps_3_init + eps_4_init)
                            v_nu = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused % T.int64(32) // T.int64(16) * T.int64(2) + eps_2_nu_2_co_2_p_2_fused // T.int64(64) + nu_3_init + nu_4_init)
                            v_co = T.axis.spatial(T.int64(512), eps_0_nu_0_co_0_p_0_fused % T.int64(16) * T.int64(32) + eps_1_nu_1_co_1_p_1_fused // T.int64(2) * T.int64(16) + eps_2_nu_2_co_2_p_2_fused % T.int64(64) // T.int64(8) * T.int64(2) + co_3_init + co_4_init)
                            v_p = T.axis.spatial(T.int64(16), eps_1_nu_1_co_1_p_1_fused % T.int64(2) * T.int64(8) + eps_2_nu_2_co_2_p_2_fused % T.int64(8) + p_3_init + p_4_init)
                            T.reads()
                            T.writes(bgemm_local[v_eps, v_nu, v_co, v_p])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            bgemm_local[v_eps, v_nu, v_co, v_p] = T.float32(0)
                    for ci_0 in range(T.int64(8)):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(4)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("data_pack_shared"):
                                        v0 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(32))
                                        v1 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused % T.int64(32) // T.int64(16) * T.int64(2) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(512) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) // T.int64(1024))
                                        v2 = T.axis.spatial(T.int64(512), ci_0 * T.int64(64) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(512) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) % T.int64(1024) // T.int64(16))
                                        v3 = T.axis.spatial(T.int64(16), (ax0_ax1_ax2_ax3_fused_0 * T.int64(512) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) % T.int64(16))
                                        T.reads(data_pack[v0, v1, v2, v3])
                                        T.writes(data_pack_shared[v0, v1, v2, v3])
                                        data_pack_shared[v0, v1, v2, v3] = data_pack[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(32)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                                with T.block("p1_shared"):
                                    v0 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(32))
                                    v1 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused % T.int64(32) // T.int64(16) * T.int64(2) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1) // T.int64(2048))
                                    v2 = T.axis.spatial(T.int64(512), ci_0 * T.int64(64) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1) % T.int64(2048) // T.int64(32))
                                    v3 = T.axis.spatial(T.int64(512), eps_0_nu_0_co_0_p_0_fused % T.int64(16) * T.int64(32) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1) % T.int64(32))
                                    T.reads(p1[v0, v1, v2, v3])
                                    T.writes(p1_shared[v0, v1, v2, v3])
                                    p1_shared[v0, v1, v2, v3] = p1[v0, v1, v2, v3]
                        for ci_1, eps_3, nu_3, co_3, p_3, ci_2, eps_4, nu_4, co_4, p_4 in T.grid(T.int64(2), T.int64(1), T.int64(1), T.int64(2), T.int64(1), T.int64(32), T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                            with T.block("bgemm_update"):
                                v_eps = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(32) + eps_3 + eps_4)
                                v_nu = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused % T.int64(32) // T.int64(16) * T.int64(2) + eps_2_nu_2_co_2_p_2_fused // T.int64(64) + nu_3 + nu_4)
                                v_co = T.axis.spatial(T.int64(512), eps_0_nu_0_co_0_p_0_fused % T.int64(16) * T.int64(32) + eps_1_nu_1_co_1_p_1_fused // T.int64(2) * T.int64(16) + eps_2_nu_2_co_2_p_2_fused % T.int64(64) // T.int64(8) * T.int64(2) + co_3 + co_4)
                                v_p = T.axis.spatial(T.int64(16), eps_1_nu_1_co_1_p_1_fused % T.int64(2) * T.int64(8) + eps_2_nu_2_co_2_p_2_fused % T.int64(8) + p_3 + p_4)
                                v_ci = T.axis.reduce(T.int64(512), ci_0 * T.int64(64) + ci_1 * T.int64(32) + ci_2)
                                T.reads(bgemm_local[v_eps, v_nu, v_co, v_p], data_pack_shared[v_eps, v_nu, v_ci, v_p], p1_shared[v_eps, v_nu, v_ci, v_co])
                                T.writes(bgemm_local[v_eps, v_nu, v_co, v_p])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                bgemm_local[v_eps, v_nu, v_co, v_p] = bgemm_local[v_eps, v_nu, v_co, v_p] + data_pack_shared[v_eps, v_nu, v_ci, v_p] * p1_shared[v_eps, v_nu, v_ci, v_co]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(1), T.int64(2), T.int64(1)):
                        with T.block("bgemm_local"):
                            v0 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(32) + ax0)
                            v1 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused % T.int64(32) // T.int64(16) * T.int64(2) + eps_2_nu_2_co_2_p_2_fused // T.int64(64) + ax1)
                            v2 = T.axis.spatial(T.int64(512), eps_0_nu_0_co_0_p_0_fused % T.int64(16) * T.int64(32) + eps_1_nu_1_co_1_p_1_fused // T.int64(2) * T.int64(16) + eps_2_nu_2_co_2_p_2_fused % T.int64(64) // T.int64(8) * T.int64(2) + ax2)
                            v3 = T.axis.spatial(T.int64(16), eps_1_nu_1_co_1_p_1_fused % T.int64(2) * T.int64(8) + eps_2_nu_2_co_2_p_2_fused % T.int64(8) + ax3)
                            T.reads(bgemm_local[v0, v1, v2, v3])
                            T.writes(bgemm[v0, v1, v2, v3])
                            bgemm[v0, v1, v2, v3] = bgemm_local[v0, v1, v2, v3]
        for n_co_h_0_w_0_fused_0 in T.thread_binding(T.int64(256), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 1024, "pragma_unroll_explicit": 1}):
            for n_co_h_0_w_0_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                    for ax2 in T.unroll(T.int64(2)):
                        for ax3 in T.unroll(T.int64(2)):
                            with T.block("inverse_init"):
                                v_co = T.axis.spatial(T.int64(512), (n_co_h_0_w_0_fused_0 * T.int64(32) + n_co_h_0_w_0_fused_1) // T.int64(16) + ax0)
                                v_p = T.axis.spatial(T.int64(16), (n_co_h_0_w_0_fused_0 * T.int64(32) + n_co_h_0_w_0_fused_1) % T.int64(16) + ax1)
                                v_vh, v_vw = T.axis.remap("SS", [ax2, ax3])
                                T.reads()
                                T.writes(inverse_local[v_co, v_p, v_vh, v_vw])
                                T.block_attr({"schedule_rule": "conv2d_nchw_winograd_inverse"})
                                inverse_local[v_co, v_p, v_vh, v_vw] = T.float32(0)
                            for ax4 in T.unroll(T.int64(4)):
                                for ax5 in T.unroll(T.int64(4)):
                                    with T.block("inverse_update"):
                                        v_co = T.axis.spatial(T.int64(512), (n_co_h_0_w_0_fused_0 * T.int64(32) + n_co_h_0_w_0_fused_1) // T.int64(16) + ax0)
                                        v_p = T.axis.spatial(T.int64(16), (n_co_h_0_w_0_fused_0 * T.int64(32) + n_co_h_0_w_0_fused_1) % T.int64(16) + ax1)
                                        v_vh, v_vw, v_r_a, v_r_b = T.axis.remap("SSRR", [ax2, ax3, ax4, ax5])
                                        T.reads(inverse_local[v_co, v_p, v_vh, v_vw], bgemm[v_r_a, v_r_b, v_co, v_p])
                                        T.writes(inverse_local[v_co, v_p, v_vh, v_vw])
                                        T.block_attr({"schedule_rule": "conv2d_nchw_winograd_inverse"})
                                        inverse_local[v_co, v_p, v_vh, v_vw] = inverse_local[v_co, v_p, v_vh, v_vw] + bgemm[v_r_a, v_r_b, v_co, v_p] * T.Select(v_r_a % T.int64(4) == T.int64(3) and v_vh % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_vh % T.int64(2) == T.int64(0), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_vh % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_vh % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_vh % T.int64(2) == T.int64(1), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_vh % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_vh % T.int64(2) == T.int64(1), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_vh % T.int64(2) == T.int64(0), T.float32(1), T.float32(0))))))))) * T.Select(v_r_b % T.int64(4) == T.int64(3) and v_vw % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_vw % T.int64(2) == T.int64(0), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_vw % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_vw % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_vw % T.int64(2) == T.int64(1), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_vw % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_vw % T.int64(2) == T.int64(1), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_vw % T.int64(2) == T.int64(0), T.float32(1), T.float32(0)))))))))
                for h_1, w_1 in T.grid(T.int64(2), T.int64(2)):
                    with T.block("conv2d_winograd"):
                        v_n = T.axis.spatial(T.int64(1), T.int64(0))
                        v_co = T.axis.spatial(T.int64(512), (n_co_h_0_w_0_fused_0 * T.int64(32) + n_co_h_0_w_0_fused_1) // T.int64(16))
                        v_h = T.axis.spatial(T.int64(7), (n_co_h_0_w_0_fused_0 * T.int64(32) + n_co_h_0_w_0_fused_1) % T.int64(16) // T.int64(4) * T.int64(2) + h_1)
                        v_w = T.axis.spatial(T.int64(7), (n_co_h_0_w_0_fused_0 * T.int64(32) + n_co_h_0_w_0_fused_1) % T.int64(4) * T.int64(2) + w_1)
                        T.where((n_co_h_0_w_0_fused_0 * T.int64(32) + n_co_h_0_w_0_fused_1) % T.int64(16) // T.int64(4) * T.int64(2) + h_1 < T.int64(7) and (n_co_h_0_w_0_fused_0 * T.int64(32) + n_co_h_0_w_0_fused_1) % T.int64(4) * T.int64(2) + w_1 < T.int64(7))
                        T.reads(inverse_local[v_co, v_n * T.int64(16) + v_h // T.int64(2) * T.int64(4) + v_w // T.int64(2), v_h % T.int64(2), v_w % T.int64(2)], p2[v_n, v_co, v_h, v_w], p3[v_n, v_co, T.int64(0), T.int64(0)], p4[v_n, v_co, T.int64(0), T.int64(0)])
                        T.writes(T_relu[v_n, v_co, v_h, v_w])
                        T_relu[v_n, v_co, v_h, v_w] = T.max((inverse_local[v_co, v_n * T.int64(16) + v_h // T.int64(2) * T.int64(4) + v_w // T.int64(2), v_h % T.int64(2), v_w % T.int64(2)] + p2[v_n, v_co, v_h, v_w]) * p3[v_n, v_co, T.int64(0), T.int64(0)] + p4[v_n, v_co, T.int64(0), T.int64(0)], T.float32(0))
b0 = sch.get_block(name="data_pack", func_name="main")
b1 = sch.get_block(name="bgemm", func_name="main")
b2 = sch.get_block(name="inverse", func_name="main")
b3 = sch.get_block(name="conv2d_winograd", func_name="main")
b4 = sch.get_block(name="T_add", func_name="main")
b5 = sch.get_block(name="T_multiply", func_name="main")
b6 = sch.get_block(name="T_add_1", func_name="main")
b7 = sch.get_block(name="T_relu", func_name="main")
b8 = sch.get_block(name="root", func_name="main")
b9, b10 = sch.get_producers(block=b2)
sch.compute_inline(block=b10)
b11, = sch.get_consumers(block=b2)
l12, l13, l14, l15 = sch.get_loops(block=b11)
l16, l17 = sch.split(loop=l14, factors=[None, 2], preserve_unit_iters=True)
l18, l19 = sch.split(loop=l15, factors=[None, 2], preserve_unit_iters=True)
sch.reorder(l16, l18, l17, l19)
sch.compute_at(block=b2, loop=l18, preserve_unit_loops=True, index=-1)
sch.set_scope(block=b2, buffer_index=0, storage_scope="local")
l20, l21, l22, l23, l24, l25, l26, l27, l28, l29 = sch.get_loops(block=b2)
sch.unroll(loop=l26)
sch.unroll(loop=l27)
sch.unroll(loop=l28)
sch.unroll(loop=l29)
b30, b31 = sch.get_producers(block=b0)
sch.compute_inline(block=b31)
b32, = sch.get_producers(block=b30)
l33, l34, l35, l36, l37, l38 = sch.get_loops(block=b0)
sch.reorder(l35, l36, l33, l34, l37, l38)
sch.unroll(loop=l33)
sch.unroll(loop=l34)
sch.unroll(loop=l37)
sch.unroll(loop=l38)
l39 = sch.fuse(l35, l36, preserve_unit_iters=True)
v40 = sch.sample_categorical(candidates=[32, 64, 128, 256, 512, 1024], probs=[0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666], decision=2)
l41, l42 = sch.split(loop=l39, factors=[None, v40], preserve_unit_iters=True)
sch.bind(loop=l41, thread_axis="blockIdx.x")
sch.bind(loop=l42, thread_axis="threadIdx.x")
b43 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b43, loop=l42, preserve_unit_loops=True, index=-1)
sch.compute_at(block=b30, loop=l42, preserve_unit_loops=True, index=-1)
sch.set_scope(block=b30, buffer_index=0, storage_scope="local")
sch.compute_inline(block=b32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l44, l45, l46, l47, l48 = sch.get_loops(block=b1)
v49, v50, v51, v52, v53 = sch.sample_perfect_tile(loop=l44, n=5, max_innermost_factor=64, decision=[4, 1, 1, 1, 1])
l54, l55, l56, l57, l58 = sch.split(loop=l44, factors=[v49, v50, v51, v52, v53], preserve_unit_iters=True)
v59, v60, v61, v62, v63 = sch.sample_perfect_tile(loop=l45, n=5, max_innermost_factor=64, decision=[2, 1, 2, 1, 1])
l64, l65, l66, l67, l68 = sch.split(loop=l45, factors=[v59, v60, v61, v62, v63], preserve_unit_iters=True)
v69, v70, v71, v72, v73 = sch.sample_perfect_tile(loop=l46, n=5, max_innermost_factor=64, decision=[16, 2, 8, 2, 1])
l74, l75, l76, l77, l78 = sch.split(loop=l46, factors=[v69, v70, v71, v72, v73], preserve_unit_iters=True)
v79, v80, v81, v82, v83 = sch.sample_perfect_tile(loop=l47, n=5, max_innermost_factor=64, decision=[1, 2, 8, 1, 1])
l84, l85, l86, l87, l88 = sch.split(loop=l47, factors=[v79, v80, v81, v82, v83], preserve_unit_iters=True)
v89, v90, v91 = sch.sample_perfect_tile(loop=l48, n=3, max_innermost_factor=64, decision=[8, 2, 32])
l92, l93, l94 = sch.split(loop=l48, factors=[v89, v90, v91], preserve_unit_iters=True)
sch.reorder(l54, l64, l74, l84, l55, l65, l75, l85, l56, l66, l76, l86, l92, l93, l57, l67, l77, l87, l94, l58, l68, l78, l88)
l95 = sch.fuse(l54, l64, l74, l84, preserve_unit_iters=True)
sch.bind(loop=l95, thread_axis="blockIdx.x")
l96 = sch.fuse(l55, l65, l75, l85, preserve_unit_iters=True)
sch.bind(loop=l96, thread_axis="vthread.x")
l97 = sch.fuse(l56, l66, l76, l86, preserve_unit_iters=True)
sch.bind(loop=l97, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b98 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b98, loop=l97, preserve_unit_loops=True, index=-1)
b99 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b99, loop=l92, preserve_unit_loops=True, index=-1)
l100, l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b99)
l108 = sch.fuse(l104, l105, l106, l107, preserve_unit_iters=True)
v109 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b99, ann_key="meta_schedule.cooperative_fetch", ann_val=v109)
b110 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b110, loop=l92, preserve_unit_loops=True, index=-1)
l111, l112, l113, l114, l115, l116, l117, l118 = sch.get_loops(block=b110)
l119 = sch.fuse(l115, l116, l117, l118, preserve_unit_iters=True)
v120 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b110, ann_key="meta_schedule.cooperative_fetch", ann_val=v120)
sch.reverse_compute_inline(block=b7)
sch.reverse_compute_inline(block=b6)
sch.reverse_compute_inline(block=b5)
sch.reverse_compute_inline(block=b4)
v121 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b8, ann_key="meta_schedule.unroll_explicit", ann_val=v121)
l122, l123, l124, l125, l126, l127 = sch.get_loops(block=b3)
l128 = sch.fuse(l122, l123, l124, l125, preserve_unit_iters=True)
v129 = sch.sample_categorical(candidates=[32, 64, 128, 256, 512, 1024], probs=[0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666], decision=0)
l130, l131 = sch.split(loop=l128, factors=[None, v129], preserve_unit_iters=True)
sch.bind(loop=l130, thread_axis="blockIdx.x")
sch.bind(loop=l131, thread_axis="threadIdx.x")
sch.enter_postproc()
sch.unannotate(block_or_loop=b99, ann_key="meta_schedule.cooperative_fetch")
l132, l133, l134, l135, l136 = sch.get_loops(block=b99)
l137, l138, l139 = sch.split(loop=l136, factors=[None, 128, 4], preserve_unit_iters=True)
sch.vectorize(loop=l139)
sch.bind(loop=l138, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b110, ann_key="meta_schedule.cooperative_fetch")
l140, l141, l142, l143, l144 = sch.get_loops(block=b110)
l145, l146 = sch.split(loop=l144, factors=[None, 128], preserve_unit_iters=True)
sch.bind(loop=l146, thread_axis="threadIdx.x")
b147 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b147, ann_key="meta_schedule.unroll_explicit")
b148, b149, b150, b151, b152, b153, b154, b155, b156 = sch.get_child_blocks(b147)
l157, l158, l159, l160, l161, l162 = sch.get_loops(block=b148)
l163, l164, l165, l166, l167, l168 = sch.get_loops(block=b149)
sch.annotate(block_or_loop=l163, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l163, ann_key="pragma_unroll_explicit", ann_val=1)
l169, l170, l171, l172, l173, l174 = sch.get_loops(block=b150)
l175, l176, l177, l178, l179, l180, l181 = sch.get_loops(block=b151)
l182, l183, l184, l185, l186, l187 = sch.get_loops(block=b152)
l188, l189, l190, l191, l192, l193, l194, l195, l196, l197, l198, l199, l200, l201 = sch.get_loops(block=b153)
sch.annotate(block_or_loop=l188, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l188, ann_key="pragma_unroll_explicit", ann_val=1)
l202, l203, l204, l205, l206, l207, l208 = sch.get_loops(block=b154)
l209, l210, l211, l212, l213, l214, l215, l216 = sch.get_loops(block=b155)
sch.annotate(block_or_loop=l209, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l209, ann_key="pragma_unroll_explicit", ann_val=1)
l217, l218, l219, l220 = sch.get_loops(block=b156)
b221 = sch.get_block(name="data_pack", func_name="main")
l222, l223, l224, l225, l226, l227 = sch.get_loops(block=b221)
b228 = sch.decompose_reduction(block=b221, loop=l226)
b229 = sch.get_block(name="bgemm", func_name="main")
l230, l231, l232, l233, l234, l235, l236, l237, l238, l239, l240, l241, l242, l243 = sch.get_loops(block=b229)
b244 = sch.decompose_reduction(block=b229, loop=l233)
b245 = sch.get_block(name="inverse", func_name="main")
l246, l247, l248, l249, l250, l251, l252, l253 = sch.get_loops(block=b245)
b254 = sch.decompose_reduction(block=b245, loop=l252)
2023-11-11 02:29:36 [INFO] [task_scheduler.cc:121] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #197: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/home2/xiachunwei/Software/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/home2/xiachunwei/Software/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/home2/xiachunwei/Software/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/home2/xiachunwei/Software/tvm/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/home2/xiachunwei/Software/tvm/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/home2/xiachunwei/Software/tvm/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/home2/xiachunwei/Software/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/home2/xiachunwei/Software/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  4: _ZN3tvm7runtime13PackedFun
  3: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  2: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  0: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  File "/home2/xiachunwei/Software/tvm/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(512), T.int64(7), T.int64(7)), "float32"), p1: T.Buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(512)), "float32"), p2: T.Buffer((T.int64(1), T.int64(512), T.int64(7), T.int64(7)), "float32"), p3: T.Buffer((T.int64(1), T.int64(512), T.int64(1), T.int64(1)), "float32"), p4: T.Buffer((T.int64(1), T.int64(512), T.int64(1), T.int64(1)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(512), T.int64(7), T.int64(7)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        input_tile_local = T.alloc_buffer((T.int64(512), T.int64(16), T.int64(4), T.int64(4)), scope="local")
        data_pack = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(16)))
        bgemm = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(16)))
        inverse_local = T.alloc_buffer((T.int64(512), T.int64(16), T.int64(2), T.int64(2)), scope="local")
        data_pack_local = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(16)), scope="local")
        bgemm_local = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(16)), scope="local")
        data_pack_shared = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(16)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(512)), scope="shared")
        for ci_p_fused_0 in T.thread_binding(T.int64(64), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 512, "pragma_unroll_explicit": 1}):
            for ci_p_fused_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(1), T.int64(4), T.int64(4)):
                    with T.block("input_tile"):
                        v_ci = T.axis.spatial(T.int64(512), ci_p_fused_0 * T.int64(8) + ci_p_fused_1 // T.int64(16) + ax0)
                        v_p = T.axis.spatial(T.int64(16), ci_p_fused_1 % T.int64(16) + ax1)
                        v_eps, v_nu = T.axis.remap("SS", [ax2, ax3])
                        T.reads(p0[v_p // T.int64(16), v_ci, v_p % T.int64(16) // T.int64(4) * T.int64(2) + v_eps - T.int64(1), v_p % T.int64(4) * T.int64(2) + v_nu - T.int64(1)])
                        T.writes(input_tile_local[v_ci, v_p, v_eps, v_nu])
                        T.block_attr({"schedule_rule": "None"})
                        input_tile_local[v_ci, v_p, v_eps, v_nu] = T.if_then_else(T.int64(1) <= v_p % T.int64(16) // T.int64(4) * T.int64(2) + v_eps and v_p % T.int64(16) // T.int64(4) * T.int64(2) + v_eps < T.int64(8) and T.int64(1) <= v_p % T.int64(4) * T.int64(2) + v_nu and v_p % T.int64(4) * T.int64(2) + v_nu < T.int64(8), p0[v_p // T.int64(16), v_ci, v_p % T.int64(16) // T.int64(4) * T.int64(2) + v_eps - T.int64(1), v_p % T.int64(4) * T.int64(2) + v_nu - T.int64(1)], T.float32(0))
                for eps in T.unroll(T.int64(4)):
                    for nu in T.unroll(T.int64(4)):
                        with T.block("data_pack_init"):
                            v_eps, v_nu = T.axis.remap("SS", [eps, nu])
                            v_ci = T.axis.spatial(T.int64(512), (ci_p_fused_0 * T.int64(128) + ci_p_fused_1) // T.int64(16))
                            v_p = T.axis.spatial(T.int64(16), (ci_p_fused_0 * T.int64(128) + ci_p_fused_1) % T.int64(16))
                            T.reads()
                            T.writes(data_pack_local[v_eps, v_nu, v_ci, v_p])
                            T.block_attr({"schedule_rule": "conv2d_nchw_winograd_data_pack"})
                            data_pack_local[v_eps, v_nu, v_ci, v_p] = T.float32(0)
                        for r_a in T.unroll(T.int64(4)):
                            for r_b in T.unroll(T.int64(4)):
                                with T.block("data_pack_update"):
                                    v_eps, v_nu = T.axis.remap("SS", [eps, nu])
                                    v_ci = T.axis.spatial(T.int64(512), (ci_p_fused_0 * T.int64(128) + ci_p_fused_1) // T.int64(16))
                                    v_p = T.axis.spatial(T.int64(16), (ci_p_fused_0 * T.int64(128) + ci_p_fused_1) % T.int64(16))
                                    v_r_a, v_r_b = T.axis.remap("RR", [r_a, r_b])
                                    T.reads(data_pack_local[v_eps, v_nu, v_ci, v_p], input_tile_local[v_ci, v_p, v_r_a, v_r_b])
                                    T.writes(data_pack_local[v_eps, v_nu, v_ci, v_p])
                                    T.block_attr({"schedule_rule": "conv2d_nchw_winograd_data_pack"})
                                    data_pack_local[v_eps, v_nu, v_ci, v_p] = data_pack_local[v_eps, v_nu, v_ci, v_p] + input_tile_local[v_ci, v_p, v_r_a, v_r_b] * T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(3), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(1), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(0), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(3), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(1), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(0), T.float32(1), T.float32(0))))))))))))))))) * T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(3), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(1), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(0), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(3), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(1), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(0), T.float32(1), T.float32(0)))))))))))))))))
                for ax0, ax1, ax2, ax3 in T.grid(T.int64(4), T.int64(4), T.int64(1), T.int64(1)):
                    with T.block("data_pack_local"):
                        v0, v1 = T.axis.remap("SS", [ax0, ax1])
                        v2 = T.axis.spatial(T.int64(512), ci_p_fused_0 * T.int64(8) + ci_p_fused_1 // T.int64(16) + ax2)
                        v3 = T.axis.spatial(T.int64(16), ci_p_fused_1 % T.int64(16) + ax3)
                        T.reads(data_pack_local[v0, v1, v2, v3])
                        T.writes(data_pack[v0, v1, v2, v3])
                        data_pack[v0, v1, v2, v3] = data_pack_local[v0, v1, v2, v3]
        for eps_0_nu_0_co_0_p_0_fused in T.thread_binding(T.int64(128), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 512, "pragma_unroll_explicit": 1}):
            for eps_1_nu_1_co_1_p_1_fused in T.thread_binding(T.int64(4), thread="vthread.x"):
                for eps_2_nu_2_co_2_p_2_fused in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                    for eps_3_init, nu_3_init, co_3_init, p_3_init, eps_4_init, nu_4_init, co_4_init, p_4_init in T.grid(T.int64(1), T.int64(1), T.int64(2), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("bgemm_init"):
                            v_eps = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(32) + eps_3_init + eps_4_init)
                            v_nu = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused % T.int64(32) // T.int64(16) * T.int64(2) + eps_2_nu_2_co_2_p_2_fused // T.int64(64) + nu_3_init + nu_4_init)
                            v_co = T.axis.spatial(T.int64(512), eps_0_nu_0_co_0_p_0_fused % T.int64(16) * T.int64(32) + eps_1_nu_1_co_1_p_1_fused // T.int64(2) * T.int64(16) + eps_2_nu_2_co_2_p_2_fused % T.int64(64) // T.int64(8) * T.int64(2) + co_3_init + co_4_init)
                            v_p = T.axis.spatial(T.int64(16), eps_1_nu_1_co_1_p_1_fused % T.int64(2) * T.int64(8) + eps_2_nu_2_co_2_p_2_fused % T.int64(8) + p_3_init + p_4_init)
                            T.reads()
                            T.writes(bgemm_local[v_eps, v_nu, v_co, v_p])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            bgemm_local[v_eps, v_nu, v_co, v_p] = T.float32(0)
                    for ci_0 in range(T.int64(16)):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(4)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("data_pack_shared"):
                                        v0 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(32))
                                        v1 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused % T.int64(32) // T.int64(16) * T.int64(2) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) // T.int64(512))
                                        v2 = T.axis.spatial(T.int64(512), ci_0 * T.int64(32) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) % T.int64(512) // T.int64(16))
                                        v3 = T.axis.spatial(T.int64(16), (ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) % T.int64(16))
                                        T.reads(data_pack[v0, v1, v2, v3])
                                        T.writes(data_pack_shared[v0, v1, v2, v3])
                                        data_pack_shared[v0, v1, v2, v3] = data_pack[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(8)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("p1_shared"):
                                        v0 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(32))
                                        v1 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused % T.int64(32) // T.int64(16) * T.int64(2) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) // T.int64(1024))
                                        v2 = T.axis.spatial(T.int64(512), ci_0 * T.int64(32) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) % T.int64(1024) // T.int64(32))
                                        v3 = T.axis.spatial(T.int64(512), eps_0_nu_0_co_0_p_0_fused % T.int64(16) * T.int64(32) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) % T.int64(32))
                                        T.reads(p1[v0, v1, v2, v3])
                                        T.writes(p1_shared[v0, v1, v2, v3])
                                        p1_shared[v0, v1, v2, v3] = p1[v0, v1, v2, v3]
                        for ci_1, eps_3, nu_3, co_3, p_3, ci_2, eps_4, nu_4, co_4, p_4 in T.grid(T.int64(2), T.int64(1), T.int64(1), T.int64(2), T.int64(1), T.int64(16), T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                            with T.block("bgemm_update"):
                                v_eps = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(32) + eps_3 + eps_4)
                                v_nu = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused % T.int64(32) // T.int64(16) * T.int64(2) + eps_2_nu_2_co_2_p_2_fused // T.int64(64) + nu_3 + nu_4)
                                v_co = T.axis.spatial(T.int64(512), eps_0_nu_0_co_0_p_0_fused % T.int64(16) * T.int64(32) + eps_1_nu_1_co_1_p_1_fused // T.int64(2) * T.int64(16) + eps_2_nu_2_co_2_p_2_fused % T.int64(64) // T.int64(8) * T.int64(2) + co_3 + co_4)
                                v_p = T.axis.spatial(T.int64(16), eps_1_nu_1_co_1_p_1_fused % T.int64(2) * T.int64(8) + eps_2_nu_2_co_2_p_2_fused % T.int64(8) + p_3 + p_4)
                                v_ci = T.axis.reduce(T.int64(512), ci_0 * T.int64(32) + ci_1 * T.int64(16) + ci_2)
                                T.reads(bgemm_local[v_eps, v_nu, v_co, v_p], data_pack_shared[v_eps, v_nu, v_ci, v_p], p1_shared[v_eps, v_nu, v_ci, v_co])
                                T.writes(bgemm_local[v_eps, v_nu, v_co, v_p])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                bgemm_local[v_eps, v_nu, v_co, v_p] = bgemm_local[v_eps, v_nu, v_co, v_p] + data_pack_shared[v_eps, v_nu, v_ci, v_p] * p1_shared[v_eps, v_nu, v_ci, v_co]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(1), T.int64(2), T.int64(1)):
                        with T.block("bgemm_local"):
                            v0 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(32) + ax0)
                            v1 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused % T.int64(32) // T.int64(16) * T.int64(2) + eps_2_nu_2_co_2_p_2_fused // T.int64(64) + ax1)
                            v2 = T.axis.spatial(T.int64(512), eps_0_nu_0_co_0_p_0_fused % T.int64(16) * T.int64(32) + eps_1_nu_1_co_1_p_1_fused // T.int64(2) * T.int64(16) + eps_2_nu_2_co_2_p_2_fused % T.int64(64) // T.int64(8) * T.int64(2) + ax2)
                            v3 = T.axis.spatial(T.int64(16), eps_1_nu_1_co_1_p_1_fused % T.int64(2) * T.int64(8) + eps_2_nu_2_co_2_p_2_fused % T.int64(8) + ax3)
                            T.reads(bgemm_local[v0, v1, v2, v3])
                            T.writes(bgemm[v0, v1, v2, v3])
                            bgemm[v0, v1, v2, v3] = bgemm_local[v0, v1, v2, v3]
        for n_co_h_0_w_0_fused_0 in T.thread_binding(T.int64(256), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 512, "pragma_unroll_explicit": 1}):
            for n_co_h_0_w_0_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                    for ax2 in T.unroll(T.int64(2)):
                        for ax3 in T.unroll(T.int64(2)):
                            with T.block("inverse_init"):
                                v_co = T.axis.spatial(T.int64(512), (n_co_h_0_w_0_fused_0 * T.int64(32) + n_co_h_0_w_0_fused_1) // T.int64(16) + ax0)
                                v_p = T.axis.spatial(T.int64(16), (n_co_h_0_w_0_fused_0 * T.int64(32) + n_co_h_0_w_0_fused_1) % T.int64(16) + ax1)
                                v_vh, v_vw = T.axis.remap("SS", [ax2, ax3])
                                T.reads()
                                T.writes(inverse_local[v_co, v_p, v_vh, v_vw])
                                T.block_attr({"schedule_rule": "conv2d_nchw_winograd_inverse"})
                                inverse_local[v_co, v_p, v_vh, v_vw] = T.float32(0)
                            for ax4 in T.unroll(T.int64(4)):
                                for ax5 in T.unroll(T.int64(4)):
                                    with T.block("inverse_update"):
                                        v_co = T.axis.spatial(T.int64(512), (n_co_h_0_w_0_fused_0 * T.int64(32) + n_co_h_0_w_0_fused_1) // T.int64(16) + ax0)
                                        v_p = T.axis.spatial(T.int64(16), (n_co_h_0_w_0_fused_0 * T.int64(32) + n_co_h_0_w_0_fused_1) % T.int64(16) + ax1)
                                        v_vh, v_vw, v_r_a, v_r_b = T.axis.remap("SSRR", [ax2, ax3, ax4, ax5])
                                        T.reads(inverse_local[v_co, v_p, v_vh, v_vw], bgemm[v_r_a, v_r_b, v_co, v_p])
                                        T.writes(inverse_local[v_co, v_p, v_vh, v_vw])
                                        T.block_attr({"schedule_rule": "conv2d_nchw_winograd_inverse"})
                                        inverse_local[v_co, v_p, v_vh, v_vw] = inverse_local[v_co, v_p, v_vh, v_vw] + bgemm[v_r_a, v_r_b, v_co, v_p] * T.Select(v_r_a % T.int64(4) == T.int64(3) and v_vh % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_vh % T.int64(2) == T.int64(0), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_vh % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_vh % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_vh % T.int64(2) == T.int64(1), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_vh % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_vh % T.int64(2) == T.int64(1), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_vh % T.int64(2) == T.int64(0), T.float32(1), T.float32(0))))))))) * T.Select(v_r_b % T.int64(4) == T.int64(3) and v_vw % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_vw % T.int64(2) == T.int64(0), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_vw % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_vw % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_vw % T.int64(2) == T.int64(1), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_vw % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_vw % T.int64(2) == T.int64(1), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_vw % T.int64(2) == T.int64(0), T.float32(1), T.float32(0)))))))))
                for h_1, w_1 in T.grid(T.int64(2), T.int64(2)):
                    with T.block("conv2d_winograd"):
                        v_n = T.axis.spatial(T.int64(1), T.int64(0))
                        v_co = T.axis.spatial(T.int64(512), (n_co_h_0_w_0_fused_0 * T.int64(32) + n_co_h_0_w_0_fused_1) // T.int64(16))
                        v_h = T.axis.spatial(T.int64(7), (n_co_h_0_w_0_fused_0 * T.int64(32) + n_co_h_0_w_0_fused_1) % T.int64(16) // T.int64(4) * T.int64(2) + h_1)
                        v_w = T.axis.spatial(T.int64(7), (n_co_h_0_w_0_fused_0 * T.int64(32) + n_co_h_0_w_0_fused_1) % T.int64(4) * T.int64(2) + w_1)
                        T.where((n_co_h_0_w_0_fused_0 * T.int64(32) + n_co_h_0_w_0_fused_1) % T.int64(16) // T.int64(4) * T.int64(2) + h_1 < T.int64(7) and (n_co_h_0_w_0_fused_0 * T.int64(32) + n_co_h_0_w_0_fused_1) % T.int64(4) * T.int64(2) + w_1 < T.int64(7))
                        T.reads(inverse_local[v_co, v_n * T.int64(16) + v_h // T.int64(2) * T.int64(4) + v_w // T.int64(2), v_h % T.int64(2), v_w % T.int64(2)], p2[v_n, v_co, v_h, v_w], p3[v_n, v_co, T.int64(0), T.int64(0)], p4[v_n, v_co, T.int64(0), T.int64(0)])
                        T.writes(T_relu[v_n, v_co, v_h, v_w])
                        T_relu[v_n, v_co, v_h, v_w] = T.max((inverse_local[v_co, v_n * T.int64(16) + v_h // T.int64(2) * T.int64(4) + v_w // T.int64(2), v_h % T.int64(2), v_w % T.int64(2)] + p2[v_n, v_co, v_h, v_w]) * p3[v_n, v_co, T.int64(0), T.int64(0)] + p4[v_n, v_co, T.int64(0), T.int64(0)], T.float32(0))
b0 = sch.get_block(name="data_pack", func_name="main")
b1 = sch.get_block(name="bgemm", func_name="main")
b2 = sch.get_block(name="inverse", func_name="main")
b3 = sch.get_block(name="conv2d_winograd", func_name="main")
b4 = sch.get_block(name="T_add", func_name="main")
b5 = sch.get_block(name="T_multiply", func_name="main")
b6 = sch.get_block(name="T_add_1", func_name="main")
b7 = sch.get_block(name="T_relu", func_name="main")
b8 = sch.get_block(name="root", func_name="main")
b9, b10 = sch.get_producers(block=b2)
sch.compute_inline(block=b10)
b11, = sch.get_consumers(block=b2)
l12, l13, l14, l15 = sch.get_loops(block=b11)
l16, l17 = sch.split(loop=l14, factors=[None, 2], preserve_unit_iters=True)
l18, l19 = sch.split(loop=l15, factors=[None, 2], preserve_unit_iters=True)
sch.reorder(l16, l18, l17, l19)
sch.compute_at(block=b2, loop=l18, preserve_unit_loops=True, index=-1)
sch.set_scope(block=b2, buffer_index=0, storage_scope="local")
l20, l21, l22, l23, l24, l25, l26, l27, l28, l29 = sch.get_loops(block=b2)
sch.unroll(loop=l26)
sch.unroll(loop=l27)
sch.unroll(loop=l28)
sch.unroll(loop=l29)
b30, b31 = sch.get_producers(block=b0)
sch.compute_inline(block=b31)
b32, = sch.get_producers(block=b30)
l33, l34, l35, l36, l37, l38 = sch.get_loops(block=b0)
sch.reorder(l35, l36, l33, l34, l37, l38)
sch.unroll(loop=l33)
sch.unroll(loop=l34)
sch.unroll(loop=l37)
sch.unroll(loop=l38)
l39 = sch.fuse(l35, l36, preserve_unit_iters=True)
v40 = sch.sample_categorical(candidates=[32, 64, 128, 256, 512, 1024], probs=[0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666], decision=2)
l41, l42 = sch.split(loop=l39, factors=[None, v40], preserve_unit_iters=True)
sch.bind(loop=l41, thread_axis="blockIdx.x")
sch.bind(loop=l42, thread_axis="threadIdx.x")
b43 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b43, loop=l42, preserve_unit_loops=True, index=-1)
sch.compute_at(block=b30, loop=l42, preserve_unit_loops=True, index=-1)
sch.set_scope(block=b30, buffer_index=0, storage_scope="local")
sch.compute_inline(block=b32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l44, l45, l46, l47, l48 = sch.get_loops(block=b1)
v49, v50, v51, v52, v53 = sch.sample_perfect_tile(loop=l44, n=5, max_innermost_factor=64, decision=[4, 1, 1, 1, 1])
l54, l55, l56, l57, l58 = sch.split(loop=l44, factors=[v49, v50, v51, v52, v53], preserve_unit_iters=True)
v59, v60, v61, v62, v63 = sch.sample_perfect_tile(loop=l45, n=5, max_innermost_factor=64, decision=[2, 1, 2, 1, 1])
l64, l65, l66, l67, l68 = sch.split(loop=l45, factors=[v59, v60, v61, v62, v63], preserve_unit_iters=True)
v69, v70, v71, v72, v73 = sch.sample_perfect_tile(loop=l46, n=5, max_innermost_factor=64, decision=[16, 2, 8, 2, 1])
l74, l75, l76, l77, l78 = sch.split(loop=l46, factors=[v69, v70, v71, v72, v73], preserve_unit_iters=True)
v79, v80, v81, v82, v83 = sch.sample_perfect_tile(loop=l47, n=5, max_innermost_factor=64, decision=[1, 2, 8, 1, 1])
l84, l85, l86, l87, l88 = sch.split(loop=l47, factors=[v79, v80, v81, v82, v83], preserve_unit_iters=True)
v89, v90, v91 = sch.sample_perfect_tile(loop=l48, n=3, max_innermost_factor=64, decision=[16, 2, 16])
l92, l93, l94 = sch.split(loop=l48, factors=[v89, v90, v91], preserve_unit_iters=True)
sch.reorder(l54, l64, l74, l84, l55, l65, l75, l85, l56, l66, l76, l86, l92, l93, l57, l67, l77, l87, l94, l58, l68, l78, l88)
l95 = sch.fuse(l54, l64, l74, l84, preserve_unit_iters=True)
sch.bind(loop=l95, thread_axis="blockIdx.x")
l96 = sch.fuse(l55, l65, l75, l85, preserve_unit_iters=True)
sch.bind(loop=l96, thread_axis="vthread.x")
l97 = sch.fuse(l56, l66, l76, l86, preserve_unit_iters=True)
sch.bind(loop=l97, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b98 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b98, loop=l97, preserve_unit_loops=True, index=-1)
b99 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b99, loop=l92, preserve_unit_loops=True, index=-1)
l100, l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b99)
l108 = sch.fuse(l104, l105, l106, l107, preserve_unit_iters=True)
v109 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b99, ann_key="meta_schedule.cooperative_fetch", ann_val=v109)
b110 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b110, loop=l92, preserve_unit_loops=True, index=-1)
l111, l112, l113, l114, l115, l116, l117, l118 = sch.get_loops(block=b110)
l119 = sch.fuse(l115, l116, l117, l118, preserve_unit_iters=True)
v120 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b110, ann_key="meta_schedule.cooperative_fetch", ann_val=v120)
sch.reverse_compute_inline(block=b7)
sch.reverse_compute_inline(block=b6)
sch.reverse_compute_inline(block=b5)
sch.reverse_compute_inline(block=b4)
v121 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b8, ann_key="meta_schedule.unroll_explicit", ann_val=v121)
l122, l123, l124, l125, l126, l127 = sch.get_loops(block=b3)
l128 = sch.fuse(l122, l123, l124, l125, preserve_unit_iters=True)
v129 = sch.sample_categorical(candidates=[32, 64, 128, 256, 512, 1024], probs=[0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666], decision=0)
l130, l131 = sch.split(loop=l128, factors=[None, v129], preserve_unit_iters=True)
sch.bind(loop=l130, thread_axis="blockIdx.x")
sch.bind(loop=l131, thread_axis="threadIdx.x")
sch.enter_postproc()
sch.unannotate(block_or_loop=b99, ann_key="meta_schedule.cooperative_fetch")
l132, l133, l134, l135, l136 = sch.get_loops(block=b99)
l137, l138, l139 = sch.split(loop=l136, factors=[None, 128, 2], preserve_unit_iters=True)
sch.vectorize(loop=l139)
sch.bind(loop=l138, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b110, ann_key="meta_schedule.cooperative_fetch")
l140, l141, l142, l143, l144 = sch.get_loops(block=b110)
l145, l146, l147 = sch.split(loop=l144, factors=[None, 128, 2], preserve_unit_iters=True)
sch.vectorize(loop=l147)
sch.bind(loop=l146, thread_axis="threadIdx.x")
b148 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b148, ann_key="meta_schedule.unroll_explicit")
b149, b150, b151, b152, b153, b154, b155, b156, b157 = sch.get_child_blocks(b148)
l158, l159, l160, l161, l162, l163 = sch.get_loops(block=b149)
l164, l165, l166, l167, l168, l169 = sch.get_loops(block=b150)
sch.annotate(block_or_loop=l164, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l164, ann_key="pragma_unroll_explicit", ann_val=1)
l170, l171, l172, l173, l174, l175 = sch.get_loops(block=b151)
l176, l177, l178, l179, l180, l181, l182 = sch.get_loops(block=b152)
l183, l184, l185, l186, l187, l188, l189 = sch.get_loops(block=b153)
l190, l191, l192, l193, l194, l195, l196, l197, l198, l199, l200, l201, l202, l203 = sch.get_loops(block=b154)
sch.annotate(block_or_loop=l190, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l190, ann_key="pragma_unroll_explicit", ann_val=1)
l204, l205, l206, l207, l208, l209, l210 = sch.get_loops(block=b155)
l211, l212, l213, l214, l215, l216, l217, l218 = sch.get_loops(block=b156)
sch.annotate(block_or_loop=l211, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l211, ann_key="pragma_unroll_explicit", ann_val=1)
l219, l220, l221, l222 = sch.get_loops(block=b157)
b223 = sch.get_block(name="data_pack", func_name="main")
l224, l225, l226, l227, l228, l229 = sch.get_loops(block=b223)
b230 = sch.decompose_reduction(block=b223, loop=l228)
b231 = sch.get_block(name="bgemm", func_name="main")
l232, l233, l234, l235, l236, l237, l238, l239, l240, l241, l242, l243, l244, l245 = sch.get_loops(block=b231)
b246 = sch.decompose_reduction(block=b231, loop=l235)
b247 = sch.get_block(name="inverse", func_name="main")
l248, l249, l250, l251, l252, l253, l254, l255 = sch.get_loops(block=b247)
b256 = sch.decompose_reduction(block=b247, loop=l254)
2023-11-11 02:29:36 [INFO] [task_scheduler.cc:121] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #198: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/home2/xiachunwei/Software/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/home2/xiachunwei/Software/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/home2/xiachunwei/Software/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/home2/xiachunwei/Software/tvm/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/home2/xiachunwei/Software/tvm/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/home2/xiachunwei/Software/tvm/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/home2/xiachunwei/Software/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/home2/xiachunwei/Software/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  4: _ZN3tvm7runtime13PackedFun
  3: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  2: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  0: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  File "/home2/xiachunwei/Software/tvm/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(512), T.int64(7), T.int64(7)), "float32"), p1: T.Buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(512)), "float32"), p2: T.Buffer((T.int64(1), T.int64(512), T.int64(7), T.int64(7)), "float32"), p3: T.Buffer((T.int64(1), T.int64(512), T.int64(1), T.int64(1)), "float32"), p4: T.Buffer((T.int64(1), T.int64(512), T.int64(1), T.int64(1)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(512), T.int64(7), T.int64(7)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        input_tile_local = T.alloc_buffer((T.int64(512), T.int64(16), T.int64(4), T.int64(4)), scope="local")
        data_pack = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(16)))
        bgemm = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(16)))
        inverse_local = T.alloc_buffer((T.int64(512), T.int64(16), T.int64(2), T.int64(2)), scope="local")
        data_pack_local = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(16)), scope="local")
        bgemm_local = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(16)), scope="local")
        data_pack_shared = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(16)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(512)), scope="shared")
        for ci_p_fused_0 in T.thread_binding(T.int64(64), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 1024, "pragma_unroll_explicit": 1}):
            for ci_p_fused_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(1), T.int64(4), T.int64(4)):
                    with T.block("input_tile"):
                        v_ci = T.axis.spatial(T.int64(512), ci_p_fused_0 * T.int64(8) + ci_p_fused_1 // T.int64(16) + ax0)
                        v_p = T.axis.spatial(T.int64(16), ci_p_fused_1 % T.int64(16) + ax1)
                        v_eps, v_nu = T.axis.remap("SS", [ax2, ax3])
                        T.reads(p0[v_p // T.int64(16), v_ci, v_p % T.int64(16) // T.int64(4) * T.int64(2) + v_eps - T.int64(1), v_p % T.int64(4) * T.int64(2) + v_nu - T.int64(1)])
                        T.writes(input_tile_local[v_ci, v_p, v_eps, v_nu])
                        T.block_attr({"schedule_rule": "None"})
                        input_tile_local[v_ci, v_p, v_eps, v_nu] = T.if_then_else(T.int64(1) <= v_p % T.int64(16) // T.int64(4) * T.int64(2) + v_eps and v_p % T.int64(16) // T.int64(4) * T.int64(2) + v_eps < T.int64(8) and T.int64(1) <= v_p % T.int64(4) * T.int64(2) + v_nu and v_p % T.int64(4) * T.int64(2) + v_nu < T.int64(8), p0[v_p // T.int64(16), v_ci, v_p % T.int64(16) // T.int64(4) * T.int64(2) + v_eps - T.int64(1), v_p % T.int64(4) * T.int64(2) + v_nu - T.int64(1)], T.float32(0))
                for eps in T.unroll(T.int64(4)):
                    for nu in T.unroll(T.int64(4)):
                        with T.block("data_pack_init"):
                            v_eps, v_nu = T.axis.remap("SS", [eps, nu])
                            v_ci = T.axis.spatial(T.int64(512), (ci_p_fused_0 * T.int64(128) + ci_p_fused_1) // T.int64(16))
                            v_p = T.axis.spatial(T.int64(16), (ci_p_fused_0 * T.int64(128) + ci_p_fused_1) % T.int64(16))
                            T.reads()
                            T.writes(data_pack_local[v_eps, v_nu, v_ci, v_p])
                            T.block_attr({"schedule_rule": "conv2d_nchw_winograd_data_pack"})
                            data_pack_local[v_eps, v_nu, v_ci, v_p] = T.float32(0)
                        for r_a in T.unroll(T.int64(4)):
                            for r_b in T.unroll(T.int64(4)):
                                with T.block("data_pack_update"):
                                    v_eps, v_nu = T.axis.remap("SS", [eps, nu])
                                    v_ci = T.axis.spatial(T.int64(512), (ci_p_fused_0 * T.int64(128) + ci_p_fused_1) // T.int64(16))
                                    v_p = T.axis.spatial(T.int64(16), (ci_p_fused_0 * T.int64(128) + ci_p_fused_1) % T.int64(16))
                                    v_r_a, v_r_b = T.axis.remap("RR", [r_a, r_b])
                                    T.reads(data_pack_local[v_eps, v_nu, v_ci, v_p], input_tile_local[v_ci, v_p, v_r_a, v_r_b])
                                    T.writes(data_pack_local[v_eps, v_nu, v_ci, v_p])
                                    T.block_attr({"schedule_rule": "conv2d_nchw_winograd_data_pack"})
                                    data_pack_local[v_eps, v_nu, v_ci, v_p] = data_pack_local[v_eps, v_nu, v_ci, v_p] + input_tile_local[v_ci, v_p, v_r_a, v_r_b] * T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(3), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(1), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(0), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(3), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(1), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(0), T.float32(1), T.float32(0))))))))))))))))) * T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(3), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(1), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(0), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(3), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(1), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(0), T.float32(1), T.float32(0)))))))))))))))))
                for ax0, ax1, ax2, ax3 in T.grid(T.int64(4), T.int64(4), T.int64(1), T.int64(1)):
                    with T.block("data_pack_local"):
                        v0, v1 = T.axis.remap("SS", [ax0, ax1])
                        v2 = T.axis.spatial(T.int64(512), ci_p_fused_0 * T.int64(8) + ci_p_fused_1 // T.int64(16) + ax2)
                        v3 = T.axis.spatial(T.int64(16), ci_p_fused_1 % T.int64(16) + ax3)
                        T.reads(data_pack_local[v0, v1, v2, v3])
                        T.writes(data_pack[v0, v1, v2, v3])
                        data_pack[v0, v1, v2, v3] = data_pack_local[v0, v1, v2, v3]
        for eps_0_nu_0_co_0_p_0_fused in T.thread_binding(T.int64(128), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 1024, "pragma_unroll_explicit": 1}):
            for eps_1_nu_1_co_1_p_1_fused in T.thread_binding(T.int64(4), thread="vthread.x"):
                for eps_2_nu_2_co_2_p_2_fused in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                    for eps_3_init, nu_3_init, co_3_init, p_3_init, eps_4_init, nu_4_init, co_4_init, p_4_init in T.grid(T.int64(1), T.int64(1), T.int64(2), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("bgemm_init"):
                            v_eps = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(32) + eps_3_init + eps_4_init)
                            v_nu = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused % T.int64(32) // T.int64(16) * T.int64(2) + eps_2_nu_2_co_2_p_2_fused // T.int64(64) + nu_3_init + nu_4_init)
                            v_co = T.axis.spatial(T.int64(512), eps_0_nu_0_co_0_p_0_fused % T.int64(16) * T.int64(32) + eps_1_nu_1_co_1_p_1_fused // T.int64(2) * T.int64(16) + eps_2_nu_2_co_2_p_2_fused % T.int64(64) // T.int64(8) * T.int64(2) + co_3_init + co_4_init)
                            v_p = T.axis.spatial(T.int64(16), eps_1_nu_1_co_1_p_1_fused % T.int64(2) * T.int64(8) + eps_2_nu_2_co_2_p_2_fused % T.int64(8) + p_3_init + p_4_init)
                            T.reads()
                            T.writes(bgemm_local[v_eps, v_nu, v_co, v_p])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            bgemm_local[v_eps, v_nu, v_co, v_p] = T.float32(0)
                    for ci_0 in range(T.int64(16)):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(4)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("data_pack_shared"):
                                        v0 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(32))
                                        v1 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused % T.int64(32) // T.int64(16) * T.int64(2) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) // T.int64(512))
                                        v2 = T.axis.spatial(T.int64(512), ci_0 * T.int64(32) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) % T.int64(512) // T.int64(16))
                                        v3 = T.axis.spatial(T.int64(16), (ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) % T.int64(16))
                                        T.reads(data_pack[v0, v1, v2, v3])
                                        T.writes(data_pack_shared[v0, v1, v2, v3])
                                        data_pack_shared[v0, v1, v2, v3] = data_pack[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(8)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("p1_shared"):
                                        v0 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(32))
                                        v1 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused % T.int64(32) // T.int64(16) * T.int64(2) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) // T.int64(1024))
                                        v2 = T.axis.spatial(T.int64(512), ci_0 * T.int64(32) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) % T.int64(1024) // T.int64(32))
                                        v3 = T.axis.spatial(T.int64(512), eps_0_nu_0_co_0_p_0_fused % T.int64(16) * T.int64(32) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) % T.int64(32))
                                        T.reads(p1[v0, v1, v2, v3])
                                        T.writes(p1_shared[v0, v1, v2, v3])
                                        p1_shared[v0, v1, v2, v3] = p1[v0, v1, v2, v3]
                        for ci_1, eps_3, nu_3, co_3, p_3, ci_2, eps_4, nu_4, co_4, p_4 in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(1), T.int64(32), T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                            with T.block("bgemm_update"):
                                v_eps = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(32) + eps_3 + eps_4)
                                v_nu = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused % T.int64(32) // T.int64(16) * T.int64(2) + eps_2_nu_2_co_2_p_2_fused // T.int64(64) + nu_3 + nu_4)
                                v_co = T.axis.spatial(T.int64(512), eps_0_nu_0_co_0_p_0_fused % T.int64(16) * T.int64(32) + eps_1_nu_1_co_1_p_1_fused // T.int64(2) * T.int64(16) + eps_2_nu_2_co_2_p_2_fused % T.int64(64) // T.int64(8) * T.int64(2) + co_3 + co_4)
                                v_p = T.axis.spatial(T.int64(16), eps_1_nu_1_co_1_p_1_fused % T.int64(2) * T.int64(8) + eps_2_nu_2_co_2_p_2_fused % T.int64(8) + p_3 + p_4)
                                v_ci = T.axis.reduce(T.int64(512), ci_0 * T.int64(32) + ci_1 * T.int64(32) + ci_2)
                                T.reads(bgemm_local[v_eps, v_nu, v_co, v_p], data_pack_shared[v_eps, v_nu, v_ci, v_p], p1_shared[v_eps, v_nu, v_ci, v_co])
                                T.writes(bgemm_local[v_eps, v_nu, v_co, v_p])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                bgemm_local[v_eps, v_nu, v_co, v_p] = bgemm_local[v_eps, v_nu, v_co, v_p] + data_pack_shared[v_eps, v_nu, v_ci, v_p] * p1_shared[v_eps, v_nu, v_ci, v_co]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(1), T.int64(2), T.int64(1)):
                        with T.block("bgemm_local"):
                            v0 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(32) + ax0)
                            v1 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused % T.int64(32) // T.int64(16) * T.int64(2) + eps_2_nu_2_co_2_p_2_fused // T.int64(64) + ax1)
                            v2 = T.axis.spatial(T.int64(512), eps_0_nu_0_co_0_p_0_fused % T.int64(16) * T.int64(32) + eps_1_nu_1_co_1_p_1_fused // T.int64(2) * T.int64(16) + eps_2_nu_2_co_2_p_2_fused % T.int64(64) // T.int64(8) * T.int64(2) + ax2)
                            v3 = T.axis.spatial(T.int64(16), eps_1_nu_1_co_1_p_1_fused % T.int64(2) * T.int64(8) + eps_2_nu_2_co_2_p_2_fused % T.int64(8) + ax3)
                            T.reads(bgemm_local[v0, v1, v2, v3])
                            T.writes(bgemm[v0, v1, v2, v3])
                            bgemm[v0, v1, v2, v3] = bgemm_local[v0, v1, v2, v3]
        for n_co_h_0_w_0_fused_0 in T.thread_binding(T.int64(256), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 1024, "pragma_unroll_explicit": 1}):
            for n_co_h_0_w_0_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                    for ax2 in T.unroll(T.int64(2)):
                        for ax3 in T.unroll(T.int64(2)):
                            with T.block("inverse_init"):
                                v_co = T.axis.spatial(T.int64(512), (n_co_h_0_w_0_fused_0 * T.int64(32) + n_co_h_0_w_0_fused_1) // T.int64(16) + ax0)
                                v_p = T.axis.spatial(T.int64(16), (n_co_h_0_w_0_fused_0 * T.int64(32) + n_co_h_0_w_0_fused_1) % T.int64(16) + ax1)
                                v_vh, v_vw = T.axis.remap("SS", [ax2, ax3])
                                T.reads()
                                T.writes(inverse_local[v_co, v_p, v_vh, v_vw])
                                T.block_attr({"schedule_rule": "conv2d_nchw_winograd_inverse"})
                                inverse_local[v_co, v_p, v_vh, v_vw] = T.float32(0)
                            for ax4 in T.unroll(T.int64(4)):
                                for ax5 in T.unroll(T.int64(4)):
                                    with T.block("inverse_update"):
                                        v_co = T.axis.spatial(T.int64(512), (n_co_h_0_w_0_fused_0 * T.int64(32) + n_co_h_0_w_0_fused_1) // T.int64(16) + ax0)
                                        v_p = T.axis.spatial(T.int64(16), (n_co_h_0_w_0_fused_0 * T.int64(32) + n_co_h_0_w_0_fused_1) % T.int64(16) + ax1)
                                        v_vh, v_vw, v_r_a, v_r_b = T.axis.remap("SSRR", [ax2, ax3, ax4, ax5])
                                        T.reads(inverse_local[v_co, v_p, v_vh, v_vw], bgemm[v_r_a, v_r_b, v_co, v_p])
                                        T.writes(inverse_local[v_co, v_p, v_vh, v_vw])
                                        T.block_attr({"schedule_rule": "conv2d_nchw_winograd_inverse"})
                                        inverse_local[v_co, v_p, v_vh, v_vw] = inverse_local[v_co, v_p, v_vh, v_vw] + bgemm[v_r_a, v_r_b, v_co, v_p] * T.Select(v_r_a % T.int64(4) == T.int64(3) and v_vh % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_vh % T.int64(2) == T.int64(0), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_vh % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_vh % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_vh % T.int64(2) == T.int64(1), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_vh % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_vh % T.int64(2) == T.int64(1), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_vh % T.int64(2) == T.int64(0), T.float32(1), T.float32(0))))))))) * T.Select(v_r_b % T.int64(4) == T.int64(3) and v_vw % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_vw % T.int64(2) == T.int64(0), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_vw % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_vw % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_vw % T.int64(2) == T.int64(1), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_vw % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_vw % T.int64(2) == T.int64(1), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_vw % T.int64(2) == T.int64(0), T.float32(1), T.float32(0)))))))))
                for h_1, w_1 in T.grid(T.int64(2), T.int64(2)):
                    with T.block("conv2d_winograd"):
                        v_n = T.axis.spatial(T.int64(1), T.int64(0))
                        v_co = T.axis.spatial(T.int64(512), (n_co_h_0_w_0_fused_0 * T.int64(32) + n_co_h_0_w_0_fused_1) // T.int64(16))
                        v_h = T.axis.spatial(T.int64(7), (n_co_h_0_w_0_fused_0 * T.int64(32) + n_co_h_0_w_0_fused_1) % T.int64(16) // T.int64(4) * T.int64(2) + h_1)
                        v_w = T.axis.spatial(T.int64(7), (n_co_h_0_w_0_fused_0 * T.int64(32) + n_co_h_0_w_0_fused_1) % T.int64(4) * T.int64(2) + w_1)
                        T.where((n_co_h_0_w_0_fused_0 * T.int64(32) + n_co_h_0_w_0_fused_1) % T.int64(16) // T.int64(4) * T.int64(2) + h_1 < T.int64(7) and (n_co_h_0_w_0_fused_0 * T.int64(32) + n_co_h_0_w_0_fused_1) % T.int64(4) * T.int64(2) + w_1 < T.int64(7))
                        T.reads(inverse_local[v_co, v_n * T.int64(16) + v_h // T.int64(2) * T.int64(4) + v_w // T.int64(2), v_h % T.int64(2), v_w % T.int64(2)], p2[v_n, v_co, v_h, v_w], p3[v_n, v_co, T.int64(0), T.int64(0)], p4[v_n, v_co, T.int64(0), T.int64(0)])
                        T.writes(T_relu[v_n, v_co, v_h, v_w])
                        T_relu[v_n, v_co, v_h, v_w] = T.max((inverse_local[v_co, v_n * T.int64(16) + v_h // T.int64(2) * T.int64(4) + v_w // T.int64(2), v_h % T.int64(2), v_w % T.int64(2)] + p2[v_n, v_co, v_h, v_w]) * p3[v_n, v_co, T.int64(0), T.int64(0)] + p4[v_n, v_co, T.int64(0), T.int64(0)], T.float32(0))
b0 = sch.get_block(name="data_pack", func_name="main")
b1 = sch.get_block(name="bgemm", func_name="main")
b2 = sch.get_block(name="inverse", func_name="main")
b3 = sch.get_block(name="conv2d_winograd", func_name="main")
b4 = sch.get_block(name="T_add", func_name="main")
b5 = sch.get_block(name="T_multiply", func_name="main")
b6 = sch.get_block(name="T_add_1", func_name="main")
b7 = sch.get_block(name="T_relu", func_name="main")
b8 = sch.get_block(name="root", func_name="main")
b9, b10 = sch.get_producers(block=b2)
sch.compute_inline(block=b10)
b11, = sch.get_consumers(block=b2)
l12, l13, l14, l15 = sch.get_loops(block=b11)
l16, l17 = sch.split(loop=l14, factors=[None, 2], preserve_unit_iters=True)
l18, l19 = sch.split(loop=l15, factors=[None, 2], preserve_unit_iters=True)
sch.reorder(l16, l18, l17, l19)
sch.compute_at(block=b2, loop=l18, preserve_unit_loops=True, index=-1)
sch.set_scope(block=b2, buffer_index=0, storage_scope="local")
l20, l21, l22, l23, l24, l25, l26, l27, l28, l29 = sch.get_loops(block=b2)
sch.unroll(loop=l26)
sch.unroll(loop=l27)
sch.unroll(loop=l28)
sch.unroll(loop=l29)
b30, b31 = sch.get_producers(block=b0)
sch.compute_inline(block=b31)
b32, = sch.get_producers(block=b30)
l33, l34, l35, l36, l37, l38 = sch.get_loops(block=b0)
sch.reorder(l35, l36, l33, l34, l37, l38)
sch.unroll(loop=l33)
sch.unroll(loop=l34)
sch.unroll(loop=l37)
sch.unroll(loop=l38)
l39 = sch.fuse(l35, l36, preserve_unit_iters=True)
v40 = sch.sample_categorical(candidates=[32, 64, 128, 256, 512, 1024], probs=[0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666], decision=2)
l41, l42 = sch.split(loop=l39, factors=[None, v40], preserve_unit_iters=True)
sch.bind(loop=l41, thread_axis="blockIdx.x")
sch.bind(loop=l42, thread_axis="threadIdx.x")
b43 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b43, loop=l42, preserve_unit_loops=True, index=-1)
sch.compute_at(block=b30, loop=l42, preserve_unit_loops=True, index=-1)
sch.set_scope(block=b30, buffer_index=0, storage_scope="local")
sch.compute_inline(block=b32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l44, l45, l46, l47, l48 = sch.get_loops(block=b1)
v49, v50, v51, v52, v53 = sch.sample_perfect_tile(loop=l44, n=5, max_innermost_factor=64, decision=[4, 1, 1, 1, 1])
l54, l55, l56, l57, l58 = sch.split(loop=l44, factors=[v49, v50, v51, v52, v53], preserve_unit_iters=True)
v59, v60, v61, v62, v63 = sch.sample_perfect_tile(loop=l45, n=5, max_innermost_factor=64, decision=[2, 1, 2, 1, 1])
l64, l65, l66, l67, l68 = sch.split(loop=l45, factors=[v59, v60, v61, v62, v63], preserve_unit_iters=True)
v69, v70, v71, v72, v73 = sch.sample_perfect_tile(loop=l46, n=5, max_innermost_factor=64, decision=[16, 2, 8, 2, 1])
l74, l75, l76, l77, l78 = sch.split(loop=l46, factors=[v69, v70, v71, v72, v73], preserve_unit_iters=True)
v79, v80, v81, v82, v83 = sch.sample_perfect_tile(loop=l47, n=5, max_innermost_factor=64, decision=[1, 2, 8, 1, 1])
l84, l85, l86, l87, l88 = sch.split(loop=l47, factors=[v79, v80, v81, v82, v83], preserve_unit_iters=True)
v89, v90, v91 = sch.sample_perfect_tile(loop=l48, n=3, max_innermost_factor=64, decision=[16, 1, 32])
l92, l93, l94 = sch.split(loop=l48, factors=[v89, v90, v91], preserve_unit_iters=True)
sch.reorder(l54, l64, l74, l84, l55, l65, l75, l85, l56, l66, l76, l86, l92, l93, l57, l67, l77, l87, l94, l58, l68, l78, l88)
l95 = sch.fuse(l54, l64, l74, l84, preserve_unit_iters=True)
sch.bind(loop=l95, thread_axis="blockIdx.x")
l96 = sch.fuse(l55, l65, l75, l85, preserve_unit_iters=True)
sch.bind(loop=l96, thread_axis="vthread.x")
l97 = sch.fuse(l56, l66, l76, l86, preserve_unit_iters=True)
sch.bind(loop=l97, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b98 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b98, loop=l97, preserve_unit_loops=True, index=-1)
b99 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b99, loop=l92, preserve_unit_loops=True, index=-1)
l100, l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b99)
l108 = sch.fuse(l104, l105, l106, l107, preserve_unit_iters=True)
v109 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b99, ann_key="meta_schedule.cooperative_fetch", ann_val=v109)
b110 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b110, loop=l92, preserve_unit_loops=True, index=-1)
l111, l112, l113, l114, l115, l116, l117, l118 = sch.get_loops(block=b110)
l119 = sch.fuse(l115, l116, l117, l118, preserve_unit_iters=True)
v120 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b110, ann_key="meta_schedule.cooperative_fetch", ann_val=v120)
sch.reverse_compute_inline(block=b7)
sch.reverse_compute_inline(block=b6)
sch.reverse_compute_inline(block=b5)
sch.reverse_compute_inline(block=b4)
v121 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b8, ann_key="meta_schedule.unroll_explicit", ann_val=v121)
l122, l123, l124, l125, l126, l127 = sch.get_loops(block=b3)
l128 = sch.fuse(l122, l123, l124, l125, preserve_unit_iters=True)
v129 = sch.sample_categorical(candidates=[32, 64, 128, 256, 512, 1024], probs=[0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666], decision=0)
l130, l131 = sch.split(loop=l128, factors=[None, v129], preserve_unit_iters=True)
sch.bind(loop=l130, thread_axis="blockIdx.x")
sch.bind(loop=l131, thread_axis="threadIdx.x")
sch.enter_postproc()
sch.unannotate(block_or_loop=b99, ann_key="meta_schedule.cooperative_fetch")
l132, l133, l134, l135, l136 = sch.get_loops(block=b99)
l137, l138, l139 = sch.split(loop=l136, factors=[None, 128, 2], preserve_unit_iters=True)
sch.vectorize(loop=l139)
sch.bind(loop=l138, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b110, ann_key="meta_schedule.cooperative_fetch")
l140, l141, l142, l143, l144 = sch.get_loops(block=b110)
l145, l146, l147 = sch.split(loop=l144, factors=[None, 128, 2], preserve_unit_iters=True)
sch.vectorize(loop=l147)
sch.bind(loop=l146, thread_axis="threadIdx.x")
b148 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b148, ann_key="meta_schedule.unroll_explicit")
b149, b150, b151, b152, b153, b154, b155, b156, b157 = sch.get_child_blocks(b148)
l158, l159, l160, l161, l162, l163 = sch.get_loops(block=b149)
l164, l165, l166, l167, l168, l169 = sch.get_loops(block=b150)
sch.annotate(block_or_loop=l164, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l164, ann_key="pragma_unroll_explicit", ann_val=1)
l170, l171, l172, l173, l174, l175 = sch.get_loops(block=b151)
l176, l177, l178, l179, l180, l181, l182 = sch.get_loops(block=b152)
l183, l184, l185, l186, l187, l188, l189 = sch.get_loops(block=b153)
l190, l191, l192, l193, l194, l195, l196, l197, l198, l199, l200, l201, l202, l203 = sch.get_loops(block=b154)
sch.annotate(block_or_loop=l190, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l190, ann_key="pragma_unroll_explicit", ann_val=1)
l204, l205, l206, l207, l208, l209, l210 = sch.get_loops(block=b155)
l211, l212, l213, l214, l215, l216, l217, l218 = sch.get_loops(block=b156)
sch.annotate(block_or_loop=l211, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l211, ann_key="pragma_unroll_explicit", ann_val=1)
l219, l220, l221, l222 = sch.get_loops(block=b157)
b223 = sch.get_block(name="data_pack", func_name="main")
l224, l225, l226, l227, l228, l229 = sch.get_loops(block=b223)
b230 = sch.decompose_reduction(block=b223, loop=l228)
b231 = sch.get_block(name="bgemm", func_name="main")
l232, l233, l234, l235, l236, l237, l238, l239, l240, l241, l242, l243, l244, l245 = sch.get_loops(block=b231)
b246 = sch.decompose_reduction(block=b231, loop=l235)
b247 = sch.get_block(name="inverse", func_name="main")
l248, l249, l250, l251, l252, l253, l254, l255 = sch.get_loops(block=b247)
b256 = sch.decompose_reduction(block=b247, loop=l254)
2023-11-11 02:29:36 [INFO] [task_scheduler.cc:121] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #199: Error in running:
LocalRunner: An exception occurred
Subprocess terminated
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(512), T.int64(7), T.int64(7)), "float32"), p1: T.Buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(512)), "float32"), p2: T.Buffer((T.int64(1), T.int64(512), T.int64(7), T.int64(7)), "float32"), p3: T.Buffer((T.int64(1), T.int64(512), T.int64(1), T.int64(1)), "float32"), p4: T.Buffer((T.int64(1), T.int64(512), T.int64(1), T.int64(1)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(512), T.int64(7), T.int64(7)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        input_tile_local = T.alloc_buffer((T.int64(512), T.int64(16), T.int64(4), T.int64(4)), scope="local")
        data_pack = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(16)))
        bgemm = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(16)))
        inverse_local = T.alloc_buffer((T.int64(512), T.int64(16), T.int64(2), T.int64(2)), scope="local")
        data_pack_local = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(16)), scope="local")
        bgemm_local = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(16)), scope="local")
        data_pack_shared = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(16)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(512)), scope="shared")
        for ci_p_fused_0 in T.thread_binding(T.int64(64), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 1024, "pragma_unroll_explicit": 1}):
            for ci_p_fused_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(1), T.int64(4), T.int64(4)):
                    with T.block("input_tile"):
                        v_ci = T.axis.spatial(T.int64(512), ci_p_fused_0 * T.int64(8) + ci_p_fused_1 // T.int64(16) + ax0)
                        v_p = T.axis.spatial(T.int64(16), ci_p_fused_1 % T.int64(16) + ax1)
                        v_eps, v_nu = T.axis.remap("SS", [ax2, ax3])
                        T.reads(p0[v_p // T.int64(16), v_ci, v_p % T.int64(16) // T.int64(4) * T.int64(2) + v_eps - T.int64(1), v_p % T.int64(4) * T.int64(2) + v_nu - T.int64(1)])
                        T.writes(input_tile_local[v_ci, v_p, v_eps, v_nu])
                        T.block_attr({"schedule_rule": "None"})
                        input_tile_local[v_ci, v_p, v_eps, v_nu] = T.if_then_else(T.int64(1) <= v_p % T.int64(16) // T.int64(4) * T.int64(2) + v_eps and v_p % T.int64(16) // T.int64(4) * T.int64(2) + v_eps < T.int64(8) and T.int64(1) <= v_p % T.int64(4) * T.int64(2) + v_nu and v_p % T.int64(4) * T.int64(2) + v_nu < T.int64(8), p0[v_p // T.int64(16), v_ci, v_p % T.int64(16) // T.int64(4) * T.int64(2) + v_eps - T.int64(1), v_p % T.int64(4) * T.int64(2) + v_nu - T.int64(1)], T.float32(0))
                for eps in T.unroll(T.int64(4)):
                    for nu in T.unroll(T.int64(4)):
                        with T.block("data_pack_init"):
                            v_eps, v_nu = T.axis.remap("SS", [eps, nu])
                            v_ci = T.axis.spatial(T.int64(512), (ci_p_fused_0 * T.int64(128) + ci_p_fused_1) // T.int64(16))
                            v_p = T.axis.spatial(T.int64(16), (ci_p_fused_0 * T.int64(128) + ci_p_fused_1) % T.int64(16))
                            T.reads()
                            T.writes(data_pack_local[v_eps, v_nu, v_ci, v_p])
                            T.block_attr({"schedule_rule": "conv2d_nchw_winograd_data_pack"})
                            data_pack_local[v_eps, v_nu, v_ci, v_p] = T.float32(0)
                        for r_a in T.unroll(T.int64(4)):
                            for r_b in T.unroll(T.int64(4)):
                                with T.block("data_pack_update"):
                                    v_eps, v_nu = T.axis.remap("SS", [eps, nu])
                                    v_ci = T.axis.spatial(T.int64(512), (ci_p_fused_0 * T.int64(128) + ci_p_fused_1) // T.int64(16))
                                    v_p = T.axis.spatial(T.int64(16), (ci_p_fused_0 * T.int64(128) + ci_p_fused_1) % T.int64(16))
                                    v_r_a, v_r_b = T.axis.remap("RR", [r_a, r_b])
                                    T.reads(data_pack_local[v_eps, v_nu, v_ci, v_p], input_tile_local[v_ci, v_p, v_r_a, v_r_b])
                                    T.writes(data_pack_local[v_eps, v_nu, v_ci, v_p])
                                    T.block_attr({"schedule_rule": "conv2d_nchw_winograd_data_pack"})
                                    data_pack_local[v_eps, v_nu, v_ci, v_p] = data_pack_local[v_eps, v_nu, v_ci, v_p] + input_tile_local[v_ci, v_p, v_r_a, v_r_b] * T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(3), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(1), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(0), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(3), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(1), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(0), T.float32(1), T.float32(0))))))))))))))))) * T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(3), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(1), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(0), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(3), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(1), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(0), T.float32(1), T.float32(0)))))))))))))))))
                for ax0, ax1, ax2, ax3 in T.grid(T.int64(4), T.int64(4), T.int64(1), T.int64(1)):
                    with T.block("data_pack_local"):
                        v0, v1 = T.axis.remap("SS", [ax0, ax1])
                        v2 = T.axis.spatial(T.int64(512), ci_p_fused_0 * T.int64(8) + ci_p_fused_1 // T.int64(16) + ax2)
                        v3 = T.axis.spatial(T.int64(16), ci_p_fused_1 % T.int64(16) + ax3)
                        T.reads(data_pack_local[v0, v1, v2, v3])
                        T.writes(data_pack[v0, v1, v2, v3])
                        data_pack[v0, v1, v2, v3] = data_pack_local[v0, v1, v2, v3]
        for eps_0_nu_0_co_0_p_0_fused in T.thread_binding(T.int64(256), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 1024, "pragma_unroll_explicit": 1}):
            for eps_1_nu_1_co_1_p_1_fused in T.thread_binding(T.int64(2), thread="vthread.x"):
                for eps_2_nu_2_co_2_p_2_fused in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                    for eps_3_init, nu_3_init, co_3_init, p_3_init, eps_4_init, nu_4_init, co_4_init, p_4_init in T.grid(T.int64(1), T.int64(1), T.int64(2), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("bgemm_init"):
                            v_eps = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(64) + eps_3_init + eps_4_init)
                            v_nu = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused % T.int64(64) // T.int64(32) * T.int64(2) + eps_2_nu_2_co_2_p_2_fused // T.int64(64) + nu_3_init + nu_4_init)
                            v_co = T.axis.spatial(T.int64(512), eps_0_nu_0_co_0_p_0_fused % T.int64(32) // T.int64(2) * T.int64(32) + eps_1_nu_1_co_1_p_1_fused * T.int64(16) + eps_2_nu_2_co_2_p_2_fused % T.int64(64) // T.int64(8) * T.int64(2) + co_3_init + co_4_init)
                            v_p = T.axis.spatial(T.int64(16), eps_0_nu_0_co_0_p_0_fused % T.int64(2) * T.int64(8) + eps_2_nu_2_co_2_p_2_fused % T.int64(8) + p_3_init + p_4_init)
                            T.reads()
                            T.writes(bgemm_local[v_eps, v_nu, v_co, v_p])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            bgemm_local[v_eps, v_nu, v_co, v_p] = T.float32(0)
                    for ci_0 in range(T.int64(8)):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(8)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                                with T.block("data_pack_shared"):
                                    v0 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(64))
                                    v1 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused % T.int64(64) // T.int64(32) * T.int64(2) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1) // T.int64(512))
                                    v2 = T.axis.spatial(T.int64(512), ci_0 * T.int64(64) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1) % T.int64(512) // T.int64(8))
                                    v3 = T.axis.spatial(T.int64(16), eps_0_nu_0_co_0_p_0_fused % T.int64(2) * T.int64(8) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1) % T.int64(8))
                                    T.reads(data_pack[v0, v1, v2, v3])
                                    T.writes(data_pack_shared[v0, v1, v2, v3])
                                    data_pack_shared[v0, v1, v2, v3] = data_pack[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(32)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                                with T.block("p1_shared"):
                                    v0 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(64))
                                    v1 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused % T.int64(64) // T.int64(32) * T.int64(2) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1) // T.int64(2048))
                                    v2 = T.axis.spatial(T.int64(512), ci_0 * T.int64(64) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1) % T.int64(2048) // T.int64(32))
                                    v3 = T.axis.spatial(T.int64(512), eps_0_nu_0_co_0_p_0_fused % T.int64(32) // T.int64(2) * T.int64(32) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1) % T.int64(32))
                                    T.reads(p1[v0, v1, v2, v3])
                                    T.writes(p1_shared[v0, v1, v2, v3])
                                    p1_shared[v0, v1, v2, v3] = p1[v0, v1, v2, v3]
                        for ci_1, eps_3, nu_3, co_3, p_3, ci_2, eps_4, nu_4, co_4, p_4 in T.grid(T.int64(4), T.int64(1), T.int64(1), T.int64(2), T.int64(1), T.int64(16), T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                            with T.block("bgemm_update"):
                                v_eps = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(64) + eps_3 + eps_4)
                                v_nu = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused % T.int64(64) // T.int64(32) * T.int64(2) + eps_2_nu_2_co_2_p_2_fused // T.int64(64) + nu_3 + nu_4)
                                v_co = T.axis.spatial(T.int64(512), eps_0_nu_0_co_0_p_0_fused % T.int64(32) // T.int64(2) * T.int64(32) + eps_1_nu_1_co_1_p_1_fused * T.int64(16) + eps_2_nu_2_co_2_p_2_fused % T.int64(64) // T.int64(8) * T.int64(2) + co_3 + co_4)
                                v_p = T.axis.spatial(T.int64(16), eps_0_nu_0_co_0_p_0_fused % T.int64(2) * T.int64(8) + eps_2_nu_2_co_2_p_2_fused % T.int64(8) + p_3 + p_4)
                                v_ci = T.axis.reduce(T.int64(512), ci_0 * T.int64(64) + ci_1 * T.int64(16) + ci_2)
                                T.reads(bgemm_local[v_eps, v_nu, v_co, v_p], data_pack_shared[v_eps, v_nu, v_ci, v_p], p1_shared[v_eps, v_nu, v_ci, v_co])
                                T.writes(bgemm_local[v_eps, v_nu, v_co, v_p])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                bgemm_local[v_eps, v_nu, v_co, v_p] = bgemm_local[v_eps, v_nu, v_co, v_p] + data_pack_shared[v_eps, v_nu, v_ci, v_p] * p1_shared[v_eps, v_nu, v_ci, v_co]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(1), T.int64(2), T.int64(1)):
                        with T.block("bgemm_local"):
                            v0 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(64) + ax0)
                            v1 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused % T.int64(64) // T.int64(32) * T.int64(2) + eps_2_nu_2_co_2_p_2_fused // T.int64(64) + ax1)
                            v2 = T.axis.spatial(T.int64(512), eps_0_nu_0_co_0_p_0_fused % T.int64(32) // T.int64(2) * T.int64(32) + eps_1_nu_1_co_1_p_1_fused * T.int64(16) + eps_2_nu_2_co_2_p_2_fused % T.int64(64) // T.int64(8) * T.int64(2) + ax2)
                            v3 = T.axis.spatial(T.int64(16), eps_0_nu_0_co_0_p_0_fused % T.int64(2) * T.int64(8) + eps_2_nu_2_co_2_p_2_fused % T.int64(8) + ax3)
                            T.reads(bgemm_local[v0, v1, v2, v3])
                            T.writes(bgemm[v0, v1, v2, v3])
                            bgemm[v0, v1, v2, v3] = bgemm_local[v0, v1, v2, v3]
        for n_co_h_0_w_0_fused_0 in T.thread_binding(T.int64(256), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 1024, "pragma_unroll_explicit": 1}):
            for n_co_h_0_w_0_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                    for ax2 in T.unroll(T.int64(2)):
                        for ax3 in T.unroll(T.int64(2)):
                            with T.block("inverse_init"):
                                v_co = T.axis.spatial(T.int64(512), (n_co_h_0_w_0_fused_0 * T.int64(32) + n_co_h_0_w_0_fused_1) // T.int64(16) + ax0)
                                v_p = T.axis.spatial(T.int64(16), (n_co_h_0_w_0_fused_0 * T.int64(32) + n_co_h_0_w_0_fused_1) % T.int64(16) + ax1)
                                v_vh, v_vw = T.axis.remap("SS", [ax2, ax3])
                                T.reads()
                                T.writes(inverse_local[v_co, v_p, v_vh, v_vw])
                                T.block_attr({"schedule_rule": "conv2d_nchw_winograd_inverse"})
                                inverse_local[v_co, v_p, v_vh, v_vw] = T.float32(0)
                            for ax4 in T.unroll(T.int64(4)):
                                for ax5 in T.unroll(T.int64(4)):
                                    with T.block("inverse_update"):
                                        v_co = T.axis.spatial(T.int64(512), (n_co_h_0_w_0_fused_0 * T.int64(32) + n_co_h_0_w_0_fused_1) // T.int64(16) + ax0)
                                        v_p = T.axis.spatial(T.int64(16), (n_co_h_0_w_0_fused_0 * T.int64(32) + n_co_h_0_w_0_fused_1) % T.int64(16) + ax1)
                                        v_vh, v_vw, v_r_a, v_r_b = T.axis.remap("SSRR", [ax2, ax3, ax4, ax5])
                                        T.reads(inverse_local[v_co, v_p, v_vh, v_vw], bgemm[v_r_a, v_r_b, v_co, v_p])
                                        T.writes(inverse_local[v_co, v_p, v_vh, v_vw])
                                        T.block_attr({"schedule_rule": "conv2d_nchw_winograd_inverse"})
                                        inverse_local[v_co, v_p, v_vh, v_vw] = inverse_local[v_co, v_p, v_vh, v_vw] + bgemm[v_r_a, v_r_b, v_co, v_p] * T.Select(v_r_a % T.int64(4) == T.int64(3) and v_vh % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_vh % T.int64(2) == T.int64(0), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_vh % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_vh % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_vh % T.int64(2) == T.int64(1), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_vh % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_vh % T.int64(2) == T.int64(1), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_vh % T.int64(2) == T.int64(0), T.float32(1), T.float32(0))))))))) * T.Select(v_r_b % T.int64(4) == T.int64(3) and v_vw % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_vw % T.int64(2) == T.int64(0), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_vw % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_vw % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_vw % T.int64(2) == T.int64(1), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_vw % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_vw % T.int64(2) == T.int64(1), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_vw % T.int64(2) == T.int64(0), T.float32(1), T.float32(0)))))))))
                for h_1, w_1 in T.grid(T.int64(2), T.int64(2)):
                    with T.block("conv2d_winograd"):
                        v_n = T.axis.spatial(T.int64(1), T.int64(0))
                        v_co = T.axis.spatial(T.int64(512), (n_co_h_0_w_0_fused_0 * T.int64(32) + n_co_h_0_w_0_fused_1) // T.int64(16))
                        v_h = T.axis.spatial(T.int64(7), (n_co_h_0_w_0_fused_0 * T.int64(32) + n_co_h_0_w_0_fused_1) % T.int64(16) // T.int64(4) * T.int64(2) + h_1)
                        v_w = T.axis.spatial(T.int64(7), (n_co_h_0_w_0_fused_0 * T.int64(32) + n_co_h_0_w_0_fused_1) % T.int64(4) * T.int64(2) + w_1)
                        T.where((n_co_h_0_w_0_fused_0 * T.int64(32) + n_co_h_0_w_0_fused_1) % T.int64(16) // T.int64(4) * T.int64(2) + h_1 < T.int64(7) and (n_co_h_0_w_0_fused_0 * T.int64(32) + n_co_h_0_w_0_fused_1) % T.int64(4) * T.int64(2) + w_1 < T.int64(7))
                        T.reads(inverse_local[v_co, v_n * T.int64(16) + v_h // T.int64(2) * T.int64(4) + v_w // T.int64(2), v_h % T.int64(2), v_w % T.int64(2)], p2[v_n, v_co, v_h, v_w], p3[v_n, v_co, T.int64(0), T.int64(0)], p4[v_n, v_co, T.int64(0), T.int64(0)])
                        T.writes(T_relu[v_n, v_co, v_h, v_w])
                        T_relu[v_n, v_co, v_h, v_w] = T.max((inverse_local[v_co, v_n * T.int64(16) + v_h // T.int64(2) * T.int64(4) + v_w // T.int64(2), v_h % T.int64(2), v_w % T.int64(2)] + p2[v_n, v_co, v_h, v_w]) * p3[v_n, v_co, T.int64(0), T.int64(0)] + p4[v_n, v_co, T.int64(0), T.int64(0)], T.float32(0))
b0 = sch.get_block(name="data_pack", func_name="main")
b1 = sch.get_block(name="bgemm", func_name="main")
b2 = sch.get_block(name="inverse", func_name="main")
b3 = sch.get_block(name="conv2d_winograd", func_name="main")
b4 = sch.get_block(name="T_add", func_name="main")
b5 = sch.get_block(name="T_multiply", func_name="main")
b6 = sch.get_block(name="T_add_1", func_name="main")
b7 = sch.get_block(name="T_relu", func_name="main")
b8 = sch.get_block(name="root", func_name="main")
b9, b10 = sch.get_producers(block=b2)
sch.compute_inline(block=b10)
b11, = sch.get_consumers(block=b2)
l12, l13, l14, l15 = sch.get_loops(block=b11)
l16, l17 = sch.split(loop=l14, factors=[None, 2], preserve_unit_iters=True)
l18, l19 = sch.split(loop=l15, factors=[None, 2], preserve_unit_iters=True)
sch.reorder(l16, l18, l17, l19)
sch.compute_at(block=b2, loop=l18, preserve_unit_loops=True, index=-1)
sch.set_scope(block=b2, buffer_index=0, storage_scope="local")
l20, l21, l22, l23, l24, l25, l26, l27, l28, l29 = sch.get_loops(block=b2)
sch.unroll(loop=l26)
sch.unroll(loop=l27)
sch.unroll(loop=l28)
sch.unroll(loop=l29)
b30, b31 = sch.get_producers(block=b0)
sch.compute_inline(block=b31)
b32, = sch.get_producers(block=b30)
l33, l34, l35, l36, l37, l38 = sch.get_loops(block=b0)
sch.reorder(l35, l36, l33, l34, l37, l38)
sch.unroll(loop=l33)
sch.unroll(loop=l34)
sch.unroll(loop=l37)
sch.unroll(loop=l38)
l39 = sch.fuse(l35, l36, preserve_unit_iters=True)
v40 = sch.sample_categorical(candidates=[32, 64, 128, 256, 512, 1024], probs=[0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666], decision=2)
l41, l42 = sch.split(loop=l39, factors=[None, v40], preserve_unit_iters=True)
sch.bind(loop=l41, thread_axis="blockIdx.x")
sch.bind(loop=l42, thread_axis="threadIdx.x")
b43 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b43, loop=l42, preserve_unit_loops=True, index=-1)
sch.compute_at(block=b30, loop=l42, preserve_unit_loops=True, index=-1)
sch.set_scope(block=b30, buffer_index=0, storage_scope="local")
sch.compute_inline(block=b32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l44, l45, l46, l47, l48 = sch.get_loops(block=b1)
v49, v50, v51, v52, v53 = sch.sample_perfect_tile(loop=l44, n=5, max_innermost_factor=64, decision=[4, 1, 1, 1, 1])
l54, l55, l56, l57, l58 = sch.split(loop=l44, factors=[v49, v50, v51, v52, v53], preserve_unit_iters=True)
v59, v60, v61, v62, v63 = sch.sample_perfect_tile(loop=l45, n=5, max_innermost_factor=64, decision=[2, 1, 2, 1, 1])
l64, l65, l66, l67, l68 = sch.split(loop=l45, factors=[v59, v60, v61, v62, v63], preserve_unit_iters=True)
v69, v70, v71, v72, v73 = sch.sample_perfect_tile(loop=l46, n=5, max_innermost_factor=64, decision=[16, 2, 8, 2, 1])
l74, l75, l76, l77, l78 = sch.split(loop=l46, factors=[v69, v70, v71, v72, v73], preserve_unit_iters=True)
v79, v80, v81, v82, v83 = sch.sample_perfect_tile(loop=l47, n=5, max_innermost_factor=64, decision=[2, 1, 8, 1, 1])
l84, l85, l86, l87, l88 = sch.split(loop=l47, factors=[v79, v80, v81, v82, v83], preserve_unit_iters=True)
v89, v90, v91 = sch.sample_perfect_tile(loop=l48, n=3, max_innermost_factor=64, decision=[8, 4, 16])
l92, l93, l94 = sch.split(loop=l48, factors=[v89, v90, v91], preserve_unit_iters=True)
sch.reorder(l54, l64, l74, l84, l55, l65, l75, l85, l56, l66, l76, l86, l92, l93, l57, l67, l77, l87, l94, l58, l68, l78, l88)
l95 = sch.fuse(l54, l64, l74, l84, preserve_unit_iters=True)
sch.bind(loop=l95, thread_axis="blockIdx.x")
l96 = sch.fuse(l55, l65, l75, l85, preserve_unit_iters=True)
sch.bind(loop=l96, thread_axis="vthread.x")
l97 = sch.fuse(l56, l66, l76, l86, preserve_unit_iters=True)
sch.bind(loop=l97, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b98 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b98, loop=l97, preserve_unit_loops=True, index=-1)
b99 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b99, loop=l92, preserve_unit_loops=True, index=-1)
l100, l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b99)
l108 = sch.fuse(l104, l105, l106, l107, preserve_unit_iters=True)
v109 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b99, ann_key="meta_schedule.cooperative_fetch", ann_val=v109)
b110 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b110, loop=l92, preserve_unit_loops=True, index=-1)
l111, l112, l113, l114, l115, l116, l117, l118 = sch.get_loops(block=b110)
l119 = sch.fuse(l115, l116, l117, l118, preserve_unit_iters=True)
v120 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b110, ann_key="meta_schedule.cooperative_fetch", ann_val=v120)
sch.reverse_compute_inline(block=b7)
sch.reverse_compute_inline(block=b6)
sch.reverse_compute_inline(block=b5)
sch.reverse_compute_inline(block=b4)
v121 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b8, ann_key="meta_schedule.unroll_explicit", ann_val=v121)
l122, l123, l124, l125, l126, l127 = sch.get_loops(block=b3)
l128 = sch.fuse(l122, l123, l124, l125, preserve_unit_iters=True)
v129 = sch.sample_categorical(candidates=[32, 64, 128, 256, 512, 1024], probs=[0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666], decision=0)
l130, l131 = sch.split(loop=l128, factors=[None, v129], preserve_unit_iters=True)
sch.bind(loop=l130, thread_axis="blockIdx.x")
sch.bind(loop=l131, thread_axis="threadIdx.x")
sch.enter_postproc()
sch.unannotate(block_or_loop=b99, ann_key="meta_schedule.cooperative_fetch")
l132, l133, l134, l135, l136 = sch.get_loops(block=b99)
l137, l138 = sch.split(loop=l136, factors=[None, 128], preserve_unit_iters=True)
sch.bind(loop=l138, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b110, ann_key="meta_schedule.cooperative_fetch")
l139, l140, l141, l142, l143 = sch.get_loops(block=b110)
l144, l145 = sch.split(loop=l143, factors=[None, 128], preserve_unit_iters=True)
sch.bind(loop=l145, thread_axis="threadIdx.x")
b146 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b146, ann_key="meta_schedule.unroll_explicit")
b147, b148, b149, b150, b151, b152, b153, b154, b155 = sch.get_child_blocks(b146)
l156, l157, l158, l159, l160, l161 = sch.get_loops(block=b147)
l162, l163, l164, l165, l166, l167 = sch.get_loops(block=b148)
sch.annotate(block_or_loop=l162, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l162, ann_key="pragma_unroll_explicit", ann_val=1)
l168, l169, l170, l171, l172, l173 = sch.get_loops(block=b149)
l174, l175, l176, l177, l178, l179 = sch.get_loops(block=b150)
l180, l181, l182, l183, l184, l185 = sch.get_loops(block=b151)
l186, l187, l188, l189, l190, l191, l192, l193, l194, l195, l196, l197, l198, l199 = sch.get_loops(block=b152)
sch.annotate(block_or_loop=l186, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l186, ann_key="pragma_unroll_explicit", ann_val=1)
l200, l201, l202, l203, l204, l205, l206 = sch.get_loops(block=b153)
l207, l208, l209, l210, l211, l212, l213, l214 = sch.get_loops(block=b154)
sch.annotate(block_or_loop=l207, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l207, ann_key="pragma_unroll_explicit", ann_val=1)
l215, l216, l217, l218 = sch.get_loops(block=b155)
b219 = sch.get_block(name="data_pack", func_name="main")
l220, l221, l222, l223, l224, l225 = sch.get_loops(block=b219)
b226 = sch.decompose_reduction(block=b219, loop=l224)
b227 = sch.get_block(name="bgemm", func_name="main")
l228, l229, l230, l231, l232, l233, l234, l235, l236, l237, l238, l239, l240, l241 = sch.get_loops(block=b227)
b242 = sch.decompose_reduction(block=b227, loop=l231)
b243 = sch.get_block(name="inverse", func_name="main")
l244, l245, l246, l247, l248, l249, l250, l251 = sch.get_loops(block=b243)
b252 = sch.decompose_reduction(block=b243, loop=l250)
2023-11-11 02:29:36 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #200: GFLOPs: 4314.2047. Time: 32.9568 us. Best GFLOPs: 4314.2047
2023-11-11 02:29:36 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #201: GFLOPs: 4524.9699. Time: 31.4217 us. Best GFLOPs: 4524.9699
2023-11-11 02:29:36 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #202: GFLOPs: 4438.0987. Time: 32.0368 us. Best GFLOPs: 4524.9699
2023-11-11 02:29:36 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #203: GFLOPs: 4503.4196. Time: 31.5721 us. Best GFLOPs: 4524.9699
2023-11-11 02:29:36 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #204: GFLOPs: 4816.7146. Time: 29.5185 us. Best GFLOPs: 4816.7146
2023-11-11 02:29:36 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #205: GFLOPs: 3662.3929. Time: 38.8223 us. Best GFLOPs: 4816.7146
2023-11-11 02:29:36 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #206: GFLOPs: 4632.9572. Time: 30.6893 us. Best GFLOPs: 4816.7146
2023-11-11 02:29:36 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #207: GFLOPs: 4226.1329. Time: 33.6436 us. Best GFLOPs: 4816.7146
2023-11-11 02:29:36 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #208: GFLOPs: 4370.7571. Time: 32.5304 us. Best GFLOPs: 4816.7146
2023-11-11 02:29:36 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #209: GFLOPs: 4822.9229. Time: 29.4805 us. Best GFLOPs: 4822.9229
2023-11-11 02:29:36 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #210: GFLOPs: 4343.3717. Time: 32.7355 us. Best GFLOPs: 4822.9229
2023-11-11 02:29:36 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #211: GFLOPs: 4395.5172. Time: 32.3471 us. Best GFLOPs: 4822.9229
2023-11-11 02:29:36 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #212: GFLOPs: 4633.0189. Time: 30.6889 us. Best GFLOPs: 4822.9229
2023-11-11 02:29:36 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #213: GFLOPs: 4481.7837. Time: 31.7245 us. Best GFLOPs: 4822.9229
2023-11-11 02:29:36 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #214: GFLOPs: 3284.5037. Time: 43.2889 us. Best GFLOPs: 4822.9229
2023-11-11 02:29:36 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #215: GFLOPs: 4778.0512. Time: 29.7574 us. Best GFLOPs: 4822.9229
2023-11-11 02:29:36 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #216: GFLOPs: 3847.2385. Time: 36.9570 us. Best GFLOPs: 4822.9229
2023-11-11 02:29:36 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #217: GFLOPs: 3860.8048. Time: 36.8271 us. Best GFLOPs: 4822.9229
2023-11-11 02:29:36 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #218: GFLOPs: 4692.5513. Time: 30.2996 us. Best GFLOPs: 4822.9229
2023-11-11 02:29:36 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #219: GFLOPs: 4084.7369. Time: 34.8082 us. Best GFLOPs: 4822.9229
2023-11-11 02:29:36 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #220: GFLOPs: 4485.6944. Time: 31.6969 us. Best GFLOPs: 4822.9229
2023-11-11 02:29:36 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #221: GFLOPs: 4413.6565. Time: 32.2142 us. Best GFLOPs: 4822.9229
2023-11-11 02:29:36 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #222: GFLOPs: 4480.9941. Time: 31.7301 us. Best GFLOPs: 4822.9229
2023-11-11 02:29:36 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #223: GFLOPs: 4201.6286. Time: 33.8398 us. Best GFLOPs: 4822.9229
2023-11-11 02:29:36 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #224: GFLOPs: 3986.1813. Time: 35.6688 us. Best GFLOPs: 4822.9229
2023-11-11 02:29:36 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #225: GFLOPs: 2779.5819. Time: 51.1524 us. Best GFLOPs: 4822.9229
2023-11-11 02:29:36 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #226: GFLOPs: 4637.7748. Time: 30.6575 us. Best GFLOPs: 4822.9229
2023-11-11 02:29:36 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #227: GFLOPs: 4085.1276. Time: 34.8049 us. Best GFLOPs: 4822.9229
2023-11-11 02:29:36 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #228: GFLOPs: 3341.9180. Time: 42.5451 us. Best GFLOPs: 4822.9229
2023-11-11 02:29:36 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #229: GFLOPs: 4372.3116. Time: 32.5188 us. Best GFLOPs: 4822.9229
2023-11-11 02:29:36 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #230: GFLOPs: 4398.1840. Time: 32.3275 us. Best GFLOPs: 4822.9229
2023-11-11 02:29:36 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #231: GFLOPs: 4397.6895. Time: 32.3312 us. Best GFLOPs: 4822.9229
2023-11-11 02:29:36 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #232: GFLOPs: 4317.7290. Time: 32.9299 us. Best GFLOPs: 4822.9229
2023-11-11 02:29:36 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #233: GFLOPs: 3056.8268. Time: 46.5131 us. Best GFLOPs: 4822.9229
2023-11-11 02:29:36 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #234: GFLOPs: 4766.7890. Time: 29.8277 us. Best GFLOPs: 4822.9229
2023-11-11 02:29:36 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #235: GFLOPs: 4454.6561. Time: 31.9177 us. Best GFLOPs: 4822.9229
2023-11-11 02:29:36 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #236: GFLOPs: 4382.2843. Time: 32.4448 us. Best GFLOPs: 4822.9229
2023-11-11 02:29:36 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #237: GFLOPs: 4723.9424. Time: 30.0983 us. Best GFLOPs: 4822.9229
2023-11-11 02:29:36 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #238: GFLOPs: 4296.6896. Time: 33.0912 us. Best GFLOPs: 4822.9229
2023-11-11 02:29:36 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #239: GFLOPs: 4447.6210. Time: 31.9682 us. Best GFLOPs: 4822.9229
2023-11-11 02:29:36 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #240: GFLOPs: 3625.8705. Time: 39.2133 us. Best GFLOPs: 4822.9229
2023-11-11 02:29:36 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #241: GFLOPs: 4255.1811. Time: 33.4139 us. Best GFLOPs: 4822.9229
2023-11-11 02:29:36 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #242: GFLOPs: 4308.0235. Time: 33.0041 us. Best GFLOPs: 4822.9229
2023-11-11 02:29:36 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #243: GFLOPs: 4097.8043. Time: 34.6972 us. Best GFLOPs: 4822.9229
2023-11-11 02:29:36 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #244: GFLOPs: 3957.5489. Time: 35.9269 us. Best GFLOPs: 4822.9229
2023-11-11 02:29:36 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #245: GFLOPs: 3711.1787. Time: 38.3119 us. Best GFLOPs: 4822.9229
2023-11-11 02:29:36 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #246: GFLOPs: 3395.3489. Time: 41.8756 us. Best GFLOPs: 4822.9229
2023-11-11 02:29:36 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #247: GFLOPs: 3578.0813. Time: 39.7371 us. Best GFLOPs: 4822.9229
2023-11-11 02:29:36 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #248: GFLOPs: 4381.3334. Time: 32.4519 us. Best GFLOPs: 4822.9229
2023-11-11 02:29:36 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #249: GFLOPs: 3956.8866. Time: 35.9329 us. Best GFLOPs: 4822.9229
2023-11-11 02:29:36 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #250: GFLOPs: 4326.7315. Time: 32.8614 us. Best GFLOPs: 4822.9229
2023-11-11 02:29:36 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #251: GFLOPs: 3241.7697. Time: 43.8595 us. Best GFLOPs: 4822.9229
2023-11-11 02:29:36 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #252: GFLOPs: 4557.7548. Time: 31.1957 us. Best GFLOPs: 4822.9229
2023-11-11 02:29:36 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #253: GFLOPs: 4234.8417. Time: 33.5744 us. Best GFLOPs: 4822.9229
2023-11-11 02:29:36 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #254: GFLOPs: 2506.8724. Time: 56.7170 us. Best GFLOPs: 4822.9229
2023-11-11 02:29:36 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #255: GFLOPs: 33.6060. Time: 4230.8693 us. Best GFLOPs: 4822.9229
2023-11-11 02:29:36 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #256: GFLOPs: 555.6588. Time: 255.8808 us. Best GFLOPs: 4822.9229
2023-11-11 02:37:29 [INFO] [evolutionary_search.cc:713] Generating candidates......
2023-11-11 02:37:32 [INFO] [evolutionary_search.cc:715] Picked top 102 candidate(s) from database
2023-11-11 02:37:38 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310fb03b28)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310eec3b08)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310dc5caf8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310f7cc2d8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x563110643e78)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310dc32028)]: 410 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310dc5cfb8)]: 0 failure(s)
2023-11-11 02:37:44 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310fb03b28)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310eec3b08)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310dc5caf8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310f7cc2d8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x563110643e78)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310dc32028)]: 813 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310dc5cfb8)]: 0 failure(s)
2023-11-11 02:37:50 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310fb03b28)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310eec3b08)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310dc5caf8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310f7cc2d8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x563110643e78)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310dc32028)]: 1214 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310dc5cfb8)]: 0 failure(s)
2023-11-11 02:37:56 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310fb03b28)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310eec3b08)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310dc5caf8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310f7cc2d8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x563110643e78)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310dc32028)]: 1618 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310dc5cfb8)]: 0 failure(s)
2023-11-11 02:38:03 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310fb03b28)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310eec3b08)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310dc5caf8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310f7cc2d8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x563110643e78)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310dc32028)]: 2019 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310dc5cfb8)]: 0 failure(s)
2023-11-11 02:38:09 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310fb03b28)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310eec3b08)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310dc5caf8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310f7cc2d8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x563110643e78)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310dc32028)]: 2421 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310dc5cfb8)]: 0 failure(s)
2023-11-11 02:38:15 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310fb03b28)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310eec3b08)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310dc5caf8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310f7cc2d8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x563110643e78)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310dc32028)]: 2826 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310dc5cfb8)]: 0 failure(s)
2023-11-11 02:38:21 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310fb03b28)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310eec3b08)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310dc5caf8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310f7cc2d8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x563110643e78)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310dc32028)]: 3232 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310dc5cfb8)]: 0 failure(s)
2023-11-11 02:38:27 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310fb03b28)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310eec3b08)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310dc5caf8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310f7cc2d8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x563110643e78)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310dc32028)]: 3635 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310dc5cfb8)]: 0 failure(s)
2023-11-11 02:38:27 [INFO] [evolutionary_search.cc:723] Sampled 55 candidate(s)
2023-11-11 02:38:44 [INFO] [evolutionary_search.cc:621] Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310fb03b28)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310eec3b08)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310dc5caf8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310f7cc2d8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x563110643e78)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310dc32028)]: 80 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310dc5cfb8)]: 0 failure(s)
2023-11-11 02:39:02 [INFO] [evolutionary_search.cc:621] Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310fb03b28)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310eec3b08)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310dc5caf8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310f7cc2d8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x563110643e78)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310dc32028)]: 68 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310dc5cfb8)]: 0 failure(s)
2023-11-11 02:39:21 [INFO] [evolutionary_search.cc:621] Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310fb03b28)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310eec3b08)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310dc5caf8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310f7cc2d8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x563110643e78)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310dc32028)]: 77 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310dc5cfb8)]: 0 failure(s)
2023-11-11 02:39:41 [INFO] [evolutionary_search.cc:621] Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310fb03b28)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310eec3b08)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310dc5caf8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310f7cc2d8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x563110643e78)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310dc32028)]: 83 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310dc5cfb8)]: 0 failure(s)
2023-11-11 02:39:46 [INFO] [evolutionary_search.cc:649] Scores of the best 64 candidates:
[1 : 16]:	1.0357  1.0328  1.0311  1.0225  1.0119  1.0099  1.0081  1.0050  1.0047  1.0018  1.0015  1.0005  0.9983  0.9981  0.9979  0.9894
[17 : 32]:	0.9880  0.9866  0.9833  0.9813  0.9811  0.9805  0.9793  0.9781  0.9780  0.9758  0.9744  0.9741  0.9739  0.9718  0.9670  0.9670
[33 : 48]:	0.9650  0.9648  0.9642  0.9634  0.9626  0.9606  0.9597  0.9590  0.9589  0.9588  0.9586  0.9584  0.9582  0.9578  0.9546  0.9539
[49 : 64]:	0.9527  0.9527  0.9522  0.9512  0.9512  0.9484  0.9465  0.9463  0.9450  0.9447  0.9441  0.9441  0.9412  0.9410  0.9409  0.9408
2023-11-11 02:39:47 [INFO] [evolutionary_search.cc:727] Got 64 candidate(s) with evolutionary search
2023-11-11 02:39:47 [INFO] [evolutionary_search.cc:730] Sending 64 candidates(s) for measurement
2023-11-11 02:40:51 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #257: GFLOPs: 3923.3648. Time: 36.2399 us. Best GFLOPs: 4822.9229
2023-11-11 02:40:51 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #258: GFLOPs: 5156.9033. Time: 27.5713 us. Best GFLOPs: 5156.9033
2023-11-11 02:40:51 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #259: GFLOPs: 4935.3323. Time: 28.8091 us. Best GFLOPs: 5156.9033
2023-11-11 02:40:51 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #260: GFLOPs: 3957.3053. Time: 35.9291 us. Best GFLOPs: 5156.9033
2023-11-11 02:40:51 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #261: GFLOPs: 4833.2030. Time: 29.4178 us. Best GFLOPs: 5156.9033
2023-11-11 02:40:51 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #262: GFLOPs: 4831.5573. Time: 29.4279 us. Best GFLOPs: 5156.9033
2023-11-11 02:40:51 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #263: GFLOPs: 4826.3989. Time: 29.4593 us. Best GFLOPs: 5156.9033
2023-11-11 02:40:51 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #264: GFLOPs: 4764.4334. Time: 29.8425 us. Best GFLOPs: 5156.9033
2023-11-11 02:40:51 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #265: GFLOPs: 4816.4875. Time: 29.5199 us. Best GFLOPs: 5156.9033
2023-11-11 02:40:51 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #266: GFLOPs: 4723.1423. Time: 30.1033 us. Best GFLOPs: 5156.9033
2023-11-11 02:40:51 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #267: GFLOPs: 4710.1106. Time: 30.1866 us. Best GFLOPs: 5156.9033
2023-11-11 02:40:51 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #268: GFLOPs: 4645.5599. Time: 30.6061 us. Best GFLOPs: 5156.9033
2023-11-11 02:40:51 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #269: GFLOPs: 4698.9058. Time: 30.2586 us. Best GFLOPs: 5156.9033
2023-11-11 02:40:51 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #270: GFLOPs: 4748.7176. Time: 29.9412 us. Best GFLOPs: 5156.9033
2023-11-11 02:40:51 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #271: GFLOPs: 4779.7677. Time: 29.7467 us. Best GFLOPs: 5156.9033
2023-11-11 02:40:51 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #272: GFLOPs: 4722.4524. Time: 30.1077 us. Best GFLOPs: 5156.9033
2023-11-11 02:40:51 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #273: GFLOPs: 4789.6620. Time: 29.6853 us. Best GFLOPs: 5156.9033
2023-11-11 02:40:51 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #274: GFLOPs: 4086.2153. Time: 34.7956 us. Best GFLOPs: 5156.9033
2023-11-11 02:40:51 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #275: GFLOPs: 4080.8710. Time: 34.8412 us. Best GFLOPs: 5156.9033
2023-11-11 02:40:51 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #276: GFLOPs: 4086.2422. Time: 34.7954 us. Best GFLOPs: 5156.9033
2023-11-11 02:40:51 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #277: GFLOPs: 4722.9530. Time: 30.1046 us. Best GFLOPs: 5156.9033
2023-11-11 02:40:51 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #278: GFLOPs: 4244.6855. Time: 33.4966 us. Best GFLOPs: 5156.9033
2023-11-11 02:40:51 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #279: GFLOPs: 4659.9976. Time: 30.5113 us. Best GFLOPs: 5156.9033
2023-11-11 02:40:51 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #280: GFLOPs: 4080.9250. Time: 34.8407 us. Best GFLOPs: 5156.9033
2023-11-11 02:40:51 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #281: GFLOPs: 4697.4403. Time: 30.2681 us. Best GFLOPs: 5156.9033
2023-11-11 02:40:51 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #282: GFLOPs: 4737.2713. Time: 30.0136 us. Best GFLOPs: 5156.9033
2023-11-11 02:40:51 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #283: GFLOPs: 4048.1132. Time: 35.1231 us. Best GFLOPs: 5156.9033
2023-11-11 02:40:51 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #284: GFLOPs: 4048.5391. Time: 35.1194 us. Best GFLOPs: 5156.9033
2023-11-11 02:40:51 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #285: GFLOPs: 4698.1741. Time: 30.2633 us. Best GFLOPs: 5156.9033
2023-11-11 02:40:51 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #286: GFLOPs: 4710.2875. Time: 30.1855 us. Best GFLOPs: 5156.9033
2023-11-11 02:40:51 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #287: GFLOPs: 4741.3856. Time: 29.9875 us. Best GFLOPs: 5156.9033
2023-11-11 02:40:51 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #288: GFLOPs: 4750.1028. Time: 29.9325 us. Best GFLOPs: 5156.9033
2023-11-11 02:40:51 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #289: GFLOPs: 4832.2032. Time: 29.4239 us. Best GFLOPs: 5156.9033
2023-11-11 02:40:51 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #290: GFLOPs: 4835.7571. Time: 29.4023 us. Best GFLOPs: 5156.9033
2023-11-11 02:40:51 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #291: GFLOPs: 4739.8467. Time: 29.9973 us. Best GFLOPs: 5156.9033
2023-11-11 02:40:51 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #292: GFLOPs: 4753.1345. Time: 29.9134 us. Best GFLOPs: 5156.9033
2023-11-11 02:40:51 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #293: GFLOPs: 4748.2766. Time: 29.9440 us. Best GFLOPs: 5156.9033
2023-11-11 02:40:51 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #294: GFLOPs: 4790.7841. Time: 29.6783 us. Best GFLOPs: 5156.9033
2023-11-11 02:40:51 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #295: GFLOPs: 4642.8440. Time: 30.6240 us. Best GFLOPs: 5156.9033
2023-11-11 02:40:51 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #296: GFLOPs: 4041.8145. Time: 35.1779 us. Best GFLOPs: 5156.9033
2023-11-11 02:40:51 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #297: GFLOPs: 4777.3495. Time: 29.7618 us. Best GFLOPs: 5156.9033
2023-11-11 02:40:51 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #298: GFLOPs: 4831.3197. Time: 29.4293 us. Best GFLOPs: 5156.9033
2023-11-11 02:40:51 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #299: GFLOPs: 4844.8558. Time: 29.3471 us. Best GFLOPs: 5156.9033
2023-11-11 02:40:51 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #300: GFLOPs: 4713.2271. Time: 30.1667 us. Best GFLOPs: 5156.9033
2023-11-11 02:40:51 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #301: GFLOPs: 4763.6807. Time: 29.8472 us. Best GFLOPs: 5156.9033
2023-11-11 02:40:51 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #302: GFLOPs: 4200.8184. Time: 33.8464 us. Best GFLOPs: 5156.9033
2023-11-11 02:40:51 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #303: GFLOPs: 4722.4524. Time: 30.1077 us. Best GFLOPs: 5156.9033
2023-11-11 02:40:51 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #304: GFLOPs: 5083.3026. Time: 27.9705 us. Best GFLOPs: 5156.9033
2023-11-11 02:40:51 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #305: GFLOPs: 4745.0961. Time: 29.9641 us. Best GFLOPs: 5156.9033
2023-11-11 02:40:51 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #306: GFLOPs: 4773.5842. Time: 29.7852 us. Best GFLOPs: 5156.9033
2023-11-11 02:40:51 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #307: GFLOPs: 4650.5186. Time: 30.5735 us. Best GFLOPs: 5156.9033
2023-11-11 02:40:51 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #308: GFLOPs: 4680.6984. Time: 30.3763 us. Best GFLOPs: 5156.9033
2023-11-11 02:40:51 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #309: GFLOPs: 4700.6726. Time: 30.2472 us. Best GFLOPs: 5156.9033
2023-11-11 02:40:51 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #310: GFLOPs: 4317.5862. Time: 32.9310 us. Best GFLOPs: 5156.9033
2023-11-11 02:40:51 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #311: GFLOPs: 4556.0072. Time: 31.2077 us. Best GFLOPs: 5156.9033
2023-11-11 02:40:51 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #312: GFLOPs: 4672.5722. Time: 30.4292 us. Best GFLOPs: 5156.9033
2023-11-11 02:40:51 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #313: GFLOPs: 4893.2450. Time: 29.0569 us. Best GFLOPs: 5156.9033
2023-11-11 02:40:51 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #314: GFLOPs: 4747.8039. Time: 29.9470 us. Best GFLOPs: 5156.9033
2023-11-11 02:40:51 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #315: GFLOPs: 4920.4237. Time: 28.8964 us. Best GFLOPs: 5156.9033
2023-11-11 02:40:51 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #316: GFLOPs: 4920.4237. Time: 28.8964 us. Best GFLOPs: 5156.9033
2023-11-11 02:40:51 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #317: GFLOPs: 4683.8095. Time: 30.3561 us. Best GFLOPs: 5156.9033
2023-11-11 02:40:51 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #318: GFLOPs: 1682.2861. Time: 84.5174 us. Best GFLOPs: 5156.9033
2023-11-11 02:40:51 [INFO] [task_scheduler.cc:121] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #319: Error in building:
LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(512), T.int64(7), T.int64(7)), "float32"), p1: T.Buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(512)), "float32"), p2: T.Buffer((T.int64(1), T.int64(512), T.int64(7), T.int64(7)), "float32"), p3: T.Buffer((T.int64(1), T.int64(512), T.int64(1), T.int64(1)), "float32"), p4: T.Buffer((T.int64(1), T.int64(512), T.int64(1), T.int64(1)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(512), T.int64(7), T.int64(7)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        input_tile_local = T.alloc_buffer((T.int64(512), T.int64(16), T.int64(4), T.int64(4)), scope="local")
        data_pack = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(16)))
        bgemm = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(16)))
        inverse_local = T.alloc_buffer((T.int64(512), T.int64(16), T.int64(2), T.int64(2)), scope="local")
        data_pack_local = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(16)), scope="local")
        bgemm_local = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(16)), scope="local")
        data_pack_shared = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(16)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(512)), scope="shared")
        for ci_p_fused_0 in T.thread_binding(T.int64(32), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 1024, "pragma_unroll_explicit": 1}):
            for ci_p_fused_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(1), T.int64(4), T.int64(4)):
                    with T.block("input_tile"):
                        v_ci = T.axis.spatial(T.int64(512), ci_p_fused_0 * T.int64(16) + ci_p_fused_1 // T.int64(16) + ax0)
                        v_p = T.axis.spatial(T.int64(16), ci_p_fused_1 % T.int64(16) + ax1)
                        v_eps, v_nu = T.axis.remap("SS", [ax2, ax3])
                        T.reads(p0[v_p // T.int64(16), v_ci, v_p % T.int64(16) // T.int64(4) * T.int64(2) + v_eps - T.int64(1), v_p % T.int64(4) * T.int64(2) + v_nu - T.int64(1)])
                        T.writes(input_tile_local[v_ci, v_p, v_eps, v_nu])
                        T.block_attr({"schedule_rule": "None"})
                        input_tile_local[v_ci, v_p, v_eps, v_nu] = T.if_then_else(T.int64(1) <= v_p % T.int64(16) // T.int64(4) * T.int64(2) + v_eps and v_p % T.int64(16) // T.int64(4) * T.int64(2) + v_eps < T.int64(8) and T.int64(1) <= v_p % T.int64(4) * T.int64(2) + v_nu and v_p % T.int64(4) * T.int64(2) + v_nu < T.int64(8), p0[v_p // T.int64(16), v_ci, v_p % T.int64(16) // T.int64(4) * T.int64(2) + v_eps - T.int64(1), v_p % T.int64(4) * T.int64(2) + v_nu - T.int64(1)], T.float32(0))
                for eps in T.unroll(T.int64(4)):
                    for nu in T.unroll(T.int64(4)):
                        with T.block("data_pack_init"):
                            v_eps, v_nu = T.axis.remap("SS", [eps, nu])
                            v_ci = T.axis.spatial(T.int64(512), (ci_p_fused_0 * T.int64(256) + ci_p_fused_1) // T.int64(16))
                            v_p = T.axis.spatial(T.int64(16), (ci_p_fused_0 * T.int64(256) + ci_p_fused_1) % T.int64(16))
                            T.reads()
                            T.writes(data_pack_local[v_eps, v_nu, v_ci, v_p])
                            T.block_attr({"schedule_rule": "conv2d_nchw_winograd_data_pack"})
                            data_pack_local[v_eps, v_nu, v_ci, v_p] = T.float32(0)
                        for r_a in T.unroll(T.int64(4)):
                            for r_b in T.unroll(T.int64(4)):
                                with T.block("data_pack_update"):
                                    v_eps, v_nu = T.axis.remap("SS", [eps, nu])
                                    v_ci = T.axis.spatial(T.int64(512), (ci_p_fused_0 * T.int64(256) + ci_p_fused_1) // T.int64(16))
                                    v_p = T.axis.spatial(T.int64(16), (ci_p_fused_0 * T.int64(256) + ci_p_fused_1) % T.int64(16))
                                    v_r_a, v_r_b = T.axis.remap("RR", [r_a, r_b])
                                    T.reads(data_pack_local[v_eps, v_nu, v_ci, v_p], input_tile_local[v_ci, v_p, v_r_a, v_r_b])
                                    T.writes(data_pack_local[v_eps, v_nu, v_ci, v_p])
                                    T.block_attr({"schedule_rule": "conv2d_nchw_winograd_data_pack"})
                                    data_pack_local[v_eps, v_nu, v_ci, v_p] = data_pack_local[v_eps, v_nu, v_ci, v_p] + input_tile_local[v_ci, v_p, v_r_a, v_r_b] * T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(3), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(1), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(0), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(3), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(1), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(0), T.float32(1), T.float32(0))))))))))))))))) * T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(3), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(1), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(0), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(3), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(1), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(0), T.float32(1), T.float32(0)))))))))))))))))
                for ax0, ax1, ax2, ax3 in T.grid(T.int64(4), T.int64(4), T.int64(1), T.int64(1)):
                    with T.block("data_pack_local"):
                        v0, v1 = T.axis.remap("SS", [ax0, ax1])
                        v2 = T.axis.spatial(T.int64(512), ci_p_fused_0 * T.int64(16) + ci_p_fused_1 // T.int64(16) + ax2)
                        v3 = T.axis.spatial(T.int64(16), ci_p_fused_1 % T.int64(16) + ax3)
                        T.reads(data_pack_local[v0, v1, v2, v3])
                        T.writes(data_pack[v0, v1, v2, v3])
                        data_pack[v0, v1, v2, v3] = data_pack_local[v0, v1, v2, v3]
        for eps_0_nu_0_co_0_p_0_fused in T.thread_binding(T.int64(4), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 1024, "pragma_unroll_explicit": 1}):
            for eps_1_nu_1_co_1_p_1_fused in T.thread_binding(T.int64(4), thread="vthread.x"):
                for eps_2_nu_2_co_2_p_2_fused in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                    for eps_3_init, nu_3_init, co_3_init, p_3_init, eps_4_init, nu_4_init, co_4_init, p_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(2), T.int64(2), T.int64(8), T.int64(4)):
                        with T.block("bgemm_init"):
                            v_eps = T.axis.spatial(T.int64(4), eps_2_nu_2_co_2_p_2_fused // T.int64(16) * T.int64(2) + eps_3_init * T.int64(2) + eps_4_init)
                            v_nu = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(2) * T.int64(2) + nu_3_init * T.int64(2) + nu_4_init)
                            v_co = T.axis.spatial(T.int64(512), eps_0_nu_0_co_0_p_0_fused % T.int64(2) * T.int64(256) + eps_1_nu_1_co_1_p_1_fused // T.int64(2) * T.int64(128) + eps_2_nu_2_co_2_p_2_fused % T.int64(16) * T.int64(8) + co_3_init * T.int64(8) + co_4_init)
                            v_p = T.axis.spatial(T.int64(16), eps_1_nu_1_co_1_p_1_fused % T.int64(2) * T.int64(8) + p_3_init * T.int64(4) + p_4_init)
                            T.reads()
                            T.writes(bgemm_local[v_eps, v_nu, v_co, v_p])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            bgemm_local[v_eps, v_nu, v_co, v_p] = T.float32(0)
                    for ci_0_fused in T.serial(T.int64(512), annotations={"software_pipeline_async_stages": [0], "software_pipeline_order": [0, 1, 2], "software_pipeline_stage": [0, 0, 3]}):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(4)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                                with T.block("data_pack_shared"):
                                    v0 = T.axis.spatial(T.int64(4), (ax0_ax1_ax2_ax3_fused_0 * T.int64(32) + ax0_ax1_ax2_ax3_fused_1) // T.int64(32))
                                    v1 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(2) * T.int64(2) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(32) + ax0_ax1_ax2_ax3_fused_1) % T.int64(32) // T.int64(16))
                                    v2 = T.axis.spatial(T.int64(512), ci_0_fused)
                                    v3 = T.axis.spatial(T.int64(16), (ax0_ax1_ax2_ax3_fused_0 * T.int64(32) + ax0_ax1_ax2_ax3_fused_1) % T.int64(16))
                                    T.reads(data_pack[v0, v1, v2, v3])
                                    T.writes(data_pack_shared[v0, v1, v2, v3])
                                    data_pack_shared[v0, v1, v2, v3] = data_pack[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(64)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                                with T.block("p1_shared"):
                                    v0 = T.axis.spatial(T.int64(4), (ax0_ax1_ax2_ax3_fused_0 * T.int64(32) + ax0_ax1_ax2_ax3_fused_1) // T.int64(512))
                                    v1 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(2) * T.int64(2) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(32) + ax0_ax1_ax2_ax3_fused_1) % T.int64(512) // T.int64(256))
                                    v2 = T.axis.spatial(T.int64(512), ci_0_fused)
                                    v3 = T.axis.spatial(T.int64(512), eps_0_nu_0_co_0_p_0_fused % T.int64(2) * T.int64(256) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(32) + ax0_ax1_ax2_ax3_fused_1) % T.int64(256))
                                    T.reads(p1[v0, v1, v2, v3])
                                    T.writes(p1_shared[v0, v1, v2, v3])
                                    p1_shared[v0, v1, v2, v3] = p1[v0, v1, v2, v3]
                        for ci_1, eps_3, nu_3, co_3, p_3, ci_2, eps_4, nu_4, co_4, p_4 in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(1), T.int64(2), T.int64(2), T.int64(8), T.int64(4)):
                            with T.block("bgemm_update"):
                                v_eps = T.axis.spatial(T.int64(4), eps_2_nu_2_co_2_p_2_fused // T.int64(16) * T.int64(2) + eps_3 * T.int64(2) + eps_4)
                                v_nu = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(2) * T.int64(2) + nu_3 * T.int64(2) + nu_4)
                                v_co = T.axis.spatial(T.int64(512), eps_0_nu_0_co_0_p_0_fused % T.int64(2) * T.int64(256) + eps_1_nu_1_co_1_p_1_fused // T.int64(2) * T.int64(128) + eps_2_nu_2_co_2_p_2_fused % T.int64(16) * T.int64(8) + co_3 * T.int64(8) + co_4)
                                v_p = T.axis.spatial(T.int64(16), eps_1_nu_1_co_1_p_1_fused % T.int64(2) * T.int64(8) + p_3 * T.int64(4) + p_4)
                                v_ci = T.axis.reduce(T.int64(512), ci_0_fused + ci_1 + ci_2)
                                T.reads(bgemm_local[v_eps, v_nu, v_co, v_p], data_pack_shared[v_eps, v_nu, v_ci, v_p], p1_shared[v_eps, v_nu, v_ci, v_co])
                                T.writes(bgemm_local[v_eps, v_nu, v_co, v_p])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                bgemm_local[v_eps, v_nu, v_co, v_p] = bgemm_local[v_eps, v_nu, v_co, v_p] + data_pack_shared[v_eps, v_nu, v_ci, v_p] * p1_shared[v_eps, v_nu, v_ci, v_co]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(2), T.int64(2), T.int64(8), T.int64(8)):
                        with T.block("bgemm_local"):
                            v0 = T.axis.spatial(T.int64(4), eps_2_nu_2_co_2_p_2_fused // T.int64(16) * T.int64(2) + ax0)
                            v1 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(2) * T.int64(2) + ax1)
                            v2 = T.axis.spatial(T.int64(512), eps_0_nu_0_co_0_p_0_fused % T.int64(2) * T.int64(256) + eps_1_nu_1_co_1_p_1_fused // T.int64(2) * T.int64(128) + eps_2_nu_2_co_2_p_2_fused % T.int64(16) * T.int64(8) + ax2)
                            v3 = T.axis.spatial(T.int64(16), eps_1_nu_1_co_1_p_1_fused % T.int64(2) * T.int64(8) + ax3)
                            T.reads(bgemm_local[v0, v1, v2, v3])
                            T.writes(bgemm[v0, v1, v2, v3])
                            bgemm[v0, v1, v2, v3] = bgemm_local[v0, v1, v2, v3]
        for n_co_h_0_w_0_fused_0 in T.thread_binding(T.int64(64), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 1024, "pragma_unroll_explicit": 1}):
            for n_co_h_0_w_0_fused_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                    for ax2 in T.unroll(T.int64(2)):
                        for ax3 in T.unroll(T.int64(2)):
                            with T.block("inverse_init"):
                                v_co = T.axis.spatial(T.int64(512), (n_co_h_0_w_0_fused_0 * T.int64(128) + n_co_h_0_w_0_fused_1) // T.int64(16) + ax0)
                                v_p = T.axis.spatial(T.int64(16), (n_co_h_0_w_0_fused_0 * T.int64(128) + n_co_h_0_w_0_fused_1) % T.int64(16) + ax1)
                                v_vh, v_vw = T.axis.remap("SS", [ax2, ax3])
                                T.reads()
                                T.writes(inverse_local[v_co, v_p, v_vh, v_vw])
                                T.block_attr({"schedule_rule": "conv2d_nchw_winograd_inverse"})
                                inverse_local[v_co, v_p, v_vh, v_vw] = T.float32(0)
                            for ax4 in T.unroll(T.int64(4)):
                                for ax5 in T.unroll(T.int64(4)):
                                    with T.block("inverse_update"):
                                        v_co = T.axis.spatial(T.int64(512), (n_co_h_0_w_0_fused_0 * T.int64(128) + n_co_h_0_w_0_fused_1) // T.int64(16) + ax0)
                                        v_p = T.axis.spatial(T.int64(16), (n_co_h_0_w_0_fused_0 * T.int64(128) + n_co_h_0_w_0_fused_1) % T.int64(16) + ax1)
                                        v_vh, v_vw, v_r_a, v_r_b = T.axis.remap("SSRR", [ax2, ax3, ax4, ax5])
                                        T.reads(inverse_local[v_co, v_p, v_vh, v_vw], bgemm[v_r_a, v_r_b, v_co, v_p])
                                        T.writes(inverse_local[v_co, v_p, v_vh, v_vw])
                                        T.block_attr({"schedule_rule": "conv2d_nchw_winograd_inverse"})
                                        inverse_local[v_co, v_p, v_vh, v_vw] = inverse_local[v_co, v_p, v_vh, v_vw] + bgemm[v_r_a, v_r_b, v_co, v_p] * T.Select(v_r_a % T.int64(4) == T.int64(3) and v_vh % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_vh % T.int64(2) == T.int64(0), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_vh % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_vh % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_vh % T.int64(2) == T.int64(1), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_vh % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_vh % T.int64(2) == T.int64(1), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_vh % T.int64(2) == T.int64(0), T.float32(1), T.float32(0))))))))) * T.Select(v_r_b % T.int64(4) == T.int64(3) and v_vw % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_vw % T.int64(2) == T.int64(0), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_vw % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_vw % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_vw % T.int64(2) == T.int64(1), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_vw % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_vw % T.int64(2) == T.int64(1), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_vw % T.int64(2) == T.int64(0), T.float32(1), T.float32(0)))))))))
                for h_1, w_1 in T.grid(T.int64(2), T.int64(2)):
                    with T.block("conv2d_winograd"):
                        v_n = T.axis.spatial(T.int64(1), T.int64(0))
                        v_co = T.axis.spatial(T.int64(512), (n_co_h_0_w_0_fused_0 * T.int64(128) + n_co_h_0_w_0_fused_1) // T.int64(16))
                        v_h = T.axis.spatial(T.int64(7), (n_co_h_0_w_0_fused_0 * T.int64(128) + n_co_h_0_w_0_fused_1) % T.int64(16) // T.int64(4) * T.int64(2) + h_1)
                        v_w = T.axis.spatial(T.int64(7), (n_co_h_0_w_0_fused_0 * T.int64(128) + n_co_h_0_w_0_fused_1) % T.int64(4) * T.int64(2) + w_1)
                        T.where((n_co_h_0_w_0_fused_0 * T.int64(128) + n_co_h_0_w_0_fused_1) % T.int64(16) // T.int64(4) * T.int64(2) + h_1 < T.int64(7) and (n_co_h_0_w_0_fused_0 * T.int64(128) + n_co_h_0_w_0_fused_1) % T.int64(4) * T.int64(2) + w_1 < T.int64(7))
                        T.reads(inverse_local[v_co, v_n * T.int64(16) + v_h // T.int64(2) * T.int64(4) + v_w // T.int64(2), v_h % T.int64(2), v_w % T.int64(2)], p2[v_n, v_co, v_h, v_w], p3[v_n, v_co, T.int64(0), T.int64(0)], p4[v_n, v_co, T.int64(0), T.int64(0)])
                        T.writes(T_relu[v_n, v_co, v_h, v_w])
                        T_relu[v_n, v_co, v_h, v_w] = T.max((inverse_local[v_co, v_n * T.int64(16) + v_h // T.int64(2) * T.int64(4) + v_w // T.int64(2), v_h % T.int64(2), v_w % T.int64(2)] + p2[v_n, v_co, v_h, v_w]) * p3[v_n, v_co, T.int64(0), T.int64(0)] + p4[v_n, v_co, T.int64(0), T.int64(0)], T.float32(0))
b0 = sch.get_block(name="data_pack", func_name="main")
b1 = sch.get_block(name="bgemm", func_name="main")
b2 = sch.get_block(name="inverse", func_name="main")
b3 = sch.get_block(name="conv2d_winograd", func_name="main")
b4 = sch.get_block(name="T_add", func_name="main")
b5 = sch.get_block(name="T_multiply", func_name="main")
b6 = sch.get_block(name="T_add_1", func_name="main")
b7 = sch.get_block(name="T_relu", func_name="main")
b8 = sch.get_block(name="root", func_name="main")
b9, b10 = sch.get_producers(block=b2)
sch.compute_inline(block=b10)
b11, = sch.get_consumers(block=b2)
l12, l13, l14, l15 = sch.get_loops(block=b11)
l16, l17 = sch.split(loop=l14, factors=[None, 2], preserve_unit_iters=True)
l18, l19 = sch.split(loop=l15, factors=[None, 2], preserve_unit_iters=True)
sch.reorder(l16, l18, l17, l19)
sch.compute_at(block=b2, loop=l18, preserve_unit_loops=True, index=-1)
sch.set_scope(block=b2, buffer_index=0, storage_scope="local")
l20, l21, l22, l23, l24, l25, l26, l27, l28, l29 = sch.get_loops(block=b2)
sch.unroll(loop=l26)
sch.unroll(loop=l27)
sch.unroll(loop=l28)
sch.unroll(loop=l29)
b30, b31 = sch.get_producers(block=b0)
sch.compute_inline(block=b31)
b32, = sch.get_producers(block=b30)
l33, l34, l35, l36, l37, l38 = sch.get_loops(block=b0)
sch.reorder(l35, l36, l33, l34, l37, l38)
sch.unroll(loop=l33)
sch.unroll(loop=l34)
sch.unroll(loop=l37)
sch.unroll(loop=l38)
l39 = sch.fuse(l35, l36, preserve_unit_iters=True)
v40 = sch.sample_categorical(candidates=[32, 64, 128, 256, 512, 1024], probs=[0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666], decision=3)
l41, l42 = sch.split(loop=l39, factors=[None, v40], preserve_unit_iters=True)
sch.bind(loop=l41, thread_axis="blockIdx.x")
sch.bind(loop=l42, thread_axis="threadIdx.x")
b43 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b43, loop=l42, preserve_unit_loops=True, index=-1)
sch.compute_at(block=b30, loop=l42, preserve_unit_loops=True, index=-1)
sch.set_scope(block=b30, buffer_index=0, storage_scope="local")
sch.compute_inline(block=b32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l44, l45, l46, l47, l48 = sch.get_loops(block=b1)
v49, v50, v51, v52, v53 = sch.sample_perfect_tile(loop=l44, n=5, max_innermost_factor=64, decision=[1, 1, 2, 1, 2])
l54, l55, l56, l57, l58 = sch.split(loop=l44, factors=[v49, v50, v51, v52, v53], preserve_unit_iters=True)
v59, v60, v61, v62, v63 = sch.sample_perfect_tile(loop=l45, n=5, max_innermost_factor=64, decision=[2, 1, 1, 1, 2])
l64, l65, l66, l67, l68 = sch.split(loop=l45, factors=[v59, v60, v61, v62, v63], preserve_unit_iters=True)
v69, v70, v71, v72, v73 = sch.sample_perfect_tile(loop=l46, n=5, max_innermost_factor=64, decision=[2, 2, 16, 1, 8])
l74, l75, l76, l77, l78 = sch.split(loop=l46, factors=[v69, v70, v71, v72, v73], preserve_unit_iters=True)
v79, v80, v81, v82, v83 = sch.sample_perfect_tile(loop=l47, n=5, max_innermost_factor=64, decision=[1, 2, 1, 2, 4])
l84, l85, l86, l87, l88 = sch.split(loop=l47, factors=[v79, v80, v81, v82, v83], preserve_unit_iters=True)
v89, v90, v91 = sch.sample_perfect_tile(loop=l48, n=3, max_innermost_factor=64, decision=[512, 1, 1])
l92, l93, l94 = sch.split(loop=l48, factors=[v89, v90, v91], preserve_unit_iters=True)
sch.reorder(l54, l64, l74, l84, l55, l65, l75, l85, l56, l66, l76, l86, l92, l93, l57, l67, l77, l87, l94, l58, l68, l78, l88)
l95 = sch.fuse(l54, l64, l74, l84, preserve_unit_iters=True)
sch.bind(loop=l95, thread_axis="blockIdx.x")
l96 = sch.fuse(l55, l65, l75, l85, preserve_unit_iters=True)
sch.bind(loop=l96, thread_axis="vthread.x")
l97 = sch.fuse(l56, l66, l76, l86, preserve_unit_iters=True)
sch.bind(loop=l97, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b98 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b98, loop=l97, preserve_unit_loops=True, index=-1)
b99 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b99, loop=l92, preserve_unit_loops=True, index=-1)
l100, l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b99)
l108 = sch.fuse(l104, l105, l106, l107, preserve_unit_iters=True)
v109 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b99, ann_key="meta_schedule.cooperative_fetch", ann_val=v109)
b110 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b110, loop=l92, preserve_unit_loops=True, index=-1)
l111, l112, l113, l114, l115, l116, l117, l118 = sch.get_loops(block=b110)
l119 = sch.fuse(l115, l116, l117, l118, preserve_unit_iters=True)
v120 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b110, ann_key="meta_schedule.cooperative_fetch", ann_val=v120)
l121 = sch.fuse(l92, preserve_unit_iters=True)
sch.annotate(block_or_loop=l121, ann_key="software_pipeline_stage", ann_val=[0, 0, 3])
sch.annotate(block_or_loop=l121, ann_key="software_pipeline_order", ann_val=[0, 1, 2])
sch.annotate(block_or_loop=l121, ann_key="software_pipeline_async_stages", ann_val=[0])
sch.reverse_compute_inline(block=b7)
sch.reverse_compute_inline(block=b6)
sch.reverse_compute_inline(block=b5)
sch.reverse_compute_inline(block=b4)
v122 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b8, ann_key="meta_schedule.unroll_explicit", ann_val=v122)
l123, l124, l125, l126, l127, l128 = sch.get_loops(block=b3)
l129 = sch.fuse(l123, l124, l125, l126, preserve_unit_iters=True)
v130 = sch.sample_categorical(candidates=[32, 64, 128, 256, 512, 1024], probs=[0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666], decision=2)
l131, l132 = sch.split(loop=l129, factors=[None, v130], preserve_unit_iters=True)
sch.bind(loop=l131, thread_axis="blockIdx.x")
sch.bind(loop=l132, thread_axis="threadIdx.x")
sch.enter_postproc()
sch.unannotate(block_or_loop=b99, ann_key="meta_schedule.cooperative_fetch")
l133, l134, l135, l136, l137 = sch.get_loops(block=b99)
l138, l139 = sch.split(loop=l137, factors=[None, 32], preserve_unit_iters=True)
sch.bind(loop=l139, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b110, ann_key="meta_schedule.cooperative_fetch")
l140, l141, l142, l143, l144 = sch.get_loops(block=b110)
l145, l146 = sch.split(loop=l144, factors=[None, 32], preserve_unit_iters=True)
sch.bind(loop=l146, thread_axis="threadIdx.x")
b147 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b147, ann_key="meta_schedule.unroll_explicit")
b148, b149, b150, b151, b152, b153, b154, b155, b156 = sch.get_child_blocks(b147)
l157, l158, l159, l160, l161, l162 = sch.get_loops(block=b148)
l163, l164, l165, l166, l167, l168 = sch.get_loops(block=b149)
sch.annotate(block_or_loop=l163, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l163, ann_key="pragma_unroll_explicit", ann_val=1)
l169, l170, l171, l172, l173, l174 = sch.get_loops(block=b150)
l175, l176, l177, l178, l179, l180 = sch.get_loops(block=b151)
l181, l182, l183, l184, l185, l186 = sch.get_loops(block=b152)
l187, l188, l189, l190, l191, l192, l193, l194, l195, l196, l197, l198, l199, l200 = sch.get_loops(block=b153)
sch.annotate(block_or_loop=l187, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l187, ann_key="pragma_unroll_explicit", ann_val=1)
l201, l202, l203, l204, l205, l206, l207 = sch.get_loops(block=b154)
l208, l209, l210, l211, l212, l213, l214, l215 = sch.get_loops(block=b155)
sch.annotate(block_or_loop=l208, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l208, ann_key="pragma_unroll_explicit", ann_val=1)
l216, l217, l218, l219 = sch.get_loops(block=b156)
b220 = sch.get_block(name="data_pack", func_name="main")
l221, l222, l223, l224, l225, l226 = sch.get_loops(block=b220)
b227 = sch.decompose_reduction(block=b220, loop=l225)
b228 = sch.get_block(name="bgemm", func_name="main")
l229, l230, l231, l232, l233, l234, l235, l236, l237, l238, l239, l240, l241, l242 = sch.get_loops(block=b228)
b243 = sch.decompose_reduction(block=b228, loop=l232)
b244 = sch.get_block(name="inverse", func_name="main")
l245, l246, l247, l248, l249, l250, l251, l252 = sch.get_loops(block=b244)
b253 = sch.decompose_reduction(block=b244, loop=l251)
2023-11-11 02:40:51 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #320: GFLOPs: 26.5050. Time: 5364.3586 us. Best GFLOPs: 5156.9033
2023-11-11 03:44:30 [INFO] [evolutionary_search.cc:713] Generating candidates......
2023-11-11 03:44:33 [INFO] [evolutionary_search.cc:715] Picked top 102 candidate(s) from database
2023-11-11 03:44:39 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310fb03b28)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310eec3b08)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310dc5caf8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310f7cc2d8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x563110643e78)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310dc32028)]: 399 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310dc5cfb8)]: 0 failure(s)
2023-11-11 03:44:46 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310fb03b28)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310eec3b08)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310dc5caf8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310f7cc2d8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x563110643e78)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310dc32028)]: 796 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310dc5cfb8)]: 0 failure(s)
2023-11-11 03:44:52 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310fb03b28)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310eec3b08)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310dc5caf8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310f7cc2d8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x563110643e78)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310dc32028)]: 1195 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310dc5cfb8)]: 0 failure(s)
2023-11-11 03:44:58 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310fb03b28)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310eec3b08)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310dc5caf8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310f7cc2d8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x563110643e78)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310dc32028)]: 1598 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310dc5cfb8)]: 0 failure(s)
2023-11-11 03:45:05 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310fb03b28)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310eec3b08)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310dc5caf8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310f7cc2d8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x563110643e78)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310dc32028)]: 1995 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310dc5cfb8)]: 0 failure(s)
2023-11-11 03:45:05 [INFO] [evolutionary_search.cc:723] Sampled 55 candidate(s)
2023-11-11 03:45:21 [INFO] [evolutionary_search.cc:621] Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310fb03b28)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310eec3b08)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310dc5caf8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310f7cc2d8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x563110643e78)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310dc32028)]: 92 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310dc5cfb8)]: 0 failure(s)
2023-11-11 03:45:41 [INFO] [evolutionary_search.cc:621] Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310fb03b28)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310eec3b08)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310dc5caf8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310f7cc2d8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x563110643e78)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310dc32028)]: 95 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310dc5cfb8)]: 0 failure(s)
2023-11-11 03:46:00 [INFO] [evolutionary_search.cc:621] Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310fb03b28)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310eec3b08)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310dc5caf8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310f7cc2d8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x563110643e78)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310dc32028)]: 100 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310dc5cfb8)]: 0 failure(s)
2023-11-11 03:46:19 [INFO] [evolutionary_search.cc:621] Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310fb03b28)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310eec3b08)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310dc5caf8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310f7cc2d8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x563110643e78)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310dc32028)]: 75 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310dc5cfb8)]: 0 failure(s)
2023-11-11 03:46:25 [INFO] [evolutionary_search.cc:649] Scores of the best 64 candidates:
[1 : 16]:	1.2249  1.2137  1.1918  1.1810  1.1754  1.1752  1.1484  1.1402  1.1156  1.0677  1.0601  1.0187  1.0176  1.0040  1.0035  1.0009
[17 : 32]:	0.9985  0.9879  0.9837  0.9786  0.9779  0.9776  0.9753  0.9734  0.9730  0.9713  0.9705  0.9704  0.9687  0.9674  0.9651  0.9633
[33 : 48]:	0.9631  0.9602  0.9597  0.9595  0.9591  0.9589  0.9580  0.9576  0.9564  0.9563  0.9548  0.9548  0.9523  0.9523  0.9514  0.9502
[49 : 64]:	0.9484  0.9465  0.9464  0.9457  0.9453  0.9453  0.9453  0.9446  0.9442  0.9436  0.9431  0.9424  0.9424  0.9421  0.9421  0.9417
2023-11-11 03:46:25 [INFO] [evolutionary_search.cc:727] Got 64 candidate(s) with evolutionary search
2023-11-11 03:46:25 [INFO] [evolutionary_search.cc:730] Sending 64 candidates(s) for measurement
2023-11-11 03:49:43 [INFO] [task_scheduler.cc:121] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #321: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/home2/xiachunwei/Software/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/home2/xiachunwei/Software/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/home2/xiachunwei/Software/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/home2/xiachunwei/Software/tvm/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/home2/xiachunwei/Software/tvm/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/home2/xiachunwei/Software/tvm/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/home2/xiachunwei/Software/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/home2/xiachunwei/Software/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  4: _ZN3tvm7runtime13PackedFun
  3: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  2: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  0: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  File "/home2/xiachunwei/Software/tvm/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(512), T.int64(7), T.int64(7)), "float32"), p1: T.Buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(512)), "float32"), p2: T.Buffer((T.int64(1), T.int64(512), T.int64(7), T.int64(7)), "float32"), p3: T.Buffer((T.int64(1), T.int64(512), T.int64(1), T.int64(1)), "float32"), p4: T.Buffer((T.int64(1), T.int64(512), T.int64(1), T.int64(1)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(512), T.int64(7), T.int64(7)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        input_tile_local = T.alloc_buffer((T.int64(512), T.int64(16), T.int64(4), T.int64(4)), scope="local")
        data_pack = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(16)))
        bgemm = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(16)))
        inverse_local = T.alloc_buffer((T.int64(512), T.int64(16), T.int64(2), T.int64(2)), scope="local")
        data_pack_local = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(16)), scope="local")
        bgemm_local = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(16)), scope="local")
        data_pack_shared = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(16)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(512)), scope="shared")
        for ci_p_fused_0 in T.thread_binding(T.int64(16), thread="blockIdx.x"):
            for ci_p_fused_1 in T.thread_binding(T.int64(512), thread="threadIdx.x"):
                for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(1), T.int64(4), T.int64(4)):
                    with T.block("input_tile"):
                        v_ci = T.axis.spatial(T.int64(512), ci_p_fused_0 * T.int64(32) + ci_p_fused_1 // T.int64(16) + ax0)
                        v_p = T.axis.spatial(T.int64(16), ci_p_fused_1 % T.int64(16) + ax1)
                        v_eps, v_nu = T.axis.remap("SS", [ax2, ax3])
                        T.reads(p0[v_p // T.int64(16), v_ci, v_p % T.int64(16) // T.int64(4) * T.int64(2) + v_eps - T.int64(1), v_p % T.int64(4) * T.int64(2) + v_nu - T.int64(1)])
                        T.writes(input_tile_local[v_ci, v_p, v_eps, v_nu])
                        T.block_attr({"schedule_rule": "None"})
                        input_tile_local[v_ci, v_p, v_eps, v_nu] = T.if_then_else(T.int64(1) <= v_p % T.int64(16) // T.int64(4) * T.int64(2) + v_eps and v_p % T.int64(16) // T.int64(4) * T.int64(2) + v_eps < T.int64(8) and T.int64(1) <= v_p % T.int64(4) * T.int64(2) + v_nu and v_p % T.int64(4) * T.int64(2) + v_nu < T.int64(8), p0[v_p // T.int64(16), v_ci, v_p % T.int64(16) // T.int64(4) * T.int64(2) + v_eps - T.int64(1), v_p % T.int64(4) * T.int64(2) + v_nu - T.int64(1)], T.float32(0))
                for eps in T.unroll(T.int64(4)):
                    for nu in T.unroll(T.int64(4)):
                        with T.block("data_pack_init"):
                            v_eps, v_nu = T.axis.remap("SS", [eps, nu])
                            v_ci = T.axis.spatial(T.int64(512), (ci_p_fused_0 * T.int64(512) + ci_p_fused_1) // T.int64(16))
                            v_p = T.axis.spatial(T.int64(16), (ci_p_fused_0 * T.int64(512) + ci_p_fused_1) % T.int64(16))
                            T.reads()
                            T.writes(data_pack_local[v_eps, v_nu, v_ci, v_p])
                            T.block_attr({"schedule_rule": "conv2d_nchw_winograd_data_pack"})
                            data_pack_local[v_eps, v_nu, v_ci, v_p] = T.float32(0)
                        for r_a in T.unroll(T.int64(4)):
                            for r_b in T.unroll(T.int64(4)):
                                with T.block("data_pack_update"):
                                    v_eps, v_nu = T.axis.remap("SS", [eps, nu])
                                    v_ci = T.axis.spatial(T.int64(512), (ci_p_fused_0 * T.int64(512) + ci_p_fused_1) // T.int64(16))
                                    v_p = T.axis.spatial(T.int64(16), (ci_p_fused_0 * T.int64(512) + ci_p_fused_1) % T.int64(16))
                                    v_r_a, v_r_b = T.axis.remap("RR", [r_a, r_b])
                                    T.reads(data_pack_local[v_eps, v_nu, v_ci, v_p], input_tile_local[v_ci, v_p, v_r_a, v_r_b])
                                    T.writes(data_pack_local[v_eps, v_nu, v_ci, v_p])
                                    T.block_attr({"schedule_rule": "conv2d_nchw_winograd_data_pack"})
                                    data_pack_local[v_eps, v_nu, v_ci, v_p] = data_pack_local[v_eps, v_nu, v_ci, v_p] + input_tile_local[v_ci, v_p, v_r_a, v_r_b] * T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(3), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(1), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(0), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(3), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(1), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(0), T.float32(1), T.float32(0))))))))))))))))) * T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(3), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(1), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(0), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(3), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(1), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(0), T.float32(1), T.float32(0)))))))))))))))))
                for ax0, ax1, ax2, ax3 in T.grid(T.int64(4), T.int64(4), T.int64(1), T.int64(1)):
                    with T.block("data_pack_local"):
                        v0, v1 = T.axis.remap("SS", [ax0, ax1])
                        v2 = T.axis.spatial(T.int64(512), ci_p_fused_0 * T.int64(32) + ci_p_fused_1 // T.int64(16) + ax2)
                        v3 = T.axis.spatial(T.int64(16), ci_p_fused_1 % T.int64(16) + ax3)
                        T.reads(data_pack_local[v0, v1, v2, v3])
                        T.writes(data_pack[v0, v1, v2, v3])
                        data_pack[v0, v1, v2, v3] = data_pack_local[v0, v1, v2, v3]
        for eps_0_nu_0_co_0_p_0_fused in T.thread_binding(T.int64(8), thread="blockIdx.x"):
            for eps_1_nu_1_co_1_p_1_fused in T.thread_binding(T.int64(2), thread="vthread.x"):
                for eps_2_nu_2_co_2_p_2_fused in T.thread_binding(T.int64(1024), thread="threadIdx.x"):
                    for eps_3_init, nu_3_init, co_3_init, p_3_init, eps_4_init, nu_4_init, co_4_init, p_4_init in T.grid(T.int64(1), T.int64(2), T.int64(2), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(2)):
                        with T.block("bgemm_init"):
                            v_eps = T.axis.spatial(T.int64(4), eps_2_nu_2_co_2_p_2_fused // T.int64(256) + eps_3_init + eps_4_init)
                            v_nu = T.axis.spatial(T.int64(4), eps_2_nu_2_co_2_p_2_fused % T.int64(256) // T.int64(128) * T.int64(2) + nu_3_init + nu_4_init)
                            v_co = T.axis.spatial(T.int64(512), eps_0_nu_0_co_0_p_0_fused // T.int64(2) * T.int64(128) + eps_1_nu_1_co_1_p_1_fused * T.int64(64) + eps_2_nu_2_co_2_p_2_fused % T.int64(128) // T.int64(4) * T.int64(2) + co_3_init + co_4_init)
                            v_p = T.axis.spatial(T.int64(16), eps_0_nu_0_co_0_p_0_fused % T.int64(2) * T.int64(8) + eps_2_nu_2_co_2_p_2_fused % T.int64(4) * T.int64(2) + p_3_init * T.int64(2) + p_4_init)
                            T.reads()
                            T.writes(bgemm_local[v_eps, v_nu, v_co, v_p])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            bgemm_local[v_eps, v_nu, v_co, v_p] = T.float32(0)
                    for ci_0 in range(T.int64(128)):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(1)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(1024), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("data_pack_shared"):
                                        v0 = T.axis.spatial(T.int64(4), (ax0_ax1_ax2_ax3_fused_0 * T.int64(4096) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) // T.int64(128))
                                        v1 = T.axis.spatial(T.int64(4), (ax0_ax1_ax2_ax3_fused_0 * T.int64(4096) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) % T.int64(128) // T.int64(32))
                                        v2 = T.axis.spatial(T.int64(512), ci_0 * T.int64(4) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(4096) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) % T.int64(32) // T.int64(8))
                                        v3 = T.axis.spatial(T.int64(16), eps_0_nu_0_co_0_p_0_fused % T.int64(2) * T.int64(8) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(4096) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) % T.int64(8))
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * T.int64(1024) + ax0_ax1_ax2_ax3_fused_1) * T.int64(4) + ax0_ax1_ax2_ax3_fused_2 < T.int64(512))
                                        T.reads(data_pack[v0, v1, v2, v3])
                                        T.writes(data_pack_shared[v0, v1, v2, v3])
                                        data_pack_shared[v0, v1, v2, v3] = data_pack[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(8)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(1024), thread="threadIdx.x"):
                                with T.block("p1_shared"):
                                    v0 = T.axis.spatial(T.int64(4), (ax0_ax1_ax2_ax3_fused_0 * T.int64(1024) + ax0_ax1_ax2_ax3_fused_1) // T.int64(2048))
                                    v1 = T.axis.spatial(T.int64(4), (ax0_ax1_ax2_ax3_fused_0 * T.int64(1024) + ax0_ax1_ax2_ax3_fused_1) % T.int64(2048) // T.int64(512))
                                    v2 = T.axis.spatial(T.int64(512), ci_0 * T.int64(4) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(1024) + ax0_ax1_ax2_ax3_fused_1) % T.int64(512) // T.int64(128))
                                    v3 = T.axis.spatial(T.int64(512), eps_0_nu_0_co_0_p_0_fused // T.int64(2) * T.int64(128) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(1024) + ax0_ax1_ax2_ax3_fused_1) % T.int64(128))
                                    T.reads(p1[v0, v1, v2, v3])
                                    T.writes(p1_shared[v0, v1, v2, v3])
                                    p1_shared[v0, v1, v2, v3] = p1[v0, v1, v2, v3]
                        for ci_1, eps_3, nu_3, co_3, p_3, ci_2, eps_4, nu_4, co_4, p_4 in T.grid(T.int64(4), T.int64(1), T.int64(2), T.int64(2), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(2)):
                            with T.block("bgemm_update"):
                                v_eps = T.axis.spatial(T.int64(4), eps_2_nu_2_co_2_p_2_fused // T.int64(256) + eps_3 + eps_4)
                                v_nu = T.axis.spatial(T.int64(4), eps_2_nu_2_co_2_p_2_fused % T.int64(256) // T.int64(128) * T.int64(2) + nu_3 + nu_4)
                                v_co = T.axis.spatial(T.int64(512), eps_0_nu_0_co_0_p_0_fused // T.int64(2) * T.int64(128) + eps_1_nu_1_co_1_p_1_fused * T.int64(64) + eps_2_nu_2_co_2_p_2_fused % T.int64(128) // T.int64(4) * T.int64(2) + co_3 + co_4)
                                v_p = T.axis.spatial(T.int64(16), eps_0_nu_0_co_0_p_0_fused % T.int64(2) * T.int64(8) + eps_2_nu_2_co_2_p_2_fused % T.int64(4) * T.int64(2) + p_3 * T.int64(2) + p_4)
                                v_ci = T.axis.reduce(T.int64(512), ci_0 * T.int64(4) + ci_1 + ci_2)
                                T.reads(bgemm_local[v_eps, v_nu, v_co, v_p], data_pack_shared[v_eps, v_nu, v_ci, v_p], p1_shared[v_eps, v_nu, v_ci, v_co])
                                T.writes(bgemm_local[v_eps, v_nu, v_co, v_p])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                bgemm_local[v_eps, v_nu, v_co, v_p] = bgemm_local[v_eps, v_nu, v_co, v_p] + data_pack_shared[v_eps, v_nu, v_ci, v_p] * p1_shared[v_eps, v_nu, v_ci, v_co]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(2), T.int64(2), T.int64(2)):
                        with T.block("bgemm_local"):
                            v0 = T.axis.spatial(T.int64(4), eps_2_nu_2_co_2_p_2_fused // T.int64(256) + ax0)
                            v1 = T.axis.spatial(T.int64(4), eps_2_nu_2_co_2_p_2_fused % T.int64(256) // T.int64(128) * T.int64(2) + ax1)
                            v2 = T.axis.spatial(T.int64(512), eps_0_nu_0_co_0_p_0_fused // T.int64(2) * T.int64(128) + eps_1_nu_1_co_1_p_1_fused * T.int64(64) + eps_2_nu_2_co_2_p_2_fused % T.int64(128) // T.int64(4) * T.int64(2) + ax2)
                            v3 = T.axis.spatial(T.int64(16), eps_0_nu_0_co_0_p_0_fused % T.int64(2) * T.int64(8) + eps_2_nu_2_co_2_p_2_fused % T.int64(4) * T.int64(2) + ax3)
                            T.reads(bgemm_local[v0, v1, v2, v3])
                            T.writes(bgemm[v0, v1, v2, v3])
                            bgemm[v0, v1, v2, v3] = bgemm_local[v0, v1, v2, v3]
        for n_co_h_0_w_0_fused_0 in T.thread_binding(T.int64(128), thread="blockIdx.x"):
            for n_co_h_0_w_0_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                    for ax2 in T.unroll(T.int64(2)):
                        for ax3 in T.unroll(T.int64(2)):
                            with T.block("inverse_init"):
                                v_co = T.axis.spatial(T.int64(512), (n_co_h_0_w_0_fused_0 * T.int64(64) + n_co_h_0_w_0_fused_1) // T.int64(16) + ax0)
                                v_p = T.axis.spatial(T.int64(16), (n_co_h_0_w_0_fused_0 * T.int64(64) + n_co_h_0_w_0_fused_1) % T.int64(16) + ax1)
                                v_vh, v_vw = T.axis.remap("SS", [ax2, ax3])
                                T.reads()
                                T.writes(inverse_local[v_co, v_p, v_vh, v_vw])
                                T.block_attr({"schedule_rule": "conv2d_nchw_winograd_inverse"})
                                inverse_local[v_co, v_p, v_vh, v_vw] = T.float32(0)
                            for ax4 in T.unroll(T.int64(4)):
                                for ax5 in T.unroll(T.int64(4)):
                                    with T.block("inverse_update"):
                                        v_co = T.axis.spatial(T.int64(512), (n_co_h_0_w_0_fused_0 * T.int64(64) + n_co_h_0_w_0_fused_1) // T.int64(16) + ax0)
                                        v_p = T.axis.spatial(T.int64(16), (n_co_h_0_w_0_fused_0 * T.int64(64) + n_co_h_0_w_0_fused_1) % T.int64(16) + ax1)
                                        v_vh, v_vw, v_r_a, v_r_b = T.axis.remap("SSRR", [ax2, ax3, ax4, ax5])
                                        T.reads(inverse_local[v_co, v_p, v_vh, v_vw], bgemm[v_r_a, v_r_b, v_co, v_p])
                                        T.writes(inverse_local[v_co, v_p, v_vh, v_vw])
                                        T.block_attr({"schedule_rule": "conv2d_nchw_winograd_inverse"})
                                        inverse_local[v_co, v_p, v_vh, v_vw] = inverse_local[v_co, v_p, v_vh, v_vw] + bgemm[v_r_a, v_r_b, v_co, v_p] * T.Select(v_r_a % T.int64(4) == T.int64(3) and v_vh % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_vh % T.int64(2) == T.int64(0), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_vh % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_vh % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_vh % T.int64(2) == T.int64(1), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_vh % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_vh % T.int64(2) == T.int64(1), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_vh % T.int64(2) == T.int64(0), T.float32(1), T.float32(0))))))))) * T.Select(v_r_b % T.int64(4) == T.int64(3) and v_vw % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_vw % T.int64(2) == T.int64(0), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_vw % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_vw % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_vw % T.int64(2) == T.int64(1), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_vw % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_vw % T.int64(2) == T.int64(1), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_vw % T.int64(2) == T.int64(0), T.float32(1), T.float32(0)))))))))
                for h_1, w_1 in T.grid(T.int64(2), T.int64(2)):
                    with T.block("conv2d_winograd"):
                        v_n = T.axis.spatial(T.int64(1), T.int64(0))
                        v_co = T.axis.spatial(T.int64(512), (n_co_h_0_w_0_fused_0 * T.int64(64) + n_co_h_0_w_0_fused_1) // T.int64(16))
                        v_h = T.axis.spatial(T.int64(7), (n_co_h_0_w_0_fused_0 * T.int64(64) + n_co_h_0_w_0_fused_1) % T.int64(16) // T.int64(4) * T.int64(2) + h_1)
                        v_w = T.axis.spatial(T.int64(7), (n_co_h_0_w_0_fused_0 * T.int64(64) + n_co_h_0_w_0_fused_1) % T.int64(4) * T.int64(2) + w_1)
                        T.where((n_co_h_0_w_0_fused_0 * T.int64(64) + n_co_h_0_w_0_fused_1) % T.int64(16) // T.int64(4) * T.int64(2) + h_1 < T.int64(7) and (n_co_h_0_w_0_fused_0 * T.int64(64) + n_co_h_0_w_0_fused_1) % T.int64(4) * T.int64(2) + w_1 < T.int64(7))
                        T.reads(inverse_local[v_co, v_n * T.int64(16) + v_h // T.int64(2) * T.int64(4) + v_w // T.int64(2), v_h % T.int64(2), v_w % T.int64(2)], p2[v_n, v_co, v_h, v_w], p3[v_n, v_co, T.int64(0), T.int64(0)], p4[v_n, v_co, T.int64(0), T.int64(0)])
                        T.writes(T_relu[v_n, v_co, v_h, v_w])
                        T_relu[v_n, v_co, v_h, v_w] = T.max((inverse_local[v_co, v_n * T.int64(16) + v_h // T.int64(2) * T.int64(4) + v_w // T.int64(2), v_h % T.int64(2), v_w % T.int64(2)] + p2[v_n, v_co, v_h, v_w]) * p3[v_n, v_co, T.int64(0), T.int64(0)] + p4[v_n, v_co, T.int64(0), T.int64(0)], T.float32(0))
b0 = sch.get_block(name="data_pack", func_name="main")
b1 = sch.get_block(name="bgemm", func_name="main")
b2 = sch.get_block(name="inverse", func_name="main")
b3 = sch.get_block(name="conv2d_winograd", func_name="main")
b4 = sch.get_block(name="T_add", func_name="main")
b5 = sch.get_block(name="T_multiply", func_name="main")
b6 = sch.get_block(name="T_add_1", func_name="main")
b7 = sch.get_block(name="T_relu", func_name="main")
b8 = sch.get_block(name="root", func_name="main")
b9, b10 = sch.get_producers(block=b2)
sch.compute_inline(block=b10)
b11, = sch.get_consumers(block=b2)
l12, l13, l14, l15 = sch.get_loops(block=b11)
l16, l17 = sch.split(loop=l14, factors=[None, 2], preserve_unit_iters=True)
l18, l19 = sch.split(loop=l15, factors=[None, 2], preserve_unit_iters=True)
sch.reorder(l16, l18, l17, l19)
sch.compute_at(block=b2, loop=l18, preserve_unit_loops=True, index=-1)
sch.set_scope(block=b2, buffer_index=0, storage_scope="local")
l20, l21, l22, l23, l24, l25, l26, l27, l28, l29 = sch.get_loops(block=b2)
sch.unroll(loop=l26)
sch.unroll(loop=l27)
sch.unroll(loop=l28)
sch.unroll(loop=l29)
b30, b31 = sch.get_producers(block=b0)
sch.compute_inline(block=b31)
b32, = sch.get_producers(block=b30)
l33, l34, l35, l36, l37, l38 = sch.get_loops(block=b0)
sch.reorder(l35, l36, l33, l34, l37, l38)
sch.unroll(loop=l33)
sch.unroll(loop=l34)
sch.unroll(loop=l37)
sch.unroll(loop=l38)
l39 = sch.fuse(l35, l36, preserve_unit_iters=True)
v40 = sch.sample_categorical(candidates=[32, 64, 128, 256, 512, 1024], probs=[0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666], decision=4)
l41, l42 = sch.split(loop=l39, factors=[None, v40], preserve_unit_iters=True)
sch.bind(loop=l41, thread_axis="blockIdx.x")
sch.bind(loop=l42, thread_axis="threadIdx.x")
b43 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b43, loop=l42, preserve_unit_loops=True, index=-1)
sch.compute_at(block=b30, loop=l42, preserve_unit_loops=True, index=-1)
sch.set_scope(block=b30, buffer_index=0, storage_scope="local")
sch.compute_inline(block=b32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l44, l45, l46, l47, l48 = sch.get_loops(block=b1)
v49, v50, v51, v52, v53 = sch.sample_perfect_tile(loop=l44, n=5, max_innermost_factor=64, decision=[1, 1, 4, 1, 1])
l54, l55, l56, l57, l58 = sch.split(loop=l44, factors=[v49, v50, v51, v52, v53], preserve_unit_iters=True)
v59, v60, v61, v62, v63 = sch.sample_perfect_tile(loop=l45, n=5, max_innermost_factor=64, decision=[1, 1, 2, 2, 1])
l64, l65, l66, l67, l68 = sch.split(loop=l45, factors=[v59, v60, v61, v62, v63], preserve_unit_iters=True)
v69, v70, v71, v72, v73 = sch.sample_perfect_tile(loop=l46, n=5, max_innermost_factor=64, decision=[4, 2, 32, 2, 1])
l74, l75, l76, l77, l78 = sch.split(loop=l46, factors=[v69, v70, v71, v72, v73], preserve_unit_iters=True)
v79, v80, v81, v82, v83 = sch.sample_perfect_tile(loop=l47, n=5, max_innermost_factor=64, decision=[2, 1, 4, 1, 2])
l84, l85, l86, l87, l88 = sch.split(loop=l47, factors=[v79, v80, v81, v82, v83], preserve_unit_iters=True)
v89, v90, v91 = sch.sample_perfect_tile(loop=l48, n=3, max_innermost_factor=64, decision=[128, 4, 1])
l92, l93, l94 = sch.split(loop=l48, factors=[v89, v90, v91], preserve_unit_iters=True)
sch.reorder(l54, l64, l74, l84, l55, l65, l75, l85, l56, l66, l76, l86, l92, l93, l57, l67, l77, l87, l94, l58, l68, l78, l88)
l95 = sch.fuse(l54, l64, l74, l84, preserve_unit_iters=True)
sch.bind(loop=l95, thread_axis="blockIdx.x")
l96 = sch.fuse(l55, l65, l75, l85, preserve_unit_iters=True)
sch.bind(loop=l96, thread_axis="vthread.x")
l97 = sch.fuse(l56, l66, l76, l86, preserve_unit_iters=True)
sch.bind(loop=l97, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b98 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b98, loop=l97, preserve_unit_loops=True, index=-1)
b99 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b99, loop=l92, preserve_unit_loops=True, index=-1)
l100, l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b99)
l108 = sch.fuse(l104, l105, l106, l107, preserve_unit_iters=True)
v109 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b99, ann_key="meta_schedule.cooperative_fetch", ann_val=v109)
b110 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b110, loop=l92, preserve_unit_loops=True, index=-1)
l111, l112, l113, l114, l115, l116, l117, l118 = sch.get_loops(block=b110)
l119 = sch.fuse(l115, l116, l117, l118, preserve_unit_iters=True)
v120 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b110, ann_key="meta_schedule.cooperative_fetch", ann_val=v120)
sch.reverse_compute_inline(block=b7)
sch.reverse_compute_inline(block=b6)
sch.reverse_compute_inline(block=b5)
sch.reverse_compute_inline(block=b4)
v121 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=0)
sch.annotate(block_or_loop=b8, ann_key="meta_schedule.unroll_explicit", ann_val=v121)
l122, l123, l124, l125, l126, l127 = sch.get_loops(block=b3)
l128 = sch.fuse(l122, l123, l124, l125, preserve_unit_iters=True)
v129 = sch.sample_categorical(candidates=[32, 64, 128, 256, 512, 1024], probs=[0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666], decision=1)
l130, l131 = sch.split(loop=l128, factors=[None, v129], preserve_unit_iters=True)
sch.bind(loop=l130, thread_axis="blockIdx.x")
sch.bind(loop=l131, thread_axis="threadIdx.x")
sch.enter_postproc()
sch.unannotate(block_or_loop=b99, ann_key="meta_schedule.cooperative_fetch")
l132, l133, l134, l135, l136 = sch.get_loops(block=b99)
l137, l138, l139 = sch.split(loop=l136, factors=[None, 1024, 4], preserve_unit_iters=True)
sch.vectorize(loop=l139)
sch.bind(loop=l138, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b110, ann_key="meta_schedule.cooperative_fetch")
l140, l141, l142, l143, l144 = sch.get_loops(block=b110)
l145, l146 = sch.split(loop=l144, factors=[None, 1024], preserve_unit_iters=True)
sch.bind(loop=l146, thread_axis="threadIdx.x")
b147 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b147, ann_key="meta_schedule.unroll_explicit")
b148, b149, b150, b151, b152, b153, b154, b155, b156 = sch.get_child_blocks(b147)
l157, l158, l159, l160, l161, l162 = sch.get_loops(block=b148)
l163, l164, l165, l166, l167, l168 = sch.get_loops(block=b149)
l169, l170, l171, l172, l173, l174 = sch.get_loops(block=b150)
l175, l176, l177, l178, l179, l180, l181 = sch.get_loops(block=b151)
l182, l183, l184, l185, l186, l187 = sch.get_loops(block=b152)
l188, l189, l190, l191, l192, l193, l194, l195, l196, l197, l198, l199, l200, l201 = sch.get_loops(block=b153)
l202, l203, l204, l205, l206, l207, l208 = sch.get_loops(block=b154)
l209, l210, l211, l212, l213, l214, l215, l216 = sch.get_loops(block=b155)
l217, l218, l219, l220 = sch.get_loops(block=b156)
b221 = sch.get_block(name="data_pack", func_name="main")
l222, l223, l224, l225, l226, l227 = sch.get_loops(block=b221)
b228 = sch.decompose_reduction(block=b221, loop=l226)
b229 = sch.get_block(name="bgemm", func_name="main")
l230, l231, l232, l233, l234, l235, l236, l237, l238, l239, l240, l241, l242, l243 = sch.get_loops(block=b229)
b244 = sch.decompose_reduction(block=b229, loop=l233)
b245 = sch.get_block(name="inverse", func_name="main")
l246, l247, l248, l249, l250, l251, l252, l253 = sch.get_loops(block=b245)
b254 = sch.decompose_reduction(block=b245, loop=l252)
2023-11-11 03:49:43 [INFO] [task_scheduler.cc:121] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #322: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/home2/xiachunwei/Software/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/home2/xiachunwei/Software/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/home2/xiachunwei/Software/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/home2/xiachunwei/Software/tvm/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/home2/xiachunwei/Software/tvm/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/home2/xiachunwei/Software/tvm/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/home2/xiachunwei/Software/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/home2/xiachunwei/Software/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  4: _ZN3tvm7runtime13PackedFun
  3: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  2: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  0: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  File "/home2/xiachunwei/Software/tvm/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(512), T.int64(7), T.int64(7)), "float32"), p1: T.Buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(512)), "float32"), p2: T.Buffer((T.int64(1), T.int64(512), T.int64(7), T.int64(7)), "float32"), p3: T.Buffer((T.int64(1), T.int64(512), T.int64(1), T.int64(1)), "float32"), p4: T.Buffer((T.int64(1), T.int64(512), T.int64(1), T.int64(1)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(512), T.int64(7), T.int64(7)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        input_tile_local = T.alloc_buffer((T.int64(512), T.int64(16), T.int64(4), T.int64(4)), scope="local")
        data_pack = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(16)))
        bgemm = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(16)))
        inverse_local = T.alloc_buffer((T.int64(512), T.int64(16), T.int64(2), T.int64(2)), scope="local")
        data_pack_local = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(16)), scope="local")
        bgemm_local = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(16)), scope="local")
        data_pack_shared = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(16)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(512)), scope="shared")
        for ci_p_fused_0 in T.thread_binding(T.int64(64), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 1024, "pragma_unroll_explicit": 1}):
            for ci_p_fused_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(1), T.int64(4), T.int64(4)):
                    with T.block("input_tile"):
                        v_ci = T.axis.spatial(T.int64(512), ci_p_fused_0 * T.int64(8) + ci_p_fused_1 // T.int64(16) + ax0)
                        v_p = T.axis.spatial(T.int64(16), ci_p_fused_1 % T.int64(16) + ax1)
                        v_eps, v_nu = T.axis.remap("SS", [ax2, ax3])
                        T.reads(p0[v_p // T.int64(16), v_ci, v_p % T.int64(16) // T.int64(4) * T.int64(2) + v_eps - T.int64(1), v_p % T.int64(4) * T.int64(2) + v_nu - T.int64(1)])
                        T.writes(input_tile_local[v_ci, v_p, v_eps, v_nu])
                        T.block_attr({"schedule_rule": "None"})
                        input_tile_local[v_ci, v_p, v_eps, v_nu] = T.if_then_else(T.int64(1) <= v_p % T.int64(16) // T.int64(4) * T.int64(2) + v_eps and v_p % T.int64(16) // T.int64(4) * T.int64(2) + v_eps < T.int64(8) and T.int64(1) <= v_p % T.int64(4) * T.int64(2) + v_nu and v_p % T.int64(4) * T.int64(2) + v_nu < T.int64(8), p0[v_p // T.int64(16), v_ci, v_p % T.int64(16) // T.int64(4) * T.int64(2) + v_eps - T.int64(1), v_p % T.int64(4) * T.int64(2) + v_nu - T.int64(1)], T.float32(0))
                for eps in T.unroll(T.int64(4)):
                    for nu in T.unroll(T.int64(4)):
                        with T.block("data_pack_init"):
                            v_eps, v_nu = T.axis.remap("SS", [eps, nu])
                            v_ci = T.axis.spatial(T.int64(512), (ci_p_fused_0 * T.int64(128) + ci_p_fused_1) // T.int64(16))
                            v_p = T.axis.spatial(T.int64(16), (ci_p_fused_0 * T.int64(128) + ci_p_fused_1) % T.int64(16))
                            T.reads()
                            T.writes(data_pack_local[v_eps, v_nu, v_ci, v_p])
                            T.block_attr({"schedule_rule": "conv2d_nchw_winograd_data_pack"})
                            data_pack_local[v_eps, v_nu, v_ci, v_p] = T.float32(0)
                        for r_a in T.unroll(T.int64(4)):
                            for r_b in T.unroll(T.int64(4)):
                                with T.block("data_pack_update"):
                                    v_eps, v_nu = T.axis.remap("SS", [eps, nu])
                                    v_ci = T.axis.spatial(T.int64(512), (ci_p_fused_0 * T.int64(128) + ci_p_fused_1) // T.int64(16))
                                    v_p = T.axis.spatial(T.int64(16), (ci_p_fused_0 * T.int64(128) + ci_p_fused_1) % T.int64(16))
                                    v_r_a, v_r_b = T.axis.remap("RR", [r_a, r_b])
                                    T.reads(data_pack_local[v_eps, v_nu, v_ci, v_p], input_tile_local[v_ci, v_p, v_r_a, v_r_b])
                                    T.writes(data_pack_local[v_eps, v_nu, v_ci, v_p])
                                    T.block_attr({"schedule_rule": "conv2d_nchw_winograd_data_pack"})
                                    data_pack_local[v_eps, v_nu, v_ci, v_p] = data_pack_local[v_eps, v_nu, v_ci, v_p] + input_tile_local[v_ci, v_p, v_r_a, v_r_b] * T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(3), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(1), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(0), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(3), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(1), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(0), T.float32(1), T.float32(0))))))))))))))))) * T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(3), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(1), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(0), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(3), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(1), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(0), T.float32(1), T.float32(0)))))))))))))))))
                for ax0, ax1, ax2, ax3 in T.grid(T.int64(4), T.int64(4), T.int64(1), T.int64(1)):
                    with T.block("data_pack_local"):
                        v0, v1 = T.axis.remap("SS", [ax0, ax1])
                        v2 = T.axis.spatial(T.int64(512), ci_p_fused_0 * T.int64(8) + ci_p_fused_1 // T.int64(16) + ax2)
                        v3 = T.axis.spatial(T.int64(16), ci_p_fused_1 % T.int64(16) + ax3)
                        T.reads(data_pack_local[v0, v1, v2, v3])
                        T.writes(data_pack[v0, v1, v2, v3])
                        data_pack[v0, v1, v2, v3] = data_pack_local[v0, v1, v2, v3]
        for eps_0_nu_0_co_0_p_0_fused in T.thread_binding(T.int64(128), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 1024, "pragma_unroll_explicit": 1}):
            for eps_1_nu_1_co_1_p_1_fused in T.thread_binding(T.int64(2), thread="vthread.x"):
                for eps_2_nu_2_co_2_p_2_fused in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                    for eps_3_init, nu_3_init, co_3_init, p_3_init, eps_4_init, nu_4_init, co_4_init, p_4_init in T.grid(T.int64(1), T.int64(2), T.int64(2), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("bgemm_init"):
                            v_eps = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(32) + eps_3_init + eps_4_init)
                            v_nu = T.axis.spatial(T.int64(4), eps_2_nu_2_co_2_p_2_fused // T.int64(64) * T.int64(2) + nu_3_init + nu_4_init)
                            v_co = T.axis.spatial(T.int64(512), eps_0_nu_0_co_0_p_0_fused % T.int64(32) // T.int64(2) * T.int64(32) + eps_1_nu_1_co_1_p_1_fused * T.int64(16) + eps_2_nu_2_co_2_p_2_fused % T.int64(64) // T.int64(8) * T.int64(2) + co_3_init + co_4_init)
                            v_p = T.axis.spatial(T.int64(16), eps_0_nu_0_co_0_p_0_fused % T.int64(2) * T.int64(8) + eps_2_nu_2_co_2_p_2_fused % T.int64(8) + p_3_init + p_4_init)
                            T.reads()
                            T.writes(bgemm_local[v_eps, v_nu, v_co, v_p])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            bgemm_local[v_eps, v_nu, v_co, v_p] = T.float32(0)
                    for ci_0 in range(T.int64(8)):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(16)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                                with T.block("data_pack_shared"):
                                    v0 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(32))
                                    v1 = T.axis.spatial(T.int64(4), (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1) // T.int64(512))
                                    v2 = T.axis.spatial(T.int64(512), ci_0 * T.int64(64) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1) % T.int64(512) // T.int64(8))
                                    v3 = T.axis.spatial(T.int64(16), eps_0_nu_0_co_0_p_0_fused % T.int64(2) * T.int64(8) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1) % T.int64(8))
                                    T.reads(data_pack[v0, v1, v2, v3])
                                    T.writes(data_pack_shared[v0, v1, v2, v3])
                                    data_pack_shared[v0, v1, v2, v3] = data_pack[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(64)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                                with T.block("p1_shared"):
                                    v0 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(32))
                                    v1 = T.axis.spatial(T.int64(4), (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1) // T.int64(2048))
                                    v2 = T.axis.spatial(T.int64(512), ci_0 * T.int64(64) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1) % T.int64(2048) // T.int64(32))
                                    v3 = T.axis.spatial(T.int64(512), eps_0_nu_0_co_0_p_0_fused % T.int64(32) // T.int64(2) * T.int64(32) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1) % T.int64(32))
                                    T.reads(p1[v0, v1, v2, v3])
                                    T.writes(p1_shared[v0, v1, v2, v3])
                                    p1_shared[v0, v1, v2, v3] = p1[v0, v1, v2, v3]
                        for ci_1, eps_3, nu_3, co_3, p_3, ci_2, eps_4, nu_4, co_4, p_4 in T.grid(T.int64(16), T.int64(1), T.int64(2), T.int64(2), T.int64(1), T.int64(4), T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                            with T.block("bgemm_update"):
                                v_eps = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(32) + eps_3 + eps_4)
                                v_nu = T.axis.spatial(T.int64(4), eps_2_nu_2_co_2_p_2_fused // T.int64(64) * T.int64(2) + nu_3 + nu_4)
                                v_co = T.axis.spatial(T.int64(512), eps_0_nu_0_co_0_p_0_fused % T.int64(32) // T.int64(2) * T.int64(32) + eps_1_nu_1_co_1_p_1_fused * T.int64(16) + eps_2_nu_2_co_2_p_2_fused % T.int64(64) // T.int64(8) * T.int64(2) + co_3 + co_4)
                                v_p = T.axis.spatial(T.int64(16), eps_0_nu_0_co_0_p_0_fused % T.int64(2) * T.int64(8) + eps_2_nu_2_co_2_p_2_fused % T.int64(8) + p_3 + p_4)
                                v_ci = T.axis.reduce(T.int64(512), ci_0 * T.int64(64) + ci_1 * T.int64(4) + ci_2)
                                T.reads(bgemm_local[v_eps, v_nu, v_co, v_p], data_pack_shared[v_eps, v_nu, v_ci, v_p], p1_shared[v_eps, v_nu, v_ci, v_co])
                                T.writes(bgemm_local[v_eps, v_nu, v_co, v_p])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                bgemm_local[v_eps, v_nu, v_co, v_p] = bgemm_local[v_eps, v_nu, v_co, v_p] + data_pack_shared[v_eps, v_nu, v_ci, v_p] * p1_shared[v_eps, v_nu, v_ci, v_co]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(2), T.int64(2), T.int64(1)):
                        with T.block("bgemm_local"):
                            v0 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(32) + ax0)
                            v1 = T.axis.spatial(T.int64(4), eps_2_nu_2_co_2_p_2_fused // T.int64(64) * T.int64(2) + ax1)
                            v2 = T.axis.spatial(T.int64(512), eps_0_nu_0_co_0_p_0_fused % T.int64(32) // T.int64(2) * T.int64(32) + eps_1_nu_1_co_1_p_1_fused * T.int64(16) + eps_2_nu_2_co_2_p_2_fused % T.int64(64) // T.int64(8) * T.int64(2) + ax2)
                            v3 = T.axis.spatial(T.int64(16), eps_0_nu_0_co_0_p_0_fused % T.int64(2) * T.int64(8) + eps_2_nu_2_co_2_p_2_fused % T.int64(8) + ax3)
                            T.reads(bgemm_local[v0, v1, v2, v3])
                            T.writes(bgemm[v0, v1, v2, v3])
                            bgemm[v0, v1, v2, v3] = bgemm_local[v0, v1, v2, v3]
        for n_co_h_0_w_0_fused_0 in T.thread_binding(T.int64(256), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 1024, "pragma_unroll_explicit": 1}):
            for n_co_h_0_w_0_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                    for ax2 in T.unroll(T.int64(2)):
                        for ax3 in T.unroll(T.int64(2)):
                            with T.block("inverse_init"):
                                v_co = T.axis.spatial(T.int64(512), (n_co_h_0_w_0_fused_0 * T.int64(32) + n_co_h_0_w_0_fused_1) // T.int64(16) + ax0)
                                v_p = T.axis.spatial(T.int64(16), (n_co_h_0_w_0_fused_0 * T.int64(32) + n_co_h_0_w_0_fused_1) % T.int64(16) + ax1)
                                v_vh, v_vw = T.axis.remap("SS", [ax2, ax3])
                                T.reads()
                                T.writes(inverse_local[v_co, v_p, v_vh, v_vw])
                                T.block_attr({"schedule_rule": "conv2d_nchw_winograd_inverse"})
                                inverse_local[v_co, v_p, v_vh, v_vw] = T.float32(0)
                            for ax4 in T.unroll(T.int64(4)):
                                for ax5 in T.unroll(T.int64(4)):
                                    with T.block("inverse_update"):
                                        v_co = T.axis.spatial(T.int64(512), (n_co_h_0_w_0_fused_0 * T.int64(32) + n_co_h_0_w_0_fused_1) // T.int64(16) + ax0)
                                        v_p = T.axis.spatial(T.int64(16), (n_co_h_0_w_0_fused_0 * T.int64(32) + n_co_h_0_w_0_fused_1) % T.int64(16) + ax1)
                                        v_vh, v_vw, v_r_a, v_r_b = T.axis.remap("SSRR", [ax2, ax3, ax4, ax5])
                                        T.reads(inverse_local[v_co, v_p, v_vh, v_vw], bgemm[v_r_a, v_r_b, v_co, v_p])
                                        T.writes(inverse_local[v_co, v_p, v_vh, v_vw])
                                        T.block_attr({"schedule_rule": "conv2d_nchw_winograd_inverse"})
                                        inverse_local[v_co, v_p, v_vh, v_vw] = inverse_local[v_co, v_p, v_vh, v_vw] + bgemm[v_r_a, v_r_b, v_co, v_p] * T.Select(v_r_a % T.int64(4) == T.int64(3) and v_vh % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_vh % T.int64(2) == T.int64(0), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_vh % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_vh % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_vh % T.int64(2) == T.int64(1), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_vh % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_vh % T.int64(2) == T.int64(1), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_vh % T.int64(2) == T.int64(0), T.float32(1), T.float32(0))))))))) * T.Select(v_r_b % T.int64(4) == T.int64(3) and v_vw % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_vw % T.int64(2) == T.int64(0), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_vw % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_vw % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_vw % T.int64(2) == T.int64(1), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_vw % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_vw % T.int64(2) == T.int64(1), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_vw % T.int64(2) == T.int64(0), T.float32(1), T.float32(0)))))))))
                for h_1, w_1 in T.grid(T.int64(2), T.int64(2)):
                    with T.block("conv2d_winograd"):
                        v_n = T.axis.spatial(T.int64(1), T.int64(0))
                        v_co = T.axis.spatial(T.int64(512), (n_co_h_0_w_0_fused_0 * T.int64(32) + n_co_h_0_w_0_fused_1) // T.int64(16))
                        v_h = T.axis.spatial(T.int64(7), (n_co_h_0_w_0_fused_0 * T.int64(32) + n_co_h_0_w_0_fused_1) % T.int64(16) // T.int64(4) * T.int64(2) + h_1)
                        v_w = T.axis.spatial(T.int64(7), (n_co_h_0_w_0_fused_0 * T.int64(32) + n_co_h_0_w_0_fused_1) % T.int64(4) * T.int64(2) + w_1)
                        T.where((n_co_h_0_w_0_fused_0 * T.int64(32) + n_co_h_0_w_0_fused_1) % T.int64(16) // T.int64(4) * T.int64(2) + h_1 < T.int64(7) and (n_co_h_0_w_0_fused_0 * T.int64(32) + n_co_h_0_w_0_fused_1) % T.int64(4) * T.int64(2) + w_1 < T.int64(7))
                        T.reads(inverse_local[v_co, v_n * T.int64(16) + v_h // T.int64(2) * T.int64(4) + v_w // T.int64(2), v_h % T.int64(2), v_w % T.int64(2)], p2[v_n, v_co, v_h, v_w], p3[v_n, v_co, T.int64(0), T.int64(0)], p4[v_n, v_co, T.int64(0), T.int64(0)])
                        T.writes(T_relu[v_n, v_co, v_h, v_w])
                        T_relu[v_n, v_co, v_h, v_w] = T.max((inverse_local[v_co, v_n * T.int64(16) + v_h // T.int64(2) * T.int64(4) + v_w // T.int64(2), v_h % T.int64(2), v_w % T.int64(2)] + p2[v_n, v_co, v_h, v_w]) * p3[v_n, v_co, T.int64(0), T.int64(0)] + p4[v_n, v_co, T.int64(0), T.int64(0)], T.float32(0))
b0 = sch.get_block(name="data_pack", func_name="main")
b1 = sch.get_block(name="bgemm", func_name="main")
b2 = sch.get_block(name="inverse", func_name="main")
b3 = sch.get_block(name="conv2d_winograd", func_name="main")
b4 = sch.get_block(name="T_add", func_name="main")
b5 = sch.get_block(name="T_multiply", func_name="main")
b6 = sch.get_block(name="T_add_1", func_name="main")
b7 = sch.get_block(name="T_relu", func_name="main")
b8 = sch.get_block(name="root", func_name="main")
b9, b10 = sch.get_producers(block=b2)
sch.compute_inline(block=b10)
b11, = sch.get_consumers(block=b2)
l12, l13, l14, l15 = sch.get_loops(block=b11)
l16, l17 = sch.split(loop=l14, factors=[None, 2], preserve_unit_iters=True)
l18, l19 = sch.split(loop=l15, factors=[None, 2], preserve_unit_iters=True)
sch.reorder(l16, l18, l17, l19)
sch.compute_at(block=b2, loop=l18, preserve_unit_loops=True, index=-1)
sch.set_scope(block=b2, buffer_index=0, storage_scope="local")
l20, l21, l22, l23, l24, l25, l26, l27, l28, l29 = sch.get_loops(block=b2)
sch.unroll(loop=l26)
sch.unroll(loop=l27)
sch.unroll(loop=l28)
sch.unroll(loop=l29)
b30, b31 = sch.get_producers(block=b0)
sch.compute_inline(block=b31)
b32, = sch.get_producers(block=b30)
l33, l34, l35, l36, l37, l38 = sch.get_loops(block=b0)
sch.reorder(l35, l36, l33, l34, l37, l38)
sch.unroll(loop=l33)
sch.unroll(loop=l34)
sch.unroll(loop=l37)
sch.unroll(loop=l38)
l39 = sch.fuse(l35, l36, preserve_unit_iters=True)
v40 = sch.sample_categorical(candidates=[32, 64, 128, 256, 512, 1024], probs=[0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666], decision=2)
l41, l42 = sch.split(loop=l39, factors=[None, v40], preserve_unit_iters=True)
sch.bind(loop=l41, thread_axis="blockIdx.x")
sch.bind(loop=l42, thread_axis="threadIdx.x")
b43 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b43, loop=l42, preserve_unit_loops=True, index=-1)
sch.compute_at(block=b30, loop=l42, preserve_unit_loops=True, index=-1)
sch.set_scope(block=b30, buffer_index=0, storage_scope="local")
sch.compute_inline(block=b32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l44, l45, l46, l47, l48 = sch.get_loops(block=b1)
v49, v50, v51, v52, v53 = sch.sample_perfect_tile(loop=l44, n=5, max_innermost_factor=64, decision=[4, 1, 1, 1, 1])
l54, l55, l56, l57, l58 = sch.split(loop=l44, factors=[v49, v50, v51, v52, v53], preserve_unit_iters=True)
v59, v60, v61, v62, v63 = sch.sample_perfect_tile(loop=l45, n=5, max_innermost_factor=64, decision=[1, 1, 2, 2, 1])
l64, l65, l66, l67, l68 = sch.split(loop=l45, factors=[v59, v60, v61, v62, v63], preserve_unit_iters=True)
v69, v70, v71, v72, v73 = sch.sample_perfect_tile(loop=l46, n=5, max_innermost_factor=64, decision=[16, 2, 8, 2, 1])
l74, l75, l76, l77, l78 = sch.split(loop=l46, factors=[v69, v70, v71, v72, v73], preserve_unit_iters=True)
v79, v80, v81, v82, v83 = sch.sample_perfect_tile(loop=l47, n=5, max_innermost_factor=64, decision=[2, 1, 8, 1, 1])
l84, l85, l86, l87, l88 = sch.split(loop=l47, factors=[v79, v80, v81, v82, v83], preserve_unit_iters=True)
v89, v90, v91 = sch.sample_perfect_tile(loop=l48, n=3, max_innermost_factor=64, decision=[8, 16, 4])
l92, l93, l94 = sch.split(loop=l48, factors=[v89, v90, v91], preserve_unit_iters=True)
sch.reorder(l54, l64, l74, l84, l55, l65, l75, l85, l56, l66, l76, l86, l92, l93, l57, l67, l77, l87, l94, l58, l68, l78, l88)
l95 = sch.fuse(l54, l64, l74, l84, preserve_unit_iters=True)
sch.bind(loop=l95, thread_axis="blockIdx.x")
l96 = sch.fuse(l55, l65, l75, l85, preserve_unit_iters=True)
sch.bind(loop=l96, thread_axis="vthread.x")
l97 = sch.fuse(l56, l66, l76, l86, preserve_unit_iters=True)
sch.bind(loop=l97, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b98 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b98, loop=l97, preserve_unit_loops=True, index=-1)
b99 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b99, loop=l92, preserve_unit_loops=True, index=-1)
l100, l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b99)
l108 = sch.fuse(l104, l105, l106, l107, preserve_unit_iters=True)
v109 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b99, ann_key="meta_schedule.cooperative_fetch", ann_val=v109)
b110 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b110, loop=l92, preserve_unit_loops=True, index=-1)
l111, l112, l113, l114, l115, l116, l117, l118 = sch.get_loops(block=b110)
l119 = sch.fuse(l115, l116, l117, l118, preserve_unit_iters=True)
v120 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b110, ann_key="meta_schedule.cooperative_fetch", ann_val=v120)
sch.reverse_compute_inline(block=b7)
sch.reverse_compute_inline(block=b6)
sch.reverse_compute_inline(block=b5)
sch.reverse_compute_inline(block=b4)
v121 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b8, ann_key="meta_schedule.unroll_explicit", ann_val=v121)
l122, l123, l124, l125, l126, l127 = sch.get_loops(block=b3)
l128 = sch.fuse(l122, l123, l124, l125, preserve_unit_iters=True)
v129 = sch.sample_categorical(candidates=[32, 64, 128, 256, 512, 1024], probs=[0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666], decision=0)
l130, l131 = sch.split(loop=l128, factors=[None, v129], preserve_unit_iters=True)
sch.bind(loop=l130, thread_axis="blockIdx.x")
sch.bind(loop=l131, thread_axis="threadIdx.x")
sch.enter_postproc()
sch.unannotate(block_or_loop=b99, ann_key="meta_schedule.cooperative_fetch")
l132, l133, l134, l135, l136 = sch.get_loops(block=b99)
l137, l138 = sch.split(loop=l136, factors=[None, 128], preserve_unit_iters=True)
sch.bind(loop=l138, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b110, ann_key="meta_schedule.cooperative_fetch")
l139, l140, l141, l142, l143 = sch.get_loops(block=b110)
l144, l145 = sch.split(loop=l143, factors=[None, 128], preserve_unit_iters=True)
sch.bind(loop=l145, thread_axis="threadIdx.x")
b146 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b146, ann_key="meta_schedule.unroll_explicit")
b147, b148, b149, b150, b151, b152, b153, b154, b155 = sch.get_child_blocks(b146)
l156, l157, l158, l159, l160, l161 = sch.get_loops(block=b147)
l162, l163, l164, l165, l166, l167 = sch.get_loops(block=b148)
sch.annotate(block_or_loop=l162, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l162, ann_key="pragma_unroll_explicit", ann_val=1)
l168, l169, l170, l171, l172, l173 = sch.get_loops(block=b149)
l174, l175, l176, l177, l178, l179 = sch.get_loops(block=b150)
l180, l181, l182, l183, l184, l185 = sch.get_loops(block=b151)
l186, l187, l188, l189, l190, l191, l192, l193, l194, l195, l196, l197, l198, l199 = sch.get_loops(block=b152)
sch.annotate(block_or_loop=l186, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l186, ann_key="pragma_unroll_explicit", ann_val=1)
l200, l201, l202, l203, l204, l205, l206 = sch.get_loops(block=b153)
l207, l208, l209, l210, l211, l212, l213, l214 = sch.get_loops(block=b154)
sch.annotate(block_or_loop=l207, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l207, ann_key="pragma_unroll_explicit", ann_val=1)
l215, l216, l217, l218 = sch.get_loops(block=b155)
b219 = sch.get_block(name="data_pack", func_name="main")
l220, l221, l222, l223, l224, l225 = sch.get_loops(block=b219)
b226 = sch.decompose_reduction(block=b219, loop=l224)
b227 = sch.get_block(name="bgemm", func_name="main")
l228, l229, l230, l231, l232, l233, l234, l235, l236, l237, l238, l239, l240, l241 = sch.get_loops(block=b227)
b242 = sch.decompose_reduction(block=b227, loop=l231)
b243 = sch.get_block(name="inverse", func_name="main")
l244, l245, l246, l247, l248, l249, l250, l251 = sch.get_loops(block=b243)
b252 = sch.decompose_reduction(block=b243, loop=l250)
2023-11-11 03:49:43 [INFO] [task_scheduler.cc:121] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #323: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/home2/xiachunwei/Software/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/home2/xiachunwei/Software/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/home2/xiachunwei/Software/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/home2/xiachunwei/Software/tvm/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/home2/xiachunwei/Software/tvm/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/home2/xiachunwei/Software/tvm/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/home2/xiachunwei/Software/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/home2/xiachunwei/Software/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  4: _ZN3tvm7runtime13PackedFun
  3: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  2: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  0: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  File "/home2/xiachunwei/Software/tvm/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(512), T.int64(7), T.int64(7)), "float32"), p1: T.Buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(512)), "float32"), p2: T.Buffer((T.int64(1), T.int64(512), T.int64(7), T.int64(7)), "float32"), p3: T.Buffer((T.int64(1), T.int64(512), T.int64(1), T.int64(1)), "float32"), p4: T.Buffer((T.int64(1), T.int64(512), T.int64(1), T.int64(1)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(512), T.int64(7), T.int64(7)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        input_tile_local = T.alloc_buffer((T.int64(512), T.int64(16), T.int64(4), T.int64(4)), scope="local")
        data_pack = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(16)))
        bgemm = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(16)))
        inverse_local = T.alloc_buffer((T.int64(512), T.int64(16), T.int64(2), T.int64(2)), scope="local")
        data_pack_local = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(16)), scope="local")
        bgemm_local = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(16)), scope="local")
        data_pack_shared = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(16)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(512)), scope="shared")
        for ci_p_fused_0 in T.thread_binding(T.int64(64), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 1024, "pragma_unroll_explicit": 1}):
            for ci_p_fused_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(1), T.int64(4), T.int64(4)):
                    with T.block("input_tile"):
                        v_ci = T.axis.spatial(T.int64(512), ci_p_fused_0 * T.int64(8) + ci_p_fused_1 // T.int64(16) + ax0)
                        v_p = T.axis.spatial(T.int64(16), ci_p_fused_1 % T.int64(16) + ax1)
                        v_eps, v_nu = T.axis.remap("SS", [ax2, ax3])
                        T.reads(p0[v_p // T.int64(16), v_ci, v_p % T.int64(16) // T.int64(4) * T.int64(2) + v_eps - T.int64(1), v_p % T.int64(4) * T.int64(2) + v_nu - T.int64(1)])
                        T.writes(input_tile_local[v_ci, v_p, v_eps, v_nu])
                        T.block_attr({"schedule_rule": "None"})
                        input_tile_local[v_ci, v_p, v_eps, v_nu] = T.if_then_else(T.int64(1) <= v_p % T.int64(16) // T.int64(4) * T.int64(2) + v_eps and v_p % T.int64(16) // T.int64(4) * T.int64(2) + v_eps < T.int64(8) and T.int64(1) <= v_p % T.int64(4) * T.int64(2) + v_nu and v_p % T.int64(4) * T.int64(2) + v_nu < T.int64(8), p0[v_p // T.int64(16), v_ci, v_p % T.int64(16) // T.int64(4) * T.int64(2) + v_eps - T.int64(1), v_p % T.int64(4) * T.int64(2) + v_nu - T.int64(1)], T.float32(0))
                for eps in T.unroll(T.int64(4)):
                    for nu in T.unroll(T.int64(4)):
                        with T.block("data_pack_init"):
                            v_eps, v_nu = T.axis.remap("SS", [eps, nu])
                            v_ci = T.axis.spatial(T.int64(512), (ci_p_fused_0 * T.int64(128) + ci_p_fused_1) // T.int64(16))
                            v_p = T.axis.spatial(T.int64(16), (ci_p_fused_0 * T.int64(128) + ci_p_fused_1) % T.int64(16))
                            T.reads()
                            T.writes(data_pack_local[v_eps, v_nu, v_ci, v_p])
                            T.block_attr({"schedule_rule": "conv2d_nchw_winograd_data_pack"})
                            data_pack_local[v_eps, v_nu, v_ci, v_p] = T.float32(0)
                        for r_a in T.unroll(T.int64(4)):
                            for r_b in T.unroll(T.int64(4)):
                                with T.block("data_pack_update"):
                                    v_eps, v_nu = T.axis.remap("SS", [eps, nu])
                                    v_ci = T.axis.spatial(T.int64(512), (ci_p_fused_0 * T.int64(128) + ci_p_fused_1) // T.int64(16))
                                    v_p = T.axis.spatial(T.int64(16), (ci_p_fused_0 * T.int64(128) + ci_p_fused_1) % T.int64(16))
                                    v_r_a, v_r_b = T.axis.remap("RR", [r_a, r_b])
                                    T.reads(data_pack_local[v_eps, v_nu, v_ci, v_p], input_tile_local[v_ci, v_p, v_r_a, v_r_b])
                                    T.writes(data_pack_local[v_eps, v_nu, v_ci, v_p])
                                    T.block_attr({"schedule_rule": "conv2d_nchw_winograd_data_pack"})
                                    data_pack_local[v_eps, v_nu, v_ci, v_p] = data_pack_local[v_eps, v_nu, v_ci, v_p] + input_tile_local[v_ci, v_p, v_r_a, v_r_b] * T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(3), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(1), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(0), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(3), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(1), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(0), T.float32(1), T.float32(0))))))))))))))))) * T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(3), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(1), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(0), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(3), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(1), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(0), T.float32(1), T.float32(0)))))))))))))))))
                for ax0, ax1, ax2, ax3 in T.grid(T.int64(4), T.int64(4), T.int64(1), T.int64(1)):
                    with T.block("data_pack_local"):
                        v0, v1 = T.axis.remap("SS", [ax0, ax1])
                        v2 = T.axis.spatial(T.int64(512), ci_p_fused_0 * T.int64(8) + ci_p_fused_1 // T.int64(16) + ax2)
                        v3 = T.axis.spatial(T.int64(16), ci_p_fused_1 % T.int64(16) + ax3)
                        T.reads(data_pack_local[v0, v1, v2, v3])
                        T.writes(data_pack[v0, v1, v2, v3])
                        data_pack[v0, v1, v2, v3] = data_pack_local[v0, v1, v2, v3]
        for eps_0_nu_0_co_0_p_0_fused in T.thread_binding(T.int64(128), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 1024, "pragma_unroll_explicit": 1}):
            for eps_1_nu_1_co_1_p_1_fused in T.thread_binding(T.int64(2), thread="vthread.x"):
                for eps_2_nu_2_co_2_p_2_fused in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                    for eps_3_init, nu_3_init, co_3_init, p_3_init, eps_4_init, nu_4_init, co_4_init, p_4_init in T.grid(T.int64(1), T.int64(2), T.int64(2), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("bgemm_init"):
                            v_eps = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(64) * T.int64(2) + eps_2_nu_2_co_2_p_2_fused // T.int64(64) + eps_3_init + eps_4_init)
                            v_nu = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused % T.int64(64) // T.int64(32) * T.int64(2) + nu_3_init + nu_4_init)
                            v_co = T.axis.spatial(T.int64(512), eps_0_nu_0_co_0_p_0_fused % T.int64(32) // T.int64(2) * T.int64(32) + eps_1_nu_1_co_1_p_1_fused * T.int64(16) + eps_2_nu_2_co_2_p_2_fused % T.int64(64) // T.int64(8) * T.int64(2) + co_3_init + co_4_init)
                            v_p = T.axis.spatial(T.int64(16), eps_0_nu_0_co_0_p_0_fused % T.int64(2) * T.int64(8) + eps_2_nu_2_co_2_p_2_fused % T.int64(8) + p_3_init + p_4_init)
                            T.reads()
                            T.writes(bgemm_local[v_eps, v_nu, v_co, v_p])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            bgemm_local[v_eps, v_nu, v_co, v_p] = T.float32(0)
                    for ci_0 in range(T.int64(8)):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(16)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                                with T.block("data_pack_shared"):
                                    v0 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(64) * T.int64(2) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1) // T.int64(1024))
                                    v1 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused % T.int64(64) // T.int64(32) * T.int64(2) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1) % T.int64(1024) // T.int64(512))
                                    v2 = T.axis.spatial(T.int64(512), ci_0 * T.int64(64) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1) % T.int64(512) // T.int64(8))
                                    v3 = T.axis.spatial(T.int64(16), eps_0_nu_0_co_0_p_0_fused % T.int64(2) * T.int64(8) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1) % T.int64(8))
                                    T.reads(data_pack[v0, v1, v2, v3])
                                    T.writes(data_pack_shared[v0, v1, v2, v3])
                                    data_pack_shared[v0, v1, v2, v3] = data_pack[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(64)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                                with T.block("p1_shared"):
                                    v0 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(64) * T.int64(2) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1) // T.int64(4096))
                                    v1 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused % T.int64(64) // T.int64(32) * T.int64(2) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1) % T.int64(4096) // T.int64(2048))
                                    v2 = T.axis.spatial(T.int64(512), ci_0 * T.int64(64) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1) % T.int64(2048) // T.int64(32))
                                    v3 = T.axis.spatial(T.int64(512), eps_0_nu_0_co_0_p_0_fused % T.int64(32) // T.int64(2) * T.int64(32) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1) % T.int64(32))
                                    T.reads(p1[v0, v1, v2, v3])
                                    T.writes(p1_shared[v0, v1, v2, v3])
                                    p1_shared[v0, v1, v2, v3] = p1[v0, v1, v2, v3]
                        for ci_1, eps_3, nu_3, co_3, p_3, ci_2, eps_4, nu_4, co_4, p_4 in T.grid(T.int64(8), T.int64(1), T.int64(2), T.int64(2), T.int64(1), T.int64(8), T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                            with T.block("bgemm_update"):
                                v_eps = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(64) * T.int64(2) + eps_2_nu_2_co_2_p_2_fused // T.int64(64) + eps_3 + eps_4)
                                v_nu = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused % T.int64(64) // T.int64(32) * T.int64(2) + nu_3 + nu_4)
                                v_co = T.axis.spatial(T.int64(512), eps_0_nu_0_co_0_p_0_fused % T.int64(32) // T.int64(2) * T.int64(32) + eps_1_nu_1_co_1_p_1_fused * T.int64(16) + eps_2_nu_2_co_2_p_2_fused % T.int64(64) // T.int64(8) * T.int64(2) + co_3 + co_4)
                                v_p = T.axis.spatial(T.int64(16), eps_0_nu_0_co_0_p_0_fused % T.int64(2) * T.int64(8) + eps_2_nu_2_co_2_p_2_fused % T.int64(8) + p_3 + p_4)
                                v_ci = T.axis.reduce(T.int64(512), ci_0 * T.int64(64) + ci_1 * T.int64(8) + ci_2)
                                T.reads(bgemm_local[v_eps, v_nu, v_co, v_p], data_pack_shared[v_eps, v_nu, v_ci, v_p], p1_shared[v_eps, v_nu, v_ci, v_co])
                                T.writes(bgemm_local[v_eps, v_nu, v_co, v_p])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                bgemm_local[v_eps, v_nu, v_co, v_p] = bgemm_local[v_eps, v_nu, v_co, v_p] + data_pack_shared[v_eps, v_nu, v_ci, v_p] * p1_shared[v_eps, v_nu, v_ci, v_co]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(2), T.int64(2), T.int64(1)):
                        with T.block("bgemm_local"):
                            v0 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(64) * T.int64(2) + eps_2_nu_2_co_2_p_2_fused // T.int64(64) + ax0)
                            v1 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused % T.int64(64) // T.int64(32) * T.int64(2) + ax1)
                            v2 = T.axis.spatial(T.int64(512), eps_0_nu_0_co_0_p_0_fused % T.int64(32) // T.int64(2) * T.int64(32) + eps_1_nu_1_co_1_p_1_fused * T.int64(16) + eps_2_nu_2_co_2_p_2_fused % T.int64(64) // T.int64(8) * T.int64(2) + ax2)
                            v3 = T.axis.spatial(T.int64(16), eps_0_nu_0_co_0_p_0_fused % T.int64(2) * T.int64(8) + eps_2_nu_2_co_2_p_2_fused % T.int64(8) + ax3)
                            T.reads(bgemm_local[v0, v1, v2, v3])
                            T.writes(bgemm[v0, v1, v2, v3])
                            bgemm[v0, v1, v2, v3] = bgemm_local[v0, v1, v2, v3]
        for n_co_h_0_w_0_fused_0 in T.thread_binding(T.int64(256), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 1024, "pragma_unroll_explicit": 1}):
            for n_co_h_0_w_0_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                    for ax2 in T.unroll(T.int64(2)):
                        for ax3 in T.unroll(T.int64(2)):
                            with T.block("inverse_init"):
                                v_co = T.axis.spatial(T.int64(512), (n_co_h_0_w_0_fused_0 * T.int64(32) + n_co_h_0_w_0_fused_1) // T.int64(16) + ax0)
                                v_p = T.axis.spatial(T.int64(16), (n_co_h_0_w_0_fused_0 * T.int64(32) + n_co_h_0_w_0_fused_1) % T.int64(16) + ax1)
                                v_vh, v_vw = T.axis.remap("SS", [ax2, ax3])
                                T.reads()
                                T.writes(inverse_local[v_co, v_p, v_vh, v_vw])
                                T.block_attr({"schedule_rule": "conv2d_nchw_winograd_inverse"})
                                inverse_local[v_co, v_p, v_vh, v_vw] = T.float32(0)
                            for ax4 in T.unroll(T.int64(4)):
                                for ax5 in T.unroll(T.int64(4)):
                                    with T.block("inverse_update"):
                                        v_co = T.axis.spatial(T.int64(512), (n_co_h_0_w_0_fused_0 * T.int64(32) + n_co_h_0_w_0_fused_1) // T.int64(16) + ax0)
                                        v_p = T.axis.spatial(T.int64(16), (n_co_h_0_w_0_fused_0 * T.int64(32) + n_co_h_0_w_0_fused_1) % T.int64(16) + ax1)
                                        v_vh, v_vw, v_r_a, v_r_b = T.axis.remap("SSRR", [ax2, ax3, ax4, ax5])
                                        T.reads(inverse_local[v_co, v_p, v_vh, v_vw], bgemm[v_r_a, v_r_b, v_co, v_p])
                                        T.writes(inverse_local[v_co, v_p, v_vh, v_vw])
                                        T.block_attr({"schedule_rule": "conv2d_nchw_winograd_inverse"})
                                        inverse_local[v_co, v_p, v_vh, v_vw] = inverse_local[v_co, v_p, v_vh, v_vw] + bgemm[v_r_a, v_r_b, v_co, v_p] * T.Select(v_r_a % T.int64(4) == T.int64(3) and v_vh % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_vh % T.int64(2) == T.int64(0), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_vh % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_vh % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_vh % T.int64(2) == T.int64(1), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_vh % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_vh % T.int64(2) == T.int64(1), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_vh % T.int64(2) == T.int64(0), T.float32(1), T.float32(0))))))))) * T.Select(v_r_b % T.int64(4) == T.int64(3) and v_vw % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_vw % T.int64(2) == T.int64(0), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_vw % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_vw % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_vw % T.int64(2) == T.int64(1), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_vw % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_vw % T.int64(2) == T.int64(1), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_vw % T.int64(2) == T.int64(0), T.float32(1), T.float32(0)))))))))
                for h_1, w_1 in T.grid(T.int64(2), T.int64(2)):
                    with T.block("conv2d_winograd"):
                        v_n = T.axis.spatial(T.int64(1), T.int64(0))
                        v_co = T.axis.spatial(T.int64(512), (n_co_h_0_w_0_fused_0 * T.int64(32) + n_co_h_0_w_0_fused_1) // T.int64(16))
                        v_h = T.axis.spatial(T.int64(7), (n_co_h_0_w_0_fused_0 * T.int64(32) + n_co_h_0_w_0_fused_1) % T.int64(16) // T.int64(4) * T.int64(2) + h_1)
                        v_w = T.axis.spatial(T.int64(7), (n_co_h_0_w_0_fused_0 * T.int64(32) + n_co_h_0_w_0_fused_1) % T.int64(4) * T.int64(2) + w_1)
                        T.where((n_co_h_0_w_0_fused_0 * T.int64(32) + n_co_h_0_w_0_fused_1) % T.int64(16) // T.int64(4) * T.int64(2) + h_1 < T.int64(7) and (n_co_h_0_w_0_fused_0 * T.int64(32) + n_co_h_0_w_0_fused_1) % T.int64(4) * T.int64(2) + w_1 < T.int64(7))
                        T.reads(inverse_local[v_co, v_n * T.int64(16) + v_h // T.int64(2) * T.int64(4) + v_w // T.int64(2), v_h % T.int64(2), v_w % T.int64(2)], p2[v_n, v_co, v_h, v_w], p3[v_n, v_co, T.int64(0), T.int64(0)], p4[v_n, v_co, T.int64(0), T.int64(0)])
                        T.writes(T_relu[v_n, v_co, v_h, v_w])
                        T_relu[v_n, v_co, v_h, v_w] = T.max((inverse_local[v_co, v_n * T.int64(16) + v_h // T.int64(2) * T.int64(4) + v_w // T.int64(2), v_h % T.int64(2), v_w % T.int64(2)] + p2[v_n, v_co, v_h, v_w]) * p3[v_n, v_co, T.int64(0), T.int64(0)] + p4[v_n, v_co, T.int64(0), T.int64(0)], T.float32(0))
b0 = sch.get_block(name="data_pack", func_name="main")
b1 = sch.get_block(name="bgemm", func_name="main")
b2 = sch.get_block(name="inverse", func_name="main")
b3 = sch.get_block(name="conv2d_winograd", func_name="main")
b4 = sch.get_block(name="T_add", func_name="main")
b5 = sch.get_block(name="T_multiply", func_name="main")
b6 = sch.get_block(name="T_add_1", func_name="main")
b7 = sch.get_block(name="T_relu", func_name="main")
b8 = sch.get_block(name="root", func_name="main")
b9, b10 = sch.get_producers(block=b2)
sch.compute_inline(block=b10)
b11, = sch.get_consumers(block=b2)
l12, l13, l14, l15 = sch.get_loops(block=b11)
l16, l17 = sch.split(loop=l14, factors=[None, 2], preserve_unit_iters=True)
l18, l19 = sch.split(loop=l15, factors=[None, 2], preserve_unit_iters=True)
sch.reorder(l16, l18, l17, l19)
sch.compute_at(block=b2, loop=l18, preserve_unit_loops=True, index=-1)
sch.set_scope(block=b2, buffer_index=0, storage_scope="local")
l20, l21, l22, l23, l24, l25, l26, l27, l28, l29 = sch.get_loops(block=b2)
sch.unroll(loop=l26)
sch.unroll(loop=l27)
sch.unroll(loop=l28)
sch.unroll(loop=l29)
b30, b31 = sch.get_producers(block=b0)
sch.compute_inline(block=b31)
b32, = sch.get_producers(block=b30)
l33, l34, l35, l36, l37, l38 = sch.get_loops(block=b0)
sch.reorder(l35, l36, l33, l34, l37, l38)
sch.unroll(loop=l33)
sch.unroll(loop=l34)
sch.unroll(loop=l37)
sch.unroll(loop=l38)
l39 = sch.fuse(l35, l36, preserve_unit_iters=True)
v40 = sch.sample_categorical(candidates=[32, 64, 128, 256, 512, 1024], probs=[0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666], decision=2)
l41, l42 = sch.split(loop=l39, factors=[None, v40], preserve_unit_iters=True)
sch.bind(loop=l41, thread_axis="blockIdx.x")
sch.bind(loop=l42, thread_axis="threadIdx.x")
b43 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b43, loop=l42, preserve_unit_loops=True, index=-1)
sch.compute_at(block=b30, loop=l42, preserve_unit_loops=True, index=-1)
sch.set_scope(block=b30, buffer_index=0, storage_scope="local")
sch.compute_inline(block=b32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l44, l45, l46, l47, l48 = sch.get_loops(block=b1)
v49, v50, v51, v52, v53 = sch.sample_perfect_tile(loop=l44, n=5, max_innermost_factor=64, decision=[2, 1, 2, 1, 1])
l54, l55, l56, l57, l58 = sch.split(loop=l44, factors=[v49, v50, v51, v52, v53], preserve_unit_iters=True)
v59, v60, v61, v62, v63 = sch.sample_perfect_tile(loop=l45, n=5, max_innermost_factor=64, decision=[2, 1, 1, 2, 1])
l64, l65, l66, l67, l68 = sch.split(loop=l45, factors=[v59, v60, v61, v62, v63], preserve_unit_iters=True)
v69, v70, v71, v72, v73 = sch.sample_perfect_tile(loop=l46, n=5, max_innermost_factor=64, decision=[16, 2, 8, 2, 1])
l74, l75, l76, l77, l78 = sch.split(loop=l46, factors=[v69, v70, v71, v72, v73], preserve_unit_iters=True)
v79, v80, v81, v82, v83 = sch.sample_perfect_tile(loop=l47, n=5, max_innermost_factor=64, decision=[2, 1, 8, 1, 1])
l84, l85, l86, l87, l88 = sch.split(loop=l47, factors=[v79, v80, v81, v82, v83], preserve_unit_iters=True)
v89, v90, v91 = sch.sample_perfect_tile(loop=l48, n=3, max_innermost_factor=64, decision=[8, 8, 8])
l92, l93, l94 = sch.split(loop=l48, factors=[v89, v90, v91], preserve_unit_iters=True)
sch.reorder(l54, l64, l74, l84, l55, l65, l75, l85, l56, l66, l76, l86, l92, l93, l57, l67, l77, l87, l94, l58, l68, l78, l88)
l95 = sch.fuse(l54, l64, l74, l84, preserve_unit_iters=True)
sch.bind(loop=l95, thread_axis="blockIdx.x")
l96 = sch.fuse(l55, l65, l75, l85, preserve_unit_iters=True)
sch.bind(loop=l96, thread_axis="vthread.x")
l97 = sch.fuse(l56, l66, l76, l86, preserve_unit_iters=True)
sch.bind(loop=l97, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b98 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b98, loop=l97, preserve_unit_loops=True, index=-1)
b99 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b99, loop=l92, preserve_unit_loops=True, index=-1)
l100, l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b99)
l108 = sch.fuse(l104, l105, l106, l107, preserve_unit_iters=True)
v109 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b99, ann_key="meta_schedule.cooperative_fetch", ann_val=v109)
b110 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b110, loop=l92, preserve_unit_loops=True, index=-1)
l111, l112, l113, l114, l115, l116, l117, l118 = sch.get_loops(block=b110)
l119 = sch.fuse(l115, l116, l117, l118, preserve_unit_iters=True)
v120 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b110, ann_key="meta_schedule.cooperative_fetch", ann_val=v120)
sch.reverse_compute_inline(block=b7)
sch.reverse_compute_inline(block=b6)
sch.reverse_compute_inline(block=b5)
sch.reverse_compute_inline(block=b4)
v121 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b8, ann_key="meta_schedule.unroll_explicit", ann_val=v121)
l122, l123, l124, l125, l126, l127 = sch.get_loops(block=b3)
l128 = sch.fuse(l122, l123, l124, l125, preserve_unit_iters=True)
v129 = sch.sample_categorical(candidates=[32, 64, 128, 256, 512, 1024], probs=[0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666], decision=0)
l130, l131 = sch.split(loop=l128, factors=[None, v129], preserve_unit_iters=True)
sch.bind(loop=l130, thread_axis="blockIdx.x")
sch.bind(loop=l131, thread_axis="threadIdx.x")
sch.enter_postproc()
sch.unannotate(block_or_loop=b99, ann_key="meta_schedule.cooperative_fetch")
l132, l133, l134, l135, l136 = sch.get_loops(block=b99)
l137, l138 = sch.split(loop=l136, factors=[None, 128], preserve_unit_iters=True)
sch.bind(loop=l138, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b110, ann_key="meta_schedule.cooperative_fetch")
l139, l140, l141, l142, l143 = sch.get_loops(block=b110)
l144, l145 = sch.split(loop=l143, factors=[None, 128], preserve_unit_iters=True)
sch.bind(loop=l145, thread_axis="threadIdx.x")
b146 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b146, ann_key="meta_schedule.unroll_explicit")
b147, b148, b149, b150, b151, b152, b153, b154, b155 = sch.get_child_blocks(b146)
l156, l157, l158, l159, l160, l161 = sch.get_loops(block=b147)
l162, l163, l164, l165, l166, l167 = sch.get_loops(block=b148)
sch.annotate(block_or_loop=l162, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l162, ann_key="pragma_unroll_explicit", ann_val=1)
l168, l169, l170, l171, l172, l173 = sch.get_loops(block=b149)
l174, l175, l176, l177, l178, l179 = sch.get_loops(block=b150)
l180, l181, l182, l183, l184, l185 = sch.get_loops(block=b151)
l186, l187, l188, l189, l190, l191, l192, l193, l194, l195, l196, l197, l198, l199 = sch.get_loops(block=b152)
sch.annotate(block_or_loop=l186, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l186, ann_key="pragma_unroll_explicit", ann_val=1)
l200, l201, l202, l203, l204, l205, l206 = sch.get_loops(block=b153)
l207, l208, l209, l210, l211, l212, l213, l214 = sch.get_loops(block=b154)
sch.annotate(block_or_loop=l207, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l207, ann_key="pragma_unroll_explicit", ann_val=1)
l215, l216, l217, l218 = sch.get_loops(block=b155)
b219 = sch.get_block(name="data_pack", func_name="main")
l220, l221, l222, l223, l224, l225 = sch.get_loops(block=b219)
b226 = sch.decompose_reduction(block=b219, loop=l224)
b227 = sch.get_block(name="bgemm", func_name="main")
l228, l229, l230, l231, l232, l233, l234, l235, l236, l237, l238, l239, l240, l241 = sch.get_loops(block=b227)
b242 = sch.decompose_reduction(block=b227, loop=l231)
b243 = sch.get_block(name="inverse", func_name="main")
l244, l245, l246, l247, l248, l249, l250, l251 = sch.get_loops(block=b243)
b252 = sch.decompose_reduction(block=b243, loop=l250)
2023-11-11 03:49:43 [INFO] [task_scheduler.cc:121] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #324: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/home2/xiachunwei/Software/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/home2/xiachunwei/Software/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/home2/xiachunwei/Software/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/home2/xiachunwei/Software/tvm/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/home2/xiachunwei/Software/tvm/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/home2/xiachunwei/Software/tvm/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/home2/xiachunwei/Software/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/home2/xiachunwei/Software/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  4: _ZN3tvm7runtime13PackedFun
  3: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  2: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  0: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  File "/home2/xiachunwei/Software/tvm/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(512), T.int64(7), T.int64(7)), "float32"), p1: T.Buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(512)), "float32"), p2: T.Buffer((T.int64(1), T.int64(512), T.int64(7), T.int64(7)), "float32"), p3: T.Buffer((T.int64(1), T.int64(512), T.int64(1), T.int64(1)), "float32"), p4: T.Buffer((T.int64(1), T.int64(512), T.int64(1), T.int64(1)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(512), T.int64(7), T.int64(7)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        input_tile_local = T.alloc_buffer((T.int64(512), T.int64(16), T.int64(4), T.int64(4)), scope="local")
        data_pack = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(16)))
        bgemm = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(16)))
        inverse_local = T.alloc_buffer((T.int64(512), T.int64(16), T.int64(2), T.int64(2)), scope="local")
        data_pack_local = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(16)), scope="local")
        bgemm_local = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(16)), scope="local")
        data_pack_shared = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(16)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(512)), scope="shared")
        for ci_p_fused_0 in T.thread_binding(T.int64(64), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 512, "pragma_unroll_explicit": 1}):
            for ci_p_fused_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(1), T.int64(4), T.int64(4)):
                    with T.block("input_tile"):
                        v_ci = T.axis.spatial(T.int64(512), ci_p_fused_0 * T.int64(8) + ci_p_fused_1 // T.int64(16) + ax0)
                        v_p = T.axis.spatial(T.int64(16), ci_p_fused_1 % T.int64(16) + ax1)
                        v_eps, v_nu = T.axis.remap("SS", [ax2, ax3])
                        T.reads(p0[v_p // T.int64(16), v_ci, v_p % T.int64(16) // T.int64(4) * T.int64(2) + v_eps - T.int64(1), v_p % T.int64(4) * T.int64(2) + v_nu - T.int64(1)])
                        T.writes(input_tile_local[v_ci, v_p, v_eps, v_nu])
                        T.block_attr({"schedule_rule": "None"})
                        input_tile_local[v_ci, v_p, v_eps, v_nu] = T.if_then_else(T.int64(1) <= v_p % T.int64(16) // T.int64(4) * T.int64(2) + v_eps and v_p % T.int64(16) // T.int64(4) * T.int64(2) + v_eps < T.int64(8) and T.int64(1) <= v_p % T.int64(4) * T.int64(2) + v_nu and v_p % T.int64(4) * T.int64(2) + v_nu < T.int64(8), p0[v_p // T.int64(16), v_ci, v_p % T.int64(16) // T.int64(4) * T.int64(2) + v_eps - T.int64(1), v_p % T.int64(4) * T.int64(2) + v_nu - T.int64(1)], T.float32(0))
                for eps in T.unroll(T.int64(4)):
                    for nu in T.unroll(T.int64(4)):
                        with T.block("data_pack_init"):
                            v_eps, v_nu = T.axis.remap("SS", [eps, nu])
                            v_ci = T.axis.spatial(T.int64(512), (ci_p_fused_0 * T.int64(128) + ci_p_fused_1) // T.int64(16))
                            v_p = T.axis.spatial(T.int64(16), (ci_p_fused_0 * T.int64(128) + ci_p_fused_1) % T.int64(16))
                            T.reads()
                            T.writes(data_pack_local[v_eps, v_nu, v_ci, v_p])
                            T.block_attr({"schedule_rule": "conv2d_nchw_winograd_data_pack"})
                            data_pack_local[v_eps, v_nu, v_ci, v_p] = T.float32(0)
                        for r_a in T.unroll(T.int64(4)):
                            for r_b in T.unroll(T.int64(4)):
                                with T.block("data_pack_update"):
                                    v_eps, v_nu = T.axis.remap("SS", [eps, nu])
                                    v_ci = T.axis.spatial(T.int64(512), (ci_p_fused_0 * T.int64(128) + ci_p_fused_1) // T.int64(16))
                                    v_p = T.axis.spatial(T.int64(16), (ci_p_fused_0 * T.int64(128) + ci_p_fused_1) % T.int64(16))
                                    v_r_a, v_r_b = T.axis.remap("RR", [r_a, r_b])
                                    T.reads(data_pack_local[v_eps, v_nu, v_ci, v_p], input_tile_local[v_ci, v_p, v_r_a, v_r_b])
                                    T.writes(data_pack_local[v_eps, v_nu, v_ci, v_p])
                                    T.block_attr({"schedule_rule": "conv2d_nchw_winograd_data_pack"})
                                    data_pack_local[v_eps, v_nu, v_ci, v_p] = data_pack_local[v_eps, v_nu, v_ci, v_p] + input_tile_local[v_ci, v_p, v_r_a, v_r_b] * T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(3), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(1), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(0), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(3), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(1), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(0), T.float32(1), T.float32(0))))))))))))))))) * T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(3), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(1), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(0), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(3), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(1), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(0), T.float32(1), T.float32(0)))))))))))))))))
                for ax0, ax1, ax2, ax3 in T.grid(T.int64(4), T.int64(4), T.int64(1), T.int64(1)):
                    with T.block("data_pack_local"):
                        v0, v1 = T.axis.remap("SS", [ax0, ax1])
                        v2 = T.axis.spatial(T.int64(512), ci_p_fused_0 * T.int64(8) + ci_p_fused_1 // T.int64(16) + ax2)
                        v3 = T.axis.spatial(T.int64(16), ci_p_fused_1 % T.int64(16) + ax3)
                        T.reads(data_pack_local[v0, v1, v2, v3])
                        T.writes(data_pack[v0, v1, v2, v3])
                        data_pack[v0, v1, v2, v3] = data_pack_local[v0, v1, v2, v3]
        for eps_0_nu_0_co_0_p_0_fused in T.thread_binding(T.int64(128), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 512, "pragma_unroll_explicit": 1}):
            for eps_1_nu_1_co_1_p_1_fused in T.thread_binding(T.int64(2), thread="vthread.x"):
                for eps_2_nu_2_co_2_p_2_fused in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                    for eps_3_init, nu_3_init, co_3_init, p_3_init, eps_4_init, nu_4_init, co_4_init, p_4_init in T.grid(T.int64(1), T.int64(2), T.int64(2), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("bgemm_init"):
                            v_eps = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(64) * T.int64(2) + eps_2_nu_2_co_2_p_2_fused // T.int64(64) + eps_3_init + eps_4_init)
                            v_nu = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused % T.int64(64) // T.int64(32) * T.int64(2) + nu_3_init + nu_4_init)
                            v_co = T.axis.spatial(T.int64(512), eps_0_nu_0_co_0_p_0_fused % T.int64(32) // T.int64(2) * T.int64(32) + eps_1_nu_1_co_1_p_1_fused * T.int64(16) + eps_2_nu_2_co_2_p_2_fused % T.int64(64) // T.int64(8) * T.int64(2) + co_3_init + co_4_init)
                            v_p = T.axis.spatial(T.int64(16), eps_0_nu_0_co_0_p_0_fused % T.int64(2) * T.int64(8) + eps_2_nu_2_co_2_p_2_fused % T.int64(8) + p_3_init + p_4_init)
                            T.reads()
                            T.writes(bgemm_local[v_eps, v_nu, v_co, v_p])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            bgemm_local[v_eps, v_nu, v_co, v_p] = T.float32(0)
                    for ci_0 in range(T.int64(8)):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(16)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                                with T.block("data_pack_shared"):
                                    v0 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(64) * T.int64(2) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1) // T.int64(1024))
                                    v1 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused % T.int64(64) // T.int64(32) * T.int64(2) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1) % T.int64(1024) // T.int64(512))
                                    v2 = T.axis.spatial(T.int64(512), ci_0 * T.int64(64) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1) % T.int64(512) // T.int64(8))
                                    v3 = T.axis.spatial(T.int64(16), eps_0_nu_0_co_0_p_0_fused % T.int64(2) * T.int64(8) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1) % T.int64(8))
                                    T.reads(data_pack[v0, v1, v2, v3])
                                    T.writes(data_pack_shared[v0, v1, v2, v3])
                                    data_pack_shared[v0, v1, v2, v3] = data_pack[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(64)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                                with T.block("p1_shared"):
                                    v0 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(64) * T.int64(2) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1) // T.int64(4096))
                                    v1 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused % T.int64(64) // T.int64(32) * T.int64(2) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1) % T.int64(4096) // T.int64(2048))
                                    v2 = T.axis.spatial(T.int64(512), ci_0 * T.int64(64) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1) % T.int64(2048) // T.int64(32))
                                    v3 = T.axis.spatial(T.int64(512), eps_0_nu_0_co_0_p_0_fused % T.int64(32) // T.int64(2) * T.int64(32) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1) % T.int64(32))
                                    T.reads(p1[v0, v1, v2, v3])
                                    T.writes(p1_shared[v0, v1, v2, v3])
                                    p1_shared[v0, v1, v2, v3] = p1[v0, v1, v2, v3]
                        for ci_1, eps_3, nu_3, co_3, p_3, ci_2, eps_4, nu_4, co_4, p_4 in T.grid(T.int64(16), T.int64(1), T.int64(2), T.int64(2), T.int64(1), T.int64(4), T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                            with T.block("bgemm_update"):
                                v_eps = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(64) * T.int64(2) + eps_2_nu_2_co_2_p_2_fused // T.int64(64) + eps_3 + eps_4)
                                v_nu = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused % T.int64(64) // T.int64(32) * T.int64(2) + nu_3 + nu_4)
                                v_co = T.axis.spatial(T.int64(512), eps_0_nu_0_co_0_p_0_fused % T.int64(32) // T.int64(2) * T.int64(32) + eps_1_nu_1_co_1_p_1_fused * T.int64(16) + eps_2_nu_2_co_2_p_2_fused % T.int64(64) // T.int64(8) * T.int64(2) + co_3 + co_4)
                                v_p = T.axis.spatial(T.int64(16), eps_0_nu_0_co_0_p_0_fused % T.int64(2) * T.int64(8) + eps_2_nu_2_co_2_p_2_fused % T.int64(8) + p_3 + p_4)
                                v_ci = T.axis.reduce(T.int64(512), ci_0 * T.int64(64) + ci_1 * T.int64(4) + ci_2)
                                T.reads(bgemm_local[v_eps, v_nu, v_co, v_p], data_pack_shared[v_eps, v_nu, v_ci, v_p], p1_shared[v_eps, v_nu, v_ci, v_co])
                                T.writes(bgemm_local[v_eps, v_nu, v_co, v_p])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                bgemm_local[v_eps, v_nu, v_co, v_p] = bgemm_local[v_eps, v_nu, v_co, v_p] + data_pack_shared[v_eps, v_nu, v_ci, v_p] * p1_shared[v_eps, v_nu, v_ci, v_co]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(2), T.int64(2), T.int64(1)):
                        with T.block("bgemm_local"):
                            v0 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(64) * T.int64(2) + eps_2_nu_2_co_2_p_2_fused // T.int64(64) + ax0)
                            v1 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused % T.int64(64) // T.int64(32) * T.int64(2) + ax1)
                            v2 = T.axis.spatial(T.int64(512), eps_0_nu_0_co_0_p_0_fused % T.int64(32) // T.int64(2) * T.int64(32) + eps_1_nu_1_co_1_p_1_fused * T.int64(16) + eps_2_nu_2_co_2_p_2_fused % T.int64(64) // T.int64(8) * T.int64(2) + ax2)
                            v3 = T.axis.spatial(T.int64(16), eps_0_nu_0_co_0_p_0_fused % T.int64(2) * T.int64(8) + eps_2_nu_2_co_2_p_2_fused % T.int64(8) + ax3)
                            T.reads(bgemm_local[v0, v1, v2, v3])
                            T.writes(bgemm[v0, v1, v2, v3])
                            bgemm[v0, v1, v2, v3] = bgemm_local[v0, v1, v2, v3]
        for n_co_h_0_w_0_fused_0 in T.thread_binding(T.int64(256), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 512, "pragma_unroll_explicit": 1}):
            for n_co_h_0_w_0_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                    for ax2 in T.unroll(T.int64(2)):
                        for ax3 in T.unroll(T.int64(2)):
                            with T.block("inverse_init"):
                                v_co = T.axis.spatial(T.int64(512), (n_co_h_0_w_0_fused_0 * T.int64(32) + n_co_h_0_w_0_fused_1) // T.int64(16) + ax0)
                                v_p = T.axis.spatial(T.int64(16), (n_co_h_0_w_0_fused_0 * T.int64(32) + n_co_h_0_w_0_fused_1) % T.int64(16) + ax1)
                                v_vh, v_vw = T.axis.remap("SS", [ax2, ax3])
                                T.reads()
                                T.writes(inverse_local[v_co, v_p, v_vh, v_vw])
                                T.block_attr({"schedule_rule": "conv2d_nchw_winograd_inverse"})
                                inverse_local[v_co, v_p, v_vh, v_vw] = T.float32(0)
                            for ax4 in T.unroll(T.int64(4)):
                                for ax5 in T.unroll(T.int64(4)):
                                    with T.block("inverse_update"):
                                        v_co = T.axis.spatial(T.int64(512), (n_co_h_0_w_0_fused_0 * T.int64(32) + n_co_h_0_w_0_fused_1) // T.int64(16) + ax0)
                                        v_p = T.axis.spatial(T.int64(16), (n_co_h_0_w_0_fused_0 * T.int64(32) + n_co_h_0_w_0_fused_1) % T.int64(16) + ax1)
                                        v_vh, v_vw, v_r_a, v_r_b = T.axis.remap("SSRR", [ax2, ax3, ax4, ax5])
                                        T.reads(inverse_local[v_co, v_p, v_vh, v_vw], bgemm[v_r_a, v_r_b, v_co, v_p])
                                        T.writes(inverse_local[v_co, v_p, v_vh, v_vw])
                                        T.block_attr({"schedule_rule": "conv2d_nchw_winograd_inverse"})
                                        inverse_local[v_co, v_p, v_vh, v_vw] = inverse_local[v_co, v_p, v_vh, v_vw] + bgemm[v_r_a, v_r_b, v_co, v_p] * T.Select(v_r_a % T.int64(4) == T.int64(3) and v_vh % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_vh % T.int64(2) == T.int64(0), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_vh % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_vh % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_vh % T.int64(2) == T.int64(1), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_vh % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_vh % T.int64(2) == T.int64(1), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_vh % T.int64(2) == T.int64(0), T.float32(1), T.float32(0))))))))) * T.Select(v_r_b % T.int64(4) == T.int64(3) and v_vw % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_vw % T.int64(2) == T.int64(0), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_vw % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_vw % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_vw % T.int64(2) == T.int64(1), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_vw % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_vw % T.int64(2) == T.int64(1), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_vw % T.int64(2) == T.int64(0), T.float32(1), T.float32(0)))))))))
                for h_1, w_1 in T.grid(T.int64(2), T.int64(2)):
                    with T.block("conv2d_winograd"):
                        v_n = T.axis.spatial(T.int64(1), T.int64(0))
                        v_co = T.axis.spatial(T.int64(512), (n_co_h_0_w_0_fused_0 * T.int64(32) + n_co_h_0_w_0_fused_1) // T.int64(16))
                        v_h = T.axis.spatial(T.int64(7), (n_co_h_0_w_0_fused_0 * T.int64(32) + n_co_h_0_w_0_fused_1) % T.int64(16) // T.int64(4) * T.int64(2) + h_1)
                        v_w = T.axis.spatial(T.int64(7), (n_co_h_0_w_0_fused_0 * T.int64(32) + n_co_h_0_w_0_fused_1) % T.int64(4) * T.int64(2) + w_1)
                        T.where((n_co_h_0_w_0_fused_0 * T.int64(32) + n_co_h_0_w_0_fused_1) % T.int64(16) // T.int64(4) * T.int64(2) + h_1 < T.int64(7) and (n_co_h_0_w_0_fused_0 * T.int64(32) + n_co_h_0_w_0_fused_1) % T.int64(4) * T.int64(2) + w_1 < T.int64(7))
                        T.reads(inverse_local[v_co, v_n * T.int64(16) + v_h // T.int64(2) * T.int64(4) + v_w // T.int64(2), v_h % T.int64(2), v_w % T.int64(2)], p2[v_n, v_co, v_h, v_w], p3[v_n, v_co, T.int64(0), T.int64(0)], p4[v_n, v_co, T.int64(0), T.int64(0)])
                        T.writes(T_relu[v_n, v_co, v_h, v_w])
                        T_relu[v_n, v_co, v_h, v_w] = T.max((inverse_local[v_co, v_n * T.int64(16) + v_h // T.int64(2) * T.int64(4) + v_w // T.int64(2), v_h % T.int64(2), v_w % T.int64(2)] + p2[v_n, v_co, v_h, v_w]) * p3[v_n, v_co, T.int64(0), T.int64(0)] + p4[v_n, v_co, T.int64(0), T.int64(0)], T.float32(0))
b0 = sch.get_block(name="data_pack", func_name="main")
b1 = sch.get_block(name="bgemm", func_name="main")
b2 = sch.get_block(name="inverse", func_name="main")
b3 = sch.get_block(name="conv2d_winograd", func_name="main")
b4 = sch.get_block(name="T_add", func_name="main")
b5 = sch.get_block(name="T_multiply", func_name="main")
b6 = sch.get_block(name="T_add_1", func_name="main")
b7 = sch.get_block(name="T_relu", func_name="main")
b8 = sch.get_block(name="root", func_name="main")
b9, b10 = sch.get_producers(block=b2)
sch.compute_inline(block=b10)
b11, = sch.get_consumers(block=b2)
l12, l13, l14, l15 = sch.get_loops(block=b11)
l16, l17 = sch.split(loop=l14, factors=[None, 2], preserve_unit_iters=True)
l18, l19 = sch.split(loop=l15, factors=[None, 2], preserve_unit_iters=True)
sch.reorder(l16, l18, l17, l19)
sch.compute_at(block=b2, loop=l18, preserve_unit_loops=True, index=-1)
sch.set_scope(block=b2, buffer_index=0, storage_scope="local")
l20, l21, l22, l23, l24, l25, l26, l27, l28, l29 = sch.get_loops(block=b2)
sch.unroll(loop=l26)
sch.unroll(loop=l27)
sch.unroll(loop=l28)
sch.unroll(loop=l29)
b30, b31 = sch.get_producers(block=b0)
sch.compute_inline(block=b31)
b32, = sch.get_producers(block=b30)
l33, l34, l35, l36, l37, l38 = sch.get_loops(block=b0)
sch.reorder(l35, l36, l33, l34, l37, l38)
sch.unroll(loop=l33)
sch.unroll(loop=l34)
sch.unroll(loop=l37)
sch.unroll(loop=l38)
l39 = sch.fuse(l35, l36, preserve_unit_iters=True)
v40 = sch.sample_categorical(candidates=[32, 64, 128, 256, 512, 1024], probs=[0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666], decision=2)
l41, l42 = sch.split(loop=l39, factors=[None, v40], preserve_unit_iters=True)
sch.bind(loop=l41, thread_axis="blockIdx.x")
sch.bind(loop=l42, thread_axis="threadIdx.x")
b43 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b43, loop=l42, preserve_unit_loops=True, index=-1)
sch.compute_at(block=b30, loop=l42, preserve_unit_loops=True, index=-1)
sch.set_scope(block=b30, buffer_index=0, storage_scope="local")
sch.compute_inline(block=b32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l44, l45, l46, l47, l48 = sch.get_loops(block=b1)
v49, v50, v51, v52, v53 = sch.sample_perfect_tile(loop=l44, n=5, max_innermost_factor=64, decision=[2, 1, 2, 1, 1])
l54, l55, l56, l57, l58 = sch.split(loop=l44, factors=[v49, v50, v51, v52, v53], preserve_unit_iters=True)
v59, v60, v61, v62, v63 = sch.sample_perfect_tile(loop=l45, n=5, max_innermost_factor=64, decision=[2, 1, 1, 2, 1])
l64, l65, l66, l67, l68 = sch.split(loop=l45, factors=[v59, v60, v61, v62, v63], preserve_unit_iters=True)
v69, v70, v71, v72, v73 = sch.sample_perfect_tile(loop=l46, n=5, max_innermost_factor=64, decision=[16, 2, 8, 2, 1])
l74, l75, l76, l77, l78 = sch.split(loop=l46, factors=[v69, v70, v71, v72, v73], preserve_unit_iters=True)
v79, v80, v81, v82, v83 = sch.sample_perfect_tile(loop=l47, n=5, max_innermost_factor=64, decision=[2, 1, 8, 1, 1])
l84, l85, l86, l87, l88 = sch.split(loop=l47, factors=[v79, v80, v81, v82, v83], preserve_unit_iters=True)
v89, v90, v91 = sch.sample_perfect_tile(loop=l48, n=3, max_innermost_factor=64, decision=[8, 16, 4])
l92, l93, l94 = sch.split(loop=l48, factors=[v89, v90, v91], preserve_unit_iters=True)
sch.reorder(l54, l64, l74, l84, l55, l65, l75, l85, l56, l66, l76, l86, l92, l93, l57, l67, l77, l87, l94, l58, l68, l78, l88)
l95 = sch.fuse(l54, l64, l74, l84, preserve_unit_iters=True)
sch.bind(loop=l95, thread_axis="blockIdx.x")
l96 = sch.fuse(l55, l65, l75, l85, preserve_unit_iters=True)
sch.bind(loop=l96, thread_axis="vthread.x")
l97 = sch.fuse(l56, l66, l76, l86, preserve_unit_iters=True)
sch.bind(loop=l97, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b98 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b98, loop=l97, preserve_unit_loops=True, index=-1)
b99 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b99, loop=l92, preserve_unit_loops=True, index=-1)
l100, l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b99)
l108 = sch.fuse(l104, l105, l106, l107, preserve_unit_iters=True)
v109 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b99, ann_key="meta_schedule.cooperative_fetch", ann_val=v109)
b110 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b110, loop=l92, preserve_unit_loops=True, index=-1)
l111, l112, l113, l114, l115, l116, l117, l118 = sch.get_loops(block=b110)
l119 = sch.fuse(l115, l116, l117, l118, preserve_unit_iters=True)
v120 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b110, ann_key="meta_schedule.cooperative_fetch", ann_val=v120)
sch.reverse_compute_inline(block=b7)
sch.reverse_compute_inline(block=b6)
sch.reverse_compute_inline(block=b5)
sch.reverse_compute_inline(block=b4)
v121 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b8, ann_key="meta_schedule.unroll_explicit", ann_val=v121)
l122, l123, l124, l125, l126, l127 = sch.get_loops(block=b3)
l128 = sch.fuse(l122, l123, l124, l125, preserve_unit_iters=True)
v129 = sch.sample_categorical(candidates=[32, 64, 128, 256, 512, 1024], probs=[0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666], decision=0)
l130, l131 = sch.split(loop=l128, factors=[None, v129], preserve_unit_iters=True)
sch.bind(loop=l130, thread_axis="blockIdx.x")
sch.bind(loop=l131, thread_axis="threadIdx.x")
sch.enter_postproc()
sch.unannotate(block_or_loop=b99, ann_key="meta_schedule.cooperative_fetch")
l132, l133, l134, l135, l136 = sch.get_loops(block=b99)
l137, l138 = sch.split(loop=l136, factors=[None, 128], preserve_unit_iters=True)
sch.bind(loop=l138, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b110, ann_key="meta_schedule.cooperative_fetch")
l139, l140, l141, l142, l143 = sch.get_loops(block=b110)
l144, l145 = sch.split(loop=l143, factors=[None, 128], preserve_unit_iters=True)
sch.bind(loop=l145, thread_axis="threadIdx.x")
b146 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b146, ann_key="meta_schedule.unroll_explicit")
b147, b148, b149, b150, b151, b152, b153, b154, b155 = sch.get_child_blocks(b146)
l156, l157, l158, l159, l160, l161 = sch.get_loops(block=b147)
l162, l163, l164, l165, l166, l167 = sch.get_loops(block=b148)
sch.annotate(block_or_loop=l162, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l162, ann_key="pragma_unroll_explicit", ann_val=1)
l168, l169, l170, l171, l172, l173 = sch.get_loops(block=b149)
l174, l175, l176, l177, l178, l179 = sch.get_loops(block=b150)
l180, l181, l182, l183, l184, l185 = sch.get_loops(block=b151)
l186, l187, l188, l189, l190, l191, l192, l193, l194, l195, l196, l197, l198, l199 = sch.get_loops(block=b152)
sch.annotate(block_or_loop=l186, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l186, ann_key="pragma_unroll_explicit", ann_val=1)
l200, l201, l202, l203, l204, l205, l206 = sch.get_loops(block=b153)
l207, l208, l209, l210, l211, l212, l213, l214 = sch.get_loops(block=b154)
sch.annotate(block_or_loop=l207, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l207, ann_key="pragma_unroll_explicit", ann_val=1)
l215, l216, l217, l218 = sch.get_loops(block=b155)
b219 = sch.get_block(name="data_pack", func_name="main")
l220, l221, l222, l223, l224, l225 = sch.get_loops(block=b219)
b226 = sch.decompose_reduction(block=b219, loop=l224)
b227 = sch.get_block(name="bgemm", func_name="main")
l228, l229, l230, l231, l232, l233, l234, l235, l236, l237, l238, l239, l240, l241 = sch.get_loops(block=b227)
b242 = sch.decompose_reduction(block=b227, loop=l231)
b243 = sch.get_block(name="inverse", func_name="main")
l244, l245, l246, l247, l248, l249, l250, l251 = sch.get_loops(block=b243)
b252 = sch.decompose_reduction(block=b243, loop=l250)
2023-11-11 03:49:43 [INFO] [task_scheduler.cc:121] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #325: Error in running:
LocalRunner: An exception occurred
Subprocess terminated
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(512), T.int64(7), T.int64(7)), "float32"), p1: T.Buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(512)), "float32"), p2: T.Buffer((T.int64(1), T.int64(512), T.int64(7), T.int64(7)), "float32"), p3: T.Buffer((T.int64(1), T.int64(512), T.int64(1), T.int64(1)), "float32"), p4: T.Buffer((T.int64(1), T.int64(512), T.int64(1), T.int64(1)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(512), T.int64(7), T.int64(7)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        input_tile_local = T.alloc_buffer((T.int64(512), T.int64(16), T.int64(4), T.int64(4)), scope="local")
        data_pack = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(16)))
        bgemm = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(16)))
        inverse_local = T.alloc_buffer((T.int64(512), T.int64(16), T.int64(2), T.int64(2)), scope="local")
        data_pack_local = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(16)), scope="local")
        bgemm_local = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(16)), scope="local")
        data_pack_shared = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(16)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(512)), scope="shared")
        for ci_p_fused_0 in T.thread_binding(T.int64(64), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 512, "pragma_unroll_explicit": 1}):
            for ci_p_fused_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(1), T.int64(4), T.int64(4)):
                    with T.block("input_tile"):
                        v_ci = T.axis.spatial(T.int64(512), ci_p_fused_0 * T.int64(8) + ci_p_fused_1 // T.int64(16) + ax0)
                        v_p = T.axis.spatial(T.int64(16), ci_p_fused_1 % T.int64(16) + ax1)
                        v_eps, v_nu = T.axis.remap("SS", [ax2, ax3])
                        T.reads(p0[v_p // T.int64(16), v_ci, v_p % T.int64(16) // T.int64(4) * T.int64(2) + v_eps - T.int64(1), v_p % T.int64(4) * T.int64(2) + v_nu - T.int64(1)])
                        T.writes(input_tile_local[v_ci, v_p, v_eps, v_nu])
                        T.block_attr({"schedule_rule": "None"})
                        input_tile_local[v_ci, v_p, v_eps, v_nu] = T.if_then_else(T.int64(1) <= v_p % T.int64(16) // T.int64(4) * T.int64(2) + v_eps and v_p % T.int64(16) // T.int64(4) * T.int64(2) + v_eps < T.int64(8) and T.int64(1) <= v_p % T.int64(4) * T.int64(2) + v_nu and v_p % T.int64(4) * T.int64(2) + v_nu < T.int64(8), p0[v_p // T.int64(16), v_ci, v_p % T.int64(16) // T.int64(4) * T.int64(2) + v_eps - T.int64(1), v_p % T.int64(4) * T.int64(2) + v_nu - T.int64(1)], T.float32(0))
                for eps in T.unroll(T.int64(4)):
                    for nu in T.unroll(T.int64(4)):
                        with T.block("data_pack_init"):
                            v_eps, v_nu = T.axis.remap("SS", [eps, nu])
                            v_ci = T.axis.spatial(T.int64(512), (ci_p_fused_0 * T.int64(128) + ci_p_fused_1) // T.int64(16))
                            v_p = T.axis.spatial(T.int64(16), (ci_p_fused_0 * T.int64(128) + ci_p_fused_1) % T.int64(16))
                            T.reads()
                            T.writes(data_pack_local[v_eps, v_nu, v_ci, v_p])
                            T.block_attr({"schedule_rule": "conv2d_nchw_winograd_data_pack"})
                            data_pack_local[v_eps, v_nu, v_ci, v_p] = T.float32(0)
                        for r_a in T.unroll(T.int64(4)):
                            for r_b in T.unroll(T.int64(4)):
                                with T.block("data_pack_update"):
                                    v_eps, v_nu = T.axis.remap("SS", [eps, nu])
                                    v_ci = T.axis.spatial(T.int64(512), (ci_p_fused_0 * T.int64(128) + ci_p_fused_1) // T.int64(16))
                                    v_p = T.axis.spatial(T.int64(16), (ci_p_fused_0 * T.int64(128) + ci_p_fused_1) % T.int64(16))
                                    v_r_a, v_r_b = T.axis.remap("RR", [r_a, r_b])
                                    T.reads(data_pack_local[v_eps, v_nu, v_ci, v_p], input_tile_local[v_ci, v_p, v_r_a, v_r_b])
                                    T.writes(data_pack_local[v_eps, v_nu, v_ci, v_p])
                                    T.block_attr({"schedule_rule": "conv2d_nchw_winograd_data_pack"})
                                    data_pack_local[v_eps, v_nu, v_ci, v_p] = data_pack_local[v_eps, v_nu, v_ci, v_p] + input_tile_local[v_ci, v_p, v_r_a, v_r_b] * T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(3), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(1), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(0), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(3), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(1), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(0), T.float32(1), T.float32(0))))))))))))))))) * T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(3), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(1), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(0), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(3), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(1), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(0), T.float32(1), T.float32(0)))))))))))))))))
                for ax0, ax1, ax2, ax3 in T.grid(T.int64(4), T.int64(4), T.int64(1), T.int64(1)):
                    with T.block("data_pack_local"):
                        v0, v1 = T.axis.remap("SS", [ax0, ax1])
                        v2 = T.axis.spatial(T.int64(512), ci_p_fused_0 * T.int64(8) + ci_p_fused_1 // T.int64(16) + ax2)
                        v3 = T.axis.spatial(T.int64(16), ci_p_fused_1 % T.int64(16) + ax3)
                        T.reads(data_pack_local[v0, v1, v2, v3])
                        T.writes(data_pack[v0, v1, v2, v3])
                        data_pack[v0, v1, v2, v3] = data_pack_local[v0, v1, v2, v3]
        for eps_0_nu_0_co_0_p_0_fused in T.thread_binding(T.int64(128), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 512, "pragma_unroll_explicit": 1}):
            for eps_1_nu_1_co_1_p_1_fused in T.thread_binding(T.int64(4), thread="vthread.x"):
                for eps_2_nu_2_co_2_p_2_fused in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                    for eps_3_init, nu_3_init, co_3_init, p_3_init, eps_4_init, nu_4_init, co_4_init, p_4_init in T.grid(T.int64(1), T.int64(1), T.int64(2), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("bgemm_init"):
                            v_eps = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(64) * T.int64(2) + eps_1_nu_1_co_1_p_1_fused // T.int64(2) + eps_3_init + eps_4_init)
                            v_nu = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused % T.int64(64) // T.int64(32) * T.int64(2) + eps_2_nu_2_co_2_p_2_fused // T.int64(64) + nu_3_init + nu_4_init)
                            v_co = T.axis.spatial(T.int64(512), eps_0_nu_0_co_0_p_0_fused % T.int64(32) // T.int64(2) * T.int64(32) + eps_1_nu_1_co_1_p_1_fused % T.int64(2) * T.int64(16) + eps_2_nu_2_co_2_p_2_fused % T.int64(64) // T.int64(8) * T.int64(2) + co_3_init + co_4_init)
                            v_p = T.axis.spatial(T.int64(16), eps_0_nu_0_co_0_p_0_fused % T.int64(2) * T.int64(8) + eps_2_nu_2_co_2_p_2_fused % T.int64(8) + p_3_init + p_4_init)
                            T.reads()
                            T.writes(bgemm_local[v_eps, v_nu, v_co, v_p])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            bgemm_local[v_eps, v_nu, v_co, v_p] = T.float32(0)
                    for ci_0 in range(T.int64(8)):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(4)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("data_pack_shared"):
                                        v0 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(64) * T.int64(2) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(512) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) // T.int64(1024))
                                        v1 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused % T.int64(64) // T.int64(32) * T.int64(2) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(512) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) % T.int64(1024) // T.int64(512))
                                        v2 = T.axis.spatial(T.int64(512), ci_0 * T.int64(64) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(512) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) % T.int64(512) // T.int64(8))
                                        v3 = T.axis.spatial(T.int64(16), eps_0_nu_0_co_0_p_0_fused % T.int64(2) * T.int64(8) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(512) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) % T.int64(8))
                                        T.reads(data_pack[v0, v1, v2, v3])
                                        T.writes(data_pack_shared[v0, v1, v2, v3])
                                        data_pack_shared[v0, v1, v2, v3] = data_pack[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(64)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                                with T.block("p1_shared"):
                                    v0 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(64) * T.int64(2) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1) // T.int64(4096))
                                    v1 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused % T.int64(64) // T.int64(32) * T.int64(2) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1) % T.int64(4096) // T.int64(2048))
                                    v2 = T.axis.spatial(T.int64(512), ci_0 * T.int64(64) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1) % T.int64(2048) // T.int64(32))
                                    v3 = T.axis.spatial(T.int64(512), eps_0_nu_0_co_0_p_0_fused % T.int64(32) // T.int64(2) * T.int64(32) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1) % T.int64(32))
                                    T.reads(p1[v0, v1, v2, v3])
                                    T.writes(p1_shared[v0, v1, v2, v3])
                                    p1_shared[v0, v1, v2, v3] = p1[v0, v1, v2, v3]
                        for ci_1, eps_3, nu_3, co_3, p_3, ci_2, eps_4, nu_4, co_4, p_4 in T.grid(T.int64(4), T.int64(1), T.int64(1), T.int64(2), T.int64(1), T.int64(16), T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                            with T.block("bgemm_update"):
                                v_eps = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(64) * T.int64(2) + eps_1_nu_1_co_1_p_1_fused // T.int64(2) + eps_3 + eps_4)
                                v_nu = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused % T.int64(64) // T.int64(32) * T.int64(2) + eps_2_nu_2_co_2_p_2_fused // T.int64(64) + nu_3 + nu_4)
                                v_co = T.axis.spatial(T.int64(512), eps_0_nu_0_co_0_p_0_fused % T.int64(32) // T.int64(2) * T.int64(32) + eps_1_nu_1_co_1_p_1_fused % T.int64(2) * T.int64(16) + eps_2_nu_2_co_2_p_2_fused % T.int64(64) // T.int64(8) * T.int64(2) + co_3 + co_4)
                                v_p = T.axis.spatial(T.int64(16), eps_0_nu_0_co_0_p_0_fused % T.int64(2) * T.int64(8) + eps_2_nu_2_co_2_p_2_fused % T.int64(8) + p_3 + p_4)
                                v_ci = T.axis.reduce(T.int64(512), ci_0 * T.int64(64) + ci_1 * T.int64(16) + ci_2)
                                T.reads(bgemm_local[v_eps, v_nu, v_co, v_p], data_pack_shared[v_eps, v_nu, v_ci, v_p], p1_shared[v_eps, v_nu, v_ci, v_co])
                                T.writes(bgemm_local[v_eps, v_nu, v_co, v_p])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                bgemm_local[v_eps, v_nu, v_co, v_p] = bgemm_local[v_eps, v_nu, v_co, v_p] + data_pack_shared[v_eps, v_nu, v_ci, v_p] * p1_shared[v_eps, v_nu, v_ci, v_co]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(1), T.int64(2), T.int64(1)):
                        with T.block("bgemm_local"):
                            v0 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(64) * T.int64(2) + eps_1_nu_1_co_1_p_1_fused // T.int64(2) + ax0)
                            v1 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused % T.int64(64) // T.int64(32) * T.int64(2) + eps_2_nu_2_co_2_p_2_fused // T.int64(64) + ax1)
                            v2 = T.axis.spatial(T.int64(512), eps_0_nu_0_co_0_p_0_fused % T.int64(32) // T.int64(2) * T.int64(32) + eps_1_nu_1_co_1_p_1_fused % T.int64(2) * T.int64(16) + eps_2_nu_2_co_2_p_2_fused % T.int64(64) // T.int64(8) * T.int64(2) + ax2)
                            v3 = T.axis.spatial(T.int64(16), eps_0_nu_0_co_0_p_0_fused % T.int64(2) * T.int64(8) + eps_2_nu_2_co_2_p_2_fused % T.int64(8) + ax3)
                            T.reads(bgemm_local[v0, v1, v2, v3])
                            T.writes(bgemm[v0, v1, v2, v3])
                            bgemm[v0, v1, v2, v3] = bgemm_local[v0, v1, v2, v3]
        for n_co_h_0_w_0_fused_0 in T.thread_binding(T.int64(256), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 512, "pragma_unroll_explicit": 1}):
            for n_co_h_0_w_0_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                    for ax2 in T.unroll(T.int64(2)):
                        for ax3 in T.unroll(T.int64(2)):
                            with T.block("inverse_init"):
                                v_co = T.axis.spatial(T.int64(512), (n_co_h_0_w_0_fused_0 * T.int64(32) + n_co_h_0_w_0_fused_1) // T.int64(16) + ax0)
                                v_p = T.axis.spatial(T.int64(16), (n_co_h_0_w_0_fused_0 * T.int64(32) + n_co_h_0_w_0_fused_1) % T.int64(16) + ax1)
                                v_vh, v_vw = T.axis.remap("SS", [ax2, ax3])
                                T.reads()
                                T.writes(inverse_local[v_co, v_p, v_vh, v_vw])
                                T.block_attr({"schedule_rule": "conv2d_nchw_winograd_inverse"})
                                inverse_local[v_co, v_p, v_vh, v_vw] = T.float32(0)
                            for ax4 in T.unroll(T.int64(4)):
                                for ax5 in T.unroll(T.int64(4)):
                                    with T.block("inverse_update"):
                                        v_co = T.axis.spatial(T.int64(512), (n_co_h_0_w_0_fused_0 * T.int64(32) + n_co_h_0_w_0_fused_1) // T.int64(16) + ax0)
                                        v_p = T.axis.spatial(T.int64(16), (n_co_h_0_w_0_fused_0 * T.int64(32) + n_co_h_0_w_0_fused_1) % T.int64(16) + ax1)
                                        v_vh, v_vw, v_r_a, v_r_b = T.axis.remap("SSRR", [ax2, ax3, ax4, ax5])
                                        T.reads(inverse_local[v_co, v_p, v_vh, v_vw], bgemm[v_r_a, v_r_b, v_co, v_p])
                                        T.writes(inverse_local[v_co, v_p, v_vh, v_vw])
                                        T.block_attr({"schedule_rule": "conv2d_nchw_winograd_inverse"})
                                        inverse_local[v_co, v_p, v_vh, v_vw] = inverse_local[v_co, v_p, v_vh, v_vw] + bgemm[v_r_a, v_r_b, v_co, v_p] * T.Select(v_r_a % T.int64(4) == T.int64(3) and v_vh % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_vh % T.int64(2) == T.int64(0), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_vh % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_vh % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_vh % T.int64(2) == T.int64(1), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_vh % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_vh % T.int64(2) == T.int64(1), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_vh % T.int64(2) == T.int64(0), T.float32(1), T.float32(0))))))))) * T.Select(v_r_b % T.int64(4) == T.int64(3) and v_vw % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_vw % T.int64(2) == T.int64(0), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_vw % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_vw % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_vw % T.int64(2) == T.int64(1), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_vw % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_vw % T.int64(2) == T.int64(1), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_vw % T.int64(2) == T.int64(0), T.float32(1), T.float32(0)))))))))
                for h_1, w_1 in T.grid(T.int64(2), T.int64(2)):
                    with T.block("conv2d_winograd"):
                        v_n = T.axis.spatial(T.int64(1), T.int64(0))
                        v_co = T.axis.spatial(T.int64(512), (n_co_h_0_w_0_fused_0 * T.int64(32) + n_co_h_0_w_0_fused_1) // T.int64(16))
                        v_h = T.axis.spatial(T.int64(7), (n_co_h_0_w_0_fused_0 * T.int64(32) + n_co_h_0_w_0_fused_1) % T.int64(16) // T.int64(4) * T.int64(2) + h_1)
                        v_w = T.axis.spatial(T.int64(7), (n_co_h_0_w_0_fused_0 * T.int64(32) + n_co_h_0_w_0_fused_1) % T.int64(4) * T.int64(2) + w_1)
                        T.where((n_co_h_0_w_0_fused_0 * T.int64(32) + n_co_h_0_w_0_fused_1) % T.int64(16) // T.int64(4) * T.int64(2) + h_1 < T.int64(7) and (n_co_h_0_w_0_fused_0 * T.int64(32) + n_co_h_0_w_0_fused_1) % T.int64(4) * T.int64(2) + w_1 < T.int64(7))
                        T.reads(inverse_local[v_co, v_n * T.int64(16) + v_h // T.int64(2) * T.int64(4) + v_w // T.int64(2), v_h % T.int64(2), v_w % T.int64(2)], p2[v_n, v_co, v_h, v_w], p3[v_n, v_co, T.int64(0), T.int64(0)], p4[v_n, v_co, T.int64(0), T.int64(0)])
                        T.writes(T_relu[v_n, v_co, v_h, v_w])
                        T_relu[v_n, v_co, v_h, v_w] = T.max((inverse_local[v_co, v_n * T.int64(16) + v_h // T.int64(2) * T.int64(4) + v_w // T.int64(2), v_h % T.int64(2), v_w % T.int64(2)] + p2[v_n, v_co, v_h, v_w]) * p3[v_n, v_co, T.int64(0), T.int64(0)] + p4[v_n, v_co, T.int64(0), T.int64(0)], T.float32(0))
b0 = sch.get_block(name="data_pack", func_name="main")
b1 = sch.get_block(name="bgemm", func_name="main")
b2 = sch.get_block(name="inverse", func_name="main")
b3 = sch.get_block(name="conv2d_winograd", func_name="main")
b4 = sch.get_block(name="T_add", func_name="main")
b5 = sch.get_block(name="T_multiply", func_name="main")
b6 = sch.get_block(name="T_add_1", func_name="main")
b7 = sch.get_block(name="T_relu", func_name="main")
b8 = sch.get_block(name="root", func_name="main")
b9, b10 = sch.get_producers(block=b2)
sch.compute_inline(block=b10)
b11, = sch.get_consumers(block=b2)
l12, l13, l14, l15 = sch.get_loops(block=b11)
l16, l17 = sch.split(loop=l14, factors=[None, 2], preserve_unit_iters=True)
l18, l19 = sch.split(loop=l15, factors=[None, 2], preserve_unit_iters=True)
sch.reorder(l16, l18, l17, l19)
sch.compute_at(block=b2, loop=l18, preserve_unit_loops=True, index=-1)
sch.set_scope(block=b2, buffer_index=0, storage_scope="local")
l20, l21, l22, l23, l24, l25, l26, l27, l28, l29 = sch.get_loops(block=b2)
sch.unroll(loop=l26)
sch.unroll(loop=l27)
sch.unroll(loop=l28)
sch.unroll(loop=l29)
b30, b31 = sch.get_producers(block=b0)
sch.compute_inline(block=b31)
b32, = sch.get_producers(block=b30)
l33, l34, l35, l36, l37, l38 = sch.get_loops(block=b0)
sch.reorder(l35, l36, l33, l34, l37, l38)
sch.unroll(loop=l33)
sch.unroll(loop=l34)
sch.unroll(loop=l37)
sch.unroll(loop=l38)
l39 = sch.fuse(l35, l36, preserve_unit_iters=True)
v40 = sch.sample_categorical(candidates=[32, 64, 128, 256, 512, 1024], probs=[0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666], decision=2)
l41, l42 = sch.split(loop=l39, factors=[None, v40], preserve_unit_iters=True)
sch.bind(loop=l41, thread_axis="blockIdx.x")
sch.bind(loop=l42, thread_axis="threadIdx.x")
b43 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b43, loop=l42, preserve_unit_loops=True, index=-1)
sch.compute_at(block=b30, loop=l42, preserve_unit_loops=True, index=-1)
sch.set_scope(block=b30, buffer_index=0, storage_scope="local")
sch.compute_inline(block=b32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l44, l45, l46, l47, l48 = sch.get_loops(block=b1)
v49, v50, v51, v52, v53 = sch.sample_perfect_tile(loop=l44, n=5, max_innermost_factor=64, decision=[2, 2, 1, 1, 1])
l54, l55, l56, l57, l58 = sch.split(loop=l44, factors=[v49, v50, v51, v52, v53], preserve_unit_iters=True)
v59, v60, v61, v62, v63 = sch.sample_perfect_tile(loop=l45, n=5, max_innermost_factor=64, decision=[2, 1, 2, 1, 1])
l64, l65, l66, l67, l68 = sch.split(loop=l45, factors=[v59, v60, v61, v62, v63], preserve_unit_iters=True)
v69, v70, v71, v72, v73 = sch.sample_perfect_tile(loop=l46, n=5, max_innermost_factor=64, decision=[16, 2, 8, 2, 1])
l74, l75, l76, l77, l78 = sch.split(loop=l46, factors=[v69, v70, v71, v72, v73], preserve_unit_iters=True)
v79, v80, v81, v82, v83 = sch.sample_perfect_tile(loop=l47, n=5, max_innermost_factor=64, decision=[2, 1, 8, 1, 1])
l84, l85, l86, l87, l88 = sch.split(loop=l47, factors=[v79, v80, v81, v82, v83], preserve_unit_iters=True)
v89, v90, v91 = sch.sample_perfect_tile(loop=l48, n=3, max_innermost_factor=64, decision=[8, 4, 16])
l92, l93, l94 = sch.split(loop=l48, factors=[v89, v90, v91], preserve_unit_iters=True)
sch.reorder(l54, l64, l74, l84, l55, l65, l75, l85, l56, l66, l76, l86, l92, l93, l57, l67, l77, l87, l94, l58, l68, l78, l88)
l95 = sch.fuse(l54, l64, l74, l84, preserve_unit_iters=True)
sch.bind(loop=l95, thread_axis="blockIdx.x")
l96 = sch.fuse(l55, l65, l75, l85, preserve_unit_iters=True)
sch.bind(loop=l96, thread_axis="vthread.x")
l97 = sch.fuse(l56, l66, l76, l86, preserve_unit_iters=True)
sch.bind(loop=l97, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b98 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b98, loop=l97, preserve_unit_loops=True, index=-1)
b99 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b99, loop=l92, preserve_unit_loops=True, index=-1)
l100, l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b99)
l108 = sch.fuse(l104, l105, l106, l107, preserve_unit_iters=True)
v109 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b99, ann_key="meta_schedule.cooperative_fetch", ann_val=v109)
b110 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b110, loop=l92, preserve_unit_loops=True, index=-1)
l111, l112, l113, l114, l115, l116, l117, l118 = sch.get_loops(block=b110)
l119 = sch.fuse(l115, l116, l117, l118, preserve_unit_iters=True)
v120 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b110, ann_key="meta_schedule.cooperative_fetch", ann_val=v120)
sch.reverse_compute_inline(block=b7)
sch.reverse_compute_inline(block=b6)
sch.reverse_compute_inline(block=b5)
sch.reverse_compute_inline(block=b4)
v121 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b8, ann_key="meta_schedule.unroll_explicit", ann_val=v121)
l122, l123, l124, l125, l126, l127 = sch.get_loops(block=b3)
l128 = sch.fuse(l122, l123, l124, l125, preserve_unit_iters=True)
v129 = sch.sample_categorical(candidates=[32, 64, 128, 256, 512, 1024], probs=[0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666], decision=0)
l130, l131 = sch.split(loop=l128, factors=[None, v129], preserve_unit_iters=True)
sch.bind(loop=l130, thread_axis="blockIdx.x")
sch.bind(loop=l131, thread_axis="threadIdx.x")
sch.enter_postproc()
sch.unannotate(block_or_loop=b99, ann_key="meta_schedule.cooperative_fetch")
l132, l133, l134, l135, l136 = sch.get_loops(block=b99)
l137, l138, l139 = sch.split(loop=l136, factors=[None, 128, 4], preserve_unit_iters=True)
sch.vectorize(loop=l139)
sch.bind(loop=l138, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b110, ann_key="meta_schedule.cooperative_fetch")
l140, l141, l142, l143, l144 = sch.get_loops(block=b110)
l145, l146 = sch.split(loop=l144, factors=[None, 128], preserve_unit_iters=True)
sch.bind(loop=l146, thread_axis="threadIdx.x")
b147 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b147, ann_key="meta_schedule.unroll_explicit")
b148, b149, b150, b151, b152, b153, b154, b155, b156 = sch.get_child_blocks(b147)
l157, l158, l159, l160, l161, l162 = sch.get_loops(block=b148)
l163, l164, l165, l166, l167, l168 = sch.get_loops(block=b149)
sch.annotate(block_or_loop=l163, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l163, ann_key="pragma_unroll_explicit", ann_val=1)
l169, l170, l171, l172, l173, l174 = sch.get_loops(block=b150)
l175, l176, l177, l178, l179, l180, l181 = sch.get_loops(block=b151)
l182, l183, l184, l185, l186, l187 = sch.get_loops(block=b152)
l188, l189, l190, l191, l192, l193, l194, l195, l196, l197, l198, l199, l200, l201 = sch.get_loops(block=b153)
sch.annotate(block_or_loop=l188, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l188, ann_key="pragma_unroll_explicit", ann_val=1)
l202, l203, l204, l205, l206, l207, l208 = sch.get_loops(block=b154)
l209, l210, l211, l212, l213, l214, l215, l216 = sch.get_loops(block=b155)
sch.annotate(block_or_loop=l209, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l209, ann_key="pragma_unroll_explicit", ann_val=1)
l217, l218, l219, l220 = sch.get_loops(block=b156)
b221 = sch.get_block(name="data_pack", func_name="main")
l222, l223, l224, l225, l226, l227 = sch.get_loops(block=b221)
b228 = sch.decompose_reduction(block=b221, loop=l226)
b229 = sch.get_block(name="bgemm", func_name="main")
l230, l231, l232, l233, l234, l235, l236, l237, l238, l239, l240, l241, l242, l243 = sch.get_loops(block=b229)
b244 = sch.decompose_reduction(block=b229, loop=l233)
b245 = sch.get_block(name="inverse", func_name="main")
l246, l247, l248, l249, l250, l251, l252, l253 = sch.get_loops(block=b245)
b254 = sch.decompose_reduction(block=b245, loop=l252)
2023-11-11 03:49:43 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #326: GFLOPs: 3936.9212. Time: 36.1151 us. Best GFLOPs: 5156.9033
2023-11-11 03:49:43 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #327: GFLOPs: 3467.6102. Time: 41.0030 us. Best GFLOPs: 5156.9033
2023-11-11 03:49:43 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #328: GFLOPs: 3033.0318. Time: 46.8780 us. Best GFLOPs: 5156.9033
2023-11-11 03:49:43 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #329: GFLOPs: 3425.3047. Time: 41.5094 us. Best GFLOPs: 5156.9033
2023-11-11 03:49:43 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #330: GFLOPs: 3420.3940. Time: 41.5690 us. Best GFLOPs: 5156.9033
2023-11-11 03:49:43 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #331: GFLOPs: 3406.7456. Time: 41.7355 us. Best GFLOPs: 5156.9033
2023-11-11 03:49:43 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #332: GFLOPs: 2700.9137. Time: 52.6423 us. Best GFLOPs: 5156.9033
2023-11-11 03:49:43 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #333: GFLOPs: 3127.0473. Time: 45.4686 us. Best GFLOPs: 5156.9033
2023-11-11 03:49:43 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #334: GFLOPs: 2541.3312. Time: 55.9480 us. Best GFLOPs: 5156.9033
2023-11-11 03:49:43 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #335: GFLOPs: 5074.5064. Time: 28.0190 us. Best GFLOPs: 5156.9033
2023-11-11 03:49:43 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #336: GFLOPs: 4934.2305. Time: 28.8155 us. Best GFLOPs: 5156.9033
2023-11-11 03:49:43 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #337: GFLOPs: 4629.8327. Time: 30.7101 us. Best GFLOPs: 5156.9033
2023-11-11 03:49:43 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #338: GFLOPs: 3450.0873. Time: 41.2112 us. Best GFLOPs: 5156.9033
2023-11-11 03:49:43 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #339: GFLOPs: 3396.3743. Time: 41.8630 us. Best GFLOPs: 5156.9033
2023-11-11 03:49:43 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #340: GFLOPs: 4974.6271. Time: 28.5815 us. Best GFLOPs: 5156.9033
2023-11-11 03:49:43 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #341: GFLOPs: 5234.6694. Time: 27.1617 us. Best GFLOPs: 5234.6694
2023-11-11 03:49:43 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #342: GFLOPs: 5311.8298. Time: 26.7671 us. Best GFLOPs: 5311.8298
2023-11-11 03:49:43 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #343: GFLOPs: 5099.4189. Time: 27.8821 us. Best GFLOPs: 5311.8298
2023-11-11 03:49:43 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #344: GFLOPs: 5052.4459. Time: 28.1413 us. Best GFLOPs: 5311.8298
2023-11-11 03:49:43 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #345: GFLOPs: 5209.7477. Time: 27.2916 us. Best GFLOPs: 5311.8298
2023-11-11 03:49:43 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #346: GFLOPs: 5173.1503. Time: 27.4847 us. Best GFLOPs: 5311.8298
2023-11-11 03:49:43 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #347: GFLOPs: 5236.4887. Time: 27.1522 us. Best GFLOPs: 5311.8298
2023-11-11 03:49:43 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #348: GFLOPs: 5174.0329. Time: 27.4800 us. Best GFLOPs: 5311.8298
2023-11-11 03:49:43 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #349: GFLOPs: 4901.1512. Time: 29.0100 us. Best GFLOPs: 5311.8298
2023-11-11 03:49:43 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #350: GFLOPs: 5058.3348. Time: 28.1085 us. Best GFLOPs: 5311.8298
2023-11-11 03:49:43 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #351: GFLOPs: 4770.3739. Time: 29.8053 us. Best GFLOPs: 5311.8298
2023-11-11 03:49:43 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #352: GFLOPs: 5030.2419. Time: 28.2655 us. Best GFLOPs: 5311.8298
2023-11-11 03:49:43 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #353: GFLOPs: 4842.3543. Time: 29.3622 us. Best GFLOPs: 5311.8298
2023-11-11 03:49:43 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #354: GFLOPs: 4987.4004. Time: 28.5083 us. Best GFLOPs: 5311.8298
2023-11-11 03:49:43 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #355: GFLOPs: 4693.3293. Time: 30.2946 us. Best GFLOPs: 5311.8298
2023-11-11 03:49:43 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #356: GFLOPs: 4925.2632. Time: 28.8680 us. Best GFLOPs: 5311.8298
2023-11-11 03:49:43 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #357: GFLOPs: 3383.7104. Time: 42.0197 us. Best GFLOPs: 5311.8298
2023-11-11 03:49:43 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #358: GFLOPs: 5032.9190. Time: 28.2505 us. Best GFLOPs: 5311.8298
2023-11-11 03:49:43 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #359: GFLOPs: 4849.9152. Time: 29.3165 us. Best GFLOPs: 5311.8298
2023-11-11 03:49:43 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #360: GFLOPs: 5012.7451. Time: 28.3642 us. Best GFLOPs: 5311.8298
2023-11-11 03:49:43 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #361: GFLOPs: 4162.9001. Time: 34.1547 us. Best GFLOPs: 5311.8298
2023-11-11 03:49:43 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #362: GFLOPs: 4993.9846. Time: 28.4707 us. Best GFLOPs: 5311.8298
2023-11-11 03:49:43 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #363: GFLOPs: 4374.6238. Time: 32.5016 us. Best GFLOPs: 5311.8298
2023-11-11 03:49:43 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #364: GFLOPs: 4446.3254. Time: 31.9775 us. Best GFLOPs: 5311.8298
2023-11-11 03:49:43 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #365: GFLOPs: 4249.7453. Time: 33.4567 us. Best GFLOPs: 5311.8298
2023-11-11 03:49:43 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #366: GFLOPs: 5112.0381. Time: 27.8133 us. Best GFLOPs: 5311.8298
2023-11-11 03:49:43 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #367: GFLOPs: 4902.9385. Time: 28.9994 us. Best GFLOPs: 5311.8298
2023-11-11 03:49:43 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #368: GFLOPs: 4650.4034. Time: 30.5742 us. Best GFLOPs: 5311.8298
2023-11-11 03:49:43 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #369: GFLOPs: 5343.5843. Time: 26.6081 us. Best GFLOPs: 5343.5843
2023-11-11 03:49:43 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #370: GFLOPs: 4822.1189. Time: 29.4855 us. Best GFLOPs: 5343.5843
2023-11-11 03:49:43 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #371: GFLOPs: 5089.1783. Time: 27.9382 us. Best GFLOPs: 5343.5843
2023-11-11 03:49:43 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #372: GFLOPs: 4121.7357. Time: 34.4958 us. Best GFLOPs: 5343.5843
2023-11-11 03:49:43 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #373: GFLOPs: 4826.7531. Time: 29.4572 us. Best GFLOPs: 5343.5843
2023-11-11 03:49:43 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #374: GFLOPs: 5260.9143. Time: 27.0262 us. Best GFLOPs: 5343.5843
2023-11-11 03:49:43 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #375: GFLOPs: 4964.4581. Time: 28.6401 us. Best GFLOPs: 5343.5843
2023-11-11 03:49:43 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #376: GFLOPs: 5014.1262. Time: 28.3564 us. Best GFLOPs: 5343.5843
2023-11-11 03:49:43 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #377: GFLOPs: 4887.5786. Time: 29.0906 us. Best GFLOPs: 5343.5843
2023-11-11 03:49:43 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #378: GFLOPs: 4888.5356. Time: 29.0849 us. Best GFLOPs: 5343.5843
2023-11-11 03:49:43 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #379: GFLOPs: 4408.9994. Time: 32.2482 us. Best GFLOPs: 5343.5843
2023-11-11 03:49:43 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #380: GFLOPs: 4819.4146. Time: 29.5020 us. Best GFLOPs: 5343.5843
2023-11-11 03:49:43 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #381: GFLOPs: 4869.8214. Time: 29.1966 us. Best GFLOPs: 5343.5843
2023-11-11 03:49:43 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #382: GFLOPs: 178.5141. Time: 796.4769 us. Best GFLOPs: 5343.5843
2023-11-11 03:49:43 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #383: GFLOPs: 1664.0431. Time: 85.4439 us. Best GFLOPs: 5343.5843
2023-11-11 03:49:43 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #384: GFLOPs: 188.8223. Time: 752.9957 us. Best GFLOPs: 5343.5843
2023-11-11 04:38:35 [INFO] [evolutionary_search.cc:713] Generating candidates......
2023-11-11 04:38:38 [INFO] [evolutionary_search.cc:715] Picked top 102 candidate(s) from database
2023-11-11 04:38:44 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310fb03b28)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310eec3b08)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310dc5caf8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310f7cc2d8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x563110643e78)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310dc32028)]: 399 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310dc5cfb8)]: 0 failure(s)
2023-11-11 04:38:50 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310fb03b28)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310eec3b08)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310dc5caf8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310f7cc2d8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x563110643e78)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310dc32028)]: 799 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310dc5cfb8)]: 0 failure(s)
2023-11-11 04:38:56 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310fb03b28)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310eec3b08)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310dc5caf8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310f7cc2d8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x563110643e78)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310dc32028)]: 1202 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310dc5cfb8)]: 0 failure(s)
2023-11-11 04:39:02 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310fb03b28)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310eec3b08)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310dc5caf8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310f7cc2d8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x563110643e78)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310dc32028)]: 1607 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310dc5cfb8)]: 0 failure(s)
2023-11-11 04:39:09 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310fb03b28)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310eec3b08)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310dc5caf8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310f7cc2d8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x563110643e78)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310dc32028)]: 2008 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310dc5cfb8)]: 0 failure(s)
2023-11-11 04:39:15 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310fb03b28)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310eec3b08)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310dc5caf8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310f7cc2d8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x563110643e78)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310dc32028)]: 2413 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310dc5cfb8)]: 0 failure(s)
2023-11-11 04:39:21 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310fb03b28)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310eec3b08)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310dc5caf8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310f7cc2d8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x563110643e78)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310dc32028)]: 2812 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310dc5cfb8)]: 0 failure(s)
2023-11-11 04:39:21 [INFO] [evolutionary_search.cc:723] Sampled 58 candidate(s)
2023-11-11 04:39:36 [INFO] [evolutionary_search.cc:621] Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310fb03b28)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310eec3b08)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310dc5caf8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310f7cc2d8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x563110643e78)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310dc32028)]: 78 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310dc5cfb8)]: 0 failure(s)
2023-11-11 04:39:55 [INFO] [evolutionary_search.cc:621] Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310fb03b28)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310eec3b08)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310dc5caf8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310f7cc2d8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x563110643e78)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310dc32028)]: 87 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310dc5cfb8)]: 0 failure(s)
2023-11-11 04:40:15 [INFO] [evolutionary_search.cc:621] Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310fb03b28)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310eec3b08)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310dc5caf8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310f7cc2d8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x563110643e78)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310dc32028)]: 92 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310dc5cfb8)]: 0 failure(s)
2023-11-11 04:40:33 [INFO] [evolutionary_search.cc:621] Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310fb03b28)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310eec3b08)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310dc5caf8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310f7cc2d8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x563110643e78)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310dc32028)]: 75 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310dc5cfb8)]: 0 failure(s)
2023-11-11 04:40:38 [INFO] [evolutionary_search.cc:649] Scores of the best 64 candidates:
[1 : 16]:	1.0207  1.0166  0.9986  0.9952  0.9942  0.9843  0.9813  0.9806  0.9799  0.9782  0.9767  0.9755  0.9748  0.9746  0.9736  0.9720
[17 : 32]:	0.9706  0.9704  0.9692  0.9683  0.9683  0.9681  0.9670  0.9662  0.9656  0.9653  0.9651  0.9648  0.9646  0.9644  0.9643  0.9639
[33 : 48]:	0.9637  0.9634  0.9628  0.9624  0.9623  0.9621  0.9620  0.9612  0.9611  0.9605  0.9604  0.9599  0.9594  0.9589  0.9587  0.9586
[49 : 64]:	0.9583  0.9583  0.9581  0.9569  0.9562  0.9560  0.9557  0.9557  0.9557  0.9554  0.9552  0.9550  0.9545  0.9542  0.9538  0.9534
2023-11-11 04:40:39 [INFO] [evolutionary_search.cc:727] Got 64 candidate(s) with evolutionary search
2023-11-11 04:40:39 [INFO] [evolutionary_search.cc:730] Sending 64 candidates(s) for measurement
2023-11-11 04:41:16 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #385: GFLOPs: 4512.9923. Time: 31.5051 us. Best GFLOPs: 5343.5843
2023-11-11 04:41:16 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #386: GFLOPs: 4676.3050. Time: 30.4049 us. Best GFLOPs: 5343.5843
2023-11-11 04:41:16 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #387: GFLOPs: 5003.1098. Time: 28.4188 us. Best GFLOPs: 5343.5843
2023-11-11 04:41:16 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #388: GFLOPs: 5147.6308. Time: 27.6209 us. Best GFLOPs: 5343.5843
2023-11-11 04:41:16 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #389: GFLOPs: 4461.5072. Time: 31.8687 us. Best GFLOPs: 5343.5843
2023-11-11 04:41:16 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #390: GFLOPs: 5451.0520. Time: 26.0835 us. Best GFLOPs: 5451.0520
2023-11-11 04:41:16 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #391: GFLOPs: 4990.7918. Time: 28.4889 us. Best GFLOPs: 5451.0520
2023-11-11 04:41:16 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #392: GFLOPs: 5014.6485. Time: 28.3534 us. Best GFLOPs: 5451.0520
2023-11-11 04:41:16 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #393: GFLOPs: 5208.7096. Time: 27.2970 us. Best GFLOPs: 5451.0520
2023-11-11 04:41:16 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #394: GFLOPs: 5050.7592. Time: 28.1507 us. Best GFLOPs: 5451.0520
2023-11-11 04:41:16 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #395: GFLOPs: 5148.5774. Time: 27.6159 us. Best GFLOPs: 5451.0520
2023-11-11 04:41:16 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #396: GFLOPs: 5172.1601. Time: 27.4899 us. Best GFLOPs: 5451.0520
2023-11-11 04:41:16 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #397: GFLOPs: 5149.5820. Time: 27.6105 us. Best GFLOPs: 5451.0520
2023-11-11 04:41:16 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #398: GFLOPs: 5075.9359. Time: 28.0111 us. Best GFLOPs: 5451.0520
2023-11-11 04:41:16 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #399: GFLOPs: 5069.8139. Time: 28.0449 us. Best GFLOPs: 5451.0520
2023-11-11 04:41:16 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #400: GFLOPs: 5173.2031. Time: 27.4844 us. Best GFLOPs: 5451.0520
2023-11-11 04:41:16 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #401: GFLOPs: 5222.4924. Time: 27.2250 us. Best GFLOPs: 5451.0520
2023-11-11 04:41:16 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #402: GFLOPs: 5035.0961. Time: 28.2383 us. Best GFLOPs: 5451.0520
2023-11-11 04:41:16 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #403: GFLOPs: 5080.2305. Time: 27.9874 us. Best GFLOPs: 5451.0520
2023-11-11 04:41:16 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #404: GFLOPs: 5089.5167. Time: 27.9363 us. Best GFLOPs: 5451.0520
2023-11-11 04:41:16 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #405: GFLOPs: 5090.7243. Time: 27.9297 us. Best GFLOPs: 5451.0520
2023-11-11 04:41:16 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #406: GFLOPs: 5175.6927. Time: 27.4712 us. Best GFLOPs: 5451.0520
2023-11-11 04:41:16 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #407: GFLOPs: 5092.6324. Time: 27.9192 us. Best GFLOPs: 5451.0520
2023-11-11 04:41:16 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #408: GFLOPs: 5006.4585. Time: 28.3998 us. Best GFLOPs: 5451.0520
2023-11-11 04:41:16 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #409: GFLOPs: 5103.7165. Time: 27.8586 us. Best GFLOPs: 5451.0520
2023-11-11 04:41:16 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #410: GFLOPs: 5011.0186. Time: 28.3740 us. Best GFLOPs: 5451.0520
2023-11-11 04:41:16 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #411: GFLOPs: 5112.7730. Time: 27.8093 us. Best GFLOPs: 5451.0520
2023-11-11 04:41:16 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #412: GFLOPs: 5044.4770. Time: 28.1858 us. Best GFLOPs: 5451.0520
2023-11-11 04:41:16 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #413: GFLOPs: 5074.0976. Time: 28.0212 us. Best GFLOPs: 5451.0520
2023-11-11 04:41:16 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #414: GFLOPs: 5092.9717. Time: 27.9174 us. Best GFLOPs: 5451.0520
2023-11-11 04:41:16 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #415: GFLOPs: 5107.5183. Time: 27.8379 us. Best GFLOPs: 5451.0520
2023-11-11 04:41:16 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #416: GFLOPs: 5128.2235. Time: 27.7255 us. Best GFLOPs: 5451.0520
2023-11-11 04:41:16 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #417: GFLOPs: 5075.8000. Time: 28.0118 us. Best GFLOPs: 5451.0520
2023-11-11 04:41:16 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #418: GFLOPs: 4571.5216. Time: 31.1018 us. Best GFLOPs: 5451.0520
2023-11-11 04:41:16 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #419: GFLOPs: 5066.3512. Time: 28.0641 us. Best GFLOPs: 5451.0520
2023-11-11 04:41:16 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #420: GFLOPs: 5128.0170. Time: 27.7266 us. Best GFLOPs: 5451.0520
2023-11-11 04:41:16 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #421: GFLOPs: 3831.4472. Time: 37.1093 us. Best GFLOPs: 5451.0520
2023-11-11 04:41:16 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #422: GFLOPs: 4603.2673. Time: 30.8873 us. Best GFLOPs: 5451.0520
2023-11-11 04:41:16 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #423: GFLOPs: 5002.7752. Time: 28.4207 us. Best GFLOPs: 5451.0520
2023-11-11 04:41:16 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #424: GFLOPs: 5146.1129. Time: 27.6291 us. Best GFLOPs: 5451.0520
2023-11-11 04:41:16 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #425: GFLOPs: 5069.4585. Time: 28.0469 us. Best GFLOPs: 5451.0520
2023-11-11 04:41:16 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #426: GFLOPs: 5149.0626. Time: 27.6133 us. Best GFLOPs: 5451.0520
2023-11-11 04:41:16 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #427: GFLOPs: 4940.3251. Time: 28.7800 us. Best GFLOPs: 5451.0520
2023-11-11 04:41:16 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #428: GFLOPs: 5065.7340. Time: 28.0675 us. Best GFLOPs: 5451.0520
2023-11-11 04:41:16 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #429: GFLOPs: 5195.3122. Time: 27.3674 us. Best GFLOPs: 5451.0520
2023-11-11 04:41:16 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #430: GFLOPs: 5093.8188. Time: 27.9127 us. Best GFLOPs: 5451.0520
2023-11-11 04:41:16 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #431: GFLOPs: 4695.8824. Time: 30.2781 us. Best GFLOPs: 5451.0520
2023-11-11 04:41:16 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #432: GFLOPs: 5099.3171. Time: 27.8826 us. Best GFLOPs: 5451.0520
2023-11-11 04:41:16 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #433: GFLOPs: 5064.0804. Time: 28.0766 us. Best GFLOPs: 5451.0520
2023-11-11 04:41:16 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #434: GFLOPs: 5123.9316. Time: 27.7487 us. Best GFLOPs: 5451.0520
2023-11-11 04:41:16 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #435: GFLOPs: 5074.3995. Time: 28.0196 us. Best GFLOPs: 5451.0520
2023-11-11 04:41:16 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #436: GFLOPs: 4992.0016. Time: 28.4820 us. Best GFLOPs: 5451.0520
2023-11-11 04:41:16 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #437: GFLOPs: 4928.0311. Time: 28.8518 us. Best GFLOPs: 5451.0520
2023-11-11 04:41:16 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #438: GFLOPs: 5045.3958. Time: 28.1806 us. Best GFLOPs: 5451.0520
2023-11-11 04:41:16 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #439: GFLOPs: 4929.1148. Time: 28.8454 us. Best GFLOPs: 5451.0520
2023-11-11 04:41:16 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #440: GFLOPs: 5142.1032. Time: 27.6506 us. Best GFLOPs: 5451.0520
2023-11-11 04:41:16 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #441: GFLOPs: 5066.6904. Time: 28.0622 us. Best GFLOPs: 5451.0520
2023-11-11 04:41:16 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #442: GFLOPs: 5137.1681. Time: 27.6772 us. Best GFLOPs: 5451.0520
2023-11-11 04:41:16 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #443: GFLOPs: 5114.9086. Time: 27.7976 us. Best GFLOPs: 5451.0520
2023-11-11 04:41:16 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #444: GFLOPs: 5116.1590. Time: 27.7908 us. Best GFLOPs: 5451.0520
2023-11-11 04:41:16 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #445: GFLOPs: 5136.9294. Time: 27.6785 us. Best GFLOPs: 5451.0520
2023-11-11 04:41:16 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #446: GFLOPs: 84.0581. Time: 1691.4773 us. Best GFLOPs: 5451.0520
2023-11-11 04:41:16 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #447: GFLOPs: 1510.8162. Time: 94.1097 us. Best GFLOPs: 5451.0520
2023-11-11 04:41:16 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #448: GFLOPs: 198.9418. Time: 714.6935 us. Best GFLOPs: 5451.0520
2023-11-11 05:28:23 [INFO] [evolutionary_search.cc:713] Generating candidates......
2023-11-11 05:28:25 [INFO] [evolutionary_search.cc:715] Picked top 102 candidate(s) from database
2023-11-11 05:28:31 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310fb03b28)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310eec3b08)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310dc5caf8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310f7cc2d8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x563110643e78)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310dc32028)]: 402 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310dc5cfb8)]: 0 failure(s)
2023-11-11 05:28:38 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310fb03b28)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310eec3b08)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310dc5caf8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310f7cc2d8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x563110643e78)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310dc32028)]: 800 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310dc5cfb8)]: 0 failure(s)
2023-11-11 05:28:44 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310fb03b28)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310eec3b08)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310dc5caf8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310f7cc2d8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x563110643e78)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310dc32028)]: 1199 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310dc5cfb8)]: 0 failure(s)
2023-11-11 05:28:50 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310fb03b28)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310eec3b08)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310dc5caf8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310f7cc2d8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x563110643e78)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310dc32028)]: 1599 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310dc5cfb8)]: 0 failure(s)
2023-11-11 05:28:56 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310fb03b28)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310eec3b08)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310dc5caf8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310f7cc2d8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x563110643e78)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310dc32028)]: 1996 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310dc5cfb8)]: 0 failure(s)
2023-11-11 05:28:56 [INFO] [evolutionary_search.cc:723] Sampled 54 candidate(s)
2023-11-11 05:29:11 [INFO] [evolutionary_search.cc:621] Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310fb03b28)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310eec3b08)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310dc5caf8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310f7cc2d8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x563110643e78)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310dc32028)]: 65 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310dc5cfb8)]: 0 failure(s)
2023-11-11 05:29:30 [INFO] [evolutionary_search.cc:621] Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310fb03b28)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310eec3b08)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310dc5caf8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310f7cc2d8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x563110643e78)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310dc32028)]: 89 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310dc5cfb8)]: 0 failure(s)
2023-11-11 05:29:48 [INFO] [evolutionary_search.cc:621] Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310fb03b28)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310eec3b08)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310dc5caf8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310f7cc2d8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x563110643e78)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310dc32028)]: 83 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310dc5cfb8)]: 0 failure(s)
2023-11-11 05:30:07 [INFO] [evolutionary_search.cc:621] Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310fb03b28)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310eec3b08)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310dc5caf8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310f7cc2d8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x563110643e78)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310dc32028)]: 78 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310dc5cfb8)]: 0 failure(s)
2023-11-11 05:30:12 [INFO] [evolutionary_search.cc:649] Scores of the best 64 candidates:
[1 : 16]:	1.0724  1.0177  1.0023  1.0018  1.0015  1.0012  0.9999  0.9995  0.9991  0.9970  0.9947  0.9934  0.9920  0.9916  0.9907  0.9890
[17 : 32]:	0.9887  0.9886  0.9879  0.9873  0.9873  0.9866  0.9866  0.9864  0.9859  0.9856  0.9855  0.9852  0.9849  0.9840  0.9821  0.9818
[33 : 48]:	0.9817  0.9814  0.9812  0.9803  0.9803  0.9798  0.9793  0.9787  0.9786  0.9776  0.9774  0.9766  0.9763  0.9755  0.9745  0.9742
[49 : 64]:	0.9742  0.9739  0.9739  0.9728  0.9721  0.9709  0.9708  0.9705  0.9702  0.9700  0.9697  0.9696  0.9692  0.9682  0.9682  0.9676
2023-11-11 05:30:12 [INFO] [evolutionary_search.cc:727] Got 64 candidate(s) with evolutionary search
2023-11-11 05:30:12 [INFO] [evolutionary_search.cc:730] Sending 64 candidates(s) for measurement
2023-11-11 05:33:55 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #449: GFLOPs: 260.6687. Time: 545.4525 us. Best GFLOPs: 5451.0520
2023-11-11 05:33:55 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #450: GFLOPs: 5048.6580. Time: 28.1624 us. Best GFLOPs: 5451.0520
2023-11-11 05:33:55 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #451: GFLOPs: 4663.4297. Time: 30.4888 us. Best GFLOPs: 5451.0520
2023-11-11 05:33:55 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #452: GFLOPs: 4925.7332. Time: 28.8652 us. Best GFLOPs: 5451.0520
2023-11-11 05:33:55 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #453: GFLOPs: 4870.8710. Time: 29.1903 us. Best GFLOPs: 5451.0520
2023-11-11 05:33:55 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #454: GFLOPs: 5045.7740. Time: 28.1785 us. Best GFLOPs: 5451.0520
2023-11-11 05:33:55 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #455: GFLOPs: 4894.2598. Time: 29.0508 us. Best GFLOPs: 5451.0520
2023-11-11 05:33:55 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #456: GFLOPs: 5051.2315. Time: 28.1481 us. Best GFLOPs: 5451.0520
2023-11-11 05:33:55 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #457: GFLOPs: 5157.9383. Time: 27.5657 us. Best GFLOPs: 5451.0520
2023-11-11 05:33:55 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #458: GFLOPs: 4905.2476. Time: 28.9858 us. Best GFLOPs: 5451.0520
2023-11-11 05:33:55 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #459: GFLOPs: 4992.0242. Time: 28.4819 us. Best GFLOPs: 5451.0520
2023-11-11 05:33:55 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #460: GFLOPs: 5038.6502. Time: 28.2184 us. Best GFLOPs: 5451.0520
2023-11-11 05:33:55 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #461: GFLOPs: 5093.3783. Time: 27.9151 us. Best GFLOPs: 5451.0520
2023-11-11 05:33:55 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #462: GFLOPs: 4354.2920. Time: 32.6534 us. Best GFLOPs: 5451.0520
2023-11-11 05:33:55 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #463: GFLOPs: 4304.1823. Time: 33.0335 us. Best GFLOPs: 5451.0520
2023-11-11 05:33:55 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #464: GFLOPs: 4927.9651. Time: 28.8522 us. Best GFLOPs: 5451.0520
2023-11-11 05:33:55 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #465: GFLOPs: 4906.5818. Time: 28.9779 us. Best GFLOPs: 5451.0520
2023-11-11 05:33:55 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #466: GFLOPs: 5141.6705. Time: 27.6530 us. Best GFLOPs: 5451.0520
2023-11-11 05:33:55 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #467: GFLOPs: 4528.9017. Time: 31.3945 us. Best GFLOPs: 5451.0520
2023-11-11 05:33:55 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #468: GFLOPs: 4991.2582. Time: 28.4863 us. Best GFLOPs: 5451.0520
2023-11-11 05:33:55 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #469: GFLOPs: 4933.3843. Time: 28.8205 us. Best GFLOPs: 5451.0520
2023-11-11 05:33:55 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #470: GFLOPs: 4975.4292. Time: 28.5769 us. Best GFLOPs: 5451.0520
2023-11-11 05:33:55 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #471: GFLOPs: 4990.8896. Time: 28.4884 us. Best GFLOPs: 5451.0520
2023-11-11 05:33:55 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #472: GFLOPs: 5034.0238. Time: 28.2443 us. Best GFLOPs: 5451.0520
2023-11-11 05:33:55 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #473: GFLOPs: 4786.9489. Time: 29.7021 us. Best GFLOPs: 5451.0520
2023-11-11 05:33:55 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #474: GFLOPs: 4933.6447. Time: 28.8189 us. Best GFLOPs: 5451.0520
2023-11-11 05:33:55 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #475: GFLOPs: 5342.6950. Time: 26.6125 us. Best GFLOPs: 5451.0520
2023-11-11 05:33:55 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #476: GFLOPs: 4907.8190. Time: 28.9706 us. Best GFLOPs: 5451.0520
2023-11-11 05:33:55 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #477: GFLOPs: 4932.5388. Time: 28.8254 us. Best GFLOPs: 5451.0520
2023-11-11 05:33:55 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #478: GFLOPs: 5029.8905. Time: 28.2675 us. Best GFLOPs: 5451.0520
2023-11-11 05:33:55 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #479: GFLOPs: 4369.7439. Time: 32.5379 us. Best GFLOPs: 5451.0520
2023-11-11 05:33:55 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #480: GFLOPs: 4897.5808. Time: 29.0311 us. Best GFLOPs: 5451.0520
2023-11-11 05:33:55 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #481: GFLOPs: 5347.9770. Time: 26.5862 us. Best GFLOPs: 5451.0520
2023-11-11 05:33:55 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #482: GFLOPs: 4718.0108. Time: 30.1361 us. Best GFLOPs: 5451.0520
2023-11-11 05:33:55 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #483: GFLOPs: 4841.4569. Time: 29.3677 us. Best GFLOPs: 5451.0520
2023-11-11 05:33:55 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #484: GFLOPs: 4391.6947. Time: 32.3753 us. Best GFLOPs: 5451.0520
2023-11-11 05:33:55 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #485: GFLOPs: 4397.0713. Time: 32.3357 us. Best GFLOPs: 5451.0520
2023-11-11 05:33:55 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #486: GFLOPs: 5081.1369. Time: 27.9824 us. Best GFLOPs: 5451.0520
2023-11-11 05:33:55 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #487: GFLOPs: 5162.9937. Time: 27.5388 us. Best GFLOPs: 5451.0520
2023-11-11 05:33:55 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #488: GFLOPs: 5341.0242. Time: 26.6208 us. Best GFLOPs: 5451.0520
2023-11-11 05:33:55 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #489: GFLOPs: 4907.6239. Time: 28.9717 us. Best GFLOPs: 5451.0520
2023-11-11 05:33:55 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #490: GFLOPs: 4955.6434. Time: 28.6910 us. Best GFLOPs: 5451.0520
2023-11-11 05:33:55 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #491: GFLOPs: 4591.8854. Time: 30.9638 us. Best GFLOPs: 5451.0520
2023-11-11 05:33:55 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #492: GFLOPs: 5350.2213. Time: 26.5751 us. Best GFLOPs: 5451.0520
2023-11-11 05:33:55 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #493: GFLOPs: 4924.4524. Time: 28.8727 us. Best GFLOPs: 5451.0520
2023-11-11 05:33:55 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #494: GFLOPs: 4869.7803. Time: 29.1969 us. Best GFLOPs: 5451.0520
2023-11-11 05:33:55 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #495: GFLOPs: 4396.2359. Time: 32.3418 us. Best GFLOPs: 5451.0520
2023-11-11 05:33:55 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #496: GFLOPs: 5036.9291. Time: 28.2280 us. Best GFLOPs: 5451.0520
2023-11-11 05:33:55 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #497: GFLOPs: 5151.3497. Time: 27.6010 us. Best GFLOPs: 5451.0520
2023-11-11 05:33:55 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #498: GFLOPs: 4854.6630. Time: 29.2878 us. Best GFLOPs: 5451.0520
2023-11-11 05:33:55 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #499: GFLOPs: 5001.5375. Time: 28.4277 us. Best GFLOPs: 5451.0520
2023-11-11 05:33:55 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #500: GFLOPs: 4990.9209. Time: 28.4882 us. Best GFLOPs: 5451.0520
2023-11-11 05:33:55 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #501: GFLOPs: 5215.2619. Time: 27.2628 us. Best GFLOPs: 5451.0520
2023-11-11 05:33:55 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #502: GFLOPs: 4880.6114. Time: 29.1321 us. Best GFLOPs: 5451.0520
2023-11-11 05:33:55 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #503: GFLOPs: 5088.0621. Time: 27.9443 us. Best GFLOPs: 5451.0520
2023-11-11 05:33:55 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #504: GFLOPs: 4933.0933. Time: 28.8222 us. Best GFLOPs: 5451.0520
2023-11-11 05:33:55 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #505: GFLOPs: 4943.4478. Time: 28.7618 us. Best GFLOPs: 5451.0520
2023-11-11 05:33:55 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #506: GFLOPs: 4944.8648. Time: 28.7535 us. Best GFLOPs: 5451.0520
2023-11-11 05:33:55 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #507: GFLOPs: 4594.3219. Time: 30.9474 us. Best GFLOPs: 5451.0520
2023-11-11 05:33:55 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #508: GFLOPs: 5046.2611. Time: 28.1758 us. Best GFLOPs: 5451.0520
2023-11-11 05:33:55 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #509: GFLOPs: 5026.4754. Time: 28.2867 us. Best GFLOPs: 5451.0520
2023-11-11 05:33:55 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #510: GFLOPs: 1724.8336. Time: 82.4325 us. Best GFLOPs: 5451.0520
2023-11-11 05:33:55 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #511: GFLOPs: 815.0132. Time: 174.4541 us. Best GFLOPs: 5451.0520
2023-11-11 05:33:55 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #512: GFLOPs: 1148.3559. Time: 123.8139 us. Best GFLOPs: 5451.0520
2023-11-11 06:20:28 [INFO] [evolutionary_search.cc:713] Generating candidates......
2023-11-11 06:20:31 [INFO] [evolutionary_search.cc:715] Picked top 102 candidate(s) from database
2023-11-11 06:20:37 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310fb03b28)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310eec3b08)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310dc5caf8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310f7cc2d8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x563110643e78)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310dc32028)]: 403 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310dc5cfb8)]: 0 failure(s)
2023-11-11 06:20:44 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310fb03b28)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310eec3b08)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310dc5caf8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310f7cc2d8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x563110643e78)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310dc32028)]: 807 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310dc5cfb8)]: 0 failure(s)
2023-11-11 06:20:50 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310fb03b28)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310eec3b08)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310dc5caf8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310f7cc2d8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x563110643e78)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310dc32028)]: 1206 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310dc5cfb8)]: 0 failure(s)
2023-11-11 06:20:56 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310fb03b28)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310eec3b08)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310dc5caf8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310f7cc2d8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x563110643e78)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310dc32028)]: 1603 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310dc5cfb8)]: 0 failure(s)
2023-11-11 06:21:02 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310fb03b28)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310eec3b08)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310dc5caf8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310f7cc2d8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x563110643e78)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310dc32028)]: 2006 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310dc5cfb8)]: 0 failure(s)
2023-11-11 06:21:08 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310fb03b28)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310eec3b08)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310dc5caf8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310f7cc2d8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x563110643e78)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310dc32028)]: 2406 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310dc5cfb8)]: 0 failure(s)
2023-11-11 06:21:08 [INFO] [evolutionary_search.cc:723] Sampled 54 candidate(s)
2023-11-11 06:21:23 [INFO] [evolutionary_search.cc:621] Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310fb03b28)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310eec3b08)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310dc5caf8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310f7cc2d8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x563110643e78)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310dc32028)]: 58 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310dc5cfb8)]: 0 failure(s)
2023-11-11 06:21:42 [INFO] [evolutionary_search.cc:621] Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310fb03b28)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310eec3b08)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310dc5caf8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310f7cc2d8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x563110643e78)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310dc32028)]: 78 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310dc5cfb8)]: 0 failure(s)
2023-11-11 06:22:00 [INFO] [evolutionary_search.cc:621] Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310fb03b28)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310eec3b08)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310dc5caf8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310f7cc2d8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x563110643e78)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310dc32028)]: 56 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310dc5cfb8)]: 0 failure(s)
2023-11-11 06:22:17 [INFO] [evolutionary_search.cc:621] Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310fb03b28)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310eec3b08)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310dc5caf8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310f7cc2d8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x563110643e78)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310dc32028)]: 54 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310dc5cfb8)]: 0 failure(s)
2023-11-11 06:22:22 [INFO] [evolutionary_search.cc:649] Scores of the best 64 candidates:
[1 : 16]:	1.2026  1.1984  1.1976  1.1907  1.1745  1.1476  1.1471  1.1436  1.1403  1.1194  1.1148  1.0873  1.0848  1.0824  1.0805  1.0510
[17 : 32]:	1.0330  1.0035  0.9841  0.9773  0.9767  0.9757  0.9723  0.9712  0.9690  0.9690  0.9681  0.9681  0.9675  0.9675  0.9656  0.9656
[33 : 48]:	0.9644  0.9642  0.9639  0.9631  0.9610  0.9602  0.9602  0.9602  0.9600  0.9596  0.9593  0.9592  0.9586  0.9582  0.9577  0.9576
[49 : 64]:	0.9573  0.9571  0.9571  0.9571  0.9569  0.9569  0.9567  0.9558  0.9557  0.9552  0.9551  0.9550  0.9549  0.9549  0.9549  0.9548
2023-11-11 06:22:23 [INFO] [evolutionary_search.cc:727] Got 64 candidate(s) with evolutionary search
2023-11-11 06:22:23 [INFO] [evolutionary_search.cc:730] Sending 64 candidates(s) for measurement
2023-11-11 06:23:00 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #513: GFLOPs: 4602.4720. Time: 30.8926 us. Best GFLOPs: 5451.0520
2023-11-11 06:23:00 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #514: GFLOPs: 4931.4487. Time: 28.8318 us. Best GFLOPs: 5451.0520
2023-11-11 06:23:00 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #515: GFLOPs: 4882.8547. Time: 29.1187 us. Best GFLOPs: 5451.0520
2023-11-11 06:23:00 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #516: GFLOPs: 4930.1001. Time: 28.8397 us. Best GFLOPs: 5451.0520
2023-11-11 06:23:00 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #517: GFLOPs: 4950.7557. Time: 28.7193 us. Best GFLOPs: 5451.0520
2023-11-11 06:23:00 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #518: GFLOPs: 4807.2002. Time: 29.5770 us. Best GFLOPs: 5451.0520
2023-11-11 06:23:00 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #519: GFLOPs: 4833.3008. Time: 29.4172 us. Best GFLOPs: 5451.0520
2023-11-11 06:23:00 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #520: GFLOPs: 4805.4911. Time: 29.5875 us. Best GFLOPs: 5451.0520
2023-11-11 06:23:00 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #521: GFLOPs: 4280.6105. Time: 33.2154 us. Best GFLOPs: 5451.0520
2023-11-11 06:23:00 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #522: GFLOPs: 4067.6322. Time: 34.9546 us. Best GFLOPs: 5451.0520
2023-11-11 06:23:00 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #523: GFLOPs: 4864.8012. Time: 29.2268 us. Best GFLOPs: 5451.0520
2023-11-11 06:23:00 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #524: GFLOPs: 4858.5647. Time: 29.2643 us. Best GFLOPs: 5451.0520
2023-11-11 06:23:00 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #525: GFLOPs: 5052.0749. Time: 28.1434 us. Best GFLOPs: 5451.0520
2023-11-11 06:23:00 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #526: GFLOPs: 5090.3134. Time: 27.9320 us. Best GFLOPs: 5451.0520
2023-11-11 06:23:00 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #527: GFLOPs: 4836.0154. Time: 29.4007 us. Best GFLOPs: 5451.0520
2023-11-11 06:23:00 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #528: GFLOPs: 4035.5007. Time: 35.2329 us. Best GFLOPs: 5451.0520
2023-11-11 06:23:00 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #529: GFLOPs: 4620.8311. Time: 30.7699 us. Best GFLOPs: 5451.0520
2023-11-11 06:23:00 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #530: GFLOPs: 4828.0034. Time: 29.4495 us. Best GFLOPs: 5451.0520
2023-11-11 06:23:00 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #531: GFLOPs: 5415.1781. Time: 26.2563 us. Best GFLOPs: 5451.0520
2023-11-11 06:23:00 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #532: GFLOPs: 5385.2057. Time: 26.4024 us. Best GFLOPs: 5451.0520
2023-11-11 06:23:00 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #533: GFLOPs: 4958.4964. Time: 28.6745 us. Best GFLOPs: 5451.0520
2023-11-11 06:23:00 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #534: GFLOPs: 5139.9302. Time: 27.6623 us. Best GFLOPs: 5451.0520
2023-11-11 06:23:00 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #535: GFLOPs: 4955.7089. Time: 28.6906 us. Best GFLOPs: 5451.0520
2023-11-11 06:23:00 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #536: GFLOPs: 4973.5029. Time: 28.5880 us. Best GFLOPs: 5451.0520
2023-11-11 06:23:00 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #537: GFLOPs: 5160.8258. Time: 27.5503 us. Best GFLOPs: 5451.0520
2023-11-11 06:23:00 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #538: GFLOPs: 5184.5984. Time: 27.4240 us. Best GFLOPs: 5451.0520
2023-11-11 06:23:00 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #539: GFLOPs: 5113.8659. Time: 27.8033 us. Best GFLOPs: 5451.0520
2023-11-11 06:23:00 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #540: GFLOPs: 5167.5208. Time: 27.5146 us. Best GFLOPs: 5451.0520
2023-11-11 06:23:00 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #541: GFLOPs: 5155.5691. Time: 27.5784 us. Best GFLOPs: 5451.0520
2023-11-11 06:23:00 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #542: GFLOPs: 5148.3026. Time: 27.6173 us. Best GFLOPs: 5451.0520
2023-11-11 06:23:00 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #543: GFLOPs: 5264.8611. Time: 27.0059 us. Best GFLOPs: 5451.0520
2023-11-11 06:23:00 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #544: GFLOPs: 5009.6108. Time: 28.3819 us. Best GFLOPs: 5451.0520
2023-11-11 06:23:00 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #545: GFLOPs: 5136.5868. Time: 27.6803 us. Best GFLOPs: 5451.0520
2023-11-11 06:23:00 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #546: GFLOPs: 5058.4684. Time: 28.1078 us. Best GFLOPs: 5451.0520
2023-11-11 06:23:00 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #547: GFLOPs: 5228.1019. Time: 27.1958 us. Best GFLOPs: 5451.0520
2023-11-11 06:23:00 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #548: GFLOPs: 5157.1485. Time: 27.5700 us. Best GFLOPs: 5451.0520
2023-11-11 06:23:00 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #549: GFLOPs: 5304.3755. Time: 26.8047 us. Best GFLOPs: 5451.0520
2023-11-11 06:23:00 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #550: GFLOPs: 5059.5048. Time: 28.1020 us. Best GFLOPs: 5451.0520
2023-11-11 06:23:00 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #551: GFLOPs: 5116.9025. Time: 27.7868 us. Best GFLOPs: 5451.0520
2023-11-11 06:23:00 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #552: GFLOPs: 5466.7855. Time: 26.0084 us. Best GFLOPs: 5466.7855
2023-11-11 06:23:00 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #553: GFLOPs: 5229.6734. Time: 27.1876 us. Best GFLOPs: 5466.7855
2023-11-11 06:23:00 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #554: GFLOPs: 5468.2942. Time: 26.0012 us. Best GFLOPs: 5468.2942
2023-11-11 06:23:00 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #555: GFLOPs: 4978.9318. Time: 28.5568 us. Best GFLOPs: 5468.2942
2023-11-11 06:23:00 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #556: GFLOPs: 5155.6377. Time: 27.5780 us. Best GFLOPs: 5468.2942
2023-11-11 06:23:00 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #557: GFLOPs: 5137.5770. Time: 27.6750 us. Best GFLOPs: 5468.2942
2023-11-11 06:23:00 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #558: GFLOPs: 5248.3792. Time: 27.0907 us. Best GFLOPs: 5468.2942
2023-11-11 06:23:00 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #559: GFLOPs: 5335.6773. Time: 26.6475 us. Best GFLOPs: 5468.2942
2023-11-11 06:23:00 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #560: GFLOPs: 4824.4349. Time: 29.4713 us. Best GFLOPs: 5468.2942
2023-11-11 06:23:00 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #561: GFLOPs: 5308.5658. Time: 26.7836 us. Best GFLOPs: 5468.2942
2023-11-11 06:23:00 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #562: GFLOPs: 5322.2121. Time: 26.7149 us. Best GFLOPs: 5468.2942
2023-11-11 06:23:00 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #563: GFLOPs: 5338.5153. Time: 26.6333 us. Best GFLOPs: 5468.2942
2023-11-11 06:23:00 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #564: GFLOPs: 5224.8957. Time: 27.2125 us. Best GFLOPs: 5468.2942
2023-11-11 06:23:00 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #565: GFLOPs: 5174.1019. Time: 27.4796 us. Best GFLOPs: 5468.2942
2023-11-11 06:23:00 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #566: GFLOPs: 5149.2611. Time: 27.6122 us. Best GFLOPs: 5468.2942
2023-11-11 06:23:00 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #567: GFLOPs: 4976.8116. Time: 28.5690 us. Best GFLOPs: 5468.2942
2023-11-11 06:23:00 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #568: GFLOPs: 5267.3613. Time: 26.9931 us. Best GFLOPs: 5468.2942
2023-11-11 06:23:00 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #569: GFLOPs: 5277.1947. Time: 26.9428 us. Best GFLOPs: 5468.2942
2023-11-11 06:23:00 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #570: GFLOPs: 5185.8484. Time: 27.4174 us. Best GFLOPs: 5468.2942
2023-11-11 06:23:00 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #571: GFLOPs: 5102.0028. Time: 27.8680 us. Best GFLOPs: 5468.2942
2023-11-11 06:23:00 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #572: GFLOPs: 4427.5952. Time: 32.1128 us. Best GFLOPs: 5468.2942
2023-11-11 06:23:00 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #573: GFLOPs: 5496.2731. Time: 25.8689 us. Best GFLOPs: 5496.2731
2023-11-11 06:23:00 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #574: GFLOPs: 431.6489. Time: 329.3936 us. Best GFLOPs: 5496.2731
2023-11-11 06:23:00 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #575: GFLOPs: 161.2972. Time: 881.4934 us. Best GFLOPs: 5496.2731
2023-11-11 06:23:00 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #576: GFLOPs: 777.9058. Time: 182.7759 us. Best GFLOPs: 5496.2731
2023-11-11 07:07:24 [INFO] [evolutionary_search.cc:713] Generating candidates......
2023-11-11 07:07:27 [INFO] [evolutionary_search.cc:715] Picked top 102 candidate(s) from database
2023-11-11 07:07:33 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310fb03b28)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310eec3b08)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310dc5caf8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310f7cc2d8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x563110643e78)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310dc32028)]: 407 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310dc5cfb8)]: 0 failure(s)
2023-11-11 07:07:39 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310fb03b28)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310eec3b08)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310dc5caf8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310f7cc2d8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x563110643e78)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310dc32028)]: 809 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310dc5cfb8)]: 0 failure(s)
2023-11-11 07:07:45 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310fb03b28)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310eec3b08)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310dc5caf8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310f7cc2d8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x563110643e78)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310dc32028)]: 1201 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310dc5cfb8)]: 0 failure(s)
2023-11-11 07:07:51 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310fb03b28)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310eec3b08)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310dc5caf8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310f7cc2d8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x563110643e78)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310dc32028)]: 1603 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310dc5cfb8)]: 0 failure(s)
2023-11-11 07:07:57 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310fb03b28)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310eec3b08)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310dc5caf8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310f7cc2d8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x563110643e78)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310dc32028)]: 2004 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310dc5cfb8)]: 0 failure(s)
2023-11-11 07:08:03 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310fb03b28)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310eec3b08)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310dc5caf8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310f7cc2d8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x563110643e78)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310dc32028)]: 2407 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310dc5cfb8)]: 0 failure(s)
2023-11-11 07:08:03 [INFO] [evolutionary_search.cc:723] Sampled 53 candidate(s)
2023-11-11 07:08:18 [INFO] [evolutionary_search.cc:621] Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310fb03b28)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310eec3b08)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310dc5caf8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310f7cc2d8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x563110643e78)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310dc32028)]: 58 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310dc5cfb8)]: 0 failure(s)
2023-11-11 07:08:35 [INFO] [evolutionary_search.cc:621] Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310fb03b28)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310eec3b08)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310dc5caf8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310f7cc2d8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x563110643e78)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310dc32028)]: 62 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310dc5cfb8)]: 0 failure(s)
2023-11-11 07:08:54 [INFO] [evolutionary_search.cc:621] Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310fb03b28)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310eec3b08)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310dc5caf8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310f7cc2d8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x563110643e78)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310dc32028)]: 104 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310dc5cfb8)]: 0 failure(s)
2023-11-11 07:09:13 [INFO] [evolutionary_search.cc:621] Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310fb03b28)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310eec3b08)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310dc5caf8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310f7cc2d8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x563110643e78)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310dc32028)]: 90 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310dc5cfb8)]: 0 failure(s)
2023-11-11 07:09:18 [INFO] [evolutionary_search.cc:649] Scores of the best 64 candidates:
[1 : 16]:	1.0941  0.9874  0.9855  0.9826  0.9819  0.9810  0.9801  0.9781  0.9757  0.9747  0.9744  0.9738  0.9733  0.9730  0.9705  0.9695
[17 : 32]:	0.9670  0.9646  0.9642  0.9637  0.9632  0.9630  0.9628  0.9622  0.9615  0.9612  0.9604  0.9604  0.9603  0.9600  0.9591  0.9591
[33 : 48]:	0.9586  0.9578  0.9574  0.9570  0.9570  0.9567  0.9560  0.9559  0.9556  0.9555  0.9552  0.9549  0.9545  0.9543  0.9543  0.9541
[49 : 64]:	0.9539  0.9539  0.9537  0.9535  0.9535  0.9530  0.9527  0.9525  0.9522  0.9519  0.9519  0.9518  0.9515  0.9513  0.9506  0.9506
2023-11-11 07:09:19 [INFO] [evolutionary_search.cc:727] Got 64 candidate(s) with evolutionary search
2023-11-11 07:09:19 [INFO] [evolutionary_search.cc:730] Sending 64 candidates(s) for measurement
2023-11-11 07:09:57 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #577: GFLOPs: 764.2730. Time: 186.0361 us. Best GFLOPs: 5496.2731
2023-11-11 07:09:57 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #578: GFLOPs: 5219.0520. Time: 27.2430 us. Best GFLOPs: 5496.2731
2023-11-11 07:09:57 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #579: GFLOPs: 5133.6264. Time: 27.6963 us. Best GFLOPs: 5496.2731
2023-11-11 07:09:57 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #580: GFLOPs: 5123.6401. Time: 27.7503 us. Best GFLOPs: 5496.2731
2023-11-11 07:09:57 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #581: GFLOPs: 5123.7946. Time: 27.7494 us. Best GFLOPs: 5496.2731
2023-11-11 07:09:57 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #582: GFLOPs: 5216.6511. Time: 27.2555 us. Best GFLOPs: 5496.2731
2023-11-11 07:09:57 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #583: GFLOPs: 5229.1845. Time: 27.1902 us. Best GFLOPs: 5496.2731
2023-11-11 07:09:57 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #584: GFLOPs: 5174.8279. Time: 27.4758 us. Best GFLOPs: 5496.2731
2023-11-11 07:09:57 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #585: GFLOPs: 5228.3777. Time: 27.1944 us. Best GFLOPs: 5496.2731
2023-11-11 07:09:57 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #586: GFLOPs: 5247.5801. Time: 27.0949 us. Best GFLOPs: 5496.2731
2023-11-11 07:09:57 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #587: GFLOPs: 5142.5924. Time: 27.6480 us. Best GFLOPs: 5496.2731
2023-11-11 07:09:57 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #588: GFLOPs: 5246.5232. Time: 27.1003 us. Best GFLOPs: 5496.2731
2023-11-11 07:09:57 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #589: GFLOPs: 5221.9737. Time: 27.2277 us. Best GFLOPs: 5496.2731
2023-11-11 07:09:57 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #590: GFLOPs: 5386.1813. Time: 26.3976 us. Best GFLOPs: 5496.2731
2023-11-11 07:09:57 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #591: GFLOPs: 5171.3725. Time: 27.4941 us. Best GFLOPs: 5496.2731
2023-11-11 07:09:57 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #592: GFLOPs: 5384.5505. Time: 26.4056 us. Best GFLOPs: 5496.2731
2023-11-11 07:09:57 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #593: GFLOPs: 5385.0503. Time: 26.4032 us. Best GFLOPs: 5496.2731
2023-11-11 07:09:57 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #594: GFLOPs: 5198.0670. Time: 27.3529 us. Best GFLOPs: 5496.2731
2023-11-11 07:09:57 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #595: GFLOPs: 5182.1147. Time: 27.4371 us. Best GFLOPs: 5496.2731
2023-11-11 07:09:57 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #596: GFLOPs: 5182.5601. Time: 27.4348 us. Best GFLOPs: 5496.2731
2023-11-11 07:09:57 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #597: GFLOPs: 5243.0761. Time: 27.1181 us. Best GFLOPs: 5496.2731
2023-11-11 07:09:57 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #598: GFLOPs: 5231.3577. Time: 27.1789 us. Best GFLOPs: 5496.2731
2023-11-11 07:09:57 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #599: GFLOPs: 5071.5468. Time: 28.0353 us. Best GFLOPs: 5496.2731
2023-11-11 07:09:57 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #600: GFLOPs: 5268.9193. Time: 26.9851 us. Best GFLOPs: 5496.2731
2023-11-11 07:09:57 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #601: GFLOPs: 5207.2555. Time: 27.3047 us. Best GFLOPs: 5496.2731
2023-11-11 07:09:57 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #602: GFLOPs: 5219.2225. Time: 27.2421 us. Best GFLOPs: 5496.2731
2023-11-11 07:09:57 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #603: GFLOPs: 5081.1711. Time: 27.9822 us. Best GFLOPs: 5496.2731
2023-11-11 07:09:57 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #604: GFLOPs: 5177.1156. Time: 27.4636 us. Best GFLOPs: 5496.2731
2023-11-11 07:09:57 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #605: GFLOPs: 5222.8058. Time: 27.2234 us. Best GFLOPs: 5496.2731
2023-11-11 07:09:57 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #606: GFLOPs: 5169.8397. Time: 27.5023 us. Best GFLOPs: 5496.2731
2023-11-11 07:09:57 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #607: GFLOPs: 5244.9770. Time: 27.1083 us. Best GFLOPs: 5496.2731
2023-11-11 07:09:57 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #608: GFLOPs: 5225.5577. Time: 27.2090 us. Best GFLOPs: 5496.2731
2023-11-11 07:09:57 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #609: GFLOPs: 5295.4846. Time: 26.8497 us. Best GFLOPs: 5496.2731
2023-11-11 07:09:57 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #610: GFLOPs: 5191.7241. Time: 27.3864 us. Best GFLOPs: 5496.2731
2023-11-11 07:09:57 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #611: GFLOPs: 5110.0817. Time: 27.8239 us. Best GFLOPs: 5496.2731
2023-11-11 07:09:57 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #612: GFLOPs: 5428.4012. Time: 26.1923 us. Best GFLOPs: 5496.2731
2023-11-11 07:09:57 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #613: GFLOPs: 5229.3593. Time: 27.1893 us. Best GFLOPs: 5496.2731
2023-11-11 07:09:57 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #614: GFLOPs: 5155.1226. Time: 27.5808 us. Best GFLOPs: 5496.2731
2023-11-11 07:09:57 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #615: GFLOPs: 5078.5124. Time: 27.9969 us. Best GFLOPs: 5496.2731
2023-11-11 07:09:57 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #616: GFLOPs: 5111.9191. Time: 27.8139 us. Best GFLOPs: 5496.2731
2023-11-11 07:09:57 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #617: GFLOPs: 5161.6321. Time: 27.5460 us. Best GFLOPs: 5496.2731
2023-11-11 07:09:57 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #618: GFLOPs: 5381.4510. Time: 26.4208 us. Best GFLOPs: 5496.2731
2023-11-11 07:09:57 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #619: GFLOPs: 5090.2955. Time: 27.9321 us. Best GFLOPs: 5496.2731
2023-11-11 07:09:57 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #620: GFLOPs: 5138.5790. Time: 27.6696 us. Best GFLOPs: 5496.2731
2023-11-11 07:09:57 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #621: GFLOPs: 5183.2457. Time: 27.4312 us. Best GFLOPs: 5496.2731
2023-11-11 07:09:57 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #622: GFLOPs: 5452.3864. Time: 26.0771 us. Best GFLOPs: 5496.2731
2023-11-11 07:09:57 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #623: GFLOPs: 5108.7333. Time: 27.8312 us. Best GFLOPs: 5496.2731
2023-11-11 07:09:57 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #624: GFLOPs: 5215.9913. Time: 27.2589 us. Best GFLOPs: 5496.2731
2023-11-11 07:09:57 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #625: GFLOPs: 5363.7585. Time: 26.5080 us. Best GFLOPs: 5496.2731
2023-11-11 07:09:57 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #626: GFLOPs: 4982.4476. Time: 28.5367 us. Best GFLOPs: 5496.2731
2023-11-11 07:09:57 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #627: GFLOPs: 5097.1097. Time: 27.8947 us. Best GFLOPs: 5496.2731
2023-11-11 07:09:57 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #628: GFLOPs: 5056.3962. Time: 28.1193 us. Best GFLOPs: 5496.2731
2023-11-11 07:09:57 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #629: GFLOPs: 5310.0236. Time: 26.7762 us. Best GFLOPs: 5496.2731
2023-11-11 07:09:57 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #630: GFLOPs: 5453.5767. Time: 26.0714 us. Best GFLOPs: 5496.2731
2023-11-11 07:09:57 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #631: GFLOPs: 5066.8937. Time: 28.0611 us. Best GFLOPs: 5496.2731
2023-11-11 07:09:57 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #632: GFLOPs: 5038.4209. Time: 28.2196 us. Best GFLOPs: 5496.2731
2023-11-11 07:09:57 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #633: GFLOPs: 5168.8016. Time: 27.5078 us. Best GFLOPs: 5496.2731
2023-11-11 07:09:57 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #634: GFLOPs: 5234.3484. Time: 27.1633 us. Best GFLOPs: 5496.2731
2023-11-11 07:09:57 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #635: GFLOPs: 5001.7762. Time: 28.4264 us. Best GFLOPs: 5496.2731
2023-11-11 07:09:57 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #636: GFLOPs: 5152.6870. Time: 27.5938 us. Best GFLOPs: 5496.2731
2023-11-11 07:09:57 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #637: GFLOPs: 5099.6908. Time: 27.8806 us. Best GFLOPs: 5496.2731
2023-11-11 07:09:57 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #638: GFLOPs: 1777.4168. Time: 79.9938 us. Best GFLOPs: 5496.2731
2023-11-11 07:09:57 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #639: GFLOPs: 731.1206. Time: 194.4719 us. Best GFLOPs: 5496.2731
2023-11-11 07:09:57 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #640: GFLOPs: 2373.0576. Time: 59.9153 us. Best GFLOPs: 5496.2731
2023-11-11 07:55:15 [INFO] [evolutionary_search.cc:713] Generating candidates......
2023-11-11 07:55:18 [INFO] [evolutionary_search.cc:715] Picked top 102 candidate(s) from database
2023-11-11 07:55:23 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310fb03b28)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310eec3b08)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310dc5caf8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310f7cc2d8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x563110643e78)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310dc32028)]: 402 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310dc5cfb8)]: 0 failure(s)
2023-11-11 07:55:29 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310fb03b28)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310eec3b08)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310dc5caf8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310f7cc2d8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x563110643e78)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310dc32028)]: 804 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310dc5cfb8)]: 0 failure(s)
2023-11-11 07:55:36 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310fb03b28)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310eec3b08)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310dc5caf8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310f7cc2d8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x563110643e78)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310dc32028)]: 1202 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310dc5cfb8)]: 0 failure(s)
2023-11-11 07:55:42 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310fb03b28)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310eec3b08)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310dc5caf8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310f7cc2d8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x563110643e78)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310dc32028)]: 1602 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310dc5cfb8)]: 0 failure(s)
2023-11-11 07:55:48 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310fb03b28)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310eec3b08)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310dc5caf8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310f7cc2d8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x563110643e78)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310dc32028)]: 2006 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310dc5cfb8)]: 0 failure(s)
2023-11-11 07:55:54 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310fb03b28)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310eec3b08)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310dc5caf8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310f7cc2d8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x563110643e78)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310dc32028)]: 2402 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310dc5cfb8)]: 0 failure(s)
2023-11-11 07:55:54 [INFO] [evolutionary_search.cc:723] Sampled 58 candidate(s)
2023-11-11 07:56:09 [INFO] [evolutionary_search.cc:621] Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310fb03b28)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310eec3b08)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310dc5caf8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310f7cc2d8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x563110643e78)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310dc32028)]: 69 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310dc5cfb8)]: 0 failure(s)
2023-11-11 07:56:26 [INFO] [evolutionary_search.cc:621] Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310fb03b28)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310eec3b08)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310dc5caf8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310f7cc2d8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x563110643e78)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310dc32028)]: 70 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310dc5cfb8)]: 0 failure(s)
2023-11-11 07:56:45 [INFO] [evolutionary_search.cc:621] Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310fb03b28)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310eec3b08)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310dc5caf8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310f7cc2d8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x563110643e78)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310dc32028)]: 97 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310dc5cfb8)]: 0 failure(s)
2023-11-11 07:57:03 [INFO] [evolutionary_search.cc:621] Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310fb03b28)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310eec3b08)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310dc5caf8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310f7cc2d8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x563110643e78)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310dc32028)]: 95 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310dc5cfb8)]: 0 failure(s)
2023-11-11 07:57:08 [INFO] [evolutionary_search.cc:649] Scores of the best 64 candidates:
[1 : 16]:	1.0009  0.9891  0.9872  0.9802  0.9798  0.9790  0.9783  0.9737  0.9716  0.9716  0.9700  0.9692  0.9679  0.9671  0.9667  0.9662
[17 : 32]:	0.9652  0.9642  0.9641  0.9639  0.9638  0.9637  0.9636  0.9634  0.9631  0.9630  0.9629  0.9627  0.9623  0.9621  0.9621  0.9620
[33 : 48]:	0.9613  0.9611  0.9603  0.9597  0.9597  0.9597  0.9592  0.9588  0.9587  0.9582  0.9577  0.9571  0.9568  0.9567  0.9564  0.9562
[49 : 64]:	0.9562  0.9561  0.9561  0.9559  0.9557  0.9556  0.9555  0.9552  0.9552  0.9547  0.9547  0.9547  0.9545  0.9545  0.9538  0.9538
2023-11-11 07:57:08 [INFO] [evolutionary_search.cc:727] Got 64 candidate(s) with evolutionary search
2023-11-11 07:57:08 [INFO] [evolutionary_search.cc:730] Sending 64 candidates(s) for measurement
2023-11-11 07:57:45 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #641: GFLOPs: 5165.8141. Time: 27.5237 us. Best GFLOPs: 5496.2731
2023-11-11 07:57:45 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #642: GFLOPs: 5148.8104. Time: 27.6146 us. Best GFLOPs: 5496.2731
2023-11-11 07:57:45 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #643: GFLOPs: 5131.7995. Time: 27.7061 us. Best GFLOPs: 5496.2731
2023-11-11 07:57:45 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #644: GFLOPs: 5170.7164. Time: 27.4976 us. Best GFLOPs: 5496.2731
2023-11-11 07:57:45 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #645: GFLOPs: 5097.7886. Time: 27.8910 us. Best GFLOPs: 5496.2731
2023-11-11 07:57:45 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #646: GFLOPs: 5147.0709. Time: 27.6239 us. Best GFLOPs: 5496.2731
2023-11-11 07:57:45 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #647: GFLOPs: 5088.4679. Time: 27.9421 us. Best GFLOPs: 5496.2731
2023-11-11 07:57:45 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #648: GFLOPs: 5116.5328. Time: 27.7888 us. Best GFLOPs: 5496.2731
2023-11-11 07:57:45 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #649: GFLOPs: 5072.5667. Time: 28.0297 us. Best GFLOPs: 5496.2731
2023-11-11 07:57:45 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #650: GFLOPs: 5122.2599. Time: 27.7577 us. Best GFLOPs: 5496.2731
2023-11-11 07:57:45 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #651: GFLOPs: 5291.3860. Time: 26.8705 us. Best GFLOPs: 5496.2731
2023-11-11 07:57:45 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #652: GFLOPs: 5175.9807. Time: 27.4697 us. Best GFLOPs: 5496.2731
2023-11-11 07:57:45 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #653: GFLOPs: 5341.0242. Time: 26.6208 us. Best GFLOPs: 5496.2731
2023-11-11 07:57:45 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #654: GFLOPs: 5122.7926. Time: 27.7549 us. Best GFLOPs: 5496.2731
2023-11-11 07:57:45 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #655: GFLOPs: 5210.7293. Time: 27.2865 us. Best GFLOPs: 5496.2731
2023-11-11 07:57:45 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #656: GFLOPs: 5325.3270. Time: 26.6993 us. Best GFLOPs: 5496.2731
2023-11-11 07:57:45 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #657: GFLOPs: 5233.4788. Time: 27.1679 us. Best GFLOPs: 5496.2731
2023-11-11 07:57:45 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #658: GFLOPs: 5400.0234. Time: 26.3300 us. Best GFLOPs: 5496.2731
2023-11-11 07:57:45 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #659: GFLOPs: 5279.8196. Time: 26.9294 us. Best GFLOPs: 5496.2731
2023-11-11 07:57:45 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #660: GFLOPs: 5105.2358. Time: 27.8503 us. Best GFLOPs: 5496.2731
2023-11-11 07:57:45 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #661: GFLOPs: 5145.7734. Time: 27.6309 us. Best GFLOPs: 5496.2731
2023-11-11 07:57:45 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #662: GFLOPs: 5228.1727. Time: 27.1954 us. Best GFLOPs: 5496.2731
2023-11-11 07:57:45 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #663: GFLOPs: 5187.0893. Time: 27.4108 us. Best GFLOPs: 5496.2731
2023-11-11 07:57:45 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #664: GFLOPs: 5453.8684. Time: 26.0700 us. Best GFLOPs: 5496.2731
2023-11-11 07:57:45 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #665: GFLOPs: 5022.4183. Time: 28.3095 us. Best GFLOPs: 5496.2731
2023-11-11 07:57:45 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #666: GFLOPs: 5097.0419. Time: 27.8951 us. Best GFLOPs: 5496.2731
2023-11-11 07:57:45 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #667: GFLOPs: 5086.0364. Time: 27.9554 us. Best GFLOPs: 5496.2731
2023-11-11 07:57:45 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #668: GFLOPs: 5182.0805. Time: 27.4373 us. Best GFLOPs: 5496.2731
2023-11-11 07:57:45 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #669: GFLOPs: 5129.3922. Time: 27.7192 us. Best GFLOPs: 5496.2731
2023-11-11 07:57:45 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #670: GFLOPs: 5156.1180. Time: 27.5755 us. Best GFLOPs: 5496.2731
2023-11-11 07:57:45 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #671: GFLOPs: 5417.6267. Time: 26.2444 us. Best GFLOPs: 5496.2731
2023-11-11 07:57:45 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #672: GFLOPs: 5245.1472. Time: 27.1074 us. Best GFLOPs: 5496.2731
2023-11-11 07:57:45 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #673: GFLOPs: 5436.6824. Time: 26.1524 us. Best GFLOPs: 5496.2731
2023-11-11 07:57:45 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #674: GFLOPs: 5121.0308. Time: 27.7644 us. Best GFLOPs: 5496.2731
2023-11-11 07:57:45 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #675: GFLOPs: 5259.7946. Time: 27.0319 us. Best GFLOPs: 5496.2731
2023-11-11 07:57:45 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #676: GFLOPs: 5186.5083. Time: 27.4139 us. Best GFLOPs: 5496.2731
2023-11-11 07:57:45 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #677: GFLOPs: 5315.1129. Time: 26.7506 us. Best GFLOPs: 5496.2731
2023-11-11 07:57:45 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #678: GFLOPs: 5140.7834. Time: 27.6577 us. Best GFLOPs: 5496.2731
2023-11-11 07:57:45 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #679: GFLOPs: 5254.5695. Time: 27.0588 us. Best GFLOPs: 5496.2731
2023-11-11 07:57:45 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #680: GFLOPs: 5140.4762. Time: 27.6594 us. Best GFLOPs: 5496.2731
2023-11-11 07:57:45 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #681: GFLOPs: 5243.2236. Time: 27.1174 us. Best GFLOPs: 5496.2731
2023-11-11 07:57:45 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #682: GFLOPs: 5121.8101. Time: 27.7602 us. Best GFLOPs: 5496.2731
2023-11-11 07:57:45 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #683: GFLOPs: 5079.9235. Time: 27.9891 us. Best GFLOPs: 5496.2731
2023-11-11 07:57:45 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #684: GFLOPs: 5144.8820. Time: 27.6357 us. Best GFLOPs: 5496.2731
2023-11-11 07:57:45 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #685: GFLOPs: 5190.6107. Time: 27.3922 us. Best GFLOPs: 5496.2731
2023-11-11 07:57:45 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #686: GFLOPs: 5158.4539. Time: 27.5630 us. Best GFLOPs: 5496.2731
2023-11-11 07:57:45 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #687: GFLOPs: 5239.3567. Time: 27.1374 us. Best GFLOPs: 5496.2731
2023-11-11 07:57:45 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #688: GFLOPs: 5069.7943. Time: 28.0450 us. Best GFLOPs: 5496.2731
2023-11-11 07:57:45 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #689: GFLOPs: 5406.5129. Time: 26.2984 us. Best GFLOPs: 5496.2731
2023-11-11 07:57:45 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #690: GFLOPs: 5294.6978. Time: 26.8537 us. Best GFLOPs: 5496.2731
2023-11-11 07:57:45 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #691: GFLOPs: 5411.5537. Time: 26.2739 us. Best GFLOPs: 5496.2731
2023-11-11 07:57:45 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #692: GFLOPs: 5397.1233. Time: 26.3441 us. Best GFLOPs: 5496.2731
2023-11-11 07:57:45 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #693: GFLOPs: 5161.0068. Time: 27.5494 us. Best GFLOPs: 5496.2731
2023-11-11 07:57:45 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #694: GFLOPs: 5257.2488. Time: 27.0450 us. Best GFLOPs: 5496.2731
2023-11-11 07:57:45 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #695: GFLOPs: 5219.5702. Time: 27.2403 us. Best GFLOPs: 5496.2731
2023-11-11 07:57:45 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #696: GFLOPs: 5126.5748. Time: 27.7344 us. Best GFLOPs: 5496.2731
2023-11-11 07:57:45 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #697: GFLOPs: 5130.0454. Time: 27.7156 us. Best GFLOPs: 5496.2731
2023-11-11 07:57:45 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #698: GFLOPs: 5276.3002. Time: 26.9474 us. Best GFLOPs: 5496.2731
2023-11-11 07:57:45 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #699: GFLOPs: 5172.8670. Time: 27.4862 us. Best GFLOPs: 5496.2731
2023-11-11 07:57:45 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #700: GFLOPs: 5203.3825. Time: 27.3250 us. Best GFLOPs: 5496.2731
2023-11-11 07:57:45 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #701: GFLOPs: 5150.8367. Time: 27.6037 us. Best GFLOPs: 5496.2731
2023-11-11 07:57:45 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #702: GFLOPs: 772.5343. Time: 184.0467 us. Best GFLOPs: 5496.2731
2023-11-11 07:57:45 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #703: GFLOPs: 9.0235. Time: 15756.8729 us. Best GFLOPs: 5496.2731
2023-11-11 07:57:45 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #704: GFLOPs: 858.8259. Time: 165.5544 us. Best GFLOPs: 5496.2731
2023-11-11 08:28:42 [INFO] [evolutionary_search.cc:713] Generating candidates......
2023-11-11 08:28:44 [INFO] [evolutionary_search.cc:715] Picked top 102 candidate(s) from database
2023-11-11 08:28:51 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310fb03b28)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310eec3b08)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310dc5caf8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310f7cc2d8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x563110643e78)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310dc32028)]: 403 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310dc5cfb8)]: 0 failure(s)
2023-11-11 08:28:57 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310fb03b28)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310eec3b08)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310dc5caf8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310f7cc2d8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x563110643e78)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310dc32028)]: 801 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310dc5cfb8)]: 0 failure(s)
2023-11-11 08:29:03 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310fb03b28)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310eec3b08)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310dc5caf8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310f7cc2d8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x563110643e78)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310dc32028)]: 1204 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310dc5cfb8)]: 0 failure(s)
2023-11-11 08:29:09 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310fb03b28)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310eec3b08)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310dc5caf8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310f7cc2d8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x563110643e78)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310dc32028)]: 1603 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310dc5cfb8)]: 0 failure(s)
2023-11-11 08:29:15 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310fb03b28)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310eec3b08)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310dc5caf8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310f7cc2d8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x563110643e78)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310dc32028)]: 2007 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310dc5cfb8)]: 0 failure(s)
2023-11-11 08:29:21 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310fb03b28)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310eec3b08)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310dc5caf8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310f7cc2d8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x563110643e78)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310dc32028)]: 2405 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310dc5cfb8)]: 0 failure(s)
2023-11-11 08:29:21 [INFO] [evolutionary_search.cc:723] Sampled 55 candidate(s)
2023-11-11 08:29:37 [INFO] [evolutionary_search.cc:621] Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310fb03b28)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310eec3b08)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310dc5caf8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310f7cc2d8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x563110643e78)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310dc32028)]: 79 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310dc5cfb8)]: 0 failure(s)
2023-11-11 08:29:56 [INFO] [evolutionary_search.cc:621] Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310fb03b28)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310eec3b08)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310dc5caf8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310f7cc2d8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x563110643e78)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310dc32028)]: 91 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310dc5cfb8)]: 0 failure(s)
2023-11-11 08:30:16 [INFO] [evolutionary_search.cc:621] Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310fb03b28)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310eec3b08)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310dc5caf8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310f7cc2d8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x563110643e78)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310dc32028)]: 125 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310dc5cfb8)]: 0 failure(s)
2023-11-11 08:30:35 [INFO] [evolutionary_search.cc:621] Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310fb03b28)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310eec3b08)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310dc5caf8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310f7cc2d8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x563110643e78)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310dc32028)]: 113 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310dc5cfb8)]: 0 failure(s)
2023-11-11 08:30:40 [INFO] [evolutionary_search.cc:649] Scores of the best 64 candidates:
[1 : 16]:	1.2799  1.2684  1.1165  1.1039  1.0985  0.9891  0.9873  0.9813  0.9799  0.9791  0.9761  0.9744  0.9699  0.9698  0.9677  0.9661
[17 : 32]:	0.9652  0.9650  0.9644  0.9639  0.9638  0.9632  0.9631  0.9630  0.9629  0.9623  0.9613  0.9608  0.9605  0.9602  0.9602  0.9598
[33 : 48]:	0.9594  0.9593  0.9591  0.9584  0.9581  0.9578  0.9577  0.9574  0.9562  0.9559  0.9559  0.9556  0.9555  0.9553  0.9552  0.9551
[49 : 64]:	0.9550  0.9550  0.9550  0.9550  0.9547  0.9545  0.9544  0.9544  0.9543  0.9541  0.9541  0.9540  0.9537  0.9536  0.9534  0.9534
2023-11-11 08:30:40 [INFO] [evolutionary_search.cc:727] Got 64 candidate(s) with evolutionary search
2023-11-11 08:30:40 [INFO] [evolutionary_search.cc:730] Sending 64 candidates(s) for measurement
2023-11-11 08:31:17 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #705: GFLOPs: 3768.0595. Time: 37.7336 us. Best GFLOPs: 5496.2731
2023-11-11 08:31:17 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #706: GFLOPs: 3748.7208. Time: 37.9282 us. Best GFLOPs: 5496.2731
2023-11-11 08:31:17 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #707: GFLOPs: 3480.7873. Time: 40.8478 us. Best GFLOPs: 5496.2731
2023-11-11 08:31:17 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #708: GFLOPs: 3432.5956. Time: 41.4213 us. Best GFLOPs: 5496.2731
2023-11-11 08:31:17 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #709: GFLOPs: 3660.6142. Time: 38.8411 us. Best GFLOPs: 5496.2731
2023-11-11 08:31:17 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #710: GFLOPs: 4871.9942. Time: 29.1836 us. Best GFLOPs: 5496.2731
2023-11-11 08:31:17 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #711: GFLOPs: 4692.4271. Time: 30.3004 us. Best GFLOPs: 5496.2731
2023-11-11 08:31:17 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #712: GFLOPs: 5032.9704. Time: 28.2502 us. Best GFLOPs: 5496.2731
2023-11-11 08:31:17 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #713: GFLOPs: 4853.3211. Time: 29.2959 us. Best GFLOPs: 5496.2731
2023-11-11 08:31:17 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #714: GFLOPs: 4928.3459. Time: 28.8499 us. Best GFLOPs: 5496.2731
2023-11-11 08:31:17 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #715: GFLOPs: 4983.0647. Time: 28.5331 us. Best GFLOPs: 5496.2731
2023-11-11 08:31:17 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #716: GFLOPs: 4928.5229. Time: 28.8489 us. Best GFLOPs: 5496.2731
2023-11-11 08:31:17 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #717: GFLOPs: 5291.3149. Time: 26.8709 us. Best GFLOPs: 5496.2731
2023-11-11 08:31:17 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #718: GFLOPs: 5192.9676. Time: 27.3798 us. Best GFLOPs: 5496.2731
2023-11-11 08:31:17 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #719: GFLOPs: 5361.5376. Time: 26.5190 us. Best GFLOPs: 5496.2731
2023-11-11 08:31:17 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #720: GFLOPs: 5498.5138. Time: 25.8583 us. Best GFLOPs: 5498.5138
2023-11-11 08:31:17 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #721: GFLOPs: 5430.2567. Time: 26.1834 us. Best GFLOPs: 5498.5138
2023-11-11 08:31:17 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #722: GFLOPs: 5437.3736. Time: 26.1491 us. Best GFLOPs: 5498.5138
2023-11-11 08:31:17 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #723: GFLOPs: 5320.4560. Time: 26.7237 us. Best GFLOPs: 5498.5138
2023-11-11 08:31:17 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #724: GFLOPs: 5352.2796. Time: 26.5648 us. Best GFLOPs: 5498.5138
2023-11-11 08:31:17 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #725: GFLOPs: 5301.8225. Time: 26.8176 us. Best GFLOPs: 5498.5138
2023-11-11 08:31:17 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #726: GFLOPs: 5389.8342. Time: 26.3797 us. Best GFLOPs: 5498.5138
2023-11-11 08:31:17 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #727: GFLOPs: 5422.6046. Time: 26.2203 us. Best GFLOPs: 5498.5138
2023-11-11 08:31:17 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #728: GFLOPs: 5306.4347. Time: 26.7943 us. Best GFLOPs: 5498.5138
2023-11-11 08:31:17 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #729: GFLOPs: 5383.2736. Time: 26.4119 us. Best GFLOPs: 5498.5138
2023-11-11 08:31:17 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #730: GFLOPs: 5323.9985. Time: 26.7059 us. Best GFLOPs: 5498.5138
2023-11-11 08:31:17 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #731: GFLOPs: 5227.5159. Time: 27.1988 us. Best GFLOPs: 5498.5138
2023-11-11 08:31:17 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #732: GFLOPs: 5330.5256. Time: 26.6732 us. Best GFLOPs: 5498.5138
2023-11-11 08:31:17 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #733: GFLOPs: 5374.5897. Time: 26.4546 us. Best GFLOPs: 5498.5138
2023-11-11 08:31:17 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #734: GFLOPs: 5130.3610. Time: 27.7139 us. Best GFLOPs: 5498.5138
2023-11-11 08:31:17 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #735: GFLOPs: 5355.9924. Time: 26.5464 us. Best GFLOPs: 5498.5138
2023-11-11 08:31:17 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #736: GFLOPs: 5246.5869. Time: 27.1000 us. Best GFLOPs: 5498.5138
2023-11-11 08:31:17 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #737: GFLOPs: 5178.1077. Time: 27.4584 us. Best GFLOPs: 5498.5138
2023-11-11 08:31:17 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #738: GFLOPs: 5333.2525. Time: 26.6596 us. Best GFLOPs: 5498.5138
2023-11-11 08:31:17 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #739: GFLOPs: 5489.8701. Time: 25.8990 us. Best GFLOPs: 5498.5138
2023-11-11 08:31:17 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #740: GFLOPs: 5482.6498. Time: 25.9332 us. Best GFLOPs: 5498.5138
2023-11-11 08:31:17 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #741: GFLOPs: 5172.3741. Time: 27.4888 us. Best GFLOPs: 5498.5138
2023-11-11 08:31:17 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #742: GFLOPs: 5220.5092. Time: 27.2354 us. Best GFLOPs: 5498.5138
2023-11-11 08:31:17 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #743: GFLOPs: 5420.8100. Time: 26.2290 us. Best GFLOPs: 5498.5138
2023-11-11 08:31:17 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #744: GFLOPs: 5149.0897. Time: 27.6131 us. Best GFLOPs: 5498.5138
2023-11-11 08:31:17 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #745: GFLOPs: 5193.2430. Time: 27.3783 us. Best GFLOPs: 5498.5138
2023-11-11 08:31:17 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #746: GFLOPs: 5236.1737. Time: 27.1539 us. Best GFLOPs: 5498.5138
2023-11-11 08:31:17 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #747: GFLOPs: 5285.2845. Time: 26.9016 us. Best GFLOPs: 5498.5138
2023-11-11 08:31:17 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #748: GFLOPs: 5447.4448. Time: 26.1008 us. Best GFLOPs: 5498.5138
2023-11-11 08:31:17 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #749: GFLOPs: 5327.8009. Time: 26.6869 us. Best GFLOPs: 5498.5138
2023-11-11 08:31:17 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #750: GFLOPs: 5318.9398. Time: 26.7313 us. Best GFLOPs: 5498.5138
2023-11-11 08:31:17 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #751: GFLOPs: 5310.2366. Time: 26.7752 us. Best GFLOPs: 5498.5138
2023-11-11 08:31:17 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #752: GFLOPs: 5210.9583. Time: 27.2853 us. Best GFLOPs: 5498.5138
2023-11-11 08:31:17 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #753: GFLOPs: 5293.2928. Time: 26.8609 us. Best GFLOPs: 5498.5138
2023-11-11 08:31:17 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #754: GFLOPs: 5387.5433. Time: 26.3910 us. Best GFLOPs: 5498.5138
2023-11-11 08:31:17 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #755: GFLOPs: 5342.3040. Time: 26.6144 us. Best GFLOPs: 5498.5138
2023-11-11 08:31:17 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #756: GFLOPs: 5269.6640. Time: 26.9813 us. Best GFLOPs: 5498.5138
2023-11-11 08:31:17 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #757: GFLOPs: 5433.2285. Time: 26.1690 us. Best GFLOPs: 5498.5138
2023-11-11 08:31:17 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #758: GFLOPs: 5319.4687. Time: 26.7287 us. Best GFLOPs: 5498.5138
2023-11-11 08:31:17 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #759: GFLOPs: 5215.5050. Time: 27.2615 us. Best GFLOPs: 5498.5138
2023-11-11 08:31:17 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #760: GFLOPs: 5077.2271. Time: 28.0039 us. Best GFLOPs: 5498.5138
2023-11-11 08:31:17 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #761: GFLOPs: 5398.7883. Time: 26.3360 us. Best GFLOPs: 5498.5138
2023-11-11 08:31:17 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #762: GFLOPs: 5288.8097. Time: 26.8836 us. Best GFLOPs: 5498.5138
2023-11-11 08:31:17 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #763: GFLOPs: 5296.4779. Time: 26.8447 us. Best GFLOPs: 5498.5138
2023-11-11 08:31:17 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #764: GFLOPs: 5171.0340. Time: 27.4959 us. Best GFLOPs: 5498.5138
2023-11-11 08:31:17 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #765: GFLOPs: 5000.4536. Time: 28.4339 us. Best GFLOPs: 5498.5138
2023-11-11 08:31:17 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #766: GFLOPs: 967.8309. Time: 146.9083 us. Best GFLOPs: 5498.5138
2023-11-11 08:31:17 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #767: GFLOPs: 748.5795. Time: 189.9363 us. Best GFLOPs: 5498.5138
2023-11-11 08:31:17 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #768: GFLOPs: 1293.7220. Time: 109.9018 us. Best GFLOPs: 5498.5138
2023-11-11 09:04:06 [INFO] [evolutionary_search.cc:713] Generating candidates......
2023-11-11 09:04:09 [INFO] [evolutionary_search.cc:715] Picked top 102 candidate(s) from database
2023-11-11 09:04:16 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310fb03b28)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310eec3b08)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310dc5caf8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310f7cc2d8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x563110643e78)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310dc32028)]: 403 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310dc5cfb8)]: 0 failure(s)
2023-11-11 09:04:22 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310fb03b28)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310eec3b08)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310dc5caf8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310f7cc2d8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x563110643e78)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310dc32028)]: 805 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310dc5cfb8)]: 0 failure(s)
2023-11-11 09:04:28 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310fb03b28)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310eec3b08)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310dc5caf8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310f7cc2d8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x563110643e78)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310dc32028)]: 1204 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310dc5cfb8)]: 0 failure(s)
2023-11-11 09:04:34 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310fb03b28)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310eec3b08)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310dc5caf8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310f7cc2d8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x563110643e78)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310dc32028)]: 1609 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310dc5cfb8)]: 0 failure(s)
2023-11-11 09:04:40 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310fb03b28)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310eec3b08)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310dc5caf8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310f7cc2d8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x563110643e78)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310dc32028)]: 2006 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310dc5cfb8)]: 0 failure(s)
2023-11-11 09:04:46 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310fb03b28)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310eec3b08)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310dc5caf8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310f7cc2d8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x563110643e78)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310dc32028)]: 2407 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310dc5cfb8)]: 0 failure(s)
2023-11-11 09:04:46 [INFO] [evolutionary_search.cc:723] Sampled 53 candidate(s)
2023-11-11 09:05:02 [INFO] [evolutionary_search.cc:621] Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310fb03b28)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310eec3b08)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310dc5caf8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310f7cc2d8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x563110643e78)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310dc32028)]: 96 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310dc5cfb8)]: 0 failure(s)
2023-11-11 09:05:20 [INFO] [evolutionary_search.cc:621] Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310fb03b28)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310eec3b08)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310dc5caf8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310f7cc2d8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x563110643e78)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310dc32028)]: 95 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310dc5cfb8)]: 0 failure(s)
2023-11-11 09:05:39 [INFO] [evolutionary_search.cc:621] Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310fb03b28)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310eec3b08)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310dc5caf8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310f7cc2d8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x563110643e78)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310dc32028)]: 108 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310dc5cfb8)]: 0 failure(s)
2023-11-11 09:05:59 [INFO] [evolutionary_search.cc:621] Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310fb03b28)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310eec3b08)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310dc5caf8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310f7cc2d8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x563110643e78)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310dc32028)]: 128 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310dc5cfb8)]: 0 failure(s)
2023-11-11 09:06:04 [INFO] [evolutionary_search.cc:649] Scores of the best 64 candidates:
[1 : 16]:	1.4234  1.3862  1.3532  1.3347  1.2424  1.1965  1.1316  1.1267  1.1223  1.0483  0.9863  0.9840  0.9837  0.9828  0.9828  0.9807
[17 : 32]:	0.9773  0.9762  0.9731  0.9707  0.9697  0.9691  0.9687  0.9680  0.9665  0.9658  0.9636  0.9631  0.9629  0.9625  0.9625  0.9624
[33 : 48]:	0.9622  0.9621  0.9616  0.9610  0.9610  0.9607  0.9600  0.9595  0.9594  0.9592  0.9592  0.9591  0.9590  0.9588  0.9585  0.9584
[49 : 64]:	0.9580  0.9579  0.9579  0.9577  0.9577  0.9575  0.9573  0.9571  0.9571  0.9567  0.9563  0.9563  0.9559  0.9559  0.9558  0.9556
2023-11-11 09:06:04 [INFO] [evolutionary_search.cc:727] Got 64 candidate(s) with evolutionary search
2023-11-11 09:06:04 [INFO] [evolutionary_search.cc:730] Sending 64 candidates(s) for measurement
2023-11-11 09:06:44 [INFO] [task_scheduler.cc:121] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #769: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/home2/xiachunwei/Software/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/home2/xiachunwei/Software/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/home2/xiachunwei/Software/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/home2/xiachunwei/Software/tvm/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/home2/xiachunwei/Software/tvm/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/home2/xiachunwei/Software/tvm/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/home2/xiachunwei/Software/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/home2/xiachunwei/Software/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  4: _ZN3tvm7runtime13PackedFun
  3: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  2: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  0: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  File "/home2/xiachunwei/Software/tvm/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(512), T.int64(7), T.int64(7)), "float32"), p1: T.Buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(512)), "float32"), p2: T.Buffer((T.int64(1), T.int64(512), T.int64(7), T.int64(7)), "float32"), p3: T.Buffer((T.int64(1), T.int64(512), T.int64(1), T.int64(1)), "float32"), p4: T.Buffer((T.int64(1), T.int64(512), T.int64(1), T.int64(1)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(512), T.int64(7), T.int64(7)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        input_tile_local = T.alloc_buffer((T.int64(512), T.int64(16), T.int64(4), T.int64(4)), scope="local")
        data_pack = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(16)))
        bgemm = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(16)))
        inverse_local = T.alloc_buffer((T.int64(512), T.int64(16), T.int64(2), T.int64(2)), scope="local")
        data_pack_local = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(16)), scope="local")
        bgemm_local = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(16)), scope="local")
        data_pack_shared = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(16)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(512)), scope="shared")
        for ci_p_fused_0 in T.thread_binding(T.int64(64), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 512, "pragma_unroll_explicit": 1}):
            for ci_p_fused_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(1), T.int64(4), T.int64(4)):
                    with T.block("input_tile"):
                        v_ci = T.axis.spatial(T.int64(512), ci_p_fused_0 * T.int64(8) + ci_p_fused_1 // T.int64(16) + ax0)
                        v_p = T.axis.spatial(T.int64(16), ci_p_fused_1 % T.int64(16) + ax1)
                        v_eps, v_nu = T.axis.remap("SS", [ax2, ax3])
                        T.reads(p0[v_p // T.int64(16), v_ci, v_p % T.int64(16) // T.int64(4) * T.int64(2) + v_eps - T.int64(1), v_p % T.int64(4) * T.int64(2) + v_nu - T.int64(1)])
                        T.writes(input_tile_local[v_ci, v_p, v_eps, v_nu])
                        T.block_attr({"schedule_rule": "None"})
                        input_tile_local[v_ci, v_p, v_eps, v_nu] = T.if_then_else(T.int64(1) <= v_p % T.int64(16) // T.int64(4) * T.int64(2) + v_eps and v_p % T.int64(16) // T.int64(4) * T.int64(2) + v_eps < T.int64(8) and T.int64(1) <= v_p % T.int64(4) * T.int64(2) + v_nu and v_p % T.int64(4) * T.int64(2) + v_nu < T.int64(8), p0[v_p // T.int64(16), v_ci, v_p % T.int64(16) // T.int64(4) * T.int64(2) + v_eps - T.int64(1), v_p % T.int64(4) * T.int64(2) + v_nu - T.int64(1)], T.float32(0))
                for eps in T.unroll(T.int64(4)):
                    for nu in T.unroll(T.int64(4)):
                        with T.block("data_pack_init"):
                            v_eps, v_nu = T.axis.remap("SS", [eps, nu])
                            v_ci = T.axis.spatial(T.int64(512), (ci_p_fused_0 * T.int64(128) + ci_p_fused_1) // T.int64(16))
                            v_p = T.axis.spatial(T.int64(16), (ci_p_fused_0 * T.int64(128) + ci_p_fused_1) % T.int64(16))
                            T.reads()
                            T.writes(data_pack_local[v_eps, v_nu, v_ci, v_p])
                            T.block_attr({"schedule_rule": "conv2d_nchw_winograd_data_pack"})
                            data_pack_local[v_eps, v_nu, v_ci, v_p] = T.float32(0)
                        for r_a in T.unroll(T.int64(4)):
                            for r_b in T.unroll(T.int64(4)):
                                with T.block("data_pack_update"):
                                    v_eps, v_nu = T.axis.remap("SS", [eps, nu])
                                    v_ci = T.axis.spatial(T.int64(512), (ci_p_fused_0 * T.int64(128) + ci_p_fused_1) // T.int64(16))
                                    v_p = T.axis.spatial(T.int64(16), (ci_p_fused_0 * T.int64(128) + ci_p_fused_1) % T.int64(16))
                                    v_r_a, v_r_b = T.axis.remap("RR", [r_a, r_b])
                                    T.reads(data_pack_local[v_eps, v_nu, v_ci, v_p], input_tile_local[v_ci, v_p, v_r_a, v_r_b])
                                    T.writes(data_pack_local[v_eps, v_nu, v_ci, v_p])
                                    T.block_attr({"schedule_rule": "conv2d_nchw_winograd_data_pack"})
                                    data_pack_local[v_eps, v_nu, v_ci, v_p] = data_pack_local[v_eps, v_nu, v_ci, v_p] + input_tile_local[v_ci, v_p, v_r_a, v_r_b] * T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(3), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(1), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(0), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(3), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(1), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(0), T.float32(1), T.float32(0))))))))))))))))) * T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(3), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(1), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(0), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(3), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(1), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(0), T.float32(1), T.float32(0)))))))))))))))))
                for ax0, ax1, ax2, ax3 in T.grid(T.int64(4), T.int64(4), T.int64(1), T.int64(1)):
                    with T.block("data_pack_local"):
                        v0, v1 = T.axis.remap("SS", [ax0, ax1])
                        v2 = T.axis.spatial(T.int64(512), ci_p_fused_0 * T.int64(8) + ci_p_fused_1 // T.int64(16) + ax2)
                        v3 = T.axis.spatial(T.int64(16), ci_p_fused_1 % T.int64(16) + ax3)
                        T.reads(data_pack_local[v0, v1, v2, v3])
                        T.writes(data_pack[v0, v1, v2, v3])
                        data_pack[v0, v1, v2, v3] = data_pack_local[v0, v1, v2, v3]
        for eps_0_nu_0_co_0_p_0_fused in T.thread_binding(T.int64(256), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 512, "pragma_unroll_explicit": 1}):
            for eps_1_nu_1_co_1_p_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for eps_2_nu_2_co_2_p_2_fused in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                    for eps_3_init, nu_3_init, co_3_init, p_3_init, eps_4_init, nu_4_init, co_4_init, p_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("bgemm_init"):
                            v_eps = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(64) + eps_3_init + eps_4_init)
                            v_nu = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused % T.int64(64) // T.int64(16) + nu_3_init + nu_4_init)
                            v_co = T.axis.spatial(T.int64(512), eps_0_nu_0_co_0_p_0_fused % T.int64(16) * T.int64(32) + eps_2_nu_2_co_2_p_2_fused // T.int64(8) + co_3_init + co_4_init)
                            v_p = T.axis.spatial(T.int64(16), eps_2_nu_2_co_2_p_2_fused % T.int64(8) * T.int64(2) + p_3_init + p_4_init)
                            T.reads()
                            T.writes(bgemm_local[v_eps, v_nu, v_co, v_p])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            bgemm_local[v_eps, v_nu, v_co, v_p] = T.float32(0)
                    for ci_0 in range(T.int64(4)):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(2)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("data_pack_shared"):
                                        v0 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(64))
                                        v1 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused % T.int64(64) // T.int64(16))
                                        v2 = T.axis.spatial(T.int64(512), ci_0 * T.int64(128) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(1024) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) // T.int64(16))
                                        v3 = T.axis.spatial(T.int64(16), (ax0_ax1_ax2_ax3_fused_0 * T.int64(1024) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) % T.int64(16))
                                        T.reads(data_pack[v0, v1, v2, v3])
                                        T.writes(data_pack_shared[v0, v1, v2, v3])
                                        data_pack_shared[v0, v1, v2, v3] = data_pack[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(4)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("p1_shared"):
                                        v0 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(64))
                                        v1 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused % T.int64(64) // T.int64(16))
                                        v2 = T.axis.spatial(T.int64(512), ci_0 * T.int64(128) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(1024) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) // T.int64(32))
                                        v3 = T.axis.spatial(T.int64(512), eps_0_nu_0_co_0_p_0_fused % T.int64(16) * T.int64(32) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(1024) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) % T.int64(32))
                                        T.reads(p1[v0, v1, v2, v3])
                                        T.writes(p1_shared[v0, v1, v2, v3])
                                        p1_shared[v0, v1, v2, v3] = p1[v0, v1, v2, v3]
                        for ci_1, eps_3, nu_3, co_3, p_3, ci_2, eps_4, nu_4, co_4, p_4 in T.grid(T.int64(32), T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(4), T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                            with T.block("bgemm_update"):
                                v_eps = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(64) + eps_3 + eps_4)
                                v_nu = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused % T.int64(64) // T.int64(16) + nu_3 + nu_4)
                                v_co = T.axis.spatial(T.int64(512), eps_0_nu_0_co_0_p_0_fused % T.int64(16) * T.int64(32) + eps_2_nu_2_co_2_p_2_fused // T.int64(8) + co_3 + co_4)
                                v_p = T.axis.spatial(T.int64(16), eps_2_nu_2_co_2_p_2_fused % T.int64(8) * T.int64(2) + p_3 + p_4)
                                v_ci = T.axis.reduce(T.int64(512), ci_0 * T.int64(128) + ci_1 * T.int64(4) + ci_2)
                                T.reads(bgemm_local[v_eps, v_nu, v_co, v_p], data_pack_shared[v_eps, v_nu, v_ci, v_p], p1_shared[v_eps, v_nu, v_ci, v_co])
                                T.writes(bgemm_local[v_eps, v_nu, v_co, v_p])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                bgemm_local[v_eps, v_nu, v_co, v_p] = bgemm_local[v_eps, v_nu, v_co, v_p] + data_pack_shared[v_eps, v_nu, v_ci, v_p] * p1_shared[v_eps, v_nu, v_ci, v_co]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(2)):
                        with T.block("bgemm_local"):
                            v0 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(64) + ax0)
                            v1 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused % T.int64(64) // T.int64(16) + ax1)
                            v2 = T.axis.spatial(T.int64(512), eps_0_nu_0_co_0_p_0_fused % T.int64(16) * T.int64(32) + eps_2_nu_2_co_2_p_2_fused // T.int64(8) + ax2)
                            v3 = T.axis.spatial(T.int64(16), eps_2_nu_2_co_2_p_2_fused % T.int64(8) * T.int64(2) + ax3)
                            T.reads(bgemm_local[v0, v1, v2, v3])
                            T.writes(bgemm[v0, v1, v2, v3])
                            bgemm[v0, v1, v2, v3] = bgemm_local[v0, v1, v2, v3]
        for n_co_h_0_w_0_fused_0 in T.thread_binding(T.int64(256), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 512, "pragma_unroll_explicit": 1}):
            for n_co_h_0_w_0_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                    for ax2 in T.unroll(T.int64(2)):
                        for ax3 in T.unroll(T.int64(2)):
                            with T.block("inverse_init"):
                                v_co = T.axis.spatial(T.int64(512), (n_co_h_0_w_0_fused_0 * T.int64(32) + n_co_h_0_w_0_fused_1) // T.int64(16) + ax0)
                                v_p = T.axis.spatial(T.int64(16), (n_co_h_0_w_0_fused_0 * T.int64(32) + n_co_h_0_w_0_fused_1) % T.int64(16) + ax1)
                                v_vh, v_vw = T.axis.remap("SS", [ax2, ax3])
                                T.reads()
                                T.writes(inverse_local[v_co, v_p, v_vh, v_vw])
                                T.block_attr({"schedule_rule": "conv2d_nchw_winograd_inverse"})
                                inverse_local[v_co, v_p, v_vh, v_vw] = T.float32(0)
                            for ax4 in T.unroll(T.int64(4)):
                                for ax5 in T.unroll(T.int64(4)):
                                    with T.block("inverse_update"):
                                        v_co = T.axis.spatial(T.int64(512), (n_co_h_0_w_0_fused_0 * T.int64(32) + n_co_h_0_w_0_fused_1) // T.int64(16) + ax0)
                                        v_p = T.axis.spatial(T.int64(16), (n_co_h_0_w_0_fused_0 * T.int64(32) + n_co_h_0_w_0_fused_1) % T.int64(16) + ax1)
                                        v_vh, v_vw, v_r_a, v_r_b = T.axis.remap("SSRR", [ax2, ax3, ax4, ax5])
                                        T.reads(inverse_local[v_co, v_p, v_vh, v_vw], bgemm[v_r_a, v_r_b, v_co, v_p])
                                        T.writes(inverse_local[v_co, v_p, v_vh, v_vw])
                                        T.block_attr({"schedule_rule": "conv2d_nchw_winograd_inverse"})
                                        inverse_local[v_co, v_p, v_vh, v_vw] = inverse_local[v_co, v_p, v_vh, v_vw] + bgemm[v_r_a, v_r_b, v_co, v_p] * T.Select(v_r_a % T.int64(4) == T.int64(3) and v_vh % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_vh % T.int64(2) == T.int64(0), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_vh % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_vh % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_vh % T.int64(2) == T.int64(1), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_vh % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_vh % T.int64(2) == T.int64(1), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_vh % T.int64(2) == T.int64(0), T.float32(1), T.float32(0))))))))) * T.Select(v_r_b % T.int64(4) == T.int64(3) and v_vw % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_vw % T.int64(2) == T.int64(0), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_vw % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_vw % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_vw % T.int64(2) == T.int64(1), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_vw % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_vw % T.int64(2) == T.int64(1), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_vw % T.int64(2) == T.int64(0), T.float32(1), T.float32(0)))))))))
                for h_1, w_1 in T.grid(T.int64(2), T.int64(2)):
                    with T.block("conv2d_winograd"):
                        v_n = T.axis.spatial(T.int64(1), T.int64(0))
                        v_co = T.axis.spatial(T.int64(512), (n_co_h_0_w_0_fused_0 * T.int64(32) + n_co_h_0_w_0_fused_1) // T.int64(16))
                        v_h = T.axis.spatial(T.int64(7), (n_co_h_0_w_0_fused_0 * T.int64(32) + n_co_h_0_w_0_fused_1) % T.int64(16) // T.int64(4) * T.int64(2) + h_1)
                        v_w = T.axis.spatial(T.int64(7), (n_co_h_0_w_0_fused_0 * T.int64(32) + n_co_h_0_w_0_fused_1) % T.int64(4) * T.int64(2) + w_1)
                        T.where((n_co_h_0_w_0_fused_0 * T.int64(32) + n_co_h_0_w_0_fused_1) % T.int64(16) // T.int64(4) * T.int64(2) + h_1 < T.int64(7) and (n_co_h_0_w_0_fused_0 * T.int64(32) + n_co_h_0_w_0_fused_1) % T.int64(4) * T.int64(2) + w_1 < T.int64(7))
                        T.reads(inverse_local[v_co, v_n * T.int64(16) + v_h // T.int64(2) * T.int64(4) + v_w // T.int64(2), v_h % T.int64(2), v_w % T.int64(2)], p2[v_n, v_co, v_h, v_w], p3[v_n, v_co, T.int64(0), T.int64(0)], p4[v_n, v_co, T.int64(0), T.int64(0)])
                        T.writes(T_relu[v_n, v_co, v_h, v_w])
                        T_relu[v_n, v_co, v_h, v_w] = T.max((inverse_local[v_co, v_n * T.int64(16) + v_h // T.int64(2) * T.int64(4) + v_w // T.int64(2), v_h % T.int64(2), v_w % T.int64(2)] + p2[v_n, v_co, v_h, v_w]) * p3[v_n, v_co, T.int64(0), T.int64(0)] + p4[v_n, v_co, T.int64(0), T.int64(0)], T.float32(0))
b0 = sch.get_block(name="data_pack", func_name="main")
b1 = sch.get_block(name="bgemm", func_name="main")
b2 = sch.get_block(name="inverse", func_name="main")
b3 = sch.get_block(name="conv2d_winograd", func_name="main")
b4 = sch.get_block(name="T_add", func_name="main")
b5 = sch.get_block(name="T_multiply", func_name="main")
b6 = sch.get_block(name="T_add_1", func_name="main")
b7 = sch.get_block(name="T_relu", func_name="main")
b8 = sch.get_block(name="root", func_name="main")
b9, b10 = sch.get_producers(block=b2)
sch.compute_inline(block=b10)
b11, = sch.get_consumers(block=b2)
l12, l13, l14, l15 = sch.get_loops(block=b11)
l16, l17 = sch.split(loop=l14, factors=[None, 2], preserve_unit_iters=True)
l18, l19 = sch.split(loop=l15, factors=[None, 2], preserve_unit_iters=True)
sch.reorder(l16, l18, l17, l19)
sch.compute_at(block=b2, loop=l18, preserve_unit_loops=True, index=-1)
sch.set_scope(block=b2, buffer_index=0, storage_scope="local")
l20, l21, l22, l23, l24, l25, l26, l27, l28, l29 = sch.get_loops(block=b2)
sch.unroll(loop=l26)
sch.unroll(loop=l27)
sch.unroll(loop=l28)
sch.unroll(loop=l29)
b30, b31 = sch.get_producers(block=b0)
sch.compute_inline(block=b31)
b32, = sch.get_producers(block=b30)
l33, l34, l35, l36, l37, l38 = sch.get_loops(block=b0)
sch.reorder(l35, l36, l33, l34, l37, l38)
sch.unroll(loop=l33)
sch.unroll(loop=l34)
sch.unroll(loop=l37)
sch.unroll(loop=l38)
l39 = sch.fuse(l35, l36, preserve_unit_iters=True)
v40 = sch.sample_categorical(candidates=[32, 64, 128, 256, 512, 1024], probs=[0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666], decision=2)
l41, l42 = sch.split(loop=l39, factors=[None, v40], preserve_unit_iters=True)
sch.bind(loop=l41, thread_axis="blockIdx.x")
sch.bind(loop=l42, thread_axis="threadIdx.x")
b43 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b43, loop=l42, preserve_unit_loops=True, index=-1)
sch.compute_at(block=b30, loop=l42, preserve_unit_loops=True, index=-1)
sch.set_scope(block=b30, buffer_index=0, storage_scope="local")
sch.compute_inline(block=b32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l44, l45, l46, l47, l48 = sch.get_loops(block=b1)
v49, v50, v51, v52, v53 = sch.sample_perfect_tile(loop=l44, n=5, max_innermost_factor=64, decision=[4, 1, 1, 1, 1])
l54, l55, l56, l57, l58 = sch.split(loop=l44, factors=[v49, v50, v51, v52, v53], preserve_unit_iters=True)
v59, v60, v61, v62, v63 = sch.sample_perfect_tile(loop=l45, n=5, max_innermost_factor=64, decision=[4, 1, 1, 1, 1])
l64, l65, l66, l67, l68 = sch.split(loop=l45, factors=[v59, v60, v61, v62, v63], preserve_unit_iters=True)
v69, v70, v71, v72, v73 = sch.sample_perfect_tile(loop=l46, n=5, max_innermost_factor=64, decision=[16, 1, 32, 1, 1])
l74, l75, l76, l77, l78 = sch.split(loop=l46, factors=[v69, v70, v71, v72, v73], preserve_unit_iters=True)
v79, v80, v81, v82, v83 = sch.sample_perfect_tile(loop=l47, n=5, max_innermost_factor=64, decision=[1, 1, 8, 2, 1])
l84, l85, l86, l87, l88 = sch.split(loop=l47, factors=[v79, v80, v81, v82, v83], preserve_unit_iters=True)
v89, v90, v91 = sch.sample_perfect_tile(loop=l48, n=3, max_innermost_factor=64, decision=[4, 32, 4])
l92, l93, l94 = sch.split(loop=l48, factors=[v89, v90, v91], preserve_unit_iters=True)
sch.reorder(l54, l64, l74, l84, l55, l65, l75, l85, l56, l66, l76, l86, l92, l93, l57, l67, l77, l87, l94, l58, l68, l78, l88)
l95 = sch.fuse(l54, l64, l74, l84, preserve_unit_iters=True)
sch.bind(loop=l95, thread_axis="blockIdx.x")
l96 = sch.fuse(l55, l65, l75, l85, preserve_unit_iters=True)
sch.bind(loop=l96, thread_axis="vthread.x")
l97 = sch.fuse(l56, l66, l76, l86, preserve_unit_iters=True)
sch.bind(loop=l97, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b98 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b98, loop=l97, preserve_unit_loops=True, index=-1)
b99 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b99, loop=l92, preserve_unit_loops=True, index=-1)
l100, l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b99)
l108 = sch.fuse(l104, l105, l106, l107, preserve_unit_iters=True)
v109 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b99, ann_key="meta_schedule.cooperative_fetch", ann_val=v109)
b110 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b110, loop=l92, preserve_unit_loops=True, index=-1)
l111, l112, l113, l114, l115, l116, l117, l118 = sch.get_loops(block=b110)
l119 = sch.fuse(l115, l116, l117, l118, preserve_unit_iters=True)
v120 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b110, ann_key="meta_schedule.cooperative_fetch", ann_val=v120)
sch.reverse_compute_inline(block=b7)
sch.reverse_compute_inline(block=b6)
sch.reverse_compute_inline(block=b5)
sch.reverse_compute_inline(block=b4)
v121 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b8, ann_key="meta_schedule.unroll_explicit", ann_val=v121)
l122, l123, l124, l125, l126, l127 = sch.get_loops(block=b3)
l128 = sch.fuse(l122, l123, l124, l125, preserve_unit_iters=True)
v129 = sch.sample_categorical(candidates=[32, 64, 128, 256, 512, 1024], probs=[0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666], decision=0)
l130, l131 = sch.split(loop=l128, factors=[None, v129], preserve_unit_iters=True)
sch.bind(loop=l130, thread_axis="blockIdx.x")
sch.bind(loop=l131, thread_axis="threadIdx.x")
sch.enter_postproc()
sch.unannotate(block_or_loop=b99, ann_key="meta_schedule.cooperative_fetch")
l132, l133, l134, l135, l136 = sch.get_loops(block=b99)
l137, l138, l139 = sch.split(loop=l136, factors=[None, 256, 4], preserve_unit_iters=True)
sch.vectorize(loop=l139)
sch.bind(loop=l138, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b110, ann_key="meta_schedule.cooperative_fetch")
l140, l141, l142, l143, l144 = sch.get_loops(block=b110)
l145, l146, l147 = sch.split(loop=l144, factors=[None, 256, 4], preserve_unit_iters=True)
sch.vectorize(loop=l147)
sch.bind(loop=l146, thread_axis="threadIdx.x")
b148 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b148, ann_key="meta_schedule.unroll_explicit")
b149, b150, b151, b152, b153, b154, b155, b156, b157 = sch.get_child_blocks(b148)
l158, l159, l160, l161, l162, l163 = sch.get_loops(block=b149)
l164, l165, l166, l167, l168, l169 = sch.get_loops(block=b150)
sch.annotate(block_or_loop=l164, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l164, ann_key="pragma_unroll_explicit", ann_val=1)
l170, l171, l172, l173, l174, l175 = sch.get_loops(block=b151)
l176, l177, l178, l179, l180, l181, l182 = sch.get_loops(block=b152)
l183, l184, l185, l186, l187, l188, l189 = sch.get_loops(block=b153)
l190, l191, l192, l193, l194, l195, l196, l197, l198, l199, l200, l201, l202, l203 = sch.get_loops(block=b154)
sch.annotate(block_or_loop=l190, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l190, ann_key="pragma_unroll_explicit", ann_val=1)
l204, l205, l206, l207, l208, l209, l210 = sch.get_loops(block=b155)
l211, l212, l213, l214, l215, l216, l217, l218 = sch.get_loops(block=b156)
sch.annotate(block_or_loop=l211, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l211, ann_key="pragma_unroll_explicit", ann_val=1)
l219, l220, l221, l222 = sch.get_loops(block=b157)
b223 = sch.get_block(name="data_pack", func_name="main")
l224, l225, l226, l227, l228, l229 = sch.get_loops(block=b223)
b230 = sch.decompose_reduction(block=b223, loop=l228)
b231 = sch.get_block(name="bgemm", func_name="main")
l232, l233, l234, l235, l236, l237, l238, l239, l240, l241, l242, l243, l244, l245 = sch.get_loops(block=b231)
b246 = sch.decompose_reduction(block=b231, loop=l235)
b247 = sch.get_block(name="inverse", func_name="main")
l248, l249, l250, l251, l252, l253, l254, l255 = sch.get_loops(block=b247)
b256 = sch.decompose_reduction(block=b247, loop=l254)
2023-11-11 09:06:44 [INFO] [task_scheduler.cc:121] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #770: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/home2/xiachunwei/Software/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/home2/xiachunwei/Software/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/home2/xiachunwei/Software/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/home2/xiachunwei/Software/tvm/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/home2/xiachunwei/Software/tvm/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/home2/xiachunwei/Software/tvm/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/home2/xiachunwei/Software/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/home2/xiachunwei/Software/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  4: _ZN3tvm7runtime13PackedFun
  3: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  2: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  0: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  File "/home2/xiachunwei/Software/tvm/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(512), T.int64(7), T.int64(7)), "float32"), p1: T.Buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(512)), "float32"), p2: T.Buffer((T.int64(1), T.int64(512), T.int64(7), T.int64(7)), "float32"), p3: T.Buffer((T.int64(1), T.int64(512), T.int64(1), T.int64(1)), "float32"), p4: T.Buffer((T.int64(1), T.int64(512), T.int64(1), T.int64(1)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(512), T.int64(7), T.int64(7)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        input_tile_local = T.alloc_buffer((T.int64(512), T.int64(16), T.int64(4), T.int64(4)), scope="local")
        data_pack = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(16)))
        bgemm = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(16)))
        inverse_local = T.alloc_buffer((T.int64(512), T.int64(16), T.int64(2), T.int64(2)), scope="local")
        data_pack_local = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(16)), scope="local")
        bgemm_local = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(16)), scope="local")
        data_pack_shared = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(16)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(512)), scope="shared")
        for ci_p_fused_0 in T.thread_binding(T.int64(64), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 512, "pragma_unroll_explicit": 1}):
            for ci_p_fused_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(1), T.int64(4), T.int64(4)):
                    with T.block("input_tile"):
                        v_ci = T.axis.spatial(T.int64(512), ci_p_fused_0 * T.int64(8) + ci_p_fused_1 // T.int64(16) + ax0)
                        v_p = T.axis.spatial(T.int64(16), ci_p_fused_1 % T.int64(16) + ax1)
                        v_eps, v_nu = T.axis.remap("SS", [ax2, ax3])
                        T.reads(p0[v_p // T.int64(16), v_ci, v_p % T.int64(16) // T.int64(4) * T.int64(2) + v_eps - T.int64(1), v_p % T.int64(4) * T.int64(2) + v_nu - T.int64(1)])
                        T.writes(input_tile_local[v_ci, v_p, v_eps, v_nu])
                        T.block_attr({"schedule_rule": "None"})
                        input_tile_local[v_ci, v_p, v_eps, v_nu] = T.if_then_else(T.int64(1) <= v_p % T.int64(16) // T.int64(4) * T.int64(2) + v_eps and v_p % T.int64(16) // T.int64(4) * T.int64(2) + v_eps < T.int64(8) and T.int64(1) <= v_p % T.int64(4) * T.int64(2) + v_nu and v_p % T.int64(4) * T.int64(2) + v_nu < T.int64(8), p0[v_p // T.int64(16), v_ci, v_p % T.int64(16) // T.int64(4) * T.int64(2) + v_eps - T.int64(1), v_p % T.int64(4) * T.int64(2) + v_nu - T.int64(1)], T.float32(0))
                for eps in T.unroll(T.int64(4)):
                    for nu in T.unroll(T.int64(4)):
                        with T.block("data_pack_init"):
                            v_eps, v_nu = T.axis.remap("SS", [eps, nu])
                            v_ci = T.axis.spatial(T.int64(512), (ci_p_fused_0 * T.int64(128) + ci_p_fused_1) // T.int64(16))
                            v_p = T.axis.spatial(T.int64(16), (ci_p_fused_0 * T.int64(128) + ci_p_fused_1) % T.int64(16))
                            T.reads()
                            T.writes(data_pack_local[v_eps, v_nu, v_ci, v_p])
                            T.block_attr({"schedule_rule": "conv2d_nchw_winograd_data_pack"})
                            data_pack_local[v_eps, v_nu, v_ci, v_p] = T.float32(0)
                        for r_a in T.unroll(T.int64(4)):
                            for r_b in T.unroll(T.int64(4)):
                                with T.block("data_pack_update"):
                                    v_eps, v_nu = T.axis.remap("SS", [eps, nu])
                                    v_ci = T.axis.spatial(T.int64(512), (ci_p_fused_0 * T.int64(128) + ci_p_fused_1) // T.int64(16))
                                    v_p = T.axis.spatial(T.int64(16), (ci_p_fused_0 * T.int64(128) + ci_p_fused_1) % T.int64(16))
                                    v_r_a, v_r_b = T.axis.remap("RR", [r_a, r_b])
                                    T.reads(data_pack_local[v_eps, v_nu, v_ci, v_p], input_tile_local[v_ci, v_p, v_r_a, v_r_b])
                                    T.writes(data_pack_local[v_eps, v_nu, v_ci, v_p])
                                    T.block_attr({"schedule_rule": "conv2d_nchw_winograd_data_pack"})
                                    data_pack_local[v_eps, v_nu, v_ci, v_p] = data_pack_local[v_eps, v_nu, v_ci, v_p] + input_tile_local[v_ci, v_p, v_r_a, v_r_b] * T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(3), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(1), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(0), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(3), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(1), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(0), T.float32(1), T.float32(0))))))))))))))))) * T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(3), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(1), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(0), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(3), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(1), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(0), T.float32(1), T.float32(0)))))))))))))))))
                for ax0, ax1, ax2, ax3 in T.grid(T.int64(4), T.int64(4), T.int64(1), T.int64(1)):
                    with T.block("data_pack_local"):
                        v0, v1 = T.axis.remap("SS", [ax0, ax1])
                        v2 = T.axis.spatial(T.int64(512), ci_p_fused_0 * T.int64(8) + ci_p_fused_1 // T.int64(16) + ax2)
                        v3 = T.axis.spatial(T.int64(16), ci_p_fused_1 % T.int64(16) + ax3)
                        T.reads(data_pack_local[v0, v1, v2, v3])
                        T.writes(data_pack[v0, v1, v2, v3])
                        data_pack[v0, v1, v2, v3] = data_pack_local[v0, v1, v2, v3]
        for eps_0_nu_0_co_0_p_0_fused in T.thread_binding(T.int64(256), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 512, "pragma_unroll_explicit": 1}):
            for eps_1_nu_1_co_1_p_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for eps_2_nu_2_co_2_p_2_fused in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                    for eps_3_init, nu_3_init, co_3_init, p_3_init, eps_4_init, nu_4_init, co_4_init, p_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("bgemm_init"):
                            v_eps = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(64) + eps_3_init + eps_4_init)
                            v_nu = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused % T.int64(64) // T.int64(16) + nu_3_init + nu_4_init)
                            v_co = T.axis.spatial(T.int64(512), eps_0_nu_0_co_0_p_0_fused % T.int64(16) * T.int64(32) + eps_2_nu_2_co_2_p_2_fused // T.int64(8) + co_3_init + co_4_init)
                            v_p = T.axis.spatial(T.int64(16), eps_2_nu_2_co_2_p_2_fused % T.int64(8) * T.int64(2) + p_3_init + p_4_init)
                            T.reads()
                            T.writes(bgemm_local[v_eps, v_nu, v_co, v_p])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            bgemm_local[v_eps, v_nu, v_co, v_p] = T.float32(0)
                    for ci_0 in range(T.int64(4)):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(2)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("data_pack_shared"):
                                        v0 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(64))
                                        v1 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused % T.int64(64) // T.int64(16))
                                        v2 = T.axis.spatial(T.int64(512), ci_0 * T.int64(128) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(1024) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) // T.int64(16))
                                        v3 = T.axis.spatial(T.int64(16), (ax0_ax1_ax2_ax3_fused_0 * T.int64(1024) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) % T.int64(16))
                                        T.reads(data_pack[v0, v1, v2, v3])
                                        T.writes(data_pack_shared[v0, v1, v2, v3])
                                        data_pack_shared[v0, v1, v2, v3] = data_pack[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(16)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                                with T.block("p1_shared"):
                                    v0 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(64))
                                    v1 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused % T.int64(64) // T.int64(16))
                                    v2 = T.axis.spatial(T.int64(512), ci_0 * T.int64(128) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1) // T.int64(32))
                                    v3 = T.axis.spatial(T.int64(512), eps_0_nu_0_co_0_p_0_fused % T.int64(16) * T.int64(32) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1) % T.int64(32))
                                    T.reads(p1[v0, v1, v2, v3])
                                    T.writes(p1_shared[v0, v1, v2, v3])
                                    p1_shared[v0, v1, v2, v3] = p1[v0, v1, v2, v3]
                        for ci_1, eps_3, nu_3, co_3, p_3, ci_2, eps_4, nu_4, co_4, p_4 in T.grid(T.int64(32), T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(4), T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                            with T.block("bgemm_update"):
                                v_eps = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(64) + eps_3 + eps_4)
                                v_nu = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused % T.int64(64) // T.int64(16) + nu_3 + nu_4)
                                v_co = T.axis.spatial(T.int64(512), eps_0_nu_0_co_0_p_0_fused % T.int64(16) * T.int64(32) + eps_2_nu_2_co_2_p_2_fused // T.int64(8) + co_3 + co_4)
                                v_p = T.axis.spatial(T.int64(16), eps_2_nu_2_co_2_p_2_fused % T.int64(8) * T.int64(2) + p_3 + p_4)
                                v_ci = T.axis.reduce(T.int64(512), ci_0 * T.int64(128) + ci_1 * T.int64(4) + ci_2)
                                T.reads(bgemm_local[v_eps, v_nu, v_co, v_p], data_pack_shared[v_eps, v_nu, v_ci, v_p], p1_shared[v_eps, v_nu, v_ci, v_co])
                                T.writes(bgemm_local[v_eps, v_nu, v_co, v_p])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                bgemm_local[v_eps, v_nu, v_co, v_p] = bgemm_local[v_eps, v_nu, v_co, v_p] + data_pack_shared[v_eps, v_nu, v_ci, v_p] * p1_shared[v_eps, v_nu, v_ci, v_co]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(2)):
                        with T.block("bgemm_local"):
                            v0 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(64) + ax0)
                            v1 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused % T.int64(64) // T.int64(16) + ax1)
                            v2 = T.axis.spatial(T.int64(512), eps_0_nu_0_co_0_p_0_fused % T.int64(16) * T.int64(32) + eps_2_nu_2_co_2_p_2_fused // T.int64(8) + ax2)
                            v3 = T.axis.spatial(T.int64(16), eps_2_nu_2_co_2_p_2_fused % T.int64(8) * T.int64(2) + ax3)
                            T.reads(bgemm_local[v0, v1, v2, v3])
                            T.writes(bgemm[v0, v1, v2, v3])
                            bgemm[v0, v1, v2, v3] = bgemm_local[v0, v1, v2, v3]
        for n_co_h_0_w_0_fused_0 in T.thread_binding(T.int64(256), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 512, "pragma_unroll_explicit": 1}):
            for n_co_h_0_w_0_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                    for ax2 in T.unroll(T.int64(2)):
                        for ax3 in T.unroll(T.int64(2)):
                            with T.block("inverse_init"):
                                v_co = T.axis.spatial(T.int64(512), (n_co_h_0_w_0_fused_0 * T.int64(32) + n_co_h_0_w_0_fused_1) // T.int64(16) + ax0)
                                v_p = T.axis.spatial(T.int64(16), (n_co_h_0_w_0_fused_0 * T.int64(32) + n_co_h_0_w_0_fused_1) % T.int64(16) + ax1)
                                v_vh, v_vw = T.axis.remap("SS", [ax2, ax3])
                                T.reads()
                                T.writes(inverse_local[v_co, v_p, v_vh, v_vw])
                                T.block_attr({"schedule_rule": "conv2d_nchw_winograd_inverse"})
                                inverse_local[v_co, v_p, v_vh, v_vw] = T.float32(0)
                            for ax4 in T.unroll(T.int64(4)):
                                for ax5 in T.unroll(T.int64(4)):
                                    with T.block("inverse_update"):
                                        v_co = T.axis.spatial(T.int64(512), (n_co_h_0_w_0_fused_0 * T.int64(32) + n_co_h_0_w_0_fused_1) // T.int64(16) + ax0)
                                        v_p = T.axis.spatial(T.int64(16), (n_co_h_0_w_0_fused_0 * T.int64(32) + n_co_h_0_w_0_fused_1) % T.int64(16) + ax1)
                                        v_vh, v_vw, v_r_a, v_r_b = T.axis.remap("SSRR", [ax2, ax3, ax4, ax5])
                                        T.reads(inverse_local[v_co, v_p, v_vh, v_vw], bgemm[v_r_a, v_r_b, v_co, v_p])
                                        T.writes(inverse_local[v_co, v_p, v_vh, v_vw])
                                        T.block_attr({"schedule_rule": "conv2d_nchw_winograd_inverse"})
                                        inverse_local[v_co, v_p, v_vh, v_vw] = inverse_local[v_co, v_p, v_vh, v_vw] + bgemm[v_r_a, v_r_b, v_co, v_p] * T.Select(v_r_a % T.int64(4) == T.int64(3) and v_vh % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_vh % T.int64(2) == T.int64(0), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_vh % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_vh % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_vh % T.int64(2) == T.int64(1), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_vh % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_vh % T.int64(2) == T.int64(1), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_vh % T.int64(2) == T.int64(0), T.float32(1), T.float32(0))))))))) * T.Select(v_r_b % T.int64(4) == T.int64(3) and v_vw % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_vw % T.int64(2) == T.int64(0), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_vw % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_vw % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_vw % T.int64(2) == T.int64(1), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_vw % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_vw % T.int64(2) == T.int64(1), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_vw % T.int64(2) == T.int64(0), T.float32(1), T.float32(0)))))))))
                for h_1, w_1 in T.grid(T.int64(2), T.int64(2)):
                    with T.block("conv2d_winograd"):
                        v_n = T.axis.spatial(T.int64(1), T.int64(0))
                        v_co = T.axis.spatial(T.int64(512), (n_co_h_0_w_0_fused_0 * T.int64(32) + n_co_h_0_w_0_fused_1) // T.int64(16))
                        v_h = T.axis.spatial(T.int64(7), (n_co_h_0_w_0_fused_0 * T.int64(32) + n_co_h_0_w_0_fused_1) % T.int64(16) // T.int64(4) * T.int64(2) + h_1)
                        v_w = T.axis.spatial(T.int64(7), (n_co_h_0_w_0_fused_0 * T.int64(32) + n_co_h_0_w_0_fused_1) % T.int64(4) * T.int64(2) + w_1)
                        T.where((n_co_h_0_w_0_fused_0 * T.int64(32) + n_co_h_0_w_0_fused_1) % T.int64(16) // T.int64(4) * T.int64(2) + h_1 < T.int64(7) and (n_co_h_0_w_0_fused_0 * T.int64(32) + n_co_h_0_w_0_fused_1) % T.int64(4) * T.int64(2) + w_1 < T.int64(7))
                        T.reads(inverse_local[v_co, v_n * T.int64(16) + v_h // T.int64(2) * T.int64(4) + v_w // T.int64(2), v_h % T.int64(2), v_w % T.int64(2)], p2[v_n, v_co, v_h, v_w], p3[v_n, v_co, T.int64(0), T.int64(0)], p4[v_n, v_co, T.int64(0), T.int64(0)])
                        T.writes(T_relu[v_n, v_co, v_h, v_w])
                        T_relu[v_n, v_co, v_h, v_w] = T.max((inverse_local[v_co, v_n * T.int64(16) + v_h // T.int64(2) * T.int64(4) + v_w // T.int64(2), v_h % T.int64(2), v_w % T.int64(2)] + p2[v_n, v_co, v_h, v_w]) * p3[v_n, v_co, T.int64(0), T.int64(0)] + p4[v_n, v_co, T.int64(0), T.int64(0)], T.float32(0))
b0 = sch.get_block(name="data_pack", func_name="main")
b1 = sch.get_block(name="bgemm", func_name="main")
b2 = sch.get_block(name="inverse", func_name="main")
b3 = sch.get_block(name="conv2d_winograd", func_name="main")
b4 = sch.get_block(name="T_add", func_name="main")
b5 = sch.get_block(name="T_multiply", func_name="main")
b6 = sch.get_block(name="T_add_1", func_name="main")
b7 = sch.get_block(name="T_relu", func_name="main")
b8 = sch.get_block(name="root", func_name="main")
b9, b10 = sch.get_producers(block=b2)
sch.compute_inline(block=b10)
b11, = sch.get_consumers(block=b2)
l12, l13, l14, l15 = sch.get_loops(block=b11)
l16, l17 = sch.split(loop=l14, factors=[None, 2], preserve_unit_iters=True)
l18, l19 = sch.split(loop=l15, factors=[None, 2], preserve_unit_iters=True)
sch.reorder(l16, l18, l17, l19)
sch.compute_at(block=b2, loop=l18, preserve_unit_loops=True, index=-1)
sch.set_scope(block=b2, buffer_index=0, storage_scope="local")
l20, l21, l22, l23, l24, l25, l26, l27, l28, l29 = sch.get_loops(block=b2)
sch.unroll(loop=l26)
sch.unroll(loop=l27)
sch.unroll(loop=l28)
sch.unroll(loop=l29)
b30, b31 = sch.get_producers(block=b0)
sch.compute_inline(block=b31)
b32, = sch.get_producers(block=b30)
l33, l34, l35, l36, l37, l38 = sch.get_loops(block=b0)
sch.reorder(l35, l36, l33, l34, l37, l38)
sch.unroll(loop=l33)
sch.unroll(loop=l34)
sch.unroll(loop=l37)
sch.unroll(loop=l38)
l39 = sch.fuse(l35, l36, preserve_unit_iters=True)
v40 = sch.sample_categorical(candidates=[32, 64, 128, 256, 512, 1024], probs=[0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666], decision=2)
l41, l42 = sch.split(loop=l39, factors=[None, v40], preserve_unit_iters=True)
sch.bind(loop=l41, thread_axis="blockIdx.x")
sch.bind(loop=l42, thread_axis="threadIdx.x")
b43 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b43, loop=l42, preserve_unit_loops=True, index=-1)
sch.compute_at(block=b30, loop=l42, preserve_unit_loops=True, index=-1)
sch.set_scope(block=b30, buffer_index=0, storage_scope="local")
sch.compute_inline(block=b32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l44, l45, l46, l47, l48 = sch.get_loops(block=b1)
v49, v50, v51, v52, v53 = sch.sample_perfect_tile(loop=l44, n=5, max_innermost_factor=64, decision=[4, 1, 1, 1, 1])
l54, l55, l56, l57, l58 = sch.split(loop=l44, factors=[v49, v50, v51, v52, v53], preserve_unit_iters=True)
v59, v60, v61, v62, v63 = sch.sample_perfect_tile(loop=l45, n=5, max_innermost_factor=64, decision=[4, 1, 1, 1, 1])
l64, l65, l66, l67, l68 = sch.split(loop=l45, factors=[v59, v60, v61, v62, v63], preserve_unit_iters=True)
v69, v70, v71, v72, v73 = sch.sample_perfect_tile(loop=l46, n=5, max_innermost_factor=64, decision=[16, 1, 32, 1, 1])
l74, l75, l76, l77, l78 = sch.split(loop=l46, factors=[v69, v70, v71, v72, v73], preserve_unit_iters=True)
v79, v80, v81, v82, v83 = sch.sample_perfect_tile(loop=l47, n=5, max_innermost_factor=64, decision=[1, 1, 8, 2, 1])
l84, l85, l86, l87, l88 = sch.split(loop=l47, factors=[v79, v80, v81, v82, v83], preserve_unit_iters=True)
v89, v90, v91 = sch.sample_perfect_tile(loop=l48, n=3, max_innermost_factor=64, decision=[4, 32, 4])
l92, l93, l94 = sch.split(loop=l48, factors=[v89, v90, v91], preserve_unit_iters=True)
sch.reorder(l54, l64, l74, l84, l55, l65, l75, l85, l56, l66, l76, l86, l92, l93, l57, l67, l77, l87, l94, l58, l68, l78, l88)
l95 = sch.fuse(l54, l64, l74, l84, preserve_unit_iters=True)
sch.bind(loop=l95, thread_axis="blockIdx.x")
l96 = sch.fuse(l55, l65, l75, l85, preserve_unit_iters=True)
sch.bind(loop=l96, thread_axis="vthread.x")
l97 = sch.fuse(l56, l66, l76, l86, preserve_unit_iters=True)
sch.bind(loop=l97, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b98 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b98, loop=l97, preserve_unit_loops=True, index=-1)
b99 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b99, loop=l92, preserve_unit_loops=True, index=-1)
l100, l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b99)
l108 = sch.fuse(l104, l105, l106, l107, preserve_unit_iters=True)
v109 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b99, ann_key="meta_schedule.cooperative_fetch", ann_val=v109)
b110 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b110, loop=l92, preserve_unit_loops=True, index=-1)
l111, l112, l113, l114, l115, l116, l117, l118 = sch.get_loops(block=b110)
l119 = sch.fuse(l115, l116, l117, l118, preserve_unit_iters=True)
v120 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b110, ann_key="meta_schedule.cooperative_fetch", ann_val=v120)
sch.reverse_compute_inline(block=b7)
sch.reverse_compute_inline(block=b6)
sch.reverse_compute_inline(block=b5)
sch.reverse_compute_inline(block=b4)
v121 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b8, ann_key="meta_schedule.unroll_explicit", ann_val=v121)
l122, l123, l124, l125, l126, l127 = sch.get_loops(block=b3)
l128 = sch.fuse(l122, l123, l124, l125, preserve_unit_iters=True)
v129 = sch.sample_categorical(candidates=[32, 64, 128, 256, 512, 1024], probs=[0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666], decision=0)
l130, l131 = sch.split(loop=l128, factors=[None, v129], preserve_unit_iters=True)
sch.bind(loop=l130, thread_axis="blockIdx.x")
sch.bind(loop=l131, thread_axis="threadIdx.x")
sch.enter_postproc()
sch.unannotate(block_or_loop=b99, ann_key="meta_schedule.cooperative_fetch")
l132, l133, l134, l135, l136 = sch.get_loops(block=b99)
l137, l138, l139 = sch.split(loop=l136, factors=[None, 256, 4], preserve_unit_iters=True)
sch.vectorize(loop=l139)
sch.bind(loop=l138, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b110, ann_key="meta_schedule.cooperative_fetch")
l140, l141, l142, l143, l144 = sch.get_loops(block=b110)
l145, l146 = sch.split(loop=l144, factors=[None, 256], preserve_unit_iters=True)
sch.bind(loop=l146, thread_axis="threadIdx.x")
b147 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b147, ann_key="meta_schedule.unroll_explicit")
b148, b149, b150, b151, b152, b153, b154, b155, b156 = sch.get_child_blocks(b147)
l157, l158, l159, l160, l161, l162 = sch.get_loops(block=b148)
l163, l164, l165, l166, l167, l168 = sch.get_loops(block=b149)
sch.annotate(block_or_loop=l163, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l163, ann_key="pragma_unroll_explicit", ann_val=1)
l169, l170, l171, l172, l173, l174 = sch.get_loops(block=b150)
l175, l176, l177, l178, l179, l180, l181 = sch.get_loops(block=b151)
l182, l183, l184, l185, l186, l187 = sch.get_loops(block=b152)
l188, l189, l190, l191, l192, l193, l194, l195, l196, l197, l198, l199, l200, l201 = sch.get_loops(block=b153)
sch.annotate(block_or_loop=l188, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l188, ann_key="pragma_unroll_explicit", ann_val=1)
l202, l203, l204, l205, l206, l207, l208 = sch.get_loops(block=b154)
l209, l210, l211, l212, l213, l214, l215, l216 = sch.get_loops(block=b155)
sch.annotate(block_or_loop=l209, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l209, ann_key="pragma_unroll_explicit", ann_val=1)
l217, l218, l219, l220 = sch.get_loops(block=b156)
b221 = sch.get_block(name="data_pack", func_name="main")
l222, l223, l224, l225, l226, l227 = sch.get_loops(block=b221)
b228 = sch.decompose_reduction(block=b221, loop=l226)
b229 = sch.get_block(name="bgemm", func_name="main")
l230, l231, l232, l233, l234, l235, l236, l237, l238, l239, l240, l241, l242, l243 = sch.get_loops(block=b229)
b244 = sch.decompose_reduction(block=b229, loop=l233)
b245 = sch.get_block(name="inverse", func_name="main")
l246, l247, l248, l249, l250, l251, l252, l253 = sch.get_loops(block=b245)
b254 = sch.decompose_reduction(block=b245, loop=l252)
2023-11-11 09:06:44 [INFO] [task_scheduler.cc:121] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #771: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/home2/xiachunwei/Software/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/home2/xiachunwei/Software/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/home2/xiachunwei/Software/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/home2/xiachunwei/Software/tvm/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/home2/xiachunwei/Software/tvm/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/home2/xiachunwei/Software/tvm/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/home2/xiachunwei/Software/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/home2/xiachunwei/Software/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  4: _ZN3tvm7runtime13PackedFun
  3: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  2: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  0: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  File "/home2/xiachunwei/Software/tvm/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(512), T.int64(7), T.int64(7)), "float32"), p1: T.Buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(512)), "float32"), p2: T.Buffer((T.int64(1), T.int64(512), T.int64(7), T.int64(7)), "float32"), p3: T.Buffer((T.int64(1), T.int64(512), T.int64(1), T.int64(1)), "float32"), p4: T.Buffer((T.int64(1), T.int64(512), T.int64(1), T.int64(1)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(512), T.int64(7), T.int64(7)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        input_tile_local = T.alloc_buffer((T.int64(512), T.int64(16), T.int64(4), T.int64(4)), scope="local")
        data_pack = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(16)))
        bgemm = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(16)))
        inverse_local = T.alloc_buffer((T.int64(512), T.int64(16), T.int64(2), T.int64(2)), scope="local")
        data_pack_local = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(16)), scope="local")
        bgemm_local = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(16)), scope="local")
        data_pack_shared = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(16)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(512)), scope="shared")
        for ci_p_fused_0 in T.thread_binding(T.int64(64), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 1024, "pragma_unroll_explicit": 1}):
            for ci_p_fused_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(1), T.int64(4), T.int64(4)):
                    with T.block("input_tile"):
                        v_ci = T.axis.spatial(T.int64(512), ci_p_fused_0 * T.int64(8) + ci_p_fused_1 // T.int64(16) + ax0)
                        v_p = T.axis.spatial(T.int64(16), ci_p_fused_1 % T.int64(16) + ax1)
                        v_eps, v_nu = T.axis.remap("SS", [ax2, ax3])
                        T.reads(p0[v_p // T.int64(16), v_ci, v_p % T.int64(16) // T.int64(4) * T.int64(2) + v_eps - T.int64(1), v_p % T.int64(4) * T.int64(2) + v_nu - T.int64(1)])
                        T.writes(input_tile_local[v_ci, v_p, v_eps, v_nu])
                        T.block_attr({"schedule_rule": "None"})
                        input_tile_local[v_ci, v_p, v_eps, v_nu] = T.if_then_else(T.int64(1) <= v_p % T.int64(16) // T.int64(4) * T.int64(2) + v_eps and v_p % T.int64(16) // T.int64(4) * T.int64(2) + v_eps < T.int64(8) and T.int64(1) <= v_p % T.int64(4) * T.int64(2) + v_nu and v_p % T.int64(4) * T.int64(2) + v_nu < T.int64(8), p0[v_p // T.int64(16), v_ci, v_p % T.int64(16) // T.int64(4) * T.int64(2) + v_eps - T.int64(1), v_p % T.int64(4) * T.int64(2) + v_nu - T.int64(1)], T.float32(0))
                for eps in T.unroll(T.int64(4)):
                    for nu in T.unroll(T.int64(4)):
                        with T.block("data_pack_init"):
                            v_eps, v_nu = T.axis.remap("SS", [eps, nu])
                            v_ci = T.axis.spatial(T.int64(512), (ci_p_fused_0 * T.int64(128) + ci_p_fused_1) // T.int64(16))
                            v_p = T.axis.spatial(T.int64(16), (ci_p_fused_0 * T.int64(128) + ci_p_fused_1) % T.int64(16))
                            T.reads()
                            T.writes(data_pack_local[v_eps, v_nu, v_ci, v_p])
                            T.block_attr({"schedule_rule": "conv2d_nchw_winograd_data_pack"})
                            data_pack_local[v_eps, v_nu, v_ci, v_p] = T.float32(0)
                        for r_a in T.unroll(T.int64(4)):
                            for r_b in T.unroll(T.int64(4)):
                                with T.block("data_pack_update"):
                                    v_eps, v_nu = T.axis.remap("SS", [eps, nu])
                                    v_ci = T.axis.spatial(T.int64(512), (ci_p_fused_0 * T.int64(128) + ci_p_fused_1) // T.int64(16))
                                    v_p = T.axis.spatial(T.int64(16), (ci_p_fused_0 * T.int64(128) + ci_p_fused_1) % T.int64(16))
                                    v_r_a, v_r_b = T.axis.remap("RR", [r_a, r_b])
                                    T.reads(data_pack_local[v_eps, v_nu, v_ci, v_p], input_tile_local[v_ci, v_p, v_r_a, v_r_b])
                                    T.writes(data_pack_local[v_eps, v_nu, v_ci, v_p])
                                    T.block_attr({"schedule_rule": "conv2d_nchw_winograd_data_pack"})
                                    data_pack_local[v_eps, v_nu, v_ci, v_p] = data_pack_local[v_eps, v_nu, v_ci, v_p] + input_tile_local[v_ci, v_p, v_r_a, v_r_b] * T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(3), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(1), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(0), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(3), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(1), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(0), T.float32(1), T.float32(0))))))))))))))))) * T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(3), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(1), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(0), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(3), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(1), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(0), T.float32(1), T.float32(0)))))))))))))))))
                for ax0, ax1, ax2, ax3 in T.grid(T.int64(4), T.int64(4), T.int64(1), T.int64(1)):
                    with T.block("data_pack_local"):
                        v0, v1 = T.axis.remap("SS", [ax0, ax1])
                        v2 = T.axis.spatial(T.int64(512), ci_p_fused_0 * T.int64(8) + ci_p_fused_1 // T.int64(16) + ax2)
                        v3 = T.axis.spatial(T.int64(16), ci_p_fused_1 % T.int64(16) + ax3)
                        T.reads(data_pack_local[v0, v1, v2, v3])
                        T.writes(data_pack[v0, v1, v2, v3])
                        data_pack[v0, v1, v2, v3] = data_pack_local[v0, v1, v2, v3]
        for eps_0_nu_0_co_0_p_0_fused in T.thread_binding(T.int64(256), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 1024, "pragma_unroll_explicit": 1}):
            for eps_1_nu_1_co_1_p_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for eps_2_nu_2_co_2_p_2_fused in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                    for eps_3_init, nu_3_init, co_3_init, p_3_init, eps_4_init, nu_4_init, co_4_init, p_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("bgemm_init"):
                            v_eps = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(64) + eps_3_init + eps_4_init)
                            v_nu = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused % T.int64(64) // T.int64(16) + nu_3_init + nu_4_init)
                            v_co = T.axis.spatial(T.int64(512), eps_0_nu_0_co_0_p_0_fused % T.int64(16) * T.int64(32) + eps_2_nu_2_co_2_p_2_fused // T.int64(8) + co_3_init + co_4_init)
                            v_p = T.axis.spatial(T.int64(16), eps_2_nu_2_co_2_p_2_fused % T.int64(8) * T.int64(2) + p_3_init + p_4_init)
                            T.reads()
                            T.writes(bgemm_local[v_eps, v_nu, v_co, v_p])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            bgemm_local[v_eps, v_nu, v_co, v_p] = T.float32(0)
                    for ci_0 in range(T.int64(4)):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(4)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("data_pack_shared"):
                                        v0 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(64))
                                        v1 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused % T.int64(64) // T.int64(16))
                                        v2 = T.axis.spatial(T.int64(512), ci_0 * T.int64(128) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(512) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) // T.int64(16))
                                        v3 = T.axis.spatial(T.int64(16), (ax0_ax1_ax2_ax3_fused_0 * T.int64(512) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) % T.int64(16))
                                        T.reads(data_pack[v0, v1, v2, v3])
                                        T.writes(data_pack_shared[v0, v1, v2, v3])
                                        data_pack_shared[v0, v1, v2, v3] = data_pack[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(4)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("p1_shared"):
                                        v0 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(64))
                                        v1 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused % T.int64(64) // T.int64(16))
                                        v2 = T.axis.spatial(T.int64(512), ci_0 * T.int64(128) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(1024) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) // T.int64(32))
                                        v3 = T.axis.spatial(T.int64(512), eps_0_nu_0_co_0_p_0_fused % T.int64(16) * T.int64(32) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(1024) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) % T.int64(32))
                                        T.reads(p1[v0, v1, v2, v3])
                                        T.writes(p1_shared[v0, v1, v2, v3])
                                        p1_shared[v0, v1, v2, v3] = p1[v0, v1, v2, v3]
                        for ci_1, eps_3, nu_3, co_3, p_3, ci_2, eps_4, nu_4, co_4, p_4 in T.grid(T.int64(8), T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(16), T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                            with T.block("bgemm_update"):
                                v_eps = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(64) + eps_3 + eps_4)
                                v_nu = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused % T.int64(64) // T.int64(16) + nu_3 + nu_4)
                                v_co = T.axis.spatial(T.int64(512), eps_0_nu_0_co_0_p_0_fused % T.int64(16) * T.int64(32) + eps_2_nu_2_co_2_p_2_fused // T.int64(8) + co_3 + co_4)
                                v_p = T.axis.spatial(T.int64(16), eps_2_nu_2_co_2_p_2_fused % T.int64(8) * T.int64(2) + p_3 + p_4)
                                v_ci = T.axis.reduce(T.int64(512), ci_0 * T.int64(128) + ci_1 * T.int64(16) + ci_2)
                                T.reads(bgemm_local[v_eps, v_nu, v_co, v_p], data_pack_shared[v_eps, v_nu, v_ci, v_p], p1_shared[v_eps, v_nu, v_ci, v_co])
                                T.writes(bgemm_local[v_eps, v_nu, v_co, v_p])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                bgemm_local[v_eps, v_nu, v_co, v_p] = bgemm_local[v_eps, v_nu, v_co, v_p] + data_pack_shared[v_eps, v_nu, v_ci, v_p] * p1_shared[v_eps, v_nu, v_ci, v_co]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(2)):
                        with T.block("bgemm_local"):
                            v0 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(64) + ax0)
                            v1 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused % T.int64(64) // T.int64(16) + ax1)
                            v2 = T.axis.spatial(T.int64(512), eps_0_nu_0_co_0_p_0_fused % T.int64(16) * T.int64(32) + eps_2_nu_2_co_2_p_2_fused // T.int64(8) + ax2)
                            v3 = T.axis.spatial(T.int64(16), eps_2_nu_2_co_2_p_2_fused % T.int64(8) * T.int64(2) + ax3)
                            T.reads(bgemm_local[v0, v1, v2, v3])
                            T.writes(bgemm[v0, v1, v2, v3])
                            bgemm[v0, v1, v2, v3] = bgemm_local[v0, v1, v2, v3]
        for n_co_h_0_w_0_fused_0 in T.thread_binding(T.int64(256), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 1024, "pragma_unroll_explicit": 1}):
            for n_co_h_0_w_0_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                    for ax2 in T.unroll(T.int64(2)):
                        for ax3 in T.unroll(T.int64(2)):
                            with T.block("inverse_init"):
                                v_co = T.axis.spatial(T.int64(512), (n_co_h_0_w_0_fused_0 * T.int64(32) + n_co_h_0_w_0_fused_1) // T.int64(16) + ax0)
                                v_p = T.axis.spatial(T.int64(16), (n_co_h_0_w_0_fused_0 * T.int64(32) + n_co_h_0_w_0_fused_1) % T.int64(16) + ax1)
                                v_vh, v_vw = T.axis.remap("SS", [ax2, ax3])
                                T.reads()
                                T.writes(inverse_local[v_co, v_p, v_vh, v_vw])
                                T.block_attr({"schedule_rule": "conv2d_nchw_winograd_inverse"})
                                inverse_local[v_co, v_p, v_vh, v_vw] = T.float32(0)
                            for ax4 in T.unroll(T.int64(4)):
                                for ax5 in T.unroll(T.int64(4)):
                                    with T.block("inverse_update"):
                                        v_co = T.axis.spatial(T.int64(512), (n_co_h_0_w_0_fused_0 * T.int64(32) + n_co_h_0_w_0_fused_1) // T.int64(16) + ax0)
                                        v_p = T.axis.spatial(T.int64(16), (n_co_h_0_w_0_fused_0 * T.int64(32) + n_co_h_0_w_0_fused_1) % T.int64(16) + ax1)
                                        v_vh, v_vw, v_r_a, v_r_b = T.axis.remap("SSRR", [ax2, ax3, ax4, ax5])
                                        T.reads(inverse_local[v_co, v_p, v_vh, v_vw], bgemm[v_r_a, v_r_b, v_co, v_p])
                                        T.writes(inverse_local[v_co, v_p, v_vh, v_vw])
                                        T.block_attr({"schedule_rule": "conv2d_nchw_winograd_inverse"})
                                        inverse_local[v_co, v_p, v_vh, v_vw] = inverse_local[v_co, v_p, v_vh, v_vw] + bgemm[v_r_a, v_r_b, v_co, v_p] * T.Select(v_r_a % T.int64(4) == T.int64(3) and v_vh % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_vh % T.int64(2) == T.int64(0), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_vh % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_vh % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_vh % T.int64(2) == T.int64(1), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_vh % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_vh % T.int64(2) == T.int64(1), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_vh % T.int64(2) == T.int64(0), T.float32(1), T.float32(0))))))))) * T.Select(v_r_b % T.int64(4) == T.int64(3) and v_vw % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_vw % T.int64(2) == T.int64(0), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_vw % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_vw % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_vw % T.int64(2) == T.int64(1), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_vw % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_vw % T.int64(2) == T.int64(1), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_vw % T.int64(2) == T.int64(0), T.float32(1), T.float32(0)))))))))
                for h_1, w_1 in T.grid(T.int64(2), T.int64(2)):
                    with T.block("conv2d_winograd"):
                        v_n = T.axis.spatial(T.int64(1), T.int64(0))
                        v_co = T.axis.spatial(T.int64(512), (n_co_h_0_w_0_fused_0 * T.int64(32) + n_co_h_0_w_0_fused_1) // T.int64(16))
                        v_h = T.axis.spatial(T.int64(7), (n_co_h_0_w_0_fused_0 * T.int64(32) + n_co_h_0_w_0_fused_1) % T.int64(16) // T.int64(4) * T.int64(2) + h_1)
                        v_w = T.axis.spatial(T.int64(7), (n_co_h_0_w_0_fused_0 * T.int64(32) + n_co_h_0_w_0_fused_1) % T.int64(4) * T.int64(2) + w_1)
                        T.where((n_co_h_0_w_0_fused_0 * T.int64(32) + n_co_h_0_w_0_fused_1) % T.int64(16) // T.int64(4) * T.int64(2) + h_1 < T.int64(7) and (n_co_h_0_w_0_fused_0 * T.int64(32) + n_co_h_0_w_0_fused_1) % T.int64(4) * T.int64(2) + w_1 < T.int64(7))
                        T.reads(inverse_local[v_co, v_n * T.int64(16) + v_h // T.int64(2) * T.int64(4) + v_w // T.int64(2), v_h % T.int64(2), v_w % T.int64(2)], p2[v_n, v_co, v_h, v_w], p3[v_n, v_co, T.int64(0), T.int64(0)], p4[v_n, v_co, T.int64(0), T.int64(0)])
                        T.writes(T_relu[v_n, v_co, v_h, v_w])
                        T_relu[v_n, v_co, v_h, v_w] = T.max((inverse_local[v_co, v_n * T.int64(16) + v_h // T.int64(2) * T.int64(4) + v_w // T.int64(2), v_h % T.int64(2), v_w % T.int64(2)] + p2[v_n, v_co, v_h, v_w]) * p3[v_n, v_co, T.int64(0), T.int64(0)] + p4[v_n, v_co, T.int64(0), T.int64(0)], T.float32(0))
b0 = sch.get_block(name="data_pack", func_name="main")
b1 = sch.get_block(name="bgemm", func_name="main")
b2 = sch.get_block(name="inverse", func_name="main")
b3 = sch.get_block(name="conv2d_winograd", func_name="main")
b4 = sch.get_block(name="T_add", func_name="main")
b5 = sch.get_block(name="T_multiply", func_name="main")
b6 = sch.get_block(name="T_add_1", func_name="main")
b7 = sch.get_block(name="T_relu", func_name="main")
b8 = sch.get_block(name="root", func_name="main")
b9, b10 = sch.get_producers(block=b2)
sch.compute_inline(block=b10)
b11, = sch.get_consumers(block=b2)
l12, l13, l14, l15 = sch.get_loops(block=b11)
l16, l17 = sch.split(loop=l14, factors=[None, 2], preserve_unit_iters=True)
l18, l19 = sch.split(loop=l15, factors=[None, 2], preserve_unit_iters=True)
sch.reorder(l16, l18, l17, l19)
sch.compute_at(block=b2, loop=l18, preserve_unit_loops=True, index=-1)
sch.set_scope(block=b2, buffer_index=0, storage_scope="local")
l20, l21, l22, l23, l24, l25, l26, l27, l28, l29 = sch.get_loops(block=b2)
sch.unroll(loop=l26)
sch.unroll(loop=l27)
sch.unroll(loop=l28)
sch.unroll(loop=l29)
b30, b31 = sch.get_producers(block=b0)
sch.compute_inline(block=b31)
b32, = sch.get_producers(block=b30)
l33, l34, l35, l36, l37, l38 = sch.get_loops(block=b0)
sch.reorder(l35, l36, l33, l34, l37, l38)
sch.unroll(loop=l33)
sch.unroll(loop=l34)
sch.unroll(loop=l37)
sch.unroll(loop=l38)
l39 = sch.fuse(l35, l36, preserve_unit_iters=True)
v40 = sch.sample_categorical(candidates=[32, 64, 128, 256, 512, 1024], probs=[0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666], decision=2)
l41, l42 = sch.split(loop=l39, factors=[None, v40], preserve_unit_iters=True)
sch.bind(loop=l41, thread_axis="blockIdx.x")
sch.bind(loop=l42, thread_axis="threadIdx.x")
b43 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b43, loop=l42, preserve_unit_loops=True, index=-1)
sch.compute_at(block=b30, loop=l42, preserve_unit_loops=True, index=-1)
sch.set_scope(block=b30, buffer_index=0, storage_scope="local")
sch.compute_inline(block=b32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l44, l45, l46, l47, l48 = sch.get_loops(block=b1)
v49, v50, v51, v52, v53 = sch.sample_perfect_tile(loop=l44, n=5, max_innermost_factor=64, decision=[4, 1, 1, 1, 1])
l54, l55, l56, l57, l58 = sch.split(loop=l44, factors=[v49, v50, v51, v52, v53], preserve_unit_iters=True)
v59, v60, v61, v62, v63 = sch.sample_perfect_tile(loop=l45, n=5, max_innermost_factor=64, decision=[4, 1, 1, 1, 1])
l64, l65, l66, l67, l68 = sch.split(loop=l45, factors=[v59, v60, v61, v62, v63], preserve_unit_iters=True)
v69, v70, v71, v72, v73 = sch.sample_perfect_tile(loop=l46, n=5, max_innermost_factor=64, decision=[16, 1, 32, 1, 1])
l74, l75, l76, l77, l78 = sch.split(loop=l46, factors=[v69, v70, v71, v72, v73], preserve_unit_iters=True)
v79, v80, v81, v82, v83 = sch.sample_perfect_tile(loop=l47, n=5, max_innermost_factor=64, decision=[1, 1, 8, 2, 1])
l84, l85, l86, l87, l88 = sch.split(loop=l47, factors=[v79, v80, v81, v82, v83], preserve_unit_iters=True)
v89, v90, v91 = sch.sample_perfect_tile(loop=l48, n=3, max_innermost_factor=64, decision=[4, 8, 16])
l92, l93, l94 = sch.split(loop=l48, factors=[v89, v90, v91], preserve_unit_iters=True)
sch.reorder(l54, l64, l74, l84, l55, l65, l75, l85, l56, l66, l76, l86, l92, l93, l57, l67, l77, l87, l94, l58, l68, l78, l88)
l95 = sch.fuse(l54, l64, l74, l84, preserve_unit_iters=True)
sch.bind(loop=l95, thread_axis="blockIdx.x")
l96 = sch.fuse(l55, l65, l75, l85, preserve_unit_iters=True)
sch.bind(loop=l96, thread_axis="vthread.x")
l97 = sch.fuse(l56, l66, l76, l86, preserve_unit_iters=True)
sch.bind(loop=l97, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b98 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b98, loop=l97, preserve_unit_loops=True, index=-1)
b99 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b99, loop=l92, preserve_unit_loops=True, index=-1)
l100, l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b99)
l108 = sch.fuse(l104, l105, l106, l107, preserve_unit_iters=True)
v109 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b99, ann_key="meta_schedule.cooperative_fetch", ann_val=v109)
b110 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b110, loop=l92, preserve_unit_loops=True, index=-1)
l111, l112, l113, l114, l115, l116, l117, l118 = sch.get_loops(block=b110)
l119 = sch.fuse(l115, l116, l117, l118, preserve_unit_iters=True)
v120 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b110, ann_key="meta_schedule.cooperative_fetch", ann_val=v120)
sch.reverse_compute_inline(block=b7)
sch.reverse_compute_inline(block=b6)
sch.reverse_compute_inline(block=b5)
sch.reverse_compute_inline(block=b4)
v121 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b8, ann_key="meta_schedule.unroll_explicit", ann_val=v121)
l122, l123, l124, l125, l126, l127 = sch.get_loops(block=b3)
l128 = sch.fuse(l122, l123, l124, l125, preserve_unit_iters=True)
v129 = sch.sample_categorical(candidates=[32, 64, 128, 256, 512, 1024], probs=[0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666], decision=0)
l130, l131 = sch.split(loop=l128, factors=[None, v129], preserve_unit_iters=True)
sch.bind(loop=l130, thread_axis="blockIdx.x")
sch.bind(loop=l131, thread_axis="threadIdx.x")
sch.enter_postproc()
sch.unannotate(block_or_loop=b99, ann_key="meta_schedule.cooperative_fetch")
l132, l133, l134, l135, l136 = sch.get_loops(block=b99)
l137, l138, l139 = sch.split(loop=l136, factors=[None, 256, 2], preserve_unit_iters=True)
sch.vectorize(loop=l139)
sch.bind(loop=l138, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b110, ann_key="meta_schedule.cooperative_fetch")
l140, l141, l142, l143, l144 = sch.get_loops(block=b110)
l145, l146, l147 = sch.split(loop=l144, factors=[None, 256, 4], preserve_unit_iters=True)
sch.vectorize(loop=l147)
sch.bind(loop=l146, thread_axis="threadIdx.x")
b148 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b148, ann_key="meta_schedule.unroll_explicit")
b149, b150, b151, b152, b153, b154, b155, b156, b157 = sch.get_child_blocks(b148)
l158, l159, l160, l161, l162, l163 = sch.get_loops(block=b149)
l164, l165, l166, l167, l168, l169 = sch.get_loops(block=b150)
sch.annotate(block_or_loop=l164, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l164, ann_key="pragma_unroll_explicit", ann_val=1)
l170, l171, l172, l173, l174, l175 = sch.get_loops(block=b151)
l176, l177, l178, l179, l180, l181, l182 = sch.get_loops(block=b152)
l183, l184, l185, l186, l187, l188, l189 = sch.get_loops(block=b153)
l190, l191, l192, l193, l194, l195, l196, l197, l198, l199, l200, l201, l202, l203 = sch.get_loops(block=b154)
sch.annotate(block_or_loop=l190, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l190, ann_key="pragma_unroll_explicit", ann_val=1)
l204, l205, l206, l207, l208, l209, l210 = sch.get_loops(block=b155)
l211, l212, l213, l214, l215, l216, l217, l218 = sch.get_loops(block=b156)
sch.annotate(block_or_loop=l211, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l211, ann_key="pragma_unroll_explicit", ann_val=1)
l219, l220, l221, l222 = sch.get_loops(block=b157)
b223 = sch.get_block(name="data_pack", func_name="main")
l224, l225, l226, l227, l228, l229 = sch.get_loops(block=b223)
b230 = sch.decompose_reduction(block=b223, loop=l228)
b231 = sch.get_block(name="bgemm", func_name="main")
l232, l233, l234, l235, l236, l237, l238, l239, l240, l241, l242, l243, l244, l245 = sch.get_loops(block=b231)
b246 = sch.decompose_reduction(block=b231, loop=l235)
b247 = sch.get_block(name="inverse", func_name="main")
l248, l249, l250, l251, l252, l253, l254, l255 = sch.get_loops(block=b247)
b256 = sch.decompose_reduction(block=b247, loop=l254)
2023-11-11 09:06:44 [INFO] [task_scheduler.cc:121] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #772: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/home2/xiachunwei/Software/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/home2/xiachunwei/Software/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/home2/xiachunwei/Software/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/home2/xiachunwei/Software/tvm/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/home2/xiachunwei/Software/tvm/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/home2/xiachunwei/Software/tvm/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/home2/xiachunwei/Software/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/home2/xiachunwei/Software/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  4: _ZN3tvm7runtime13PackedFun
  3: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  2: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  0: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  File "/home2/xiachunwei/Software/tvm/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(512), T.int64(7), T.int64(7)), "float32"), p1: T.Buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(512)), "float32"), p2: T.Buffer((T.int64(1), T.int64(512), T.int64(7), T.int64(7)), "float32"), p3: T.Buffer((T.int64(1), T.int64(512), T.int64(1), T.int64(1)), "float32"), p4: T.Buffer((T.int64(1), T.int64(512), T.int64(1), T.int64(1)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(512), T.int64(7), T.int64(7)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        input_tile_local = T.alloc_buffer((T.int64(512), T.int64(16), T.int64(4), T.int64(4)), scope="local")
        data_pack = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(16)))
        bgemm = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(16)))
        inverse_local = T.alloc_buffer((T.int64(512), T.int64(16), T.int64(2), T.int64(2)), scope="local")
        data_pack_local = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(16)), scope="local")
        bgemm_local = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(16)), scope="local")
        data_pack_shared = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(16)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(512)), scope="shared")
        for ci_p_fused_0 in T.thread_binding(T.int64(64), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 1024, "pragma_unroll_explicit": 1}):
            for ci_p_fused_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(1), T.int64(4), T.int64(4)):
                    with T.block("input_tile"):
                        v_ci = T.axis.spatial(T.int64(512), ci_p_fused_0 * T.int64(8) + ci_p_fused_1 // T.int64(16) + ax0)
                        v_p = T.axis.spatial(T.int64(16), ci_p_fused_1 % T.int64(16) + ax1)
                        v_eps, v_nu = T.axis.remap("SS", [ax2, ax3])
                        T.reads(p0[v_p // T.int64(16), v_ci, v_p % T.int64(16) // T.int64(4) * T.int64(2) + v_eps - T.int64(1), v_p % T.int64(4) * T.int64(2) + v_nu - T.int64(1)])
                        T.writes(input_tile_local[v_ci, v_p, v_eps, v_nu])
                        T.block_attr({"schedule_rule": "None"})
                        input_tile_local[v_ci, v_p, v_eps, v_nu] = T.if_then_else(T.int64(1) <= v_p % T.int64(16) // T.int64(4) * T.int64(2) + v_eps and v_p % T.int64(16) // T.int64(4) * T.int64(2) + v_eps < T.int64(8) and T.int64(1) <= v_p % T.int64(4) * T.int64(2) + v_nu and v_p % T.int64(4) * T.int64(2) + v_nu < T.int64(8), p0[v_p // T.int64(16), v_ci, v_p % T.int64(16) // T.int64(4) * T.int64(2) + v_eps - T.int64(1), v_p % T.int64(4) * T.int64(2) + v_nu - T.int64(1)], T.float32(0))
                for eps in T.unroll(T.int64(4)):
                    for nu in T.unroll(T.int64(4)):
                        with T.block("data_pack_init"):
                            v_eps, v_nu = T.axis.remap("SS", [eps, nu])
                            v_ci = T.axis.spatial(T.int64(512), (ci_p_fused_0 * T.int64(128) + ci_p_fused_1) // T.int64(16))
                            v_p = T.axis.spatial(T.int64(16), (ci_p_fused_0 * T.int64(128) + ci_p_fused_1) % T.int64(16))
                            T.reads()
                            T.writes(data_pack_local[v_eps, v_nu, v_ci, v_p])
                            T.block_attr({"schedule_rule": "conv2d_nchw_winograd_data_pack"})
                            data_pack_local[v_eps, v_nu, v_ci, v_p] = T.float32(0)
                        for r_a in T.unroll(T.int64(4)):
                            for r_b in T.unroll(T.int64(4)):
                                with T.block("data_pack_update"):
                                    v_eps, v_nu = T.axis.remap("SS", [eps, nu])
                                    v_ci = T.axis.spatial(T.int64(512), (ci_p_fused_0 * T.int64(128) + ci_p_fused_1) // T.int64(16))
                                    v_p = T.axis.spatial(T.int64(16), (ci_p_fused_0 * T.int64(128) + ci_p_fused_1) % T.int64(16))
                                    v_r_a, v_r_b = T.axis.remap("RR", [r_a, r_b])
                                    T.reads(data_pack_local[v_eps, v_nu, v_ci, v_p], input_tile_local[v_ci, v_p, v_r_a, v_r_b])
                                    T.writes(data_pack_local[v_eps, v_nu, v_ci, v_p])
                                    T.block_attr({"schedule_rule": "conv2d_nchw_winograd_data_pack"})
                                    data_pack_local[v_eps, v_nu, v_ci, v_p] = data_pack_local[v_eps, v_nu, v_ci, v_p] + input_tile_local[v_ci, v_p, v_r_a, v_r_b] * T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(3), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(1), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(0), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(3), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(1), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(0), T.float32(1), T.float32(0))))))))))))))))) * T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(3), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(1), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(0), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(3), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(1), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(0), T.float32(1), T.float32(0)))))))))))))))))
                for ax0, ax1, ax2, ax3 in T.grid(T.int64(4), T.int64(4), T.int64(1), T.int64(1)):
                    with T.block("data_pack_local"):
                        v0, v1 = T.axis.remap("SS", [ax0, ax1])
                        v2 = T.axis.spatial(T.int64(512), ci_p_fused_0 * T.int64(8) + ci_p_fused_1 // T.int64(16) + ax2)
                        v3 = T.axis.spatial(T.int64(16), ci_p_fused_1 % T.int64(16) + ax3)
                        T.reads(data_pack_local[v0, v1, v2, v3])
                        T.writes(data_pack[v0, v1, v2, v3])
                        data_pack[v0, v1, v2, v3] = data_pack_local[v0, v1, v2, v3]
        for eps_0_nu_0_co_0_p_0_fused in T.thread_binding(T.int64(256), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 1024, "pragma_unroll_explicit": 1}):
            for eps_1_nu_1_co_1_p_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for eps_2_nu_2_co_2_p_2_fused in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                    for eps_3_init, nu_3_init, co_3_init, p_3_init, eps_4_init, nu_4_init, co_4_init, p_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("bgemm_init"):
                            v_eps = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(64) + eps_3_init + eps_4_init)
                            v_nu = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused % T.int64(64) // T.int64(16) + nu_3_init + nu_4_init)
                            v_co = T.axis.spatial(T.int64(512), eps_0_nu_0_co_0_p_0_fused % T.int64(16) * T.int64(32) + eps_2_nu_2_co_2_p_2_fused // T.int64(8) + co_3_init + co_4_init)
                            v_p = T.axis.spatial(T.int64(16), eps_2_nu_2_co_2_p_2_fused % T.int64(8) * T.int64(2) + p_3_init + p_4_init)
                            T.reads()
                            T.writes(bgemm_local[v_eps, v_nu, v_co, v_p])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            bgemm_local[v_eps, v_nu, v_co, v_p] = T.float32(0)
                    for ci_0 in range(T.int64(4)):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(4)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("data_pack_shared"):
                                        v0 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(64))
                                        v1 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused % T.int64(64) // T.int64(16))
                                        v2 = T.axis.spatial(T.int64(512), ci_0 * T.int64(128) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(512) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) // T.int64(16))
                                        v3 = T.axis.spatial(T.int64(16), (ax0_ax1_ax2_ax3_fused_0 * T.int64(512) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) % T.int64(16))
                                        T.reads(data_pack[v0, v1, v2, v3])
                                        T.writes(data_pack_shared[v0, v1, v2, v3])
                                        data_pack_shared[v0, v1, v2, v3] = data_pack[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(16)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                                with T.block("p1_shared"):
                                    v0 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(64))
                                    v1 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused % T.int64(64) // T.int64(16))
                                    v2 = T.axis.spatial(T.int64(512), ci_0 * T.int64(128) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1) // T.int64(32))
                                    v3 = T.axis.spatial(T.int64(512), eps_0_nu_0_co_0_p_0_fused % T.int64(16) * T.int64(32) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1) % T.int64(32))
                                    T.reads(p1[v0, v1, v2, v3])
                                    T.writes(p1_shared[v0, v1, v2, v3])
                                    p1_shared[v0, v1, v2, v3] = p1[v0, v1, v2, v3]
                        for ci_1, eps_3, nu_3, co_3, p_3, ci_2, eps_4, nu_4, co_4, p_4 in T.grid(T.int64(8), T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(16), T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                            with T.block("bgemm_update"):
                                v_eps = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(64) + eps_3 + eps_4)
                                v_nu = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused % T.int64(64) // T.int64(16) + nu_3 + nu_4)
                                v_co = T.axis.spatial(T.int64(512), eps_0_nu_0_co_0_p_0_fused % T.int64(16) * T.int64(32) + eps_2_nu_2_co_2_p_2_fused // T.int64(8) + co_3 + co_4)
                                v_p = T.axis.spatial(T.int64(16), eps_2_nu_2_co_2_p_2_fused % T.int64(8) * T.int64(2) + p_3 + p_4)
                                v_ci = T.axis.reduce(T.int64(512), ci_0 * T.int64(128) + ci_1 * T.int64(16) + ci_2)
                                T.reads(bgemm_local[v_eps, v_nu, v_co, v_p], data_pack_shared[v_eps, v_nu, v_ci, v_p], p1_shared[v_eps, v_nu, v_ci, v_co])
                                T.writes(bgemm_local[v_eps, v_nu, v_co, v_p])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                bgemm_local[v_eps, v_nu, v_co, v_p] = bgemm_local[v_eps, v_nu, v_co, v_p] + data_pack_shared[v_eps, v_nu, v_ci, v_p] * p1_shared[v_eps, v_nu, v_ci, v_co]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(2)):
                        with T.block("bgemm_local"):
                            v0 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(64) + ax0)
                            v1 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused % T.int64(64) // T.int64(16) + ax1)
                            v2 = T.axis.spatial(T.int64(512), eps_0_nu_0_co_0_p_0_fused % T.int64(16) * T.int64(32) + eps_2_nu_2_co_2_p_2_fused // T.int64(8) + ax2)
                            v3 = T.axis.spatial(T.int64(16), eps_2_nu_2_co_2_p_2_fused % T.int64(8) * T.int64(2) + ax3)
                            T.reads(bgemm_local[v0, v1, v2, v3])
                            T.writes(bgemm[v0, v1, v2, v3])
                            bgemm[v0, v1, v2, v3] = bgemm_local[v0, v1, v2, v3]
        for n_co_h_0_w_0_fused_0 in T.thread_binding(T.int64(256), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 1024, "pragma_unroll_explicit": 1}):
            for n_co_h_0_w_0_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                    for ax2 in T.unroll(T.int64(2)):
                        for ax3 in T.unroll(T.int64(2)):
                            with T.block("inverse_init"):
                                v_co = T.axis.spatial(T.int64(512), (n_co_h_0_w_0_fused_0 * T.int64(32) + n_co_h_0_w_0_fused_1) // T.int64(16) + ax0)
                                v_p = T.axis.spatial(T.int64(16), (n_co_h_0_w_0_fused_0 * T.int64(32) + n_co_h_0_w_0_fused_1) % T.int64(16) + ax1)
                                v_vh, v_vw = T.axis.remap("SS", [ax2, ax3])
                                T.reads()
                                T.writes(inverse_local[v_co, v_p, v_vh, v_vw])
                                T.block_attr({"schedule_rule": "conv2d_nchw_winograd_inverse"})
                                inverse_local[v_co, v_p, v_vh, v_vw] = T.float32(0)
                            for ax4 in T.unroll(T.int64(4)):
                                for ax5 in T.unroll(T.int64(4)):
                                    with T.block("inverse_update"):
                                        v_co = T.axis.spatial(T.int64(512), (n_co_h_0_w_0_fused_0 * T.int64(32) + n_co_h_0_w_0_fused_1) // T.int64(16) + ax0)
                                        v_p = T.axis.spatial(T.int64(16), (n_co_h_0_w_0_fused_0 * T.int64(32) + n_co_h_0_w_0_fused_1) % T.int64(16) + ax1)
                                        v_vh, v_vw, v_r_a, v_r_b = T.axis.remap("SSRR", [ax2, ax3, ax4, ax5])
                                        T.reads(inverse_local[v_co, v_p, v_vh, v_vw], bgemm[v_r_a, v_r_b, v_co, v_p])
                                        T.writes(inverse_local[v_co, v_p, v_vh, v_vw])
                                        T.block_attr({"schedule_rule": "conv2d_nchw_winograd_inverse"})
                                        inverse_local[v_co, v_p, v_vh, v_vw] = inverse_local[v_co, v_p, v_vh, v_vw] + bgemm[v_r_a, v_r_b, v_co, v_p] * T.Select(v_r_a % T.int64(4) == T.int64(3) and v_vh % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_vh % T.int64(2) == T.int64(0), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_vh % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_vh % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_vh % T.int64(2) == T.int64(1), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_vh % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_vh % T.int64(2) == T.int64(1), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_vh % T.int64(2) == T.int64(0), T.float32(1), T.float32(0))))))))) * T.Select(v_r_b % T.int64(4) == T.int64(3) and v_vw % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_vw % T.int64(2) == T.int64(0), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_vw % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_vw % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_vw % T.int64(2) == T.int64(1), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_vw % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_vw % T.int64(2) == T.int64(1), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_vw % T.int64(2) == T.int64(0), T.float32(1), T.float32(0)))))))))
                for h_1, w_1 in T.grid(T.int64(2), T.int64(2)):
                    with T.block("conv2d_winograd"):
                        v_n = T.axis.spatial(T.int64(1), T.int64(0))
                        v_co = T.axis.spatial(T.int64(512), (n_co_h_0_w_0_fused_0 * T.int64(32) + n_co_h_0_w_0_fused_1) // T.int64(16))
                        v_h = T.axis.spatial(T.int64(7), (n_co_h_0_w_0_fused_0 * T.int64(32) + n_co_h_0_w_0_fused_1) % T.int64(16) // T.int64(4) * T.int64(2) + h_1)
                        v_w = T.axis.spatial(T.int64(7), (n_co_h_0_w_0_fused_0 * T.int64(32) + n_co_h_0_w_0_fused_1) % T.int64(4) * T.int64(2) + w_1)
                        T.where((n_co_h_0_w_0_fused_0 * T.int64(32) + n_co_h_0_w_0_fused_1) % T.int64(16) // T.int64(4) * T.int64(2) + h_1 < T.int64(7) and (n_co_h_0_w_0_fused_0 * T.int64(32) + n_co_h_0_w_0_fused_1) % T.int64(4) * T.int64(2) + w_1 < T.int64(7))
                        T.reads(inverse_local[v_co, v_n * T.int64(16) + v_h // T.int64(2) * T.int64(4) + v_w // T.int64(2), v_h % T.int64(2), v_w % T.int64(2)], p2[v_n, v_co, v_h, v_w], p3[v_n, v_co, T.int64(0), T.int64(0)], p4[v_n, v_co, T.int64(0), T.int64(0)])
                        T.writes(T_relu[v_n, v_co, v_h, v_w])
                        T_relu[v_n, v_co, v_h, v_w] = T.max((inverse_local[v_co, v_n * T.int64(16) + v_h // T.int64(2) * T.int64(4) + v_w // T.int64(2), v_h % T.int64(2), v_w % T.int64(2)] + p2[v_n, v_co, v_h, v_w]) * p3[v_n, v_co, T.int64(0), T.int64(0)] + p4[v_n, v_co, T.int64(0), T.int64(0)], T.float32(0))
b0 = sch.get_block(name="data_pack", func_name="main")
b1 = sch.get_block(name="bgemm", func_name="main")
b2 = sch.get_block(name="inverse", func_name="main")
b3 = sch.get_block(name="conv2d_winograd", func_name="main")
b4 = sch.get_block(name="T_add", func_name="main")
b5 = sch.get_block(name="T_multiply", func_name="main")
b6 = sch.get_block(name="T_add_1", func_name="main")
b7 = sch.get_block(name="T_relu", func_name="main")
b8 = sch.get_block(name="root", func_name="main")
b9, b10 = sch.get_producers(block=b2)
sch.compute_inline(block=b10)
b11, = sch.get_consumers(block=b2)
l12, l13, l14, l15 = sch.get_loops(block=b11)
l16, l17 = sch.split(loop=l14, factors=[None, 2], preserve_unit_iters=True)
l18, l19 = sch.split(loop=l15, factors=[None, 2], preserve_unit_iters=True)
sch.reorder(l16, l18, l17, l19)
sch.compute_at(block=b2, loop=l18, preserve_unit_loops=True, index=-1)
sch.set_scope(block=b2, buffer_index=0, storage_scope="local")
l20, l21, l22, l23, l24, l25, l26, l27, l28, l29 = sch.get_loops(block=b2)
sch.unroll(loop=l26)
sch.unroll(loop=l27)
sch.unroll(loop=l28)
sch.unroll(loop=l29)
b30, b31 = sch.get_producers(block=b0)
sch.compute_inline(block=b31)
b32, = sch.get_producers(block=b30)
l33, l34, l35, l36, l37, l38 = sch.get_loops(block=b0)
sch.reorder(l35, l36, l33, l34, l37, l38)
sch.unroll(loop=l33)
sch.unroll(loop=l34)
sch.unroll(loop=l37)
sch.unroll(loop=l38)
l39 = sch.fuse(l35, l36, preserve_unit_iters=True)
v40 = sch.sample_categorical(candidates=[32, 64, 128, 256, 512, 1024], probs=[0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666], decision=2)
l41, l42 = sch.split(loop=l39, factors=[None, v40], preserve_unit_iters=True)
sch.bind(loop=l41, thread_axis="blockIdx.x")
sch.bind(loop=l42, thread_axis="threadIdx.x")
b43 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b43, loop=l42, preserve_unit_loops=True, index=-1)
sch.compute_at(block=b30, loop=l42, preserve_unit_loops=True, index=-1)
sch.set_scope(block=b30, buffer_index=0, storage_scope="local")
sch.compute_inline(block=b32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l44, l45, l46, l47, l48 = sch.get_loops(block=b1)
v49, v50, v51, v52, v53 = sch.sample_perfect_tile(loop=l44, n=5, max_innermost_factor=64, decision=[4, 1, 1, 1, 1])
l54, l55, l56, l57, l58 = sch.split(loop=l44, factors=[v49, v50, v51, v52, v53], preserve_unit_iters=True)
v59, v60, v61, v62, v63 = sch.sample_perfect_tile(loop=l45, n=5, max_innermost_factor=64, decision=[4, 1, 1, 1, 1])
l64, l65, l66, l67, l68 = sch.split(loop=l45, factors=[v59, v60, v61, v62, v63], preserve_unit_iters=True)
v69, v70, v71, v72, v73 = sch.sample_perfect_tile(loop=l46, n=5, max_innermost_factor=64, decision=[16, 1, 32, 1, 1])
l74, l75, l76, l77, l78 = sch.split(loop=l46, factors=[v69, v70, v71, v72, v73], preserve_unit_iters=True)
v79, v80, v81, v82, v83 = sch.sample_perfect_tile(loop=l47, n=5, max_innermost_factor=64, decision=[1, 1, 8, 2, 1])
l84, l85, l86, l87, l88 = sch.split(loop=l47, factors=[v79, v80, v81, v82, v83], preserve_unit_iters=True)
v89, v90, v91 = sch.sample_perfect_tile(loop=l48, n=3, max_innermost_factor=64, decision=[4, 8, 16])
l92, l93, l94 = sch.split(loop=l48, factors=[v89, v90, v91], preserve_unit_iters=True)
sch.reorder(l54, l64, l74, l84, l55, l65, l75, l85, l56, l66, l76, l86, l92, l93, l57, l67, l77, l87, l94, l58, l68, l78, l88)
l95 = sch.fuse(l54, l64, l74, l84, preserve_unit_iters=True)
sch.bind(loop=l95, thread_axis="blockIdx.x")
l96 = sch.fuse(l55, l65, l75, l85, preserve_unit_iters=True)
sch.bind(loop=l96, thread_axis="vthread.x")
l97 = sch.fuse(l56, l66, l76, l86, preserve_unit_iters=True)
sch.bind(loop=l97, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b98 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b98, loop=l97, preserve_unit_loops=True, index=-1)
b99 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b99, loop=l92, preserve_unit_loops=True, index=-1)
l100, l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b99)
l108 = sch.fuse(l104, l105, l106, l107, preserve_unit_iters=True)
v109 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b99, ann_key="meta_schedule.cooperative_fetch", ann_val=v109)
b110 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b110, loop=l92, preserve_unit_loops=True, index=-1)
l111, l112, l113, l114, l115, l116, l117, l118 = sch.get_loops(block=b110)
l119 = sch.fuse(l115, l116, l117, l118, preserve_unit_iters=True)
v120 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b110, ann_key="meta_schedule.cooperative_fetch", ann_val=v120)
sch.reverse_compute_inline(block=b7)
sch.reverse_compute_inline(block=b6)
sch.reverse_compute_inline(block=b5)
sch.reverse_compute_inline(block=b4)
v121 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b8, ann_key="meta_schedule.unroll_explicit", ann_val=v121)
l122, l123, l124, l125, l126, l127 = sch.get_loops(block=b3)
l128 = sch.fuse(l122, l123, l124, l125, preserve_unit_iters=True)
v129 = sch.sample_categorical(candidates=[32, 64, 128, 256, 512, 1024], probs=[0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666], decision=0)
l130, l131 = sch.split(loop=l128, factors=[None, v129], preserve_unit_iters=True)
sch.bind(loop=l130, thread_axis="blockIdx.x")
sch.bind(loop=l131, thread_axis="threadIdx.x")
sch.enter_postproc()
sch.unannotate(block_or_loop=b99, ann_key="meta_schedule.cooperative_fetch")
l132, l133, l134, l135, l136 = sch.get_loops(block=b99)
l137, l138, l139 = sch.split(loop=l136, factors=[None, 256, 2], preserve_unit_iters=True)
sch.vectorize(loop=l139)
sch.bind(loop=l138, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b110, ann_key="meta_schedule.cooperative_fetch")
l140, l141, l142, l143, l144 = sch.get_loops(block=b110)
l145, l146 = sch.split(loop=l144, factors=[None, 256], preserve_unit_iters=True)
sch.bind(loop=l146, thread_axis="threadIdx.x")
b147 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b147, ann_key="meta_schedule.unroll_explicit")
b148, b149, b150, b151, b152, b153, b154, b155, b156 = sch.get_child_blocks(b147)
l157, l158, l159, l160, l161, l162 = sch.get_loops(block=b148)
l163, l164, l165, l166, l167, l168 = sch.get_loops(block=b149)
sch.annotate(block_or_loop=l163, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l163, ann_key="pragma_unroll_explicit", ann_val=1)
l169, l170, l171, l172, l173, l174 = sch.get_loops(block=b150)
l175, l176, l177, l178, l179, l180, l181 = sch.get_loops(block=b151)
l182, l183, l184, l185, l186, l187 = sch.get_loops(block=b152)
l188, l189, l190, l191, l192, l193, l194, l195, l196, l197, l198, l199, l200, l201 = sch.get_loops(block=b153)
sch.annotate(block_or_loop=l188, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l188, ann_key="pragma_unroll_explicit", ann_val=1)
l202, l203, l204, l205, l206, l207, l208 = sch.get_loops(block=b154)
l209, l210, l211, l212, l213, l214, l215, l216 = sch.get_loops(block=b155)
sch.annotate(block_or_loop=l209, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l209, ann_key="pragma_unroll_explicit", ann_val=1)
l217, l218, l219, l220 = sch.get_loops(block=b156)
b221 = sch.get_block(name="data_pack", func_name="main")
l222, l223, l224, l225, l226, l227 = sch.get_loops(block=b221)
b228 = sch.decompose_reduction(block=b221, loop=l226)
b229 = sch.get_block(name="bgemm", func_name="main")
l230, l231, l232, l233, l234, l235, l236, l237, l238, l239, l240, l241, l242, l243 = sch.get_loops(block=b229)
b244 = sch.decompose_reduction(block=b229, loop=l233)
b245 = sch.get_block(name="inverse", func_name="main")
l246, l247, l248, l249, l250, l251, l252, l253 = sch.get_loops(block=b245)
b254 = sch.decompose_reduction(block=b245, loop=l252)
2023-11-11 09:06:44 [INFO] [task_scheduler.cc:121] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #773: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/home2/xiachunwei/Software/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/home2/xiachunwei/Software/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/home2/xiachunwei/Software/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/home2/xiachunwei/Software/tvm/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/home2/xiachunwei/Software/tvm/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/home2/xiachunwei/Software/tvm/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/home2/xiachunwei/Software/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/home2/xiachunwei/Software/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  4: _ZN3tvm7runtime13PackedFun
  3: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  2: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  0: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  File "/home2/xiachunwei/Software/tvm/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(512), T.int64(7), T.int64(7)), "float32"), p1: T.Buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(512)), "float32"), p2: T.Buffer((T.int64(1), T.int64(512), T.int64(7), T.int64(7)), "float32"), p3: T.Buffer((T.int64(1), T.int64(512), T.int64(1), T.int64(1)), "float32"), p4: T.Buffer((T.int64(1), T.int64(512), T.int64(1), T.int64(1)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(512), T.int64(7), T.int64(7)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        input_tile_local = T.alloc_buffer((T.int64(512), T.int64(16), T.int64(4), T.int64(4)), scope="local")
        data_pack = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(16)))
        bgemm = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(16)))
        inverse_local = T.alloc_buffer((T.int64(512), T.int64(16), T.int64(2), T.int64(2)), scope="local")
        data_pack_local = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(16)), scope="local")
        bgemm_local = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(16)), scope="local")
        data_pack_shared = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(16)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(512)), scope="shared")
        for ci_p_fused_0 in T.thread_binding(T.int64(64), thread="blockIdx.x"):
            for ci_p_fused_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(1), T.int64(4), T.int64(4)):
                    with T.block("input_tile"):
                        v_ci = T.axis.spatial(T.int64(512), ci_p_fused_0 * T.int64(8) + ci_p_fused_1 // T.int64(16) + ax0)
                        v_p = T.axis.spatial(T.int64(16), ci_p_fused_1 % T.int64(16) + ax1)
                        v_eps, v_nu = T.axis.remap("SS", [ax2, ax3])
                        T.reads(p0[v_p // T.int64(16), v_ci, v_p % T.int64(16) // T.int64(4) * T.int64(2) + v_eps - T.int64(1), v_p % T.int64(4) * T.int64(2) + v_nu - T.int64(1)])
                        T.writes(input_tile_local[v_ci, v_p, v_eps, v_nu])
                        T.block_attr({"schedule_rule": "None"})
                        input_tile_local[v_ci, v_p, v_eps, v_nu] = T.if_then_else(T.int64(1) <= v_p % T.int64(16) // T.int64(4) * T.int64(2) + v_eps and v_p % T.int64(16) // T.int64(4) * T.int64(2) + v_eps < T.int64(8) and T.int64(1) <= v_p % T.int64(4) * T.int64(2) + v_nu and v_p % T.int64(4) * T.int64(2) + v_nu < T.int64(8), p0[v_p // T.int64(16), v_ci, v_p % T.int64(16) // T.int64(4) * T.int64(2) + v_eps - T.int64(1), v_p % T.int64(4) * T.int64(2) + v_nu - T.int64(1)], T.float32(0))
                for eps in T.unroll(T.int64(4)):
                    for nu in T.unroll(T.int64(4)):
                        with T.block("data_pack_init"):
                            v_eps, v_nu = T.axis.remap("SS", [eps, nu])
                            v_ci = T.axis.spatial(T.int64(512), (ci_p_fused_0 * T.int64(128) + ci_p_fused_1) // T.int64(16))
                            v_p = T.axis.spatial(T.int64(16), (ci_p_fused_0 * T.int64(128) + ci_p_fused_1) % T.int64(16))
                            T.reads()
                            T.writes(data_pack_local[v_eps, v_nu, v_ci, v_p])
                            T.block_attr({"schedule_rule": "conv2d_nchw_winograd_data_pack"})
                            data_pack_local[v_eps, v_nu, v_ci, v_p] = T.float32(0)
                        for r_a in T.unroll(T.int64(4)):
                            for r_b in T.unroll(T.int64(4)):
                                with T.block("data_pack_update"):
                                    v_eps, v_nu = T.axis.remap("SS", [eps, nu])
                                    v_ci = T.axis.spatial(T.int64(512), (ci_p_fused_0 * T.int64(128) + ci_p_fused_1) // T.int64(16))
                                    v_p = T.axis.spatial(T.int64(16), (ci_p_fused_0 * T.int64(128) + ci_p_fused_1) % T.int64(16))
                                    v_r_a, v_r_b = T.axis.remap("RR", [r_a, r_b])
                                    T.reads(data_pack_local[v_eps, v_nu, v_ci, v_p], input_tile_local[v_ci, v_p, v_r_a, v_r_b])
                                    T.writes(data_pack_local[v_eps, v_nu, v_ci, v_p])
                                    T.block_attr({"schedule_rule": "conv2d_nchw_winograd_data_pack"})
                                    data_pack_local[v_eps, v_nu, v_ci, v_p] = data_pack_local[v_eps, v_nu, v_ci, v_p] + input_tile_local[v_ci, v_p, v_r_a, v_r_b] * T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(3), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(1), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(0), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(3), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(1), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(0), T.float32(1), T.float32(0))))))))))))))))) * T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(3), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(1), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(0), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(3), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(1), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(0), T.float32(1), T.float32(0)))))))))))))))))
                for ax0, ax1, ax2, ax3 in T.grid(T.int64(4), T.int64(4), T.int64(1), T.int64(1)):
                    with T.block("data_pack_local"):
                        v0, v1 = T.axis.remap("SS", [ax0, ax1])
                        v2 = T.axis.spatial(T.int64(512), ci_p_fused_0 * T.int64(8) + ci_p_fused_1 // T.int64(16) + ax2)
                        v3 = T.axis.spatial(T.int64(16), ci_p_fused_1 % T.int64(16) + ax3)
                        T.reads(data_pack_local[v0, v1, v2, v3])
                        T.writes(data_pack[v0, v1, v2, v3])
                        data_pack[v0, v1, v2, v3] = data_pack_local[v0, v1, v2, v3]
        for eps_0_nu_0_co_0_p_0_fused in T.thread_binding(T.int64(1024), thread="blockIdx.x"):
            for eps_1_nu_1_co_1_p_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for eps_2_nu_2_co_2_p_2_fused in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                    for eps_3_init, nu_3_init, co_3_init, p_3_init, eps_4_init, nu_4_init, co_4_init, p_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("bgemm_init"):
                            v_eps = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(256) + eps_3_init + eps_4_init)
                            v_nu = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused % T.int64(256) // T.int64(64) + nu_3_init + nu_4_init)
                            v_co = T.axis.spatial(T.int64(512), eps_0_nu_0_co_0_p_0_fused % T.int64(64) * T.int64(8) + eps_2_nu_2_co_2_p_2_fused // T.int64(8) + co_3_init + co_4_init)
                            v_p = T.axis.spatial(T.int64(16), eps_2_nu_2_co_2_p_2_fused % T.int64(8) * T.int64(2) + p_3_init + p_4_init)
                            T.reads()
                            T.writes(bgemm_local[v_eps, v_nu, v_co, v_p])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            bgemm_local[v_eps, v_nu, v_co, v_p] = T.float32(0)
                    for ci_0 in range(T.int64(4)):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(8)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("data_pack_shared"):
                                        v0 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(256))
                                        v1 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused % T.int64(256) // T.int64(64))
                                        v2 = T.axis.spatial(T.int64(512), ci_0 * T.int64(128) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) // T.int64(16))
                                        v3 = T.axis.spatial(T.int64(16), (ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) % T.int64(16))
                                        T.reads(data_pack[v0, v1, v2, v3])
                                        T.writes(data_pack_shared[v0, v1, v2, v3])
                                        data_pack_shared[v0, v1, v2, v3] = data_pack[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(16)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                                with T.block("p1_shared"):
                                    v0 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(256))
                                    v1 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused % T.int64(256) // T.int64(64))
                                    v2 = T.axis.spatial(T.int64(512), ci_0 * T.int64(128) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused_1) // T.int64(8))
                                    v3 = T.axis.spatial(T.int64(512), eps_0_nu_0_co_0_p_0_fused % T.int64(64) * T.int64(8) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused_1) % T.int64(8))
                                    T.reads(p1[v0, v1, v2, v3])
                                    T.writes(p1_shared[v0, v1, v2, v3])
                                    p1_shared[v0, v1, v2, v3] = p1[v0, v1, v2, v3]
                        for ci_1, eps_3, nu_3, co_3, p_3, ci_2, eps_4, nu_4, co_4, p_4 in T.grid(T.int64(32), T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(4), T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                            with T.block("bgemm_update"):
                                v_eps = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(256) + eps_3 + eps_4)
                                v_nu = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused % T.int64(256) // T.int64(64) + nu_3 + nu_4)
                                v_co = T.axis.spatial(T.int64(512), eps_0_nu_0_co_0_p_0_fused % T.int64(64) * T.int64(8) + eps_2_nu_2_co_2_p_2_fused // T.int64(8) + co_3 + co_4)
                                v_p = T.axis.spatial(T.int64(16), eps_2_nu_2_co_2_p_2_fused % T.int64(8) * T.int64(2) + p_3 + p_4)
                                v_ci = T.axis.reduce(T.int64(512), ci_0 * T.int64(128) + ci_1 * T.int64(4) + ci_2)
                                T.reads(bgemm_local[v_eps, v_nu, v_co, v_p], data_pack_shared[v_eps, v_nu, v_ci, v_p], p1_shared[v_eps, v_nu, v_ci, v_co])
                                T.writes(bgemm_local[v_eps, v_nu, v_co, v_p])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                bgemm_local[v_eps, v_nu, v_co, v_p] = bgemm_local[v_eps, v_nu, v_co, v_p] + data_pack_shared[v_eps, v_nu, v_ci, v_p] * p1_shared[v_eps, v_nu, v_ci, v_co]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(2)):
                        with T.block("bgemm_local"):
                            v0 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(256) + ax0)
                            v1 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused % T.int64(256) // T.int64(64) + ax1)
                            v2 = T.axis.spatial(T.int64(512), eps_0_nu_0_co_0_p_0_fused % T.int64(64) * T.int64(8) + eps_2_nu_2_co_2_p_2_fused // T.int64(8) + ax2)
                            v3 = T.axis.spatial(T.int64(16), eps_2_nu_2_co_2_p_2_fused % T.int64(8) * T.int64(2) + ax3)
                            T.reads(bgemm_local[v0, v1, v2, v3])
                            T.writes(bgemm[v0, v1, v2, v3])
                            bgemm[v0, v1, v2, v3] = bgemm_local[v0, v1, v2, v3]
        for n_co_h_0_w_0_fused_0 in T.thread_binding(T.int64(256), thread="blockIdx.x"):
            for n_co_h_0_w_0_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                    for ax2 in T.unroll(T.int64(2)):
                        for ax3 in T.unroll(T.int64(2)):
                            with T.block("inverse_init"):
                                v_co = T.axis.spatial(T.int64(512), (n_co_h_0_w_0_fused_0 * T.int64(32) + n_co_h_0_w_0_fused_1) // T.int64(16) + ax0)
                                v_p = T.axis.spatial(T.int64(16), (n_co_h_0_w_0_fused_0 * T.int64(32) + n_co_h_0_w_0_fused_1) % T.int64(16) + ax1)
                                v_vh, v_vw = T.axis.remap("SS", [ax2, ax3])
                                T.reads()
                                T.writes(inverse_local[v_co, v_p, v_vh, v_vw])
                                T.block_attr({"schedule_rule": "conv2d_nchw_winograd_inverse"})
                                inverse_local[v_co, v_p, v_vh, v_vw] = T.float32(0)
                            for ax4 in T.unroll(T.int64(4)):
                                for ax5 in T.unroll(T.int64(4)):
                                    with T.block("inverse_update"):
                                        v_co = T.axis.spatial(T.int64(512), (n_co_h_0_w_0_fused_0 * T.int64(32) + n_co_h_0_w_0_fused_1) // T.int64(16) + ax0)
                                        v_p = T.axis.spatial(T.int64(16), (n_co_h_0_w_0_fused_0 * T.int64(32) + n_co_h_0_w_0_fused_1) % T.int64(16) + ax1)
                                        v_vh, v_vw, v_r_a, v_r_b = T.axis.remap("SSRR", [ax2, ax3, ax4, ax5])
                                        T.reads(inverse_local[v_co, v_p, v_vh, v_vw], bgemm[v_r_a, v_r_b, v_co, v_p])
                                        T.writes(inverse_local[v_co, v_p, v_vh, v_vw])
                                        T.block_attr({"schedule_rule": "conv2d_nchw_winograd_inverse"})
                                        inverse_local[v_co, v_p, v_vh, v_vw] = inverse_local[v_co, v_p, v_vh, v_vw] + bgemm[v_r_a, v_r_b, v_co, v_p] * T.Select(v_r_a % T.int64(4) == T.int64(3) and v_vh % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_vh % T.int64(2) == T.int64(0), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_vh % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_vh % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_vh % T.int64(2) == T.int64(1), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_vh % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_vh % T.int64(2) == T.int64(1), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_vh % T.int64(2) == T.int64(0), T.float32(1), T.float32(0))))))))) * T.Select(v_r_b % T.int64(4) == T.int64(3) and v_vw % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_vw % T.int64(2) == T.int64(0), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_vw % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_vw % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_vw % T.int64(2) == T.int64(1), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_vw % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_vw % T.int64(2) == T.int64(1), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_vw % T.int64(2) == T.int64(0), T.float32(1), T.float32(0)))))))))
                for h_1, w_1 in T.grid(T.int64(2), T.int64(2)):
                    with T.block("conv2d_winograd"):
                        v_n = T.axis.spatial(T.int64(1), T.int64(0))
                        v_co = T.axis.spatial(T.int64(512), (n_co_h_0_w_0_fused_0 * T.int64(32) + n_co_h_0_w_0_fused_1) // T.int64(16))
                        v_h = T.axis.spatial(T.int64(7), (n_co_h_0_w_0_fused_0 * T.int64(32) + n_co_h_0_w_0_fused_1) % T.int64(16) // T.int64(4) * T.int64(2) + h_1)
                        v_w = T.axis.spatial(T.int64(7), (n_co_h_0_w_0_fused_0 * T.int64(32) + n_co_h_0_w_0_fused_1) % T.int64(4) * T.int64(2) + w_1)
                        T.where((n_co_h_0_w_0_fused_0 * T.int64(32) + n_co_h_0_w_0_fused_1) % T.int64(16) // T.int64(4) * T.int64(2) + h_1 < T.int64(7) and (n_co_h_0_w_0_fused_0 * T.int64(32) + n_co_h_0_w_0_fused_1) % T.int64(4) * T.int64(2) + w_1 < T.int64(7))
                        T.reads(inverse_local[v_co, v_n * T.int64(16) + v_h // T.int64(2) * T.int64(4) + v_w // T.int64(2), v_h % T.int64(2), v_w % T.int64(2)], p2[v_n, v_co, v_h, v_w], p3[v_n, v_co, T.int64(0), T.int64(0)], p4[v_n, v_co, T.int64(0), T.int64(0)])
                        T.writes(T_relu[v_n, v_co, v_h, v_w])
                        T_relu[v_n, v_co, v_h, v_w] = T.max((inverse_local[v_co, v_n * T.int64(16) + v_h // T.int64(2) * T.int64(4) + v_w // T.int64(2), v_h % T.int64(2), v_w % T.int64(2)] + p2[v_n, v_co, v_h, v_w]) * p3[v_n, v_co, T.int64(0), T.int64(0)] + p4[v_n, v_co, T.int64(0), T.int64(0)], T.float32(0))
b0 = sch.get_block(name="data_pack", func_name="main")
b1 = sch.get_block(name="bgemm", func_name="main")
b2 = sch.get_block(name="inverse", func_name="main")
b3 = sch.get_block(name="conv2d_winograd", func_name="main")
b4 = sch.get_block(name="T_add", func_name="main")
b5 = sch.get_block(name="T_multiply", func_name="main")
b6 = sch.get_block(name="T_add_1", func_name="main")
b7 = sch.get_block(name="T_relu", func_name="main")
b8 = sch.get_block(name="root", func_name="main")
b9, b10 = sch.get_producers(block=b2)
sch.compute_inline(block=b10)
b11, = sch.get_consumers(block=b2)
l12, l13, l14, l15 = sch.get_loops(block=b11)
l16, l17 = sch.split(loop=l14, factors=[None, 2], preserve_unit_iters=True)
l18, l19 = sch.split(loop=l15, factors=[None, 2], preserve_unit_iters=True)
sch.reorder(l16, l18, l17, l19)
sch.compute_at(block=b2, loop=l18, preserve_unit_loops=True, index=-1)
sch.set_scope(block=b2, buffer_index=0, storage_scope="local")
l20, l21, l22, l23, l24, l25, l26, l27, l28, l29 = sch.get_loops(block=b2)
sch.unroll(loop=l26)
sch.unroll(loop=l27)
sch.unroll(loop=l28)
sch.unroll(loop=l29)
b30, b31 = sch.get_producers(block=b0)
sch.compute_inline(block=b31)
b32, = sch.get_producers(block=b30)
l33, l34, l35, l36, l37, l38 = sch.get_loops(block=b0)
sch.reorder(l35, l36, l33, l34, l37, l38)
sch.unroll(loop=l33)
sch.unroll(loop=l34)
sch.unroll(loop=l37)
sch.unroll(loop=l38)
l39 = sch.fuse(l35, l36, preserve_unit_iters=True)
v40 = sch.sample_categorical(candidates=[32, 64, 128, 256, 512, 1024], probs=[0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666], decision=2)
l41, l42 = sch.split(loop=l39, factors=[None, v40], preserve_unit_iters=True)
sch.bind(loop=l41, thread_axis="blockIdx.x")
sch.bind(loop=l42, thread_axis="threadIdx.x")
b43 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b43, loop=l42, preserve_unit_loops=True, index=-1)
sch.compute_at(block=b30, loop=l42, preserve_unit_loops=True, index=-1)
sch.set_scope(block=b30, buffer_index=0, storage_scope="local")
sch.compute_inline(block=b32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l44, l45, l46, l47, l48 = sch.get_loops(block=b1)
v49, v50, v51, v52, v53 = sch.sample_perfect_tile(loop=l44, n=5, max_innermost_factor=64, decision=[4, 1, 1, 1, 1])
l54, l55, l56, l57, l58 = sch.split(loop=l44, factors=[v49, v50, v51, v52, v53], preserve_unit_iters=True)
v59, v60, v61, v62, v63 = sch.sample_perfect_tile(loop=l45, n=5, max_innermost_factor=64, decision=[4, 1, 1, 1, 1])
l64, l65, l66, l67, l68 = sch.split(loop=l45, factors=[v59, v60, v61, v62, v63], preserve_unit_iters=True)
v69, v70, v71, v72, v73 = sch.sample_perfect_tile(loop=l46, n=5, max_innermost_factor=64, decision=[64, 1, 8, 1, 1])
l74, l75, l76, l77, l78 = sch.split(loop=l46, factors=[v69, v70, v71, v72, v73], preserve_unit_iters=True)
v79, v80, v81, v82, v83 = sch.sample_perfect_tile(loop=l47, n=5, max_innermost_factor=64, decision=[1, 1, 8, 2, 1])
l84, l85, l86, l87, l88 = sch.split(loop=l47, factors=[v79, v80, v81, v82, v83], preserve_unit_iters=True)
v89, v90, v91 = sch.sample_perfect_tile(loop=l48, n=3, max_innermost_factor=64, decision=[4, 32, 4])
l92, l93, l94 = sch.split(loop=l48, factors=[v89, v90, v91], preserve_unit_iters=True)
sch.reorder(l54, l64, l74, l84, l55, l65, l75, l85, l56, l66, l76, l86, l92, l93, l57, l67, l77, l87, l94, l58, l68, l78, l88)
l95 = sch.fuse(l54, l64, l74, l84, preserve_unit_iters=True)
sch.bind(loop=l95, thread_axis="blockIdx.x")
l96 = sch.fuse(l55, l65, l75, l85, preserve_unit_iters=True)
sch.bind(loop=l96, thread_axis="vthread.x")
l97 = sch.fuse(l56, l66, l76, l86, preserve_unit_iters=True)
sch.bind(loop=l97, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b98 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b98, loop=l97, preserve_unit_loops=True, index=-1)
b99 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b99, loop=l92, preserve_unit_loops=True, index=-1)
l100, l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b99)
l108 = sch.fuse(l104, l105, l106, l107, preserve_unit_iters=True)
v109 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b99, ann_key="meta_schedule.cooperative_fetch", ann_val=v109)
b110 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b110, loop=l92, preserve_unit_loops=True, index=-1)
l111, l112, l113, l114, l115, l116, l117, l118 = sch.get_loops(block=b110)
l119 = sch.fuse(l115, l116, l117, l118, preserve_unit_iters=True)
v120 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b110, ann_key="meta_schedule.cooperative_fetch", ann_val=v120)
sch.reverse_compute_inline(block=b7)
sch.reverse_compute_inline(block=b6)
sch.reverse_compute_inline(block=b5)
sch.reverse_compute_inline(block=b4)
v121 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=0)
sch.annotate(block_or_loop=b8, ann_key="meta_schedule.unroll_explicit", ann_val=v121)
l122, l123, l124, l125, l126, l127 = sch.get_loops(block=b3)
l128 = sch.fuse(l122, l123, l124, l125, preserve_unit_iters=True)
v129 = sch.sample_categorical(candidates=[32, 64, 128, 256, 512, 1024], probs=[0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666], decision=0)
l130, l131 = sch.split(loop=l128, factors=[None, v129], preserve_unit_iters=True)
sch.bind(loop=l130, thread_axis="blockIdx.x")
sch.bind(loop=l131, thread_axis="threadIdx.x")
sch.enter_postproc()
sch.unannotate(block_or_loop=b99, ann_key="meta_schedule.cooperative_fetch")
l132, l133, l134, l135, l136 = sch.get_loops(block=b99)
l137, l138, l139 = sch.split(loop=l136, factors=[None, 64, 4], preserve_unit_iters=True)
sch.vectorize(loop=l139)
sch.bind(loop=l138, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b110, ann_key="meta_schedule.cooperative_fetch")
l140, l141, l142, l143, l144 = sch.get_loops(block=b110)
l145, l146 = sch.split(loop=l144, factors=[None, 64], preserve_unit_iters=True)
sch.bind(loop=l146, thread_axis="threadIdx.x")
b147 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b147, ann_key="meta_schedule.unroll_explicit")
b148, b149, b150, b151, b152, b153, b154, b155, b156 = sch.get_child_blocks(b147)
l157, l158, l159, l160, l161, l162 = sch.get_loops(block=b148)
l163, l164, l165, l166, l167, l168 = sch.get_loops(block=b149)
l169, l170, l171, l172, l173, l174 = sch.get_loops(block=b150)
l175, l176, l177, l178, l179, l180, l181 = sch.get_loops(block=b151)
l182, l183, l184, l185, l186, l187 = sch.get_loops(block=b152)
l188, l189, l190, l191, l192, l193, l194, l195, l196, l197, l198, l199, l200, l201 = sch.get_loops(block=b153)
l202, l203, l204, l205, l206, l207, l208 = sch.get_loops(block=b154)
l209, l210, l211, l212, l213, l214, l215, l216 = sch.get_loops(block=b155)
l217, l218, l219, l220 = sch.get_loops(block=b156)
b221 = sch.get_block(name="data_pack", func_name="main")
l222, l223, l224, l225, l226, l227 = sch.get_loops(block=b221)
b228 = sch.decompose_reduction(block=b221, loop=l226)
b229 = sch.get_block(name="bgemm", func_name="main")
l230, l231, l232, l233, l234, l235, l236, l237, l238, l239, l240, l241, l242, l243 = sch.get_loops(block=b229)
b244 = sch.decompose_reduction(block=b229, loop=l233)
b245 = sch.get_block(name="inverse", func_name="main")
l246, l247, l248, l249, l250, l251, l252, l253 = sch.get_loops(block=b245)
b254 = sch.decompose_reduction(block=b245, loop=l252)
2023-11-11 09:06:44 [INFO] [task_scheduler.cc:121] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #774: Error in running:
LocalRunner: An exception occurred
Subprocess terminated
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(512), T.int64(7), T.int64(7)), "float32"), p1: T.Buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(512)), "float32"), p2: T.Buffer((T.int64(1), T.int64(512), T.int64(7), T.int64(7)), "float32"), p3: T.Buffer((T.int64(1), T.int64(512), T.int64(1), T.int64(1)), "float32"), p4: T.Buffer((T.int64(1), T.int64(512), T.int64(1), T.int64(1)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(512), T.int64(7), T.int64(7)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        input_tile_local = T.alloc_buffer((T.int64(512), T.int64(16), T.int64(4), T.int64(4)), scope="local")
        data_pack = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(16)))
        bgemm = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(16)))
        inverse_local = T.alloc_buffer((T.int64(512), T.int64(16), T.int64(2), T.int64(2)), scope="local")
        data_pack_local = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(16)), scope="local")
        bgemm_local = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(16)), scope="local")
        data_pack_shared = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(16)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(512), T.int64(512)), scope="shared")
        for ci_p_fused_0 in T.thread_binding(T.int64(64), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 64, "pragma_unroll_explicit": 1}):
            for ci_p_fused_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(1), T.int64(4), T.int64(4)):
                    with T.block("input_tile"):
                        v_ci = T.axis.spatial(T.int64(512), ci_p_fused_0 * T.int64(8) + ci_p_fused_1 // T.int64(16) + ax0)
                        v_p = T.axis.spatial(T.int64(16), ci_p_fused_1 % T.int64(16) + ax1)
                        v_eps, v_nu = T.axis.remap("SS", [ax2, ax3])
                        T.reads(p0[v_p // T.int64(16), v_ci, v_p % T.int64(16) // T.int64(4) * T.int64(2) + v_eps - T.int64(1), v_p % T.int64(4) * T.int64(2) + v_nu - T.int64(1)])
                        T.writes(input_tile_local[v_ci, v_p, v_eps, v_nu])
                        T.block_attr({"schedule_rule": "None"})
                        input_tile_local[v_ci, v_p, v_eps, v_nu] = T.if_then_else(T.int64(1) <= v_p % T.int64(16) // T.int64(4) * T.int64(2) + v_eps and v_p % T.int64(16) // T.int64(4) * T.int64(2) + v_eps < T.int64(8) and T.int64(1) <= v_p % T.int64(4) * T.int64(2) + v_nu and v_p % T.int64(4) * T.int64(2) + v_nu < T.int64(8), p0[v_p // T.int64(16), v_ci, v_p % T.int64(16) // T.int64(4) * T.int64(2) + v_eps - T.int64(1), v_p % T.int64(4) * T.int64(2) + v_nu - T.int64(1)], T.float32(0))
                for eps in T.unroll(T.int64(4)):
                    for nu in T.unroll(T.int64(4)):
                        with T.block("data_pack_init"):
                            v_eps, v_nu = T.axis.remap("SS", [eps, nu])
                            v_ci = T.axis.spatial(T.int64(512), (ci_p_fused_0 * T.int64(128) + ci_p_fused_1) // T.int64(16))
                            v_p = T.axis.spatial(T.int64(16), (ci_p_fused_0 * T.int64(128) + ci_p_fused_1) % T.int64(16))
                            T.reads()
                            T.writes(data_pack_local[v_eps, v_nu, v_ci, v_p])
                            T.block_attr({"schedule_rule": "conv2d_nchw_winograd_data_pack"})
                            data_pack_local[v_eps, v_nu, v_ci, v_p] = T.float32(0)
                        for r_a in T.unroll(T.int64(4)):
                            for r_b in T.unroll(T.int64(4)):
                                with T.block("data_pack_update"):
                                    v_eps, v_nu = T.axis.remap("SS", [eps, nu])
                                    v_ci = T.axis.spatial(T.int64(512), (ci_p_fused_0 * T.int64(128) + ci_p_fused_1) // T.int64(16))
                                    v_p = T.axis.spatial(T.int64(16), (ci_p_fused_0 * T.int64(128) + ci_p_fused_1) % T.int64(16))
                                    v_r_a, v_r_b = T.axis.remap("RR", [r_a, r_b])
                                    T.reads(data_pack_local[v_eps, v_nu, v_ci, v_p], input_tile_local[v_ci, v_p, v_r_a, v_r_b])
                                    T.writes(data_pack_local[v_eps, v_nu, v_ci, v_p])
                                    T.block_attr({"schedule_rule": "conv2d_nchw_winograd_data_pack"})
                                    data_pack_local[v_eps, v_nu, v_ci, v_p] = data_pack_local[v_eps, v_nu, v_ci, v_p] + input_tile_local[v_ci, v_p, v_r_a, v_r_b] * T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(3), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(1), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(0), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(3), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(1), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(0), T.float32(1), T.float32(0))))))))))))))))) * T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(3), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(1), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(0), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(3), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(1), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(0), T.float32(1), T.float32(0)))))))))))))))))
                for ax0, ax1, ax2, ax3 in T.grid(T.int64(4), T.int64(4), T.int64(1), T.int64(1)):
                    with T.block("data_pack_local"):
                        v0, v1 = T.axis.remap("SS", [ax0, ax1])
                        v2 = T.axis.spatial(T.int64(512), ci_p_fused_0 * T.int64(8) + ci_p_fused_1 // T.int64(16) + ax2)
                        v3 = T.axis.spatial(T.int64(16), ci_p_fused_1 % T.int64(16) + ax3)
                        T.reads(data_pack_local[v0, v1, v2, v3])
                        T.writes(data_pack[v0, v1, v2, v3])
                        data_pack[v0, v1, v2, v3] = data_pack_local[v0, v1, v2, v3]
        for eps_0_nu_0_co_0_p_0_fused in T.thread_binding(T.int64(1024), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 64, "pragma_unroll_explicit": 1}):
            for eps_1_nu_1_co_1_p_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for eps_2_nu_2_co_2_p_2_fused in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                    for eps_3_init, nu_3_init, co_3_init, p_3_init, eps_4_init, nu_4_init, co_4_init, p_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(2)):
                        with T.block("bgemm_init"):
                            v_eps = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(256) + eps_3_init + eps_4_init)
                            v_nu = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused % T.int64(256) // T.int64(64) + nu_3_init + nu_4_init)
                            v_co = T.axis.spatial(T.int64(512), eps_0_nu_0_co_0_p_0_fused % T.int64(64) * T.int64(8) + eps_2_nu_2_co_2_p_2_fused // T.int64(8) + co_3_init + co_4_init)
                            v_p = T.axis.spatial(T.int64(16), eps_2_nu_2_co_2_p_2_fused % T.int64(8) * T.int64(2) + p_3_init * T.int64(2) + p_4_init)
                            T.reads()
                            T.writes(bgemm_local[v_eps, v_nu, v_co, v_p])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            bgemm_local[v_eps, v_nu, v_co, v_p] = T.float32(0)
                    for ci_0 in range(T.int64(4)):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(16)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("data_pack_shared"):
                                        v0 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(256))
                                        v1 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused % T.int64(256) // T.int64(64))
                                        v2 = T.axis.spatial(T.int64(512), ci_0 * T.int64(128) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) // T.int64(16))
                                        v3 = T.axis.spatial(T.int64(16), (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) % T.int64(16))
                                        T.reads(data_pack[v0, v1, v2, v3])
                                        T.writes(data_pack_shared[v0, v1, v2, v3])
                                        data_pack_shared[v0, v1, v2, v3] = data_pack[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(4)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("p1_shared"):
                                        v0 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(256))
                                        v1 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused % T.int64(256) // T.int64(64))
                                        v2 = T.axis.spatial(T.int64(512), ci_0 * T.int64(128) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) // T.int64(8))
                                        v3 = T.axis.spatial(T.int64(512), eps_0_nu_0_co_0_p_0_fused % T.int64(64) * T.int64(8) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) % T.int64(8))
                                        T.reads(p1[v0, v1, v2, v3])
                                        T.writes(p1_shared[v0, v1, v2, v3])
                                        p1_shared[v0, v1, v2, v3] = p1[v0, v1, v2, v3]
                        for ci_1, eps_3, nu_3, co_3, p_3, ci_2, eps_4, nu_4, co_4, p_4 in T.grid(T.int64(8), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(16), T.int64(1), T.int64(1), T.int64(1), T.int64(2)):
                            with T.block("bgemm_update"):
                                v_eps = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(256) + eps_3 + eps_4)
                                v_nu = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused % T.int64(256) // T.int64(64) + nu_3 + nu_4)
                                v_co = T.axis.spatial(T.int64(512), eps_0_nu_0_co_0_p_0_fused % T.int64(64) * T.int64(8) + eps_2_nu_2_co_2_p_2_fused // T.int64(8) + co_3 + co_4)
                                v_p = T.axis.spatial(T.int64(16), eps_2_nu_2_co_2_p_2_fused % T.int64(8) * T.int64(2) + p_3 * T.int64(2) + p_4)
                                v_ci = T.axis.reduce(T.int64(512), ci_0 * T.int64(128) + ci_1 * T.int64(16) + ci_2)
                                T.reads(bgemm_local[v_eps, v_nu, v_co, v_p], data_pack_shared[v_eps, v_nu, v_ci, v_p], p1_shared[v_eps, v_nu, v_ci, v_co])
                                T.writes(bgemm_local[v_eps, v_nu, v_co, v_p])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                bgemm_local[v_eps, v_nu, v_co, v_p] = bgemm_local[v_eps, v_nu, v_co, v_p] + data_pack_shared[v_eps, v_nu, v_ci, v_p] * p1_shared[v_eps, v_nu, v_ci, v_co]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(2)):
                        with T.block("bgemm_local"):
                            v0 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(256) + ax0)
                            v1 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused % T.int64(256) // T.int64(64) + ax1)
                            v2 = T.axis.spatial(T.int64(512), eps_0_nu_0_co_0_p_0_fused % T.int64(64) * T.int64(8) + eps_2_nu_2_co_2_p_2_fused // T.int64(8) + ax2)
                            v3 = T.axis.spatial(T.int64(16), eps_2_nu_2_co_2_p_2_fused % T.int64(8) * T.int64(2) + ax3)
                            T.reads(bgemm_local[v0, v1, v2, v3])
                            T.writes(bgemm[v0, v1, v2, v3])
                            bgemm[v0, v1, v2, v3] = bgemm_local[v0, v1, v2, v3]
        for n_co_h_0_w_0_fused_0 in T.thread_binding(T.int64(256), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 64, "pragma_unroll_explicit": 1}):
            for n_co_h_0_w_0_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                    for ax2 in T.unroll(T.int64(2)):
                        for ax3 in T.unroll(T.int64(2)):
                            with T.block("inverse_init"):
                                v_co = T.axis.spatial(T.int64(512), (n_co_h_0_w_0_fused_0 * T.int64(32) + n_co_h_0_w_0_fused_1) // T.int64(16) + ax0)
                                v_p = T.axis.spatial(T.int64(16), (n_co_h_0_w_0_fused_0 * T.int64(32) + n_co_h_0_w_0_fused_1) % T.int64(16) + ax1)
                                v_vh, v_vw = T.axis.remap("SS", [ax2, ax3])
                                T.reads()
                                T.writes(inverse_local[v_co, v_p, v_vh, v_vw])
                                T.block_attr({"schedule_rule": "conv2d_nchw_winograd_inverse"})
                                inverse_local[v_co, v_p, v_vh, v_vw] = T.float32(0)
                            for ax4 in T.unroll(T.int64(4)):
                                for ax5 in T.unroll(T.int64(4)):
                                    with T.block("inverse_update"):
                                        v_co = T.axis.spatial(T.int64(512), (n_co_h_0_w_0_fused_0 * T.int64(32) + n_co_h_0_w_0_fused_1) // T.int64(16) + ax0)
                                        v_p = T.axis.spatial(T.int64(16), (n_co_h_0_w_0_fused_0 * T.int64(32) + n_co_h_0_w_0_fused_1) % T.int64(16) + ax1)
                                        v_vh, v_vw, v_r_a, v_r_b = T.axis.remap("SSRR", [ax2, ax3, ax4, ax5])
                                        T.reads(inverse_local[v_co, v_p, v_vh, v_vw], bgemm[v_r_a, v_r_b, v_co, v_p])
                                        T.writes(inverse_local[v_co, v_p, v_vh, v_vw])
                                        T.block_attr({"schedule_rule": "conv2d_nchw_winograd_inverse"})
                                        inverse_local[v_co, v_p, v_vh, v_vw] = inverse_local[v_co, v_p, v_vh, v_vw] + bgemm[v_r_a, v_r_b, v_co, v_p] * T.Select(v_r_a % T.int64(4) == T.int64(3) and v_vh % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_vh % T.int64(2) == T.int64(0), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_vh % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_vh % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_vh % T.int64(2) == T.int64(1), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_vh % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_vh % T.int64(2) == T.int64(1), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_vh % T.int64(2) == T.int64(0), T.float32(1), T.float32(0))))))))) * T.Select(v_r_b % T.int64(4) == T.int64(3) and v_vw % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_vw % T.int64(2) == T.int64(0), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_vw % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_vw % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_vw % T.int64(2) == T.int64(1), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_vw % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_vw % T.int64(2) == T.int64(1), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_vw % T.int64(2) == T.int64(0), T.float32(1), T.float32(0)))))))))
                for h_1, w_1 in T.grid(T.int64(2), T.int64(2)):
                    with T.block("conv2d_winograd"):
                        v_n = T.axis.spatial(T.int64(1), T.int64(0))
                        v_co = T.axis.spatial(T.int64(512), (n_co_h_0_w_0_fused_0 * T.int64(32) + n_co_h_0_w_0_fused_1) // T.int64(16))
                        v_h = T.axis.spatial(T.int64(7), (n_co_h_0_w_0_fused_0 * T.int64(32) + n_co_h_0_w_0_fused_1) % T.int64(16) // T.int64(4) * T.int64(2) + h_1)
                        v_w = T.axis.spatial(T.int64(7), (n_co_h_0_w_0_fused_0 * T.int64(32) + n_co_h_0_w_0_fused_1) % T.int64(4) * T.int64(2) + w_1)
                        T.where((n_co_h_0_w_0_fused_0 * T.int64(32) + n_co_h_0_w_0_fused_1) % T.int64(16) // T.int64(4) * T.int64(2) + h_1 < T.int64(7) and (n_co_h_0_w_0_fused_0 * T.int64(32) + n_co_h_0_w_0_fused_1) % T.int64(4) * T.int64(2) + w_1 < T.int64(7))
                        T.reads(inverse_local[v_co, v_n * T.int64(16) + v_h // T.int64(2) * T.int64(4) + v_w // T.int64(2), v_h % T.int64(2), v_w % T.int64(2)], p2[v_n, v_co, v_h, v_w], p3[v_n, v_co, T.int64(0), T.int64(0)], p4[v_n, v_co, T.int64(0), T.int64(0)])
                        T.writes(T_relu[v_n, v_co, v_h, v_w])
                        T_relu[v_n, v_co, v_h, v_w] = T.max((inverse_local[v_co, v_n * T.int64(16) + v_h // T.int64(2) * T.int64(4) + v_w // T.int64(2), v_h % T.int64(2), v_w % T.int64(2)] + p2[v_n, v_co, v_h, v_w]) * p3[v_n, v_co, T.int64(0), T.int64(0)] + p4[v_n, v_co, T.int64(0), T.int64(0)], T.float32(0))
b0 = sch.get_block(name="data_pack", func_name="main")
b1 = sch.get_block(name="bgemm", func_name="main")
b2 = sch.get_block(name="inverse", func_name="main")
b3 = sch.get_block(name="conv2d_winograd", func_name="main")
b4 = sch.get_block(name="T_add", func_name="main")
b5 = sch.get_block(name="T_multiply", func_name="main")
b6 = sch.get_block(name="T_add_1", func_name="main")
b7 = sch.get_block(name="T_relu", func_name="main")
b8 = sch.get_block(name="root", func_name="main")
b9, b10 = sch.get_producers(block=b2)
sch.compute_inline(block=b10)
b11, = sch.get_consumers(block=b2)
l12, l13, l14, l15 = sch.get_loops(block=b11)
l16, l17 = sch.split(loop=l14, factors=[None, 2], preserve_unit_iters=True)
l18, l19 = sch.split(loop=l15, factors=[None, 2], preserve_unit_iters=True)
sch.reorder(l16, l18, l17, l19)
sch.compute_at(block=b2, loop=l18, preserve_unit_loops=True, index=-1)
sch.set_scope(block=b2, buffer_index=0, storage_scope="local")
l20, l21, l22, l23, l24, l25, l26, l27, l28, l29 = sch.get_loops(block=b2)
sch.unroll(loop=l26)
sch.unroll(loop=l27)
sch.unroll(loop=l28)
sch.unroll(loop=l29)
b30, b31 = sch.get_producers(block=b0)
sch.compute_inline(block=b31)
b32, = sch.get_producers(block=b30)
l33, l34, l35, l36, l37, l38 = sch.get_loops(block=b0)
sch.reorder(l35, l36, l33, l34, l37, l38)
sch.unroll(loop=l33)
sch.unroll(loop=l34)
sch.unroll(loop=l37)
sch.unroll(loop=l38)
l39 = sch.fuse(l35, l36, preserve_unit_iters=True)
v40 = sch.sample_categorical(candidates=[32, 64, 128, 256, 512, 1024], probs=[0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666], decision=2)
l41, l42 = sch.split(loop=l39, factors=[None, v40], preserve_unit_iters=True)
sch.bind(loop=l41, thread_axis="blockIdx.x")
sch.bind(loop=l42, thread_axis="threadIdx.x")
b43 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b43, loop=l42, preserve_unit_loops=True, index=-1)
sch.compute_at(block=b30, loop=l42, preserve_unit_loops=True, index=-1)
sch.set_scope(block=b30, buffer_index=0, storage_scope="local")
sch.compute_inline(block=b32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l44, l45, l46, l47, l48 = sch.get_loops(block=b1)
v49, v50, v51, v52, v53 = sch.sample_perfect_tile(loop=l44, n=5, max_innermost_factor=64, decision=[4, 1, 1, 1, 1])
l54, l55, l56, l57, l58 = sch.split(loop=l44, factors=[v49, v50, v51, v52, v53], preserve_unit_iters=True)
v59, v60, v61, v62, v63 = sch.sample_perfect_tile(loop=l45, n=5, max_innermost_factor=64, decision=[4, 1, 1, 1, 1])
l64, l65, l66, l67, l68 = sch.split(loop=l45, factors=[v59, v60, v61, v62, v63], preserve_unit_iters=True)
v69, v70, v71, v72, v73 = sch.sample_perfect_tile(loop=l46, n=5, max_innermost_factor=64, decision=[64, 1, 8, 1, 1])
l74, l75, l76, l77, l78 = sch.split(loop=l46, factors=[v69, v70, v71, v72, v73], preserve_unit_iters=True)
v79, v80, v81, v82, v83 = sch.sample_perfect_tile(loop=l47, n=5, max_innermost_factor=64, decision=[1, 1, 8, 1, 2])
l84, l85, l86, l87, l88 = sch.split(loop=l47, factors=[v79, v80, v81, v82, v83], preserve_unit_iters=True)
v89, v90, v91 = sch.sample_perfect_tile(loop=l48, n=3, max_innermost_factor=64, decision=[4, 8, 16])
l92, l93, l94 = sch.split(loop=l48, factors=[v89, v90, v91], preserve_unit_iters=True)
sch.reorder(l54, l64, l74, l84, l55, l65, l75, l85, l56, l66, l76, l86, l92, l93, l57, l67, l77, l87, l94, l58, l68, l78, l88)
l95 = sch.fuse(l54, l64, l74, l84, preserve_unit_iters=True)
sch.bind(loop=l95, thread_axis="blockIdx.x")
l96 = sch.fuse(l55, l65, l75, l85, preserve_unit_iters=True)
sch.bind(loop=l96, thread_axis="vthread.x")
l97 = sch.fuse(l56, l66, l76, l86, preserve_unit_iters=True)
sch.bind(loop=l97, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b98 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b98, loop=l97, preserve_unit_loops=True, index=-1)
b99 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b99, loop=l92, preserve_unit_loops=True, index=-1)
l100, l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b99)
l108 = sch.fuse(l104, l105, l106, l107, preserve_unit_iters=True)
v109 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b99, ann_key="meta_schedule.cooperative_fetch", ann_val=v109)
b110 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b110, loop=l92, preserve_unit_loops=True, index=-1)
l111, l112, l113, l114, l115, l116, l117, l118 = sch.get_loops(block=b110)
l119 = sch.fuse(l115, l116, l117, l118, preserve_unit_iters=True)
v120 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b110, ann_key="meta_schedule.cooperative_fetch", ann_val=v120)
sch.reverse_compute_inline(block=b7)
sch.reverse_compute_inline(block=b6)
sch.reverse_compute_inline(block=b5)
sch.reverse_compute_inline(block=b4)
v121 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=2)
sch.annotate(block_or_loop=b8, ann_key="meta_schedule.unroll_explicit", ann_val=v121)
l122, l123, l124, l125, l126, l127 = sch.get_loops(block=b3)
l128 = sch.fuse(l122, l123, l124, l125, preserve_unit_iters=True)
v129 = sch.sample_categorical(candidates=[32, 64, 128, 256, 512, 1024], probs=[0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666], decision=0)
l130, l131 = sch.split(loop=l128, factors=[None, v129], preserve_unit_iters=True)
sch.bind(loop=l130, thread_axis="blockIdx.x")
sch.bind(loop=l131, thread_axis="threadIdx.x")
sch.enter_postproc()
sch.unannotate(block_or_loop=b99, ann_key="meta_schedule.cooperative_fetch")
l132, l133, l134, l135, l136 = sch.get_loops(block=b99)
l137, l138, l139 = sch.split(loop=l136, factors=[None, 64, 2], preserve_unit_iters=True)
sch.vectorize(loop=l139)
sch.bind(loop=l138, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b110, ann_key="meta_schedule.cooperative_fetch")
l140, l141, l142, l143, l144 = sch.get_loops(block=b110)
l145, l146, l147 = sch.split(loop=l144, factors=[None, 64, 4], preserve_unit_iters=True)
sch.vectorize(loop=l147)
sch.bind(loop=l146, thread_axis="threadIdx.x")
b148 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b148, ann_key="meta_schedule.unroll_explicit")
b149, b150, b151, b152, b153, b154, b155, b156, b157 = sch.get_child_blocks(b148)
l158, l159, l160, l161, l162, l163 = sch.get_loops(block=b149)
l164, l165, l166, l167, l168, l169 = sch.get_loops(block=b150)
sch.annotate(block_or_loop=l164, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l164, ann_key="pragma_unroll_explicit", ann_val=1)
l170, l171, l172, l173, l174, l175 = sch.get_loops(block=b151)
l176, l177, l178, l179, l180, l181, l182 = sch.get_loops(block=b152)
l183, l184, l185, l186, l187, l188, l189 = sch.get_loops(block=b153)
l190, l191, l192, l193, l194, l195, l196, l197, l198, l199, l200, l201, l202, l203 = sch.get_loops(block=b154)
sch.annotate(block_or_loop=l190, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l190, ann_key="pragma_unroll_explicit", ann_val=1)
l204, l205, l206, l207, l208, l209, l210 = sch.get_loops(block=b155)
l211, l212, l213, l214, l215, l216, l217, l218 = sch.get_loops(block=b156)
sch.annotate(block_or_loop=l211, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l211, ann_key="pragma_unroll_explicit", ann_val=1)
l219, l220, l221, l222 = sch.get_loops(block=b157)
b223 = sch.get_block(name="data_pack", func_name="main")
l224, l225, l226, l227, l228, l229 = sch.get_loops(block=b223)
b230 = sch.decompose_reduction(block=b223, loop=l228)
b231 = sch.get_block(name="bgemm", func_name="main")
l232, l233, l234, l235, l236, l237, l238, l239, l240, l241, l242, l243, l244, l245 = sch.get_loops(block=b231)
b246 = sch.decompose_reduction(block=b231, loop=l235)
b247 = sch.get_block(name="inverse", func_name="main")
l248, l249, l250, l251, l252, l253, l254, l255 = sch.get_loops(block=b247)
b256 = sch.decompose_reduction(block=b247, loop=l254)
2023-11-11 09:06:44 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #775: GFLOPs: 3488.5879. Time: 40.7564 us. Best GFLOPs: 5498.5138
2023-11-11 09:06:44 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #776: GFLOPs: 2102.9286. Time: 67.6116 us. Best GFLOPs: 5498.5138
2023-11-11 09:06:44 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #777: GFLOPs: 2103.3923. Time: 67.5967 us. Best GFLOPs: 5498.5138
2023-11-11 09:06:44 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #778: GFLOPs: 2103.3151. Time: 67.5992 us. Best GFLOPs: 5498.5138
2023-11-11 09:06:44 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #779: GFLOPs: 5010.1808. Time: 28.3787 us. Best GFLOPs: 5498.5138
2023-11-11 09:06:44 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #780: GFLOPs: 4940.2117. Time: 28.7806 us. Best GFLOPs: 5498.5138
2023-11-11 09:06:44 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #781: GFLOPs: 4717.5390. Time: 30.1391 us. Best GFLOPs: 5498.5138
2023-11-11 09:06:44 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #782: GFLOPs: 4702.7147. Time: 30.2341 us. Best GFLOPs: 5498.5138
2023-11-11 09:06:44 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #783: GFLOPs: 4996.8208. Time: 28.4546 us. Best GFLOPs: 5498.5138
2023-11-11 09:06:44 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #784: GFLOPs: 4939.8818. Time: 28.7826 us. Best GFLOPs: 5498.5138
2023-11-11 09:06:44 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #785: GFLOPs: 1907.9466. Time: 74.5212 us. Best GFLOPs: 5498.5138
2023-11-11 09:06:44 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #786: GFLOPs: 4715.9991. Time: 30.1489 us. Best GFLOPs: 5498.5138
2023-11-11 09:06:44 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #787: GFLOPs: 4450.6567. Time: 31.9464 us. Best GFLOPs: 5498.5138
2023-11-11 09:06:44 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #788: GFLOPs: 5175.2087. Time: 27.4738 us. Best GFLOPs: 5498.5138
2023-11-11 09:06:44 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #789: GFLOPs: 5256.9360. Time: 27.0466 us. Best GFLOPs: 5498.5138
2023-11-11 09:06:44 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #790: GFLOPs: 5261.5427. Time: 27.0229 us. Best GFLOPs: 5498.5138
2023-11-11 09:06:44 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #791: GFLOPs: 5389.9065. Time: 26.3794 us. Best GFLOPs: 5498.5138
2023-11-11 09:06:44 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #792: GFLOPs: 5381.8402. Time: 26.4189 us. Best GFLOPs: 5498.5138
2023-11-11 09:06:44 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #793: GFLOPs: 5095.0862. Time: 27.9058 us. Best GFLOPs: 5498.5138
2023-11-11 09:06:44 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #794: GFLOPs: 4946.6224. Time: 28.7433 us. Best GFLOPs: 5498.5138
2023-11-11 09:06:44 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #795: GFLOPs: 5281.3064. Time: 26.9218 us. Best GFLOPs: 5498.5138
2023-11-11 09:06:44 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #796: GFLOPs: 5089.0770. Time: 27.9387 us. Best GFLOPs: 5498.5138
2023-11-11 09:06:44 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #797: GFLOPs: 5154.1277. Time: 27.5861 us. Best GFLOPs: 5498.5138
2023-11-11 09:06:44 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #798: GFLOPs: 5115.8548. Time: 27.7925 us. Best GFLOPs: 5498.5138
2023-11-11 09:06:44 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #799: GFLOPs: 4874.7871. Time: 29.1669 us. Best GFLOPs: 5498.5138
2023-11-11 09:06:44 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #800: GFLOPs: 5205.2802. Time: 27.3150 us. Best GFLOPs: 5498.5138
2023-11-11 09:06:44 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #801: GFLOPs: 5251.5075. Time: 27.0746 us. Best GFLOPs: 5498.5138
2023-11-11 09:06:44 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #802: GFLOPs: 5176.5682. Time: 27.4665 us. Best GFLOPs: 5498.5138
2023-11-11 09:06:44 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #803: GFLOPs: 5018.3972. Time: 28.3322 us. Best GFLOPs: 5498.5138
2023-11-11 09:06:44 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #804: GFLOPs: 5114.5366. Time: 27.7997 us. Best GFLOPs: 5498.5138
2023-11-11 09:06:44 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #805: GFLOPs: 5037.0036. Time: 28.2276 us. Best GFLOPs: 5498.5138
2023-11-11 09:06:44 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #806: GFLOPs: 5101.6623. Time: 27.8698 us. Best GFLOPs: 5498.5138
2023-11-11 09:06:44 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #807: GFLOPs: 5412.4911. Time: 26.2693 us. Best GFLOPs: 5498.5138
2023-11-11 09:06:44 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #808: GFLOPs: 5158.4327. Time: 27.5631 us. Best GFLOPs: 5498.5138
2023-11-11 09:06:44 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #809: GFLOPs: 4953.2159. Time: 28.7051 us. Best GFLOPs: 5498.5138
2023-11-11 09:06:44 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #810: GFLOPs: 5212.0010. Time: 27.2798 us. Best GFLOPs: 5498.5138
2023-11-11 09:06:44 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #811: GFLOPs: 5128.8575. Time: 27.7220 us. Best GFLOPs: 5498.5138
2023-11-11 09:06:44 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #812: GFLOPs: 5113.5930. Time: 27.8048 us. Best GFLOPs: 5498.5138
2023-11-11 09:06:44 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #813: GFLOPs: 5176.9038. Time: 27.4648 us. Best GFLOPs: 5498.5138
2023-11-11 09:06:44 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #814: GFLOPs: 5382.6243. Time: 26.4151 us. Best GFLOPs: 5498.5138
2023-11-11 09:06:44 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #815: GFLOPs: 5274.6663. Time: 26.9557 us. Best GFLOPs: 5498.5138
2023-11-11 09:06:44 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #816: GFLOPs: 4901.3433. Time: 29.0089 us. Best GFLOPs: 5498.5138
2023-11-11 09:06:44 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #817: GFLOPs: 5115.5070. Time: 27.7944 us. Best GFLOPs: 5498.5138
2023-11-11 09:06:44 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #818: GFLOPs: 5115.1635. Time: 27.7963 us. Best GFLOPs: 5498.5138
2023-11-11 09:06:44 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #819: GFLOPs: 5114.9761. Time: 27.7973 us. Best GFLOPs: 5498.5138
2023-11-11 09:06:44 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #820: GFLOPs: 5264.8394. Time: 27.0060 us. Best GFLOPs: 5498.5138
2023-11-11 09:06:44 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #821: GFLOPs: 5346.4340. Time: 26.5939 us. Best GFLOPs: 5498.5138
2023-11-11 09:06:44 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #822: GFLOPs: 5148.2679. Time: 27.6175 us. Best GFLOPs: 5498.5138
2023-11-11 09:06:44 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #823: GFLOPs: 5124.2504. Time: 27.7470 us. Best GFLOPs: 5498.5138
2023-11-11 09:06:44 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #824: GFLOPs: 4911.6150. Time: 28.9482 us. Best GFLOPs: 5498.5138
2023-11-11 09:06:44 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #825: GFLOPs: 5125.4376. Time: 27.7405 us. Best GFLOPs: 5498.5138
2023-11-11 09:06:44 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #826: GFLOPs: 5190.6285. Time: 27.3921 us. Best GFLOPs: 5498.5138
2023-11-11 09:06:44 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #827: GFLOPs: 5297.4566. Time: 26.8397 us. Best GFLOPs: 5498.5138
2023-11-11 09:06:44 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #828: GFLOPs: 4951.1161. Time: 28.7172 us. Best GFLOPs: 5498.5138
2023-11-11 09:06:44 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #829: GFLOPs: 5116.6694. Time: 27.7881 us. Best GFLOPs: 5498.5138
2023-11-11 09:06:44 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #830: GFLOPs: 44.2598. Time: 3212.4474 us. Best GFLOPs: 5498.5138
2023-11-11 09:06:44 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #831: GFLOPs: 119.9977. Time: 1184.8764 us. Best GFLOPs: 5498.5138
2023-11-11 09:06:44 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #832: GFLOPs: 1543.8147. Time: 92.0981 us. Best GFLOPs: 5498.5138
2023-11-11 09:36:03 [INFO] [evolutionary_search.cc:713] Generating candidates......
2023-11-11 09:36:06 [INFO] [evolutionary_search.cc:715] Picked top 102 candidate(s) from database
2023-11-11 09:36:13 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310fb03b28)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310eec3b08)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310dc5caf8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310f7cc2d8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x563110643e78)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310dc32028)]: 405 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310dc5cfb8)]: 0 failure(s)
2023-11-11 09:36:19 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310fb03b28)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310eec3b08)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310dc5caf8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310f7cc2d8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x563110643e78)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310dc32028)]: 807 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310dc5cfb8)]: 0 failure(s)
2023-11-11 09:36:25 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310fb03b28)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310eec3b08)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310dc5caf8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310f7cc2d8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x563110643e78)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310dc32028)]: 1207 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310dc5cfb8)]: 0 failure(s)
2023-11-11 09:36:31 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310fb03b28)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310eec3b08)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310dc5caf8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310f7cc2d8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x563110643e78)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310dc32028)]: 1605 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310dc5cfb8)]: 0 failure(s)
2023-11-11 09:36:37 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310fb03b28)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310eec3b08)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310dc5caf8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310f7cc2d8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x563110643e78)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310dc32028)]: 2004 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310dc5cfb8)]: 0 failure(s)
2023-11-11 09:36:43 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310fb03b28)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310eec3b08)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310dc5caf8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310f7cc2d8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x563110643e78)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310dc32028)]: 2408 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310dc5cfb8)]: 0 failure(s)
2023-11-11 09:36:43 [INFO] [evolutionary_search.cc:723] Sampled 52 candidate(s)
2023-11-11 09:36:59 [INFO] [evolutionary_search.cc:621] Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310fb03b28)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310eec3b08)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310dc5caf8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310f7cc2d8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x563110643e78)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310dc32028)]: 110 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310dc5cfb8)]: 0 failure(s)
2023-11-11 09:37:18 [INFO] [evolutionary_search.cc:621] Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310fb03b28)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310eec3b08)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310dc5caf8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310f7cc2d8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x563110643e78)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310dc32028)]: 95 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310dc5cfb8)]: 0 failure(s)
2023-11-11 09:37:37 [INFO] [evolutionary_search.cc:621] Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310fb03b28)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310eec3b08)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310dc5caf8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310f7cc2d8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x563110643e78)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310dc32028)]: 115 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310dc5cfb8)]: 0 failure(s)
2023-11-11 09:37:56 [INFO] [evolutionary_search.cc:621] Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310fb03b28)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310eec3b08)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310dc5caf8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310f7cc2d8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x563110643e78)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310dc32028)]: 101 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310dc5cfb8)]: 0 failure(s)
2023-11-11 09:38:01 [INFO] [evolutionary_search.cc:649] Scores of the best 64 candidates:
[1 : 16]:	1.4649  1.4307  1.3370  0.9977  0.9923  0.9851  0.9821  0.9820  0.9789  0.9783  0.9782  0.9699  0.9696  0.9690  0.9672  0.9662
[17 : 32]:	0.9656  0.9650  0.9640  0.9640  0.9630  0.9628  0.9626  0.9625  0.9622  0.9620  0.9619  0.9617  0.9614  0.9614  0.9610  0.9610
[33 : 48]:	0.9609  0.9606  0.9605  0.9604  0.9604  0.9601  0.9601  0.9599  0.9598  0.9597  0.9595  0.9592  0.9591  0.9590  0.9586  0.9586
[49 : 64]:	0.9582  0.9577  0.9577  0.9575  0.9574  0.9573  0.9573  0.9573  0.9572  0.9571  0.9571  0.9570  0.9569  0.9566  0.9565  0.9564
2023-11-11 09:38:02 [INFO] [evolutionary_search.cc:727] Got 64 candidate(s) with evolutionary search
2023-11-11 09:38:02 [INFO] [evolutionary_search.cc:730] Sending 64 candidates(s) for measurement
2023-11-11 09:38:40 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #833: GFLOPs: 3704.5312. Time: 38.3807 us. Best GFLOPs: 5498.5138
2023-11-11 09:38:40 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #834: GFLOPs: 3738.7336. Time: 38.0296 us. Best GFLOPs: 5498.5138
2023-11-11 09:38:40 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #835: GFLOPs: 3719.2291. Time: 38.2290 us. Best GFLOPs: 5498.5138
2023-11-11 09:38:40 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #836: GFLOPs: 5315.0418. Time: 26.7509 us. Best GFLOPs: 5498.5138
2023-11-11 09:38:40 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #837: GFLOPs: 4713.2042. Time: 30.1668 us. Best GFLOPs: 5498.5138
2023-11-11 09:38:40 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #838: GFLOPs: 5203.3065. Time: 27.3254 us. Best GFLOPs: 5498.5138
2023-11-11 09:38:40 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #839: GFLOPs: 4995.0912. Time: 28.4644 us. Best GFLOPs: 5498.5138
2023-11-11 09:38:40 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #840: GFLOPs: 4724.9392. Time: 30.0919 us. Best GFLOPs: 5498.5138
2023-11-11 09:38:40 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #841: GFLOPs: 5185.8422. Time: 27.4174 us. Best GFLOPs: 5498.5138
2023-11-11 09:38:40 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #842: GFLOPs: 5415.7993. Time: 26.2533 us. Best GFLOPs: 5498.5138
2023-11-11 09:38:40 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #843: GFLOPs: 5421.0493. Time: 26.2278 us. Best GFLOPs: 5498.5138
2023-11-11 09:38:40 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #844: GFLOPs: 5413.9694. Time: 26.2621 us. Best GFLOPs: 5498.5138
2023-11-11 09:38:40 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #845: GFLOPs: 5301.0424. Time: 26.8216 us. Best GFLOPs: 5498.5138
2023-11-11 09:38:40 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #846: GFLOPs: 5413.7860. Time: 26.2630 us. Best GFLOPs: 5498.5138
2023-11-11 09:38:40 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #847: GFLOPs: 5298.9605. Time: 26.8321 us. Best GFLOPs: 5498.5138
2023-11-11 09:38:40 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #848: GFLOPs: 5106.5100. Time: 27.8434 us. Best GFLOPs: 5498.5138
2023-11-11 09:38:40 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #849: GFLOPs: 5258.0538. Time: 27.0409 us. Best GFLOPs: 5498.5138
2023-11-11 09:38:40 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #850: GFLOPs: 5414.0460. Time: 26.2618 us. Best GFLOPs: 5498.5138
2023-11-11 09:38:40 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #851: GFLOPs: 5188.7325. Time: 27.4021 us. Best GFLOPs: 5498.5138
2023-11-11 09:38:40 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #852: GFLOPs: 5380.4143. Time: 26.4259 us. Best GFLOPs: 5498.5138
2023-11-11 09:38:40 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #853: GFLOPs: 5249.0739. Time: 27.0871 us. Best GFLOPs: 5498.5138
2023-11-11 09:38:40 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #854: GFLOPs: 5218.5599. Time: 27.2455 us. Best GFLOPs: 5498.5138
2023-11-11 09:38:40 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #855: GFLOPs: 5224.6517. Time: 27.2138 us. Best GFLOPs: 5498.5138
2023-11-11 09:38:40 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #856: GFLOPs: 5425.7550. Time: 26.2051 us. Best GFLOPs: 5498.5138
2023-11-11 09:38:40 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #857: GFLOPs: 5211.6198. Time: 27.2818 us. Best GFLOPs: 5498.5138
2023-11-11 09:38:40 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #858: GFLOPs: 5233.4501. Time: 27.1680 us. Best GFLOPs: 5498.5138
2023-11-11 09:38:40 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #859: GFLOPs: 5430.5052. Time: 26.1822 us. Best GFLOPs: 5498.5138
2023-11-11 09:38:40 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #860: GFLOPs: 5373.6331. Time: 26.4593 us. Best GFLOPs: 5498.5138
2023-11-11 09:38:40 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #861: GFLOPs: 5430.1786. Time: 26.1837 us. Best GFLOPs: 5498.5138
2023-11-11 09:38:40 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #862: GFLOPs: 5252.6175. Time: 27.0689 us. Best GFLOPs: 5498.5138
2023-11-11 09:38:40 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #863: GFLOPs: 5115.9216. Time: 27.7921 us. Best GFLOPs: 5498.5138
2023-11-11 09:38:40 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #864: GFLOPs: 5253.5876. Time: 27.0639 us. Best GFLOPs: 5498.5138
2023-11-11 09:38:40 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #865: GFLOPs: 5430.6139. Time: 26.1816 us. Best GFLOPs: 5498.5138
2023-11-11 09:38:40 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #866: GFLOPs: 5338.6798. Time: 26.6325 us. Best GFLOPs: 5498.5138
2023-11-11 09:38:40 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #867: GFLOPs: 5276.0007. Time: 26.9489 us. Best GFLOPs: 5498.5138
2023-11-11 09:38:40 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #868: GFLOPs: 5289.5151. Time: 26.8800 us. Best GFLOPs: 5498.5138
2023-11-11 09:38:40 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #869: GFLOPs: 5313.7638. Time: 26.7574 us. Best GFLOPs: 5498.5138
2023-11-11 09:38:40 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #870: GFLOPs: 5167.5070. Time: 27.5147 us. Best GFLOPs: 5498.5138
2023-11-11 09:38:40 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #871: GFLOPs: 5171.2725. Time: 27.4947 us. Best GFLOPs: 5498.5138
2023-11-11 09:38:40 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #872: GFLOPs: 5315.0305. Time: 26.7510 us. Best GFLOPs: 5498.5138
2023-11-11 09:38:40 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #873: GFLOPs: 4963.6344. Time: 28.6448 us. Best GFLOPs: 5498.5138
2023-11-11 09:38:40 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #874: GFLOPs: 5136.6913. Time: 27.6798 us. Best GFLOPs: 5498.5138
2023-11-11 09:38:40 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #875: GFLOPs: 5322.6434. Time: 26.7127 us. Best GFLOPs: 5498.5138
2023-11-11 09:38:40 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #876: GFLOPs: 5220.8222. Time: 27.2337 us. Best GFLOPs: 5498.5138
2023-11-11 09:38:40 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #877: GFLOPs: 5115.3017. Time: 27.7955 us. Best GFLOPs: 5498.5138
2023-11-11 09:38:40 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #878: GFLOPs: 5285.6016. Time: 26.8999 us. Best GFLOPs: 5498.5138
2023-11-11 09:38:40 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #879: GFLOPs: 5213.8393. Time: 27.2702 us. Best GFLOPs: 5498.5138
2023-11-11 09:38:40 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #880: GFLOPs: 5360.4273. Time: 26.5245 us. Best GFLOPs: 5498.5138
2023-11-11 09:38:40 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #881: GFLOPs: 5241.0599. Time: 27.1286 us. Best GFLOPs: 5498.5138
2023-11-11 09:38:40 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #882: GFLOPs: 5184.1821. Time: 27.4262 us. Best GFLOPs: 5498.5138
2023-11-11 09:38:40 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #883: GFLOPs: 5180.0898. Time: 27.4479 us. Best GFLOPs: 5498.5138
2023-11-11 09:38:40 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #884: GFLOPs: 5166.8175. Time: 27.5184 us. Best GFLOPs: 5498.5138
2023-11-11 09:38:40 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #885: GFLOPs: 5169.6805. Time: 27.5031 us. Best GFLOPs: 5498.5138
2023-11-11 09:38:40 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #886: GFLOPs: 5304.0028. Time: 26.8066 us. Best GFLOPs: 5498.5138
2023-11-11 09:38:40 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #887: GFLOPs: 5296.2630. Time: 26.8458 us. Best GFLOPs: 5498.5138
2023-11-11 09:38:40 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #888: GFLOPs: 5352.2746. Time: 26.5649 us. Best GFLOPs: 5498.5138
2023-11-11 09:38:40 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #889: GFLOPs: 4988.3211. Time: 28.5031 us. Best GFLOPs: 5498.5138
2023-11-11 09:38:40 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #890: GFLOPs: 5213.8739. Time: 27.2700 us. Best GFLOPs: 5498.5138
2023-11-11 09:38:40 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #891: GFLOPs: 5227.6540. Time: 27.1981 us. Best GFLOPs: 5498.5138
2023-11-11 09:38:40 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #892: GFLOPs: 5171.0272. Time: 27.4960 us. Best GFLOPs: 5498.5138
2023-11-11 09:38:40 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #893: GFLOPs: 5177.9710. Time: 27.4591 us. Best GFLOPs: 5498.5138
2023-11-11 09:38:40 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #894: GFLOPs: 120.8872. Time: 1176.1575 us. Best GFLOPs: 5498.5138
2023-11-11 09:38:40 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #895: GFLOPs: 1240.9721. Time: 114.5734 us. Best GFLOPs: 5498.5138
2023-11-11 09:38:40 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #896: GFLOPs: 812.8080. Time: 174.9274 us. Best GFLOPs: 5498.5138
2023-11-11 10:12:50 [INFO] [evolutionary_search.cc:713] Generating candidates......
2023-11-11 10:12:53 [INFO] [evolutionary_search.cc:715] Picked top 102 candidate(s) from database
2023-11-11 10:12:59 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310fb03b28)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310eec3b08)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310dc5caf8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310f7cc2d8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x563110643e78)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310dc32028)]: 399 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310dc5cfb8)]: 0 failure(s)
2023-11-11 10:13:05 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310fb03b28)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310eec3b08)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310dc5caf8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310f7cc2d8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x563110643e78)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310dc32028)]: 797 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310dc5cfb8)]: 0 failure(s)
2023-11-11 10:13:12 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310fb03b28)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310eec3b08)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310dc5caf8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310f7cc2d8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x563110643e78)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310dc32028)]: 1192 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310dc5cfb8)]: 0 failure(s)
2023-11-11 10:13:17 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310fb03b28)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310eec3b08)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310dc5caf8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310f7cc2d8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x563110643e78)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310dc32028)]: 1587 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310dc5cfb8)]: 0 failure(s)
2023-11-11 10:13:17 [INFO] [evolutionary_search.cc:723] Sampled 53 candidate(s)
2023-11-11 10:13:33 [INFO] [evolutionary_search.cc:621] Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310fb03b28)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310eec3b08)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310dc5caf8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310f7cc2d8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x563110643e78)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310dc32028)]: 96 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310dc5cfb8)]: 0 failure(s)
2023-11-11 10:13:52 [INFO] [evolutionary_search.cc:621] Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310fb03b28)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310eec3b08)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310dc5caf8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310f7cc2d8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x563110643e78)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310dc32028)]: 113 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310dc5cfb8)]: 0 failure(s)
2023-11-11 10:14:11 [INFO] [evolutionary_search.cc:621] Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310fb03b28)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310eec3b08)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310dc5caf8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310f7cc2d8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x563110643e78)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310dc32028)]: 107 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310dc5cfb8)]: 0 failure(s)
2023-11-11 10:14:30 [INFO] [evolutionary_search.cc:621] Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310fb03b28)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310eec3b08)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310dc5caf8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310f7cc2d8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x563110643e78)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310dc32028)]: 115 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310dc5cfb8)]: 0 failure(s)
2023-11-11 10:14:35 [INFO] [evolutionary_search.cc:649] Scores of the best 64 candidates:
[1 : 16]:	1.0001  0.9956  0.9915  0.9909  0.9839  0.9786  0.9782  0.9714  0.9697  0.9685  0.9680  0.9679  0.9672  0.9670  0.9661  0.9654
[17 : 32]:	0.9652  0.9651  0.9642  0.9641  0.9640  0.9633  0.9627  0.9623  0.9622  0.9620  0.9618  0.9614  0.9612  0.9612  0.9612  0.9611
[33 : 48]:	0.9609  0.9609  0.9605  0.9603  0.9601  0.9599  0.9599  0.9598  0.9594  0.9591  0.9589  0.9586  0.9585  0.9583  0.9577  0.9575
[49 : 64]:	0.9573  0.9572  0.9571  0.9570  0.9568  0.9566  0.9563  0.9559  0.9559  0.9558  0.9558  0.9557  0.9556  0.9555  0.9555  0.9555
2023-11-11 10:14:35 [INFO] [evolutionary_search.cc:727] Got 64 candidate(s) with evolutionary search
2023-11-11 10:14:35 [INFO] [evolutionary_search.cc:730] Sending 64 candidates(s) for measurement
2023-11-11 10:15:13 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #897: GFLOPs: 5301.0424. Time: 26.8216 us. Best GFLOPs: 5498.5138
2023-11-11 10:15:13 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #898: GFLOPs: 5377.6877. Time: 26.4393 us. Best GFLOPs: 5498.5138
2023-11-11 10:15:13 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #899: GFLOPs: 5150.2200. Time: 27.6071 us. Best GFLOPs: 5498.5138
2023-11-11 10:15:13 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #900: GFLOPs: 5440.3100. Time: 26.1350 us. Best GFLOPs: 5498.5138
2023-11-11 10:15:13 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #901: GFLOPs: 5190.2281. Time: 27.3942 us. Best GFLOPs: 5498.5138
2023-11-11 10:15:13 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #902: GFLOPs: 5117.3882. Time: 27.7842 us. Best GFLOPs: 5498.5138
2023-11-11 10:15:13 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #903: GFLOPs: 5316.7905. Time: 26.7421 us. Best GFLOPs: 5498.5138
2023-11-11 10:15:13 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #904: GFLOPs: 5208.5708. Time: 27.2978 us. Best GFLOPs: 5498.5138
2023-11-11 10:15:13 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #905: GFLOPs: 5448.3838. Time: 26.0963 us. Best GFLOPs: 5498.5138
2023-11-11 10:15:13 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #906: GFLOPs: 5177.4578. Time: 27.4618 us. Best GFLOPs: 5498.5138
2023-11-11 10:15:13 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #907: GFLOPs: 5272.9620. Time: 26.9644 us. Best GFLOPs: 5498.5138
2023-11-11 10:15:13 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #908: GFLOPs: 5185.3972. Time: 27.4198 us. Best GFLOPs: 5498.5138
2023-11-11 10:15:13 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #909: GFLOPs: 5229.4986. Time: 27.1885 us. Best GFLOPs: 5498.5138
2023-11-11 10:15:13 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #910: GFLOPs: 5305.5824. Time: 26.7986 us. Best GFLOPs: 5498.5138
2023-11-11 10:15:13 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #911: GFLOPs: 5228.9401. Time: 27.1914 us. Best GFLOPs: 5498.5138
2023-11-11 10:15:13 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #912: GFLOPs: 5184.7207. Time: 27.4233 us. Best GFLOPs: 5498.5138
2023-11-11 10:15:13 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #913: GFLOPs: 5252.9536. Time: 27.0671 us. Best GFLOPs: 5498.5138
2023-11-11 10:15:13 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #914: GFLOPs: 5445.9373. Time: 26.1080 us. Best GFLOPs: 5498.5138
2023-11-11 10:15:13 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #915: GFLOPs: 5333.3089. Time: 26.6593 us. Best GFLOPs: 5498.5138
2023-11-11 10:15:13 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #916: GFLOPs: 5304.3755. Time: 26.8047 us. Best GFLOPs: 5498.5138
2023-11-11 10:15:13 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #917: GFLOPs: 5435.9908. Time: 26.1557 us. Best GFLOPs: 5498.5138
2023-11-11 10:15:13 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #918: GFLOPs: 5179.9515. Time: 27.4486 us. Best GFLOPs: 5498.5138
2023-11-11 10:15:13 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #919: GFLOPs: 5377.9796. Time: 26.4379 us. Best GFLOPs: 5498.5138
2023-11-11 10:15:13 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #920: GFLOPs: 5167.6796. Time: 27.5138 us. Best GFLOPs: 5498.5138
2023-11-11 10:15:13 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #921: GFLOPs: 5107.4258. Time: 27.8384 us. Best GFLOPs: 5498.5138
2023-11-11 10:15:13 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #922: GFLOPs: 5179.7434. Time: 27.4497 us. Best GFLOPs: 5498.5138
2023-11-11 10:15:13 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #923: GFLOPs: 5169.1987. Time: 27.5057 us. Best GFLOPs: 5498.5138
2023-11-11 10:15:13 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #924: GFLOPs: 5441.6337. Time: 26.1286 us. Best GFLOPs: 5498.5138
2023-11-11 10:15:13 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #925: GFLOPs: 5398.3890. Time: 26.3379 us. Best GFLOPs: 5498.5138
2023-11-11 10:15:13 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #926: GFLOPs: 5272.0754. Time: 26.9690 us. Best GFLOPs: 5498.5138
2023-11-11 10:15:13 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #927: GFLOPs: 5318.3213. Time: 26.7345 us. Best GFLOPs: 5498.5138
2023-11-11 10:15:13 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #928: GFLOPs: 5161.2389. Time: 27.5481 us. Best GFLOPs: 5498.5138
2023-11-11 10:15:13 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #929: GFLOPs: 5440.2081. Time: 26.1355 us. Best GFLOPs: 5498.5138
2023-11-11 10:15:13 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #930: GFLOPs: 5300.1570. Time: 26.8261 us. Best GFLOPs: 5498.5138
2023-11-11 10:15:13 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #931: GFLOPs: 5145.8393. Time: 27.6306 us. Best GFLOPs: 5498.5138
2023-11-11 10:15:13 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #932: GFLOPs: 5174.2405. Time: 27.4789 us. Best GFLOPs: 5498.5138
2023-11-11 10:15:13 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #933: GFLOPs: 5220.4247. Time: 27.2358 us. Best GFLOPs: 5498.5138
2023-11-11 10:15:13 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #934: GFLOPs: 5445.5308. Time: 26.1099 us. Best GFLOPs: 5498.5138
2023-11-11 10:15:13 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #935: GFLOPs: 5144.2520. Time: 27.6391 us. Best GFLOPs: 5498.5138
2023-11-11 10:15:13 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #936: GFLOPs: 5197.3344. Time: 27.3568 us. Best GFLOPs: 5498.5138
2023-11-11 10:15:13 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #937: GFLOPs: 5325.7536. Time: 26.6971 us. Best GFLOPs: 5498.5138
2023-11-11 10:15:13 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #938: GFLOPs: 5199.2537. Time: 27.3467 us. Best GFLOPs: 5498.5138
2023-11-11 10:15:13 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #939: GFLOPs: 5128.1206. Time: 27.7260 us. Best GFLOPs: 5498.5138
2023-11-11 10:15:13 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #940: GFLOPs: 5173.4106. Time: 27.4833 us. Best GFLOPs: 5498.5138
2023-11-11 10:15:13 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #941: GFLOPs: 5335.8406. Time: 26.6467 us. Best GFLOPs: 5498.5138
2023-11-11 10:15:13 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #942: GFLOPs: 5346.6398. Time: 26.5929 us. Best GFLOPs: 5498.5138
2023-11-11 10:15:13 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #943: GFLOPs: 5095.9362. Time: 27.9011 us. Best GFLOPs: 5498.5138
2023-11-11 10:15:13 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #944: GFLOPs: 5240.0957. Time: 27.1336 us. Best GFLOPs: 5498.5138
2023-11-11 10:15:13 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #945: GFLOPs: 5176.9102. Time: 27.4647 us. Best GFLOPs: 5498.5138
2023-11-11 10:15:13 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #946: GFLOPs: 5311.0193. Time: 26.7712 us. Best GFLOPs: 5498.5138
2023-11-11 10:15:13 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #947: GFLOPs: 5261.9495. Time: 27.0209 us. Best GFLOPs: 5498.5138
2023-11-11 10:15:13 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #948: GFLOPs: 5253.4816. Time: 27.0644 us. Best GFLOPs: 5498.5138
2023-11-11 10:15:13 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #949: GFLOPs: 5281.7038. Time: 26.9198 us. Best GFLOPs: 5498.5138
2023-11-11 10:15:13 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #950: GFLOPs: 5334.3156. Time: 26.6543 us. Best GFLOPs: 5498.5138
2023-11-11 10:15:13 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #951: GFLOPs: 5095.7862. Time: 27.9020 us. Best GFLOPs: 5498.5138
2023-11-11 10:15:13 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #952: GFLOPs: 5315.1710. Time: 26.7503 us. Best GFLOPs: 5498.5138
2023-11-11 10:15:13 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #953: GFLOPs: 5153.2016. Time: 27.5911 us. Best GFLOPs: 5498.5138
2023-11-11 10:15:13 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #954: GFLOPs: 5383.2193. Time: 26.4122 us. Best GFLOPs: 5498.5138
2023-11-11 10:15:13 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #955: GFLOPs: 5337.0115. Time: 26.6408 us. Best GFLOPs: 5498.5138
2023-11-11 10:15:13 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #956: GFLOPs: 5189.7062. Time: 27.3970 us. Best GFLOPs: 5498.5138
2023-11-11 10:15:13 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #957: GFLOPs: 5187.0202. Time: 27.4112 us. Best GFLOPs: 5498.5138
2023-11-11 10:15:13 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #958: GFLOPs: 397.7701. Time: 357.4487 us. Best GFLOPs: 5498.5138
2023-11-11 10:15:13 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #959: GFLOPs: 1687.5404. Time: 84.2542 us. Best GFLOPs: 5498.5138
2023-11-11 10:15:13 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #960: GFLOPs: 1386.4747. Time: 102.5496 us. Best GFLOPs: 5498.5138
2023-11-11 10:40:57 [INFO] [evolutionary_search.cc:713] Generating candidates......
2023-11-11 10:41:00 [INFO] [evolutionary_search.cc:715] Picked top 102 candidate(s) from database
2023-11-11 10:41:06 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310fb03b28)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310eec3b08)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310dc5caf8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310f7cc2d8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x563110643e78)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310dc32028)]: 395 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310dc5cfb8)]: 0 failure(s)
2023-11-11 10:41:12 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310fb03b28)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310eec3b08)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310dc5caf8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310f7cc2d8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x563110643e78)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310dc32028)]: 797 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310dc5cfb8)]: 0 failure(s)
2023-11-11 10:41:18 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310fb03b28)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310eec3b08)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310dc5caf8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310f7cc2d8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x563110643e78)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310dc32028)]: 1199 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310dc5cfb8)]: 0 failure(s)
2023-11-11 10:41:25 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310fb03b28)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310eec3b08)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310dc5caf8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310f7cc2d8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x563110643e78)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310dc32028)]: 1601 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310dc5cfb8)]: 0 failure(s)
2023-11-11 10:41:31 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310fb03b28)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310eec3b08)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310dc5caf8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310f7cc2d8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x563110643e78)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310dc32028)]: 1999 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310dc5cfb8)]: 0 failure(s)
2023-11-11 10:41:31 [INFO] [evolutionary_search.cc:723] Sampled 51 candidate(s)
2023-11-11 10:41:46 [INFO] [evolutionary_search.cc:621] Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310fb03b28)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310eec3b08)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310dc5caf8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310f7cc2d8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x563110643e78)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310dc32028)]: 71 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310dc5cfb8)]: 0 failure(s)
2023-11-11 10:42:05 [INFO] [evolutionary_search.cc:621] Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310fb03b28)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310eec3b08)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310dc5caf8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310f7cc2d8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x563110643e78)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310dc32028)]: 95 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310dc5cfb8)]: 0 failure(s)
2023-11-11 10:42:23 [INFO] [evolutionary_search.cc:621] Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310fb03b28)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310eec3b08)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310dc5caf8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310f7cc2d8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x563110643e78)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310dc32028)]: 101 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310dc5cfb8)]: 0 failure(s)
2023-11-11 10:42:43 [INFO] [evolutionary_search.cc:621] Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310fb03b28)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310eec3b08)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310dc5caf8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310f7cc2d8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x563110643e78)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310dc32028)]: 124 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310dc5cfb8)]: 0 failure(s)
2023-11-11 10:42:48 [INFO] [evolutionary_search.cc:649] Scores of the best 40 candidates:
[1 : 16]:	0.9902  0.9819  0.9808  0.9776  0.9771  0.9753  0.9735  0.9725  0.9710  0.9708  0.9701  0.9701  0.9700  0.9695  0.9690  0.9689
[17 : 32]:	0.9677  0.9668  0.9667  0.9666  0.9666  0.9662  0.9661  0.9658  0.9657  0.9655  0.9655  0.9650  0.9650  0.9644  0.9626  0.9620
[33 : 40]:	0.9617  0.9617  0.9616  0.9614  0.9611  0.9606  0.9604  0.9603
2023-11-11 10:42:49 [INFO] [evolutionary_search.cc:727] Got 40 candidate(s) with evolutionary search
2023-11-11 10:42:49 [INFO] [evolutionary_search.cc:730] Sending 40 candidates(s) for measurement
2023-11-11 10:43:14 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #961: GFLOPs: 5337.9836. Time: 26.6360 us. Best GFLOPs: 5498.5138
2023-11-11 10:43:14 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #962: GFLOPs: 5283.9106. Time: 26.9086 us. Best GFLOPs: 5498.5138
2023-11-11 10:43:14 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #963: GFLOPs: 5247.9222. Time: 27.0931 us. Best GFLOPs: 5498.5138
2023-11-11 10:43:14 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #964: GFLOPs: 5398.2626. Time: 26.3385 us. Best GFLOPs: 5498.5138
2023-11-11 10:43:14 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #965: GFLOPs: 5165.9212. Time: 27.5231 us. Best GFLOPs: 5498.5138
2023-11-11 10:43:14 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #966: GFLOPs: 5113.6741. Time: 27.8044 us. Best GFLOPs: 5498.5138
2023-11-11 10:43:14 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #967: GFLOPs: 5191.5503. Time: 27.3873 us. Best GFLOPs: 5498.5138
2023-11-11 10:43:14 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #968: GFLOPs: 5154.2640. Time: 27.5854 us. Best GFLOPs: 5498.5138
2023-11-11 10:43:14 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #969: GFLOPs: 5345.4968. Time: 26.5985 us. Best GFLOPs: 5498.5138
2023-11-11 10:43:14 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #970: GFLOPs: 5275.1582. Time: 26.9532 us. Best GFLOPs: 5498.5138
2023-11-11 10:43:14 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #971: GFLOPs: 5145.8084. Time: 27.6307 us. Best GFLOPs: 5498.5138
2023-11-11 10:43:14 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #972: GFLOPs: 5261.0537. Time: 27.0255 us. Best GFLOPs: 5498.5138
2023-11-11 10:43:14 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #973: GFLOPs: 5105.3718. Time: 27.8496 us. Best GFLOPs: 5498.5138
2023-11-11 10:43:14 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #974: GFLOPs: 5396.5727. Time: 26.3468 us. Best GFLOPs: 5498.5138
2023-11-11 10:43:14 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #975: GFLOPs: 5350.9433. Time: 26.5715 us. Best GFLOPs: 5498.5138
2023-11-11 10:43:14 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #976: GFLOPs: 5123.2115. Time: 27.7526 us. Best GFLOPs: 5498.5138
2023-11-11 10:43:14 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #977: GFLOPs: 5350.4231. Time: 26.5740 us. Best GFLOPs: 5498.5138
2023-11-11 10:43:14 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #978: GFLOPs: 5166.1282. Time: 27.5220 us. Best GFLOPs: 5498.5138
2023-11-11 10:43:14 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #979: GFLOPs: 5198.3462. Time: 27.3515 us. Best GFLOPs: 5498.5138
2023-11-11 10:43:14 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #980: GFLOPs: 5264.5117. Time: 27.0077 us. Best GFLOPs: 5498.5138
2023-11-11 10:43:14 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #981: GFLOPs: 5157.3618. Time: 27.5688 us. Best GFLOPs: 5498.5138
2023-11-11 10:43:14 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #982: GFLOPs: 5385.6030. Time: 26.4005 us. Best GFLOPs: 5498.5138
2023-11-11 10:43:14 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #983: GFLOPs: 5191.5851. Time: 27.3871 us. Best GFLOPs: 5498.5138
2023-11-11 10:43:14 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #984: GFLOPs: 5045.7624. Time: 28.1786 us. Best GFLOPs: 5498.5138
2023-11-11 10:43:14 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #985: GFLOPs: 5284.4137. Time: 26.9060 us. Best GFLOPs: 5498.5138
2023-11-11 10:43:14 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #986: GFLOPs: 5296.2272. Time: 26.8460 us. Best GFLOPs: 5498.5138
2023-11-11 10:43:14 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #987: GFLOPs: 5259.6232. Time: 27.0328 us. Best GFLOPs: 5498.5138
2023-11-11 10:43:14 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #988: GFLOPs: 5311.3024. Time: 26.7698 us. Best GFLOPs: 5498.5138
2023-11-11 10:43:14 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #989: GFLOPs: 5229.1845. Time: 27.1902 us. Best GFLOPs: 5498.5138
2023-11-11 10:43:14 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #990: GFLOPs: 5101.8125. Time: 27.8690 us. Best GFLOPs: 5498.5138
2023-11-11 10:43:14 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #991: GFLOPs: 5064.6213. Time: 28.0736 us. Best GFLOPs: 5498.5138
2023-11-11 10:43:14 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #992: GFLOPs: 5239.4300. Time: 27.1370 us. Best GFLOPs: 5498.5138
2023-11-11 10:43:14 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #993: GFLOPs: 5358.1373. Time: 26.5358 us. Best GFLOPs: 5498.5138
2023-11-11 10:43:14 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #994: GFLOPs: 5214.6024. Time: 27.2662 us. Best GFLOPs: 5498.5138
2023-11-11 10:43:14 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #995: GFLOPs: 5206.0099. Time: 27.3112 us. Best GFLOPs: 5498.5138
2023-11-11 10:43:14 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #996: GFLOPs: 5210.9743. Time: 27.2852 us. Best GFLOPs: 5498.5138
2023-11-11 10:43:14 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #997: GFLOPs: 5064.1248. Time: 28.0764 us. Best GFLOPs: 5498.5138
2023-11-11 10:43:14 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #998: GFLOPs: 5374.1012. Time: 26.4570 us. Best GFLOPs: 5498.5138
2023-11-11 10:43:14 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #999: GFLOPs: 846.5006. Time: 167.9649 us. Best GFLOPs: 5498.5138
2023-11-11 10:43:14 [INFO] [task_scheduler.cc:131] [Task #25: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_multiply_add_nn_re_a4e88f85cf7823fc_] Trial #1000: GFLOPs: 733.2513. Time: 193.9068 us. Best GFLOPs: 5498.5138
