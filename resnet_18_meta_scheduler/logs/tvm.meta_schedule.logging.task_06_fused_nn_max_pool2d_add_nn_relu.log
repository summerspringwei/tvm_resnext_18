2023-11-10 23:34:05 [INFO] [task_scheduler.cc:160] Initializing Task #6: "fused_nn_max_pool2d_add_nn_relu"
2023-11-10 23:34:05 [INFO] [task_scheduler.cc:35] 
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(64), T.int64(112), T.int64(112)), "float32"), p1: T.Buffer((T.int64(1), T.int64(64), T.int64(1), T.int64(1)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(64), T.int64(56), T.int64(56)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        pad_temp = T.alloc_buffer((T.int64(1), T.int64(64), T.int64(114), T.int64(114)))
        pool_max = T.alloc_buffer((T.int64(1), T.int64(64), T.int64(56), T.int64(56)))
        T_add = T.alloc_buffer((T.int64(1), T.int64(64), T.int64(56), T.int64(56)))
        for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(64), T.int64(114), T.int64(114)):
            with T.block("pad_temp"):
                v_ax0, v_ax1, v_ax2, v_ax3 = T.axis.remap("SSSS", [ax0, ax1, ax2, ax3])
                T.reads(p0[v_ax0, v_ax1, v_ax2 - T.int64(1), v_ax3 - T.int64(1)])
                T.writes(pad_temp[v_ax0, v_ax1, v_ax2, v_ax3])
                pad_temp[v_ax0, v_ax1, v_ax2, v_ax3] = T.if_then_else(T.int64(1) <= v_ax2 and v_ax2 < T.int64(113) and T.int64(1) <= v_ax3 and v_ax3 < T.int64(113), p0[v_ax0, v_ax1, v_ax2 - T.int64(1), v_ax3 - T.int64(1)], T.float32(-3.4028234663852886e+38))
        for ax0, ax1, ax2, ax3, rv0, rv1 in T.grid(T.int64(1), T.int64(64), T.int64(56), T.int64(56), T.int64(3), T.int64(3)):
            with T.block("pool_max"):
                v_ax0, v_ax1, v_ax2, v_ax3, v_rv0, v_rv1 = T.axis.remap("SSSSRR", [ax0, ax1, ax2, ax3, rv0, rv1])
                T.reads(pad_temp[v_ax0, v_ax1, v_ax2 * T.int64(2) + v_rv0, v_ax3 * T.int64(2) + v_rv1])
                T.writes(pool_max[v_ax0, v_ax1, v_ax2, v_ax3])
                T.block_attr({"schedule_rule": "meta_schedule.pool_max"})
                with T.init():
                    pool_max[v_ax0, v_ax1, v_ax2, v_ax3] = T.float32(-3.4028234663852886e+38)
                pool_max[v_ax0, v_ax1, v_ax2, v_ax3] = T.max(pool_max[v_ax0, v_ax1, v_ax2, v_ax3], pad_temp[v_ax0, v_ax1, v_ax2 * T.int64(2) + v_rv0, v_ax3 * T.int64(2) + v_rv1])
        for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(64), T.int64(56), T.int64(56)):
            with T.block("T_add"):
                v_ax0, v_ax1, v_ax2, v_ax3 = T.axis.remap("SSSS", [ax0, ax1, ax2, ax3])
                T.reads(pool_max[v_ax0, v_ax1, v_ax2, v_ax3], p1[v_ax0, v_ax1, T.int64(0), T.int64(0)])
                T.writes(T_add[v_ax0, v_ax1, v_ax2, v_ax3])
                T_add[v_ax0, v_ax1, v_ax2, v_ax3] = pool_max[v_ax0, v_ax1, v_ax2, v_ax3] + p1[v_ax0, v_ax1, T.int64(0), T.int64(0)]
        for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(64), T.int64(56), T.int64(56)):
            with T.block("T_relu"):
                v_ax0, v_ax1, v_ax2, v_ax3 = T.axis.remap("SSSS", [ax0, ax1, ax2, ax3])
                T.reads(T_add[v_ax0, v_ax1, v_ax2, v_ax3])
                T.writes(T_relu[v_ax0, v_ax1, v_ax2, v_ax3])
                T_relu[v_ax0, v_ax1, v_ax2, v_ax3] = T.max(T_add[v_ax0, v_ax1, v_ax2, v_ax3], T.float32(0))
2023-11-10 23:34:05 [INFO] [task_scheduler.cc:164] Total 2 design space(s) generated
2023-11-10 23:34:05 [INFO] [task_scheduler.cc:170] Design space #0:
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(64), T.int64(112), T.int64(112)), "float32"), p1: T.Buffer((T.int64(1), T.int64(64), T.int64(1), T.int64(1)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(64), T.int64(56), T.int64(56)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit": 512})
            pool_max = T.alloc_buffer((T.int64(1), T.int64(64), T.int64(56), T.int64(56)))
            for ax0_ax1_ax2_ax3_fused_0 in T.thread_binding(T.int64(784), thread="blockIdx.x"):
                for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                    for rv0, rv1 in T.grid(T.int64(3), T.int64(3)):
                        with T.block("pool_max"):
                            v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                            v_ax1 = T.axis.spatial(T.int64(64), (ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1) // T.int64(3136))
                            v_ax2 = T.axis.spatial(T.int64(56), (ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1) % T.int64(3136) // T.int64(56))
                            v_ax3 = T.axis.spatial(T.int64(56), (ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1) % T.int64(56))
                            v_rv0, v_rv1 = T.axis.remap("RR", [rv0, rv1])
                            T.reads(p0[v_ax0, v_ax1, v_ax2 * T.int64(2) + v_rv0 - T.int64(1), v_ax3 * T.int64(2) + v_rv1 - T.int64(1)])
                            T.writes(pool_max[v_ax0, v_ax1, v_ax2, v_ax3])
                            with T.init():
                                pool_max[v_ax0, v_ax1, v_ax2, v_ax3] = T.float32(-3.4028234663852886e+38)
                            pool_max[v_ax0, v_ax1, v_ax2, v_ax3] = T.max(pool_max[v_ax0, v_ax1, v_ax2, v_ax3], T.if_then_else(T.int64(1) <= v_ax2 * T.int64(2) + v_rv0 and v_ax2 * T.int64(2) + v_rv0 < T.int64(113) and T.int64(1) <= v_ax3 * T.int64(2) + v_rv1 and v_ax3 * T.int64(2) + v_rv1 < T.int64(113), p0[v_ax0, v_ax1, v_ax2 * T.int64(2) + v_rv0 - T.int64(1), v_ax3 * T.int64(2) + v_rv1 - T.int64(1)], T.float32(-3.4028234663852886e+38)))
            for ax0_ax1_ax2_ax3_fused_0 in T.thread_binding(T.int64(1568), thread="blockIdx.x"):
                for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                    with T.block("T_add"):
                        v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                        v_ax1 = T.axis.spatial(T.int64(64), (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1) // T.int64(3136))
                        v_ax2 = T.axis.spatial(T.int64(56), (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1) % T.int64(3136) // T.int64(56))
                        v_ax3 = T.axis.spatial(T.int64(56), (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1) % T.int64(56))
                        T.reads(pool_max[v_ax0, v_ax1, v_ax2, v_ax3], p1[v_ax0, v_ax1, T.int64(0), T.int64(0)])
                        T.writes(T_relu[v_ax0, v_ax1, v_ax2, v_ax3])
                        T_relu[v_ax0, v_ax1, v_ax2, v_ax3] = T.max(pool_max[v_ax0, v_ax1, v_ax2, v_ax3] + p1[v_ax0, v_ax1, T.int64(0), T.int64(0)], T.float32(0))
b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="pool_max", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b1, ann_key="schedule_rule")
sch.reverse_compute_inline(block=b3)
sch.compute_inline(block=b0)
v5 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v5)
l6, l7, l8, l9 = sch.get_loops(block=b2)
l10 = sch.fuse(l6, l7, l8, l9, preserve_unit_iters=True)
v11 = sch.sample_categorical(candidates=[32, 64, 128, 256, 512, 1024], probs=[0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666], decision=2)
l12, l13 = sch.split(loop=l10, factors=[None, v11], preserve_unit_iters=True)
sch.bind(loop=l12, thread_axis="blockIdx.x")
sch.bind(loop=l13, thread_axis="threadIdx.x")
l14, l15, l16, l17, l18, l19 = sch.get_loops(block=b1)
l20 = sch.fuse(l14, l15, l16, l17, preserve_unit_iters=True)
v21 = sch.sample_categorical(candidates=[32, 64, 128, 256, 512, 1024], probs=[0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666], decision=3)
l22, l23 = sch.split(loop=l20, factors=[None, v21], preserve_unit_iters=True)
sch.bind(loop=l22, thread_axis="blockIdx.x")
sch.bind(loop=l23, thread_axis="threadIdx.x")
2023-11-10 23:34:05 [INFO] [task_scheduler.cc:170] Design space #1:
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(64), T.int64(112), T.int64(112)), "float32"), p1: T.Buffer((T.int64(1), T.int64(64), T.int64(1), T.int64(1)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(64), T.int64(56), T.int64(56)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit": 1024})
            pool_max_shared = T.alloc_buffer((T.int64(1), T.int64(64), T.int64(56), T.int64(56)), scope="shared")
            for ax0_ax1_ax2_ax3_0_fused in T.thread_binding(T.int64(3584), thread="blockIdx.x"):
                for ax0, ax1, ax2, ax3, ax4_ax5_fused_0 in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(56), T.int64(1)):
                    for ax4_ax5_fused_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                        with T.block("pool_max"):
                            v_ax0 = T.axis.spatial(T.int64(1), ax0)
                            v_ax1 = T.axis.spatial(T.int64(64), ax0_ax1_ax2_ax3_0_fused // T.int64(56) + ax1)
                            v_ax2 = T.axis.spatial(T.int64(56), ax0_ax1_ax2_ax3_0_fused % T.int64(56) + ax2)
                            v_ax3 = T.axis.spatial(T.int64(56), ax3)
                            v_rv0 = T.axis.reduce(T.int64(3), (ax4_ax5_fused_0 * T.int64(128) + ax4_ax5_fused_1) // T.int64(3))
                            v_rv1 = T.axis.reduce(T.int64(3), (ax4_ax5_fused_0 * T.int64(128) + ax4_ax5_fused_1) % T.int64(3))
                            T.where(ax4_ax5_fused_0 * T.int64(128) + ax4_ax5_fused_1 < T.int64(9))
                            T.reads(p0[v_ax0, v_ax1, v_ax2 * T.int64(2) + v_rv0 - T.int64(1), v_ax3 * T.int64(2) + v_rv1 - T.int64(1)])
                            T.writes(pool_max_shared[v_ax0, v_ax1, v_ax2, v_ax3])
                            with T.init():
                                pool_max_shared[v_ax0, v_ax1, v_ax2, v_ax3] = T.float32(-3.4028234663852886e+38)
                            pool_max_shared[v_ax0, v_ax1, v_ax2, v_ax3] = T.max(pool_max_shared[v_ax0, v_ax1, v_ax2, v_ax3], T.if_then_else(T.int64(1) <= v_ax2 * T.int64(2) + v_rv0 and v_ax2 * T.int64(2) + v_rv0 < T.int64(113) and T.int64(1) <= v_ax3 * T.int64(2) + v_rv1 and v_ax3 * T.int64(2) + v_rv1 < T.int64(113), p0[v_ax0, v_ax1, v_ax2 * T.int64(2) + v_rv0 - T.int64(1), v_ax3 * T.int64(2) + v_rv1 - T.int64(1)], T.float32(-3.4028234663852886e+38)))
                for ax3_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                    with T.block("T_add"):
                        v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                        v_ax1 = T.axis.spatial(T.int64(64), ax0_ax1_ax2_ax3_0_fused // T.int64(56))
                        v_ax2 = T.axis.spatial(T.int64(56), ax0_ax1_ax2_ax3_0_fused % T.int64(56))
                        v_ax3 = T.axis.spatial(T.int64(56), ax3_1)
                        T.where(T.Mul(T.int64(0), T.int64(128)) + ax3_1 < T.int64(56))
                        T.reads(pool_max_shared[v_ax0, v_ax1, v_ax2, v_ax3], p1[v_ax0, v_ax1, T.int64(0), T.int64(0)])
                        T.writes(T_relu[v_ax0, v_ax1, v_ax2, v_ax3])
                        T_relu[v_ax0, v_ax1, v_ax2, v_ax3] = T.max(pool_max_shared[v_ax0, v_ax1, v_ax2, v_ax3] + p1[v_ax0, v_ax1, T.int64(0), T.int64(0)], T.float32(0))
b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="pool_max", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b1, ann_key="schedule_rule")
sch.reverse_compute_inline(block=b3)
sch.compute_inline(block=b0)
b5, = sch.get_consumers(block=b1)
l6, l7, l8, l9 = sch.get_loops(block=b5)
v10 = sch.sample_categorical(candidates=[4, 8, 16, 32, 64, 128, 256, 512], probs=[0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125], decision=5)
l11, l12 = sch.split(loop=l9, factors=[None, v10], preserve_unit_iters=True)
sch.bind(loop=l12, thread_axis="threadIdx.x")
sch.compute_at(block=b1, loop=l11, preserve_unit_loops=True, index=-1)
sch.set_scope(block=b1, buffer_index=0, storage_scope="shared")
l13, l14, l15, l16, l17, l18, l19, l20, l21, l22 = sch.get_loops(block=b1)
l23 = sch.fuse(l21, l22, preserve_unit_iters=True)
l24, l25 = sch.split(loop=l23, factors=[None, v10], preserve_unit_iters=True)
sch.bind(loop=l25, thread_axis="threadIdx.x")
v26 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v26)
l27, l28, l29, l30, l31 = sch.get_loops(block=b2)
l32 = sch.fuse(l27, l28, l29, l30, preserve_unit_iters=True)
sch.bind(loop=l32, thread_axis="blockIdx.x")
2023-11-10 23:39:06 [INFO] [evolutionary_search.cc:713] Generating candidates......
2023-11-10 23:39:06 [INFO] [evolutionary_search.cc:715] Picked top 0 candidate(s) from database
2023-11-10 23:39:09 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310fb056b8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310e891278)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310e8921e8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310e060038)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56310bbc4548)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310e896198)]: 0 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310e896608)]: 0 failure(s)
2023-11-10 23:39:09 [INFO] [evolutionary_search.cc:723] Sampled 512 candidate(s)
2023-11-10 23:39:11 [INFO] [evolutionary_search.cc:621] Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310fb056b8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310e891278)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310e8921e8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310e060038)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56310bbc4548)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310e896198)]: 0 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310e896608)]: 0 failure(s)
2023-11-10 23:39:12 [INFO] [evolutionary_search.cc:621] Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310fb056b8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310e891278)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310e8921e8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310e060038)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56310bbc4548)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310e896198)]: 0 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310e896608)]: 0 failure(s)
2023-11-10 23:39:14 [INFO] [evolutionary_search.cc:621] Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310fb056b8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310e891278)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310e8921e8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310e060038)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56310bbc4548)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310e896198)]: 0 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310e896608)]: 0 failure(s)
2023-11-10 23:39:16 [INFO] [evolutionary_search.cc:621] Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310fb056b8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310e891278)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310e8921e8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310e060038)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56310bbc4548)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310e896198)]: 0 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310e896608)]: 0 failure(s)
2023-11-10 23:39:16 [INFO] [evolutionary_search.cc:649] Scores of the best 64 candidates:
[1 : 16]:	0.9995  0.9991  0.9951  0.9875  0.9855  0.9826  0.9820  0.9801  0.9764  0.9747  0.9728  0.9675  0.9636  0.9555  0.9524  0.9517
[17 : 32]:	0.9509  0.9506  0.9452  0.9435  0.9435  0.9401  0.9230  0.9162  0.9140  0.9132  0.9109  0.9030  0.9026  0.8912  0.8905  0.8899
[33 : 48]:	0.8844  0.8776  0.8746  0.8745  0.8712  0.8584  0.8406  0.8405  0.8334  0.8219  0.8075  0.8069  0.8069  0.8056  0.8004  0.7987
[49 : 64]:	0.7982  0.7937  0.7884  0.7817  0.7800  0.7769  0.7589  0.7503  0.7470  0.7469  0.7452  0.7448  0.7389  0.7367  0.7257  0.7251
2023-11-10 23:39:16 [INFO] [evolutionary_search.cc:727] Got 64 candidate(s) with evolutionary search
2023-11-10 23:39:16 [INFO] [evolutionary_search.cc:730] Sending 63 candidates(s) for measurement
2023-11-11 00:21:27 [INFO] [task_scheduler.cc:121] [Task #6: fused_nn_max_pool2d_add_nn_relu] Trial #1: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/home2/xiachunwei/Software/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/home2/xiachunwei/Software/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/home2/xiachunwei/Software/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/home2/xiachunwei/Software/tvm/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/home2/xiachunwei/Software/tvm/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/home2/xiachunwei/Software/tvm/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/home2/xiachunwei/Software/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/home2/xiachunwei/Software/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  4: _ZN3tvm7runtime13PackedFun
  3: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  2: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  0: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  File "/home2/xiachunwei/Software/tvm/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(64), T.int64(112), T.int64(112)), "float32"), p1: T.Buffer((T.int64(1), T.int64(64), T.int64(1), T.int64(1)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(64), T.int64(56), T.int64(56)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        pool_max = T.alloc_buffer((T.int64(1), T.int64(64), T.int64(56), T.int64(56)))
        for ax0_ax1_ax2_ax3_fused_0 in T.thread_binding(T.int64(3136), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 64, "pragma_unroll_explicit": 1}):
            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                with T.block("pool_max_init"):
                    v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                    v_ax1 = T.axis.spatial(T.int64(64), (ax0_ax1_ax2_ax3_fused_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused_1) // T.int64(3136))
                    v_ax2 = T.axis.spatial(T.int64(56), (ax0_ax1_ax2_ax3_fused_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused_1) % T.int64(3136) // T.int64(56))
                    v_ax3 = T.axis.spatial(T.int64(56), (ax0_ax1_ax2_ax3_fused_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused_1) % T.int64(56))
                    T.reads()
                    T.writes(pool_max[v_ax0, v_ax1, v_ax2, v_ax3])
                    pool_max[v_ax0, v_ax1, v_ax2, v_ax3] = T.float32(-3.4028234663852886e+38)
                for rv0, rv1 in T.grid(T.int64(3), T.int64(3)):
                    with T.block("pool_max_update"):
                        v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                        v_ax1 = T.axis.spatial(T.int64(64), (ax0_ax1_ax2_ax3_fused_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused_1) // T.int64(3136))
                        v_ax2 = T.axis.spatial(T.int64(56), (ax0_ax1_ax2_ax3_fused_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused_1) % T.int64(3136) // T.int64(56))
                        v_ax3 = T.axis.spatial(T.int64(56), (ax0_ax1_ax2_ax3_fused_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused_1) % T.int64(56))
                        v_rv0, v_rv1 = T.axis.remap("RR", [rv0, rv1])
                        T.reads(pool_max[v_ax0, v_ax1, v_ax2, v_ax3], p0[v_ax0, v_ax1, v_ax2 * T.int64(2) + v_rv0 - T.int64(1), v_ax3 * T.int64(2) + v_rv1 - T.int64(1)])
                        T.writes(pool_max[v_ax0, v_ax1, v_ax2, v_ax3])
                        pool_max[v_ax0, v_ax1, v_ax2, v_ax3] = T.max(pool_max[v_ax0, v_ax1, v_ax2, v_ax3], T.if_then_else(T.int64(1) <= v_ax2 * T.int64(2) + v_rv0 and v_ax2 * T.int64(2) + v_rv0 < T.int64(113) and T.int64(1) <= v_ax3 * T.int64(2) + v_rv1 and v_ax3 * T.int64(2) + v_rv1 < T.int64(113), p0[v_ax0, v_ax1, v_ax2 * T.int64(2) + v_rv0 - T.int64(1), v_ax3 * T.int64(2) + v_rv1 - T.int64(1)], T.float32(-3.4028234663852886e+38)))
        for ax0_ax1_ax2_ax3_fused_0 in T.thread_binding(T.int64(784), thread="blockIdx.x"):
            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                with T.block("T_add"):
                    v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                    v_ax1 = T.axis.spatial(T.int64(64), (ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1) // T.int64(3136))
                    v_ax2 = T.axis.spatial(T.int64(56), (ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1) % T.int64(3136) // T.int64(56))
                    v_ax3 = T.axis.spatial(T.int64(56), (ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1) % T.int64(56))
                    T.reads(pool_max[v_ax0, v_ax1, v_ax2, v_ax3], p1[v_ax0, v_ax1, T.int64(0), T.int64(0)])
                    T.writes(T_relu[v_ax0, v_ax1, v_ax2, v_ax3])
                    T_relu[v_ax0, v_ax1, v_ax2, v_ax3] = T.max(pool_max[v_ax0, v_ax1, v_ax2, v_ax3] + p1[v_ax0, v_ax1, T.int64(0), T.int64(0)], T.float32(0))
b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="pool_max", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b1, ann_key="schedule_rule")
sch.reverse_compute_inline(block=b3)
sch.compute_inline(block=b0)
v5 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=2)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v5)
l6, l7, l8, l9 = sch.get_loops(block=b2)
l10 = sch.fuse(l6, l7, l8, l9, preserve_unit_iters=True)
v11 = sch.sample_categorical(candidates=[32, 64, 128, 256, 512, 1024], probs=[0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666], decision=3)
l12, l13 = sch.split(loop=l10, factors=[None, v11], preserve_unit_iters=True)
sch.bind(loop=l12, thread_axis="blockIdx.x")
sch.bind(loop=l13, thread_axis="threadIdx.x")
l14, l15, l16, l17, l18, l19 = sch.get_loops(block=b1)
l20 = sch.fuse(l14, l15, l16, l17, preserve_unit_iters=True)
v21 = sch.sample_categorical(candidates=[32, 64, 128, 256, 512, 1024], probs=[0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666], decision=1)
l22, l23 = sch.split(loop=l20, factors=[None, v21], preserve_unit_iters=True)
sch.bind(loop=l22, thread_axis="blockIdx.x")
sch.bind(loop=l23, thread_axis="threadIdx.x")
sch.enter_postproc()
b24 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b24, ann_key="meta_schedule.unroll_explicit")
b25, b26 = sch.get_child_blocks(b24)
l27, l28, l29, l30 = sch.get_loops(block=b25)
sch.annotate(block_or_loop=l27, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l27, ann_key="pragma_unroll_explicit", ann_val=1)
l31, l32 = sch.get_loops(block=b26)
b33 = sch.get_block(name="pool_max", func_name="main")
l34, l35, l36, l37 = sch.get_loops(block=b33)
b38 = sch.decompose_reduction(block=b33, loop=l36)
2023-11-11 00:21:27 [INFO] [task_scheduler.cc:121] [Task #6: fused_nn_max_pool2d_add_nn_relu] Trial #2: Error in running:
LocalRunner: An exception occurred
Subprocess terminated
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(64), T.int64(112), T.int64(112)), "float32"), p1: T.Buffer((T.int64(1), T.int64(64), T.int64(1), T.int64(1)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(64), T.int64(56), T.int64(56)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        pool_max_shared = T.alloc_buffer((T.int64(1), T.int64(64), T.int64(56), T.int64(56)), scope="shared")
        for ax0_ax1_ax2_ax3_0_fused in T.thread_binding(T.int64(3584), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 16, "pragma_unroll_explicit": 1}):
            for ax0, ax1, ax2, ax3, ax4_ax5_fused_0 in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(56), T.int64(1)):
                for ax4_ax5_fused_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                    with T.block("pool_max"):
                        v_ax0 = T.axis.spatial(T.int64(1), ax0)
                        v_ax1 = T.axis.spatial(T.int64(64), ax0_ax1_ax2_ax3_0_fused // T.int64(56) + ax1)
                        v_ax2 = T.axis.spatial(T.int64(56), ax0_ax1_ax2_ax3_0_fused % T.int64(56) + ax2)
                        v_ax3 = T.axis.spatial(T.int64(56), ax3)
                        v_rv0 = T.axis.reduce(T.int64(3), (ax4_ax5_fused_0 * T.int64(128) + ax4_ax5_fused_1) // T.int64(3))
                        v_rv1 = T.axis.reduce(T.int64(3), (ax4_ax5_fused_0 * T.int64(128) + ax4_ax5_fused_1) % T.int64(3))
                        T.where(ax4_ax5_fused_0 * T.int64(128) + ax4_ax5_fused_1 < T.int64(9))
                        T.reads(p0[v_ax0, v_ax1, v_ax2 * T.int64(2) + v_rv0 - T.int64(1), v_ax3 * T.int64(2) + v_rv1 - T.int64(1)])
                        T.writes(pool_max_shared[v_ax0, v_ax1, v_ax2, v_ax3])
                        with T.init():
                            pool_max_shared[v_ax0, v_ax1, v_ax2, v_ax3] = T.float32(-3.4028234663852886e+38)
                        pool_max_shared[v_ax0, v_ax1, v_ax2, v_ax3] = T.max(pool_max_shared[v_ax0, v_ax1, v_ax2, v_ax3], T.if_then_else(T.int64(1) <= v_ax2 * T.int64(2) + v_rv0 and v_ax2 * T.int64(2) + v_rv0 < T.int64(113) and T.int64(1) <= v_ax3 * T.int64(2) + v_rv1 and v_ax3 * T.int64(2) + v_rv1 < T.int64(113), p0[v_ax0, v_ax1, v_ax2 * T.int64(2) + v_rv0 - T.int64(1), v_ax3 * T.int64(2) + v_rv1 - T.int64(1)], T.float32(-3.4028234663852886e+38)))
            for ax3_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                with T.block("T_add"):
                    v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                    v_ax1 = T.axis.spatial(T.int64(64), ax0_ax1_ax2_ax3_0_fused // T.int64(56))
                    v_ax2 = T.axis.spatial(T.int64(56), ax0_ax1_ax2_ax3_0_fused % T.int64(56))
                    v_ax3 = T.axis.spatial(T.int64(56), ax3_1)
                    T.where(T.Mul(T.int64(0), T.int64(128)) + ax3_1 < T.int64(56))
                    T.reads(pool_max_shared[v_ax0, v_ax1, v_ax2, v_ax3], p1[v_ax0, v_ax1, T.int64(0), T.int64(0)])
                    T.writes(T_relu[v_ax0, v_ax1, v_ax2, v_ax3])
                    T_relu[v_ax0, v_ax1, v_ax2, v_ax3] = T.max(pool_max_shared[v_ax0, v_ax1, v_ax2, v_ax3] + p1[v_ax0, v_ax1, T.int64(0), T.int64(0)], T.float32(0))
b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="pool_max", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b1, ann_key="schedule_rule")
sch.reverse_compute_inline(block=b3)
sch.compute_inline(block=b0)
b5, = sch.get_consumers(block=b1)
l6, l7, l8, l9 = sch.get_loops(block=b5)
v10 = sch.sample_categorical(candidates=[4, 8, 16, 32, 64, 128, 256, 512], probs=[0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125], decision=5)
l11, l12 = sch.split(loop=l9, factors=[None, v10], preserve_unit_iters=True)
sch.bind(loop=l12, thread_axis="threadIdx.x")
sch.compute_at(block=b1, loop=l11, preserve_unit_loops=True, index=-1)
sch.set_scope(block=b1, buffer_index=0, storage_scope="shared")
l13, l14, l15, l16, l17, l18, l19, l20, l21, l22 = sch.get_loops(block=b1)
l23 = sch.fuse(l21, l22, preserve_unit_iters=True)
l24, l25 = sch.split(loop=l23, factors=[None, v10], preserve_unit_iters=True)
sch.bind(loop=l25, thread_axis="threadIdx.x")
v26 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=1)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v26)
l27, l28, l29, l30, l31 = sch.get_loops(block=b2)
l32 = sch.fuse(l27, l28, l29, l30, preserve_unit_iters=True)
sch.bind(loop=l32, thread_axis="blockIdx.x")
sch.enter_postproc()
b33 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b33, ann_key="meta_schedule.unroll_explicit")
b34, b35 = sch.get_child_blocks(b33)
l36, l37, l38, l39, l40, l41, l42 = sch.get_loops(block=b34)
sch.annotate(block_or_loop=l36, ann_key="pragma_auto_unroll_max_step", ann_val=16)
sch.annotate(block_or_loop=l36, ann_key="pragma_unroll_explicit", ann_val=1)
l43, l44 = sch.get_loops(block=b35)
2023-11-11 00:21:27 [INFO] [task_scheduler.cc:131] [Task #6: fused_nn_max_pool2d_add_nn_relu] Trial #3: GFLOPs: 220.9811. Time: 9.9906 us. Best GFLOPs: 220.9811
2023-11-11 00:21:27 [INFO] [task_scheduler.cc:121] [Task #6: fused_nn_max_pool2d_add_nn_relu] Trial #4: Error in building:
LocalBuilder: An exception occurred
Traceback (most recent call last):
  File "/home2/xiachunwei/Software/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/home2/xiachunwei/Software/tvm/python/tvm/meta_schedule/builder/local_builder.py", line 165, in <lambda>
    lambda x: _worker_func(*x),
  File "/home2/xiachunwei/Software/tvm/python/tvm/meta_schedule/builder/local_builder.py", line 231, in _worker_func
    rt_mod: Module = f_build(mod, target, _deserialize_params(params))
  File "/home2/xiachunwei/Software/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/home2/xiachunwei/Software/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
  File "/home2/xiachunwei/Software/tvm/python/tvm/meta_schedule/builder/local_builder.py", line 261, in default_build
    return tvm_build(mod, target=target)
  File "/home2/xiachunwei/Software/tvm/python/tvm/driver/build_module.py", line 281, in build
    rt_mod_host = _driver_ffi.tir_to_runtime(annotated_mods, target_host)
  File "/home2/xiachunwei/Software/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/home2/xiachunwei/Software/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  33: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::TypedPackedFunc<tvm::runtime::Module (tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)>::AssignTypedLambda<tvm::{lambda(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)#6}>(tvm::{lambda(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)#6}, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}> >::Call(tvm::runtime::PackedFuncObj const*, tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)
  32: tvm::TIRToRuntime(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target const&)
  31: tvm::SplitMixedModule(tvm::IRModule, tvm::Target const&, tvm::Target const&)
  30: tvm::ApplyPasses(tvm::IRModule, tvm::transform::Sequential)
  29: tvm::transform::Pass::operator()(tvm::IRModule) const
  28: tvm::transform::Pass::operator()(tvm::IRModule, tvm::transform::PassContext const&) const
  27: tvm::transform::SequentialNode::operator()(tvm::IRModule, tvm::transform::PassContext const&) const
  26: tvm::transform::Pass::operator()(tvm::IRModule, tvm::transform::PassContext const&) const
  25: tvm::tir::transform::PrimFuncPassNode::operator()(tvm::IRModule, tvm::transform::PassContext const&) const
  24: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::TypedPackedFunc<tvm::tir::PrimFunc (tvm::tir::PrimFunc, tvm::IRModule, tvm::transform::PassContext)>::AssignTypedLambda<tvm::tir::transform::LowerThreadAllreduce()::{lambda(tvm::tir::PrimFunc, tvm::IRModule, tvm::transform::PassContext)#1}>(tvm::tir::transform::LowerThreadAllreduce()::{lambda(tvm::tir::PrimFunc, tvm::IRModule, tvm::transform::PassContext)#1})::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}> >::Call(tvm::runtime::PackedFuncObj const*, tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)
  23: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7r
  22: tvm::tir::ThreadAllreduceBuilder::VisitStmt_(tvm::tir::AttrStmtNode const*)
  21: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  20: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7r
  19: tvm::tir::ThreadAllreduceBuilder::VisitStmt_(tvm::tir::AllocateNode const*)
  18: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  17: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7r
  16: tvm::tir::ThreadAllreduceBuilder::VisitStmt_(tvm::tir::AllocateNode const*)
  15: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  14: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7r
  13: tvm::tir::ThreadAllreduceBuilder::VisitStmt_(tvm::tir::AllocateNode const*)
  12: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  11: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7r
  10: tvm::tir::ThreadAllreduceBuilder::VisitStmt_(tvm::tir::AttrStmtNode const*)
  9: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  8: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runt
  7: tvm::runtime::ObjectPtr<tvm::runtime::Object> tvm::runtime::Array<tvm::tir::Stmt, void>::MapHelper<tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1}, tvm::tir::Stmt>(tvm::runtime::ObjectPtr<tvm::runtime::Object>, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1})
  6: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  5: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7r
  4: tvm::tir::ThreadAllreduceBuilder::VisitStmt_(tvm::tir::AttrStmtNode const*)
  3: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  2: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runt
  1: tvm::tir::ThreadAllreduceBuilder::VisitStmt_(tvm::tir::EvaluateNode const*)
  0: tvm::tir::ThreadAllreduceBuilder::MakeAllreduce(tvm::tir::CallNode const*)
  File "/home2/xiachunwei/Software/tvm/src/tir/transforms/lower_thread_allreduce.cc", line 383
InternalError: Check failed: (!load_remap_.count(buffers[i]->data.get())) is false: 

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(64), T.int64(112), T.int64(112)), "float32"), p1: T.Buffer((T.int64(1), T.int64(64), T.int64(1), T.int64(1)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(64), T.int64(56), T.int64(56)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        pool_max_shared = T.alloc_buffer((T.int64(1), T.int64(64), T.int64(56), T.int64(56)), scope="shared")
        for ax0_ax1_ax2_ax3_0_fused in T.thread_binding(T.int64(25088), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 64, "pragma_unroll_explicit": 1}):
            for ax0, ax1, ax2, ax3, ax4_ax5_fused_0 in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(8), T.int64(2)):
                for ax4_ax5_fused_1 in T.thread_binding(T.int64(8), thread="threadIdx.x"):
                    with T.block("pool_max"):
                        v_ax0 = T.axis.spatial(T.int64(1), ax0)
                        v_ax1 = T.axis.spatial(T.int64(64), ax0_ax1_ax2_ax3_0_fused // T.int64(392) + ax1)
                        v_ax2 = T.axis.spatial(T.int64(56), ax0_ax1_ax2_ax3_0_fused % T.int64(392) // T.int64(7) + ax2)
                        v_ax3 = T.axis.spatial(T.int64(56), ax0_ax1_ax2_ax3_0_fused % T.int64(7) * T.int64(8) + ax3)
                        v_rv0 = T.axis.reduce(T.int64(3), (ax4_ax5_fused_0 * T.int64(8) + ax4_ax5_fused_1) // T.int64(3))
                        v_rv1 = T.axis.reduce(T.int64(3), (ax4_ax5_fused_0 * T.int64(8) + ax4_ax5_fused_1) % T.int64(3))
                        T.where(ax4_ax5_fused_0 * T.int64(8) + ax4_ax5_fused_1 < T.int64(9))
                        T.reads(p0[v_ax0, v_ax1, v_ax2 * T.int64(2) + v_rv0 - T.int64(1), v_ax3 * T.int64(2) + v_rv1 - T.int64(1)])
                        T.writes(pool_max_shared[v_ax0, v_ax1, v_ax2, v_ax3])
                        with T.init():
                            pool_max_shared[v_ax0, v_ax1, v_ax2, v_ax3] = T.float32(-3.4028234663852886e+38)
                        pool_max_shared[v_ax0, v_ax1, v_ax2, v_ax3] = T.max(pool_max_shared[v_ax0, v_ax1, v_ax2, v_ax3], T.if_then_else(T.int64(1) <= v_ax2 * T.int64(2) + v_rv0 and v_ax2 * T.int64(2) + v_rv0 < T.int64(113) and T.int64(1) <= v_ax3 * T.int64(2) + v_rv1 and v_ax3 * T.int64(2) + v_rv1 < T.int64(113), p0[v_ax0, v_ax1, v_ax2 * T.int64(2) + v_rv0 - T.int64(1), v_ax3 * T.int64(2) + v_rv1 - T.int64(1)], T.float32(-3.4028234663852886e+38)))
            for ax3_1 in T.thread_binding(T.int64(8), thread="threadIdx.x"):
                with T.block("T_add"):
                    v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                    v_ax1 = T.axis.spatial(T.int64(64), ax0_ax1_ax2_ax3_0_fused // T.int64(392))
                    v_ax2 = T.axis.spatial(T.int64(56), ax0_ax1_ax2_ax3_0_fused % T.int64(392) // T.int64(7))
                    v_ax3 = T.axis.spatial(T.int64(56), ax0_ax1_ax2_ax3_0_fused % T.int64(7) * T.int64(8) + ax3_1)
                    T.reads(pool_max_shared[v_ax0, v_ax1, v_ax2, v_ax3], p1[v_ax0, v_ax1, T.int64(0), T.int64(0)])
                    T.writes(T_relu[v_ax0, v_ax1, v_ax2, v_ax3])
                    T_relu[v_ax0, v_ax1, v_ax2, v_ax3] = T.max(pool_max_shared[v_ax0, v_ax1, v_ax2, v_ax3] + p1[v_ax0, v_ax1, T.int64(0), T.int64(0)], T.float32(0))
b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="pool_max", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b1, ann_key="schedule_rule")
sch.reverse_compute_inline(block=b3)
sch.compute_inline(block=b0)
b5, = sch.get_consumers(block=b1)
l6, l7, l8, l9 = sch.get_loops(block=b5)
v10 = sch.sample_categorical(candidates=[4, 8, 16, 32, 64, 128, 256, 512], probs=[0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125], decision=1)
l11, l12 = sch.split(loop=l9, factors=[None, v10], preserve_unit_iters=True)
sch.bind(loop=l12, thread_axis="threadIdx.x")
sch.compute_at(block=b1, loop=l11, preserve_unit_loops=True, index=-1)
sch.set_scope(block=b1, buffer_index=0, storage_scope="shared")
l13, l14, l15, l16, l17, l18, l19, l20, l21, l22 = sch.get_loops(block=b1)
l23 = sch.fuse(l21, l22, preserve_unit_iters=True)
l24, l25 = sch.split(loop=l23, factors=[None, v10], preserve_unit_iters=True)
sch.bind(loop=l25, thread_axis="threadIdx.x")
v26 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=2)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v26)
l27, l28, l29, l30, l31 = sch.get_loops(block=b2)
l32 = sch.fuse(l27, l28, l29, l30, preserve_unit_iters=True)
sch.bind(loop=l32, thread_axis="blockIdx.x")
sch.enter_postproc()
b33 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b33, ann_key="meta_schedule.unroll_explicit")
b34, b35 = sch.get_child_blocks(b33)
l36, l37, l38, l39, l40, l41, l42 = sch.get_loops(block=b34)
sch.annotate(block_or_loop=l36, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l36, ann_key="pragma_unroll_explicit", ann_val=1)
l43, l44 = sch.get_loops(block=b35)
2023-11-11 00:21:27 [INFO] [task_scheduler.cc:131] [Task #6: fused_nn_max_pool2d_add_nn_relu] Trial #5: GFLOPs: 173.5693. Time: 12.7197 us. Best GFLOPs: 220.9811
2023-11-11 00:21:27 [INFO] [task_scheduler.cc:131] [Task #6: fused_nn_max_pool2d_add_nn_relu] Trial #6: GFLOPs: 269.2735. Time: 8.1989 us. Best GFLOPs: 269.2735
2023-11-11 00:21:27 [INFO] [task_scheduler.cc:131] [Task #6: fused_nn_max_pool2d_add_nn_relu] Trial #7: GFLOPs: 180.8844. Time: 12.2053 us. Best GFLOPs: 269.2735
2023-11-11 00:21:27 [INFO] [task_scheduler.cc:121] [Task #6: fused_nn_max_pool2d_add_nn_relu] Trial #8: Error in building:
LocalBuilder: An exception occurred
Traceback (most recent call last):
  File "/home2/xiachunwei/Software/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/home2/xiachunwei/Software/tvm/python/tvm/meta_schedule/builder/local_builder.py", line 165, in <lambda>
    lambda x: _worker_func(*x),
  File "/home2/xiachunwei/Software/tvm/python/tvm/meta_schedule/builder/local_builder.py", line 231, in _worker_func
    rt_mod: Module = f_build(mod, target, _deserialize_params(params))
  File "/home2/xiachunwei/Software/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/home2/xiachunwei/Software/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
  File "/home2/xiachunwei/Software/tvm/python/tvm/meta_schedule/builder/local_builder.py", line 261, in default_build
    return tvm_build(mod, target=target)
  File "/home2/xiachunwei/Software/tvm/python/tvm/driver/build_module.py", line 281, in build
    rt_mod_host = _driver_ffi.tir_to_runtime(annotated_mods, target_host)
  File "/home2/xiachunwei/Software/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/home2/xiachunwei/Software/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  33: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::TypedPackedFunc<tvm::runtime::Module (tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)>::AssignTypedLambda<tvm::{lambda(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)#6}>(tvm::{lambda(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)#6}, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}> >::Call(tvm::runtime::PackedFuncObj const*, tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)
  32: tvm::TIRToRuntime(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target const&)
  31: tvm::SplitMixedModule(tvm::IRModule, tvm::Target const&, tvm::Target const&)
  30: tvm::ApplyPasses(tvm::IRModule, tvm::transform::Sequential)
  29: tvm::transform::Pass::operator()(tvm::IRModule) const
  28: tvm::transform::Pass::operator()(tvm::IRModule, tvm::transform::PassContext const&) const
  27: tvm::transform::SequentialNode::operator()(tvm::IRModule, tvm::transform::PassContext const&) const
  26: tvm::transform::Pass::operator()(tvm::IRModule, tvm::transform::PassContext const&) const
  25: tvm::tir::transform::PrimFuncPassNode::operator()(tvm::IRModule, tvm::transform::PassContext const&) const
  24: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::TypedPackedFunc<tvm::tir::PrimFunc (tvm::tir::PrimFunc, tvm::IRModule, tvm::transform::PassContext)>::AssignTypedLambda<tvm::tir::transform::LowerThreadAllreduce()::{lambda(tvm::tir::PrimFunc, tvm::IRModule, tvm::transform::PassContext)#1}>(tvm::tir::transform::LowerThreadAllreduce()::{lambda(tvm::tir::PrimFunc, tvm::IRModule, tvm::transform::PassContext)#1})::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}> >::Call(tvm::runtime::PackedFuncObj const*, tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)
  23: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7r
  22: tvm::tir::ThreadAllreduceBuilder::VisitStmt_(tvm::tir::AttrStmtNode const*)
  21: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  20: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7r
  19: tvm::tir::ThreadAllreduceBuilder::VisitStmt_(tvm::tir::AllocateNode const*)
  18: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  17: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7r
  16: tvm::tir::ThreadAllreduceBuilder::VisitStmt_(tvm::tir::AllocateNode const*)
  15: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  14: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7r
  13: tvm::tir::ThreadAllreduceBuilder::VisitStmt_(tvm::tir::AllocateNode const*)
  12: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  11: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7r
  10: tvm::tir::ThreadAllreduceBuilder::VisitStmt_(tvm::tir::AttrStmtNode const*)
  9: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  8: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runt
  7: tvm::runtime::ObjectPtr<tvm::runtime::Object> tvm::runtime::Array<tvm::tir::Stmt, void>::MapHelper<tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1}, tvm::tir::Stmt>(tvm::runtime::ObjectPtr<tvm::runtime::Object>, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1})
  6: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  5: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7r
  4: tvm::tir::ThreadAllreduceBuilder::VisitStmt_(tvm::tir::AttrStmtNode const*)
  3: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  2: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runt
  1: tvm::tir::ThreadAllreduceBuilder::VisitStmt_(tvm::tir::EvaluateNode const*)
  0: tvm::tir::ThreadAllreduceBuilder::MakeAllreduce(tvm::tir::CallNode const*)
  File "/home2/xiachunwei/Software/tvm/src/tir/transforms/lower_thread_allreduce.cc", line 383
InternalError: Check failed: (!load_remap_.count(buffers[i]->data.get())) is false: 

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(64), T.int64(112), T.int64(112)), "float32"), p1: T.Buffer((T.int64(1), T.int64(64), T.int64(1), T.int64(1)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(64), T.int64(56), T.int64(56)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        pool_max_shared = T.alloc_buffer((T.int64(1), T.int64(64), T.int64(56), T.int64(56)), scope="shared")
        for ax0_ax1_ax2_ax3_0_fused in T.thread_binding(T.int64(25088), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 512, "pragma_unroll_explicit": 1}):
            for ax0, ax1, ax2, ax3, ax4_ax5_fused_0 in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(8), T.int64(2)):
                for ax4_ax5_fused_1 in T.thread_binding(T.int64(8), thread="threadIdx.x"):
                    with T.block("pool_max"):
                        v_ax0 = T.axis.spatial(T.int64(1), ax0)
                        v_ax1 = T.axis.spatial(T.int64(64), ax0_ax1_ax2_ax3_0_fused // T.int64(392) + ax1)
                        v_ax2 = T.axis.spatial(T.int64(56), ax0_ax1_ax2_ax3_0_fused % T.int64(392) // T.int64(7) + ax2)
                        v_ax3 = T.axis.spatial(T.int64(56), ax0_ax1_ax2_ax3_0_fused % T.int64(7) * T.int64(8) + ax3)
                        v_rv0 = T.axis.reduce(T.int64(3), (ax4_ax5_fused_0 * T.int64(8) + ax4_ax5_fused_1) // T.int64(3))
                        v_rv1 = T.axis.reduce(T.int64(3), (ax4_ax5_fused_0 * T.int64(8) + ax4_ax5_fused_1) % T.int64(3))
                        T.where(ax4_ax5_fused_0 * T.int64(8) + ax4_ax5_fused_1 < T.int64(9))
                        T.reads(p0[v_ax0, v_ax1, v_ax2 * T.int64(2) + v_rv0 - T.int64(1), v_ax3 * T.int64(2) + v_rv1 - T.int64(1)])
                        T.writes(pool_max_shared[v_ax0, v_ax1, v_ax2, v_ax3])
                        with T.init():
                            pool_max_shared[v_ax0, v_ax1, v_ax2, v_ax3] = T.float32(-3.4028234663852886e+38)
                        pool_max_shared[v_ax0, v_ax1, v_ax2, v_ax3] = T.max(pool_max_shared[v_ax0, v_ax1, v_ax2, v_ax3], T.if_then_else(T.int64(1) <= v_ax2 * T.int64(2) + v_rv0 and v_ax2 * T.int64(2) + v_rv0 < T.int64(113) and T.int64(1) <= v_ax3 * T.int64(2) + v_rv1 and v_ax3 * T.int64(2) + v_rv1 < T.int64(113), p0[v_ax0, v_ax1, v_ax2 * T.int64(2) + v_rv0 - T.int64(1), v_ax3 * T.int64(2) + v_rv1 - T.int64(1)], T.float32(-3.4028234663852886e+38)))
            for ax3_1 in T.thread_binding(T.int64(8), thread="threadIdx.x"):
                with T.block("T_add"):
                    v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                    v_ax1 = T.axis.spatial(T.int64(64), ax0_ax1_ax2_ax3_0_fused // T.int64(392))
                    v_ax2 = T.axis.spatial(T.int64(56), ax0_ax1_ax2_ax3_0_fused % T.int64(392) // T.int64(7))
                    v_ax3 = T.axis.spatial(T.int64(56), ax0_ax1_ax2_ax3_0_fused % T.int64(7) * T.int64(8) + ax3_1)
                    T.reads(pool_max_shared[v_ax0, v_ax1, v_ax2, v_ax3], p1[v_ax0, v_ax1, T.int64(0), T.int64(0)])
                    T.writes(T_relu[v_ax0, v_ax1, v_ax2, v_ax3])
                    T_relu[v_ax0, v_ax1, v_ax2, v_ax3] = T.max(pool_max_shared[v_ax0, v_ax1, v_ax2, v_ax3] + p1[v_ax0, v_ax1, T.int64(0), T.int64(0)], T.float32(0))
b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="pool_max", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b1, ann_key="schedule_rule")
sch.reverse_compute_inline(block=b3)
sch.compute_inline(block=b0)
b5, = sch.get_consumers(block=b1)
l6, l7, l8, l9 = sch.get_loops(block=b5)
v10 = sch.sample_categorical(candidates=[4, 8, 16, 32, 64, 128, 256, 512], probs=[0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125], decision=1)
l11, l12 = sch.split(loop=l9, factors=[None, v10], preserve_unit_iters=True)
sch.bind(loop=l12, thread_axis="threadIdx.x")
sch.compute_at(block=b1, loop=l11, preserve_unit_loops=True, index=-1)
sch.set_scope(block=b1, buffer_index=0, storage_scope="shared")
l13, l14, l15, l16, l17, l18, l19, l20, l21, l22 = sch.get_loops(block=b1)
l23 = sch.fuse(l21, l22, preserve_unit_iters=True)
l24, l25 = sch.split(loop=l23, factors=[None, v10], preserve_unit_iters=True)
sch.bind(loop=l25, thread_axis="threadIdx.x")
v26 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v26)
l27, l28, l29, l30, l31 = sch.get_loops(block=b2)
l32 = sch.fuse(l27, l28, l29, l30, preserve_unit_iters=True)
sch.bind(loop=l32, thread_axis="blockIdx.x")
sch.enter_postproc()
b33 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b33, ann_key="meta_schedule.unroll_explicit")
b34, b35 = sch.get_child_blocks(b33)
l36, l37, l38, l39, l40, l41, l42 = sch.get_loops(block=b34)
sch.annotate(block_or_loop=l36, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l36, ann_key="pragma_unroll_explicit", ann_val=1)
l43, l44 = sch.get_loops(block=b35)
2023-11-11 00:21:27 [INFO] [task_scheduler.cc:121] [Task #6: fused_nn_max_pool2d_add_nn_relu] Trial #9: Error in building:
LocalBuilder: An exception occurred
Traceback (most recent call last):
  File "/home2/xiachunwei/Software/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/home2/xiachunwei/Software/tvm/python/tvm/meta_schedule/builder/local_builder.py", line 165, in <lambda>
    lambda x: _worker_func(*x),
  File "/home2/xiachunwei/Software/tvm/python/tvm/meta_schedule/builder/local_builder.py", line 231, in _worker_func
    rt_mod: Module = f_build(mod, target, _deserialize_params(params))
  File "/home2/xiachunwei/Software/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/home2/xiachunwei/Software/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
  File "/home2/xiachunwei/Software/tvm/python/tvm/meta_schedule/builder/local_builder.py", line 261, in default_build
    return tvm_build(mod, target=target)
  File "/home2/xiachunwei/Software/tvm/python/tvm/driver/build_module.py", line 281, in build
    rt_mod_host = _driver_ffi.tir_to_runtime(annotated_mods, target_host)
  File "/home2/xiachunwei/Software/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/home2/xiachunwei/Software/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  33: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::TypedPackedFunc<tvm::runtime::Module (tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)>::AssignTypedLambda<tvm::{lambda(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)#6}>(tvm::{lambda(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)#6}, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}> >::Call(tvm::runtime::PackedFuncObj const*, tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)
  32: tvm::TIRToRuntime(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target const&)
  31: tvm::SplitMixedModule(tvm::IRModule, tvm::Target const&, tvm::Target const&)
  30: tvm::ApplyPasses(tvm::IRModule, tvm::transform::Sequential)
  29: tvm::transform::Pass::operator()(tvm::IRModule) const
  28: tvm::transform::Pass::operator()(tvm::IRModule, tvm::transform::PassContext const&) const
  27: tvm::transform::SequentialNode::operator()(tvm::IRModule, tvm::transform::PassContext const&) const
  26: tvm::transform::Pass::operator()(tvm::IRModule, tvm::transform::PassContext const&) const
  25: tvm::tir::transform::PrimFuncPassNode::operator()(tvm::IRModule, tvm::transform::PassContext const&) const
  24: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::TypedPackedFunc<tvm::tir::PrimFunc (tvm::tir::PrimFunc, tvm::IRModule, tvm::transform::PassContext)>::AssignTypedLambda<tvm::tir::transform::LowerThreadAllreduce()::{lambda(tvm::tir::PrimFunc, tvm::IRModule, tvm::transform::PassContext)#1}>(tvm::tir::transform::LowerThreadAllreduce()::{lambda(tvm::tir::PrimFunc, tvm::IRModule, tvm::transform::PassContext)#1})::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}> >::Call(tvm::runtime::PackedFuncObj const*, tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)
  23: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7r
  22: tvm::tir::ThreadAllreduceBuilder::VisitStmt_(tvm::tir::AttrStmtNode const*)
  21: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  20: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7r
  19: tvm::tir::ThreadAllreduceBuilder::VisitStmt_(tvm::tir::AllocateNode const*)
  18: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  17: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7r
  16: tvm::tir::ThreadAllreduceBuilder::VisitStmt_(tvm::tir::AllocateNode const*)
  15: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  14: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7r
  13: tvm::tir::ThreadAllreduceBuilder::VisitStmt_(tvm::tir::AllocateNode const*)
  12: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  11: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7r
  10: tvm::tir::ThreadAllreduceBuilder::VisitStmt_(tvm::tir::AttrStmtNode const*)
  9: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  8: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runt
  7: tvm::runtime::ObjectPtr<tvm::runtime::Object> tvm::runtime::Array<tvm::tir::Stmt, void>::MapHelper<tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1}, tvm::tir::Stmt>(tvm::runtime::ObjectPtr<tvm::runtime::Object>, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1})
  6: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  5: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7r
  4: tvm::tir::ThreadAllreduceBuilder::VisitStmt_(tvm::tir::AttrStmtNode const*)
  3: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  2: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runt
  1: tvm::tir::ThreadAllreduceBuilder::VisitStmt_(tvm::tir::EvaluateNode const*)
  0: tvm::tir::ThreadAllreduceBuilder::MakeAllreduce(tvm::tir::CallNode const*)
  File "/home2/xiachunwei/Software/tvm/src/tir/transforms/lower_thread_allreduce.cc", line 383
InternalError: Check failed: (!load_remap_.count(buffers[i]->data.get())) is false: 

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(64), T.int64(112), T.int64(112)), "float32"), p1: T.Buffer((T.int64(1), T.int64(64), T.int64(1), T.int64(1)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(64), T.int64(56), T.int64(56)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        pool_max_shared = T.alloc_buffer((T.int64(1), T.int64(64), T.int64(56), T.int64(56)), scope="shared")
        for ax0_ax1_ax2_ax3_0_fused in T.thread_binding(T.int64(3584), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 1024, "pragma_unroll_explicit": 1}):
            for ax0, ax1, ax2, ax3, ax4_ax5_fused_0 in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(56), T.int64(1)):
                for ax4_ax5_fused_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                    with T.block("pool_max"):
                        v_ax0 = T.axis.spatial(T.int64(1), ax0)
                        v_ax1 = T.axis.spatial(T.int64(64), ax0_ax1_ax2_ax3_0_fused // T.int64(56) + ax1)
                        v_ax2 = T.axis.spatial(T.int64(56), ax0_ax1_ax2_ax3_0_fused % T.int64(56) + ax2)
                        v_ax3 = T.axis.spatial(T.int64(56), ax3)
                        v_rv0 = T.axis.reduce(T.int64(3), (ax4_ax5_fused_0 * T.int64(256) + ax4_ax5_fused_1) // T.int64(3))
                        v_rv1 = T.axis.reduce(T.int64(3), (ax4_ax5_fused_0 * T.int64(256) + ax4_ax5_fused_1) % T.int64(3))
                        T.where(ax4_ax5_fused_0 * T.int64(256) + ax4_ax5_fused_1 < T.int64(9))
                        T.reads(p0[v_ax0, v_ax1, v_ax2 * T.int64(2) + v_rv0 - T.int64(1), v_ax3 * T.int64(2) + v_rv1 - T.int64(1)])
                        T.writes(pool_max_shared[v_ax0, v_ax1, v_ax2, v_ax3])
                        with T.init():
                            pool_max_shared[v_ax0, v_ax1, v_ax2, v_ax3] = T.float32(-3.4028234663852886e+38)
                        pool_max_shared[v_ax0, v_ax1, v_ax2, v_ax3] = T.max(pool_max_shared[v_ax0, v_ax1, v_ax2, v_ax3], T.if_then_else(T.int64(1) <= v_ax2 * T.int64(2) + v_rv0 and v_ax2 * T.int64(2) + v_rv0 < T.int64(113) and T.int64(1) <= v_ax3 * T.int64(2) + v_rv1 and v_ax3 * T.int64(2) + v_rv1 < T.int64(113), p0[v_ax0, v_ax1, v_ax2 * T.int64(2) + v_rv0 - T.int64(1), v_ax3 * T.int64(2) + v_rv1 - T.int64(1)], T.float32(-3.4028234663852886e+38)))
            for ax3_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                with T.block("T_add"):
                    v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                    v_ax1 = T.axis.spatial(T.int64(64), ax0_ax1_ax2_ax3_0_fused // T.int64(56))
                    v_ax2 = T.axis.spatial(T.int64(56), ax0_ax1_ax2_ax3_0_fused % T.int64(56))
                    v_ax3 = T.axis.spatial(T.int64(56), ax3_1)
                    T.where(T.Mul(T.int64(0), T.int64(256)) + ax3_1 < T.int64(56))
                    T.reads(pool_max_shared[v_ax0, v_ax1, v_ax2, v_ax3], p1[v_ax0, v_ax1, T.int64(0), T.int64(0)])
                    T.writes(T_relu[v_ax0, v_ax1, v_ax2, v_ax3])
                    T_relu[v_ax0, v_ax1, v_ax2, v_ax3] = T.max(pool_max_shared[v_ax0, v_ax1, v_ax2, v_ax3] + p1[v_ax0, v_ax1, T.int64(0), T.int64(0)], T.float32(0))
b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="pool_max", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b1, ann_key="schedule_rule")
sch.reverse_compute_inline(block=b3)
sch.compute_inline(block=b0)
b5, = sch.get_consumers(block=b1)
l6, l7, l8, l9 = sch.get_loops(block=b5)
v10 = sch.sample_categorical(candidates=[4, 8, 16, 32, 64, 128, 256, 512], probs=[0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125], decision=6)
l11, l12 = sch.split(loop=l9, factors=[None, v10], preserve_unit_iters=True)
sch.bind(loop=l12, thread_axis="threadIdx.x")
sch.compute_at(block=b1, loop=l11, preserve_unit_loops=True, index=-1)
sch.set_scope(block=b1, buffer_index=0, storage_scope="shared")
l13, l14, l15, l16, l17, l18, l19, l20, l21, l22 = sch.get_loops(block=b1)
l23 = sch.fuse(l21, l22, preserve_unit_iters=True)
l24, l25 = sch.split(loop=l23, factors=[None, v10], preserve_unit_iters=True)
sch.bind(loop=l25, thread_axis="threadIdx.x")
v26 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v26)
l27, l28, l29, l30, l31 = sch.get_loops(block=b2)
l32 = sch.fuse(l27, l28, l29, l30, preserve_unit_iters=True)
sch.bind(loop=l32, thread_axis="blockIdx.x")
sch.enter_postproc()
b33 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b33, ann_key="meta_schedule.unroll_explicit")
b34, b35 = sch.get_child_blocks(b33)
l36, l37, l38, l39, l40, l41, l42 = sch.get_loops(block=b34)
sch.annotate(block_or_loop=l36, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l36, ann_key="pragma_unroll_explicit", ann_val=1)
l43, l44 = sch.get_loops(block=b35)
2023-11-11 00:21:27 [INFO] [task_scheduler.cc:131] [Task #6: fused_nn_max_pool2d_add_nn_relu] Trial #10: GFLOPs: 272.1645. Time: 8.1118 us. Best GFLOPs: 272.1645
2023-11-11 00:21:27 [INFO] [task_scheduler.cc:131] [Task #6: fused_nn_max_pool2d_add_nn_relu] Trial #11: GFLOPs: 251.9119. Time: 8.7640 us. Best GFLOPs: 272.1645
2023-11-11 00:21:27 [INFO] [task_scheduler.cc:131] [Task #6: fused_nn_max_pool2d_add_nn_relu] Trial #12: GFLOPs: 211.0392. Time: 10.4613 us. Best GFLOPs: 272.1645
2023-11-11 00:21:27 [INFO] [task_scheduler.cc:131] [Task #6: fused_nn_max_pool2d_add_nn_relu] Trial #13: GFLOPs: 251.9056. Time: 8.7642 us. Best GFLOPs: 272.1645
2023-11-11 00:21:27 [INFO] [task_scheduler.cc:131] [Task #6: fused_nn_max_pool2d_add_nn_relu] Trial #14: GFLOPs: 183.8691. Time: 12.0071 us. Best GFLOPs: 272.1645
2023-11-11 00:21:27 [INFO] [task_scheduler.cc:131] [Task #6: fused_nn_max_pool2d_add_nn_relu] Trial #15: GFLOPs: 228.1764. Time: 9.6756 us. Best GFLOPs: 272.1645
2023-11-11 00:21:27 [INFO] [task_scheduler.cc:131] [Task #6: fused_nn_max_pool2d_add_nn_relu] Trial #16: GFLOPs: 284.2191. Time: 7.7678 us. Best GFLOPs: 284.2191
2023-11-11 00:21:27 [INFO] [task_scheduler.cc:131] [Task #6: fused_nn_max_pool2d_add_nn_relu] Trial #17: GFLOPs: 201.7019. Time: 10.9456 us. Best GFLOPs: 284.2191
2023-11-11 00:21:27 [INFO] [task_scheduler.cc:131] [Task #6: fused_nn_max_pool2d_add_nn_relu] Trial #18: GFLOPs: 288.9744. Time: 7.6399 us. Best GFLOPs: 288.9744
2023-11-11 00:21:27 [INFO] [task_scheduler.cc:131] [Task #6: fused_nn_max_pool2d_add_nn_relu] Trial #19: GFLOPs: 183.6915. Time: 12.0188 us. Best GFLOPs: 288.9744
2023-11-11 00:21:27 [INFO] [task_scheduler.cc:131] [Task #6: fused_nn_max_pool2d_add_nn_relu] Trial #20: GFLOPs: 201.5620. Time: 10.9532 us. Best GFLOPs: 288.9744
2023-11-11 00:21:27 [INFO] [task_scheduler.cc:131] [Task #6: fused_nn_max_pool2d_add_nn_relu] Trial #21: GFLOPs: 275.5402. Time: 8.0124 us. Best GFLOPs: 288.9744
2023-11-11 00:21:27 [INFO] [task_scheduler.cc:131] [Task #6: fused_nn_max_pool2d_add_nn_relu] Trial #22: GFLOPs: 275.6630. Time: 8.0089 us. Best GFLOPs: 288.9744
2023-11-11 00:21:27 [INFO] [task_scheduler.cc:131] [Task #6: fused_nn_max_pool2d_add_nn_relu] Trial #23: GFLOPs: 272.3656. Time: 8.1058 us. Best GFLOPs: 288.9744
2023-11-11 00:21:27 [INFO] [task_scheduler.cc:131] [Task #6: fused_nn_max_pool2d_add_nn_relu] Trial #24: GFLOPs: 177.2106. Time: 12.4583 us. Best GFLOPs: 288.9744
2023-11-11 00:21:27 [INFO] [task_scheduler.cc:131] [Task #6: fused_nn_max_pool2d_add_nn_relu] Trial #25: GFLOPs: 226.9248. Time: 9.7290 us. Best GFLOPs: 288.9744
2023-11-11 00:21:27 [INFO] [task_scheduler.cc:121] [Task #6: fused_nn_max_pool2d_add_nn_relu] Trial #26: Error in building:
LocalBuilder: An exception occurred
Traceback (most recent call last):
  File "/home2/xiachunwei/Software/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/home2/xiachunwei/Software/tvm/python/tvm/meta_schedule/builder/local_builder.py", line 165, in <lambda>
    lambda x: _worker_func(*x),
  File "/home2/xiachunwei/Software/tvm/python/tvm/meta_schedule/builder/local_builder.py", line 231, in _worker_func
    rt_mod: Module = f_build(mod, target, _deserialize_params(params))
  File "/home2/xiachunwei/Software/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/home2/xiachunwei/Software/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
  File "/home2/xiachunwei/Software/tvm/python/tvm/meta_schedule/builder/local_builder.py", line 261, in default_build
    return tvm_build(mod, target=target)
  File "/home2/xiachunwei/Software/tvm/python/tvm/driver/build_module.py", line 281, in build
    rt_mod_host = _driver_ffi.tir_to_runtime(annotated_mods, target_host)
  File "/home2/xiachunwei/Software/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/home2/xiachunwei/Software/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  33: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::TypedPackedFunc<tvm::runtime::Module (tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)>::AssignTypedLambda<tvm::{lambda(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)#6}>(tvm::{lambda(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)#6}, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}> >::Call(tvm::runtime::PackedFuncObj const*, tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)
  32: tvm::TIRToRuntime(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target const&)
  31: tvm::SplitMixedModule(tvm::IRModule, tvm::Target const&, tvm::Target const&)
  30: tvm::ApplyPasses(tvm::IRModule, tvm::transform::Sequential)
  29: tvm::transform::Pass::operator()(tvm::IRModule) const
  28: tvm::transform::Pass::operator()(tvm::IRModule, tvm::transform::PassContext const&) const
  27: tvm::transform::SequentialNode::operator()(tvm::IRModule, tvm::transform::PassContext const&) const
  26: tvm::transform::Pass::operator()(tvm::IRModule, tvm::transform::PassContext const&) const
  25: tvm::tir::transform::PrimFuncPassNode::operator()(tvm::IRModule, tvm::transform::PassContext const&) const
  24: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::TypedPackedFunc<tvm::tir::PrimFunc (tvm::tir::PrimFunc, tvm::IRModule, tvm::transform::PassContext)>::AssignTypedLambda<tvm::tir::transform::LowerThreadAllreduce()::{lambda(tvm::tir::PrimFunc, tvm::IRModule, tvm::transform::PassContext)#1}>(tvm::tir::transform::LowerThreadAllreduce()::{lambda(tvm::tir::PrimFunc, tvm::IRModule, tvm::transform::PassContext)#1})::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}> >::Call(tvm::runtime::PackedFuncObj const*, tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)
  23: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7r
  22: tvm::tir::ThreadAllreduceBuilder::VisitStmt_(tvm::tir::AttrStmtNode const*)
  21: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  20: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7r
  19: tvm::tir::ThreadAllreduceBuilder::VisitStmt_(tvm::tir::AllocateNode const*)
  18: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  17: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7r
  16: tvm::tir::ThreadAllreduceBuilder::VisitStmt_(tvm::tir::AllocateNode const*)
  15: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  14: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7r
  13: tvm::tir::ThreadAllreduceBuilder::VisitStmt_(tvm::tir::AllocateNode const*)
  12: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  11: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7r
  10: tvm::tir::ThreadAllreduceBuilder::VisitStmt_(tvm::tir::AttrStmtNode const*)
  9: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  8: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runt
  7: tvm::runtime::ObjectPtr<tvm::runtime::Object> tvm::runtime::Array<tvm::tir::Stmt, void>::MapHelper<tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1}, tvm::tir::Stmt>(tvm::runtime::ObjectPtr<tvm::runtime::Object>, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1})
  6: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  5: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7r
  4: tvm::tir::ThreadAllreduceBuilder::VisitStmt_(tvm::tir::AttrStmtNode const*)
  3: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  2: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runt
  1: tvm::tir::ThreadAllreduceBuilder::VisitStmt_(tvm::tir::EvaluateNode const*)
  0: tvm::tir::ThreadAllreduceBuilder::MakeAllreduce(tvm::tir::CallNode const*)
  File "/home2/xiachunwei/Software/tvm/src/tir/transforms/lower_thread_allreduce.cc", line 383
InternalError: Check failed: (!load_remap_.count(buffers[i]->data.get())) is false: 

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(64), T.int64(112), T.int64(112)), "float32"), p1: T.Buffer((T.int64(1), T.int64(64), T.int64(1), T.int64(1)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(64), T.int64(56), T.int64(56)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        pool_max_shared = T.alloc_buffer((T.int64(1), T.int64(64), T.int64(56), T.int64(56)), scope="shared")
        for ax0_ax1_ax2_ax3_0_fused in T.thread_binding(T.int64(3584), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 512, "pragma_unroll_explicit": 1}):
            for ax0, ax1, ax2, ax3, ax4_ax5_fused_0 in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(56), T.int64(1)):
                for ax4_ax5_fused_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                    with T.block("pool_max"):
                        v_ax0 = T.axis.spatial(T.int64(1), ax0)
                        v_ax1 = T.axis.spatial(T.int64(64), ax0_ax1_ax2_ax3_0_fused // T.int64(56) + ax1)
                        v_ax2 = T.axis.spatial(T.int64(56), ax0_ax1_ax2_ax3_0_fused % T.int64(56) + ax2)
                        v_ax3 = T.axis.spatial(T.int64(56), ax3)
                        v_rv0 = T.axis.reduce(T.int64(3), (ax4_ax5_fused_0 * T.int64(128) + ax4_ax5_fused_1) // T.int64(3))
                        v_rv1 = T.axis.reduce(T.int64(3), (ax4_ax5_fused_0 * T.int64(128) + ax4_ax5_fused_1) % T.int64(3))
                        T.where(ax4_ax5_fused_0 * T.int64(128) + ax4_ax5_fused_1 < T.int64(9))
                        T.reads(p0[v_ax0, v_ax1, v_ax2 * T.int64(2) + v_rv0 - T.int64(1), v_ax3 * T.int64(2) + v_rv1 - T.int64(1)])
                        T.writes(pool_max_shared[v_ax0, v_ax1, v_ax2, v_ax3])
                        with T.init():
                            pool_max_shared[v_ax0, v_ax1, v_ax2, v_ax3] = T.float32(-3.4028234663852886e+38)
                        pool_max_shared[v_ax0, v_ax1, v_ax2, v_ax3] = T.max(pool_max_shared[v_ax0, v_ax1, v_ax2, v_ax3], T.if_then_else(T.int64(1) <= v_ax2 * T.int64(2) + v_rv0 and v_ax2 * T.int64(2) + v_rv0 < T.int64(113) and T.int64(1) <= v_ax3 * T.int64(2) + v_rv1 and v_ax3 * T.int64(2) + v_rv1 < T.int64(113), p0[v_ax0, v_ax1, v_ax2 * T.int64(2) + v_rv0 - T.int64(1), v_ax3 * T.int64(2) + v_rv1 - T.int64(1)], T.float32(-3.4028234663852886e+38)))
            for ax3_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                with T.block("T_add"):
                    v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                    v_ax1 = T.axis.spatial(T.int64(64), ax0_ax1_ax2_ax3_0_fused // T.int64(56))
                    v_ax2 = T.axis.spatial(T.int64(56), ax0_ax1_ax2_ax3_0_fused % T.int64(56))
                    v_ax3 = T.axis.spatial(T.int64(56), ax3_1)
                    T.where(T.Mul(T.int64(0), T.int64(128)) + ax3_1 < T.int64(56))
                    T.reads(pool_max_shared[v_ax0, v_ax1, v_ax2, v_ax3], p1[v_ax0, v_ax1, T.int64(0), T.int64(0)])
                    T.writes(T_relu[v_ax0, v_ax1, v_ax2, v_ax3])
                    T_relu[v_ax0, v_ax1, v_ax2, v_ax3] = T.max(pool_max_shared[v_ax0, v_ax1, v_ax2, v_ax3] + p1[v_ax0, v_ax1, T.int64(0), T.int64(0)], T.float32(0))
b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="pool_max", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b1, ann_key="schedule_rule")
sch.reverse_compute_inline(block=b3)
sch.compute_inline(block=b0)
b5, = sch.get_consumers(block=b1)
l6, l7, l8, l9 = sch.get_loops(block=b5)
v10 = sch.sample_categorical(candidates=[4, 8, 16, 32, 64, 128, 256, 512], probs=[0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125], decision=5)
l11, l12 = sch.split(loop=l9, factors=[None, v10], preserve_unit_iters=True)
sch.bind(loop=l12, thread_axis="threadIdx.x")
sch.compute_at(block=b1, loop=l11, preserve_unit_loops=True, index=-1)
sch.set_scope(block=b1, buffer_index=0, storage_scope="shared")
l13, l14, l15, l16, l17, l18, l19, l20, l21, l22 = sch.get_loops(block=b1)
l23 = sch.fuse(l21, l22, preserve_unit_iters=True)
l24, l25 = sch.split(loop=l23, factors=[None, v10], preserve_unit_iters=True)
sch.bind(loop=l25, thread_axis="threadIdx.x")
v26 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v26)
l27, l28, l29, l30, l31 = sch.get_loops(block=b2)
l32 = sch.fuse(l27, l28, l29, l30, preserve_unit_iters=True)
sch.bind(loop=l32, thread_axis="blockIdx.x")
sch.enter_postproc()
b33 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b33, ann_key="meta_schedule.unroll_explicit")
b34, b35 = sch.get_child_blocks(b33)
l36, l37, l38, l39, l40, l41, l42 = sch.get_loops(block=b34)
sch.annotate(block_or_loop=l36, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l36, ann_key="pragma_unroll_explicit", ann_val=1)
l43, l44 = sch.get_loops(block=b35)
2023-11-11 00:21:27 [INFO] [task_scheduler.cc:131] [Task #6: fused_nn_max_pool2d_add_nn_relu] Trial #27: GFLOPs: 274.2628. Time: 8.0497 us. Best GFLOPs: 288.9744
2023-11-11 00:21:27 [INFO] [task_scheduler.cc:131] [Task #6: fused_nn_max_pool2d_add_nn_relu] Trial #28: GFLOPs: 220.5352. Time: 10.0108 us. Best GFLOPs: 288.9744
2023-11-11 00:21:27 [INFO] [task_scheduler.cc:131] [Task #6: fused_nn_max_pool2d_add_nn_relu] Trial #29: GFLOPs: 274.7768. Time: 8.0347 us. Best GFLOPs: 288.9744
2023-11-11 00:21:27 [INFO] [task_scheduler.cc:131] [Task #6: fused_nn_max_pool2d_add_nn_relu] Trial #30: GFLOPs: 251.8181. Time: 8.7672 us. Best GFLOPs: 288.9744
2023-11-11 00:21:27 [INFO] [task_scheduler.cc:131] [Task #6: fused_nn_max_pool2d_add_nn_relu] Trial #31: GFLOPs: 277.1658. Time: 7.9654 us. Best GFLOPs: 288.9744
2023-11-11 00:21:27 [INFO] [task_scheduler.cc:131] [Task #6: fused_nn_max_pool2d_add_nn_relu] Trial #32: GFLOPs: 277.9056. Time: 7.9442 us. Best GFLOPs: 288.9744
2023-11-11 00:21:27 [INFO] [task_scheduler.cc:131] [Task #6: fused_nn_max_pool2d_add_nn_relu] Trial #33: GFLOPs: 272.2996. Time: 8.1078 us. Best GFLOPs: 288.9744
2023-11-11 00:21:27 [INFO] [task_scheduler.cc:131] [Task #6: fused_nn_max_pool2d_add_nn_relu] Trial #34: GFLOPs: 260.4055. Time: 8.4781 us. Best GFLOPs: 288.9744
2023-11-11 00:21:27 [INFO] [task_scheduler.cc:131] [Task #6: fused_nn_max_pool2d_add_nn_relu] Trial #35: GFLOPs: 277.9065. Time: 7.9442 us. Best GFLOPs: 288.9744
2023-11-11 00:21:27 [INFO] [task_scheduler.cc:131] [Task #6: fused_nn_max_pool2d_add_nn_relu] Trial #36: GFLOPs: 272.0632. Time: 8.1148 us. Best GFLOPs: 288.9744
2023-11-11 00:21:27 [INFO] [task_scheduler.cc:121] [Task #6: fused_nn_max_pool2d_add_nn_relu] Trial #37: Error in building:
LocalBuilder: An exception occurred
Traceback (most recent call last):
  File "/home2/xiachunwei/Software/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/home2/xiachunwei/Software/tvm/python/tvm/meta_schedule/builder/local_builder.py", line 165, in <lambda>
    lambda x: _worker_func(*x),
  File "/home2/xiachunwei/Software/tvm/python/tvm/meta_schedule/builder/local_builder.py", line 231, in _worker_func
    rt_mod: Module = f_build(mod, target, _deserialize_params(params))
  File "/home2/xiachunwei/Software/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/home2/xiachunwei/Software/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
  File "/home2/xiachunwei/Software/tvm/python/tvm/meta_schedule/builder/local_builder.py", line 261, in default_build
    return tvm_build(mod, target=target)
  File "/home2/xiachunwei/Software/tvm/python/tvm/driver/build_module.py", line 281, in build
    rt_mod_host = _driver_ffi.tir_to_runtime(annotated_mods, target_host)
  File "/home2/xiachunwei/Software/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/home2/xiachunwei/Software/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  33: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::TypedPackedFunc<tvm::runtime::Module (tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)>::AssignTypedLambda<tvm::{lambda(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)#6}>(tvm::{lambda(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)#6}, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}> >::Call(tvm::runtime::PackedFuncObj const*, tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)
  32: tvm::TIRToRuntime(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target const&)
  31: tvm::SplitMixedModule(tvm::IRModule, tvm::Target const&, tvm::Target const&)
  30: tvm::ApplyPasses(tvm::IRModule, tvm::transform::Sequential)
  29: tvm::transform::Pass::operator()(tvm::IRModule) const
  28: tvm::transform::Pass::operator()(tvm::IRModule, tvm::transform::PassContext const&) const
  27: tvm::transform::SequentialNode::operator()(tvm::IRModule, tvm::transform::PassContext const&) const
  26: tvm::transform::Pass::operator()(tvm::IRModule, tvm::transform::PassContext const&) const
  25: tvm::tir::transform::PrimFuncPassNode::operator()(tvm::IRModule, tvm::transform::PassContext const&) const
  24: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::TypedPackedFunc<tvm::tir::PrimFunc (tvm::tir::PrimFunc, tvm::IRModule, tvm::transform::PassContext)>::AssignTypedLambda<tvm::tir::transform::LowerThreadAllreduce()::{lambda(tvm::tir::PrimFunc, tvm::IRModule, tvm::transform::PassContext)#1}>(tvm::tir::transform::LowerThreadAllreduce()::{lambda(tvm::tir::PrimFunc, tvm::IRModule, tvm::transform::PassContext)#1})::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}> >::Call(tvm::runtime::PackedFuncObj const*, tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)
  23: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7r
  22: tvm::tir::ThreadAllreduceBuilder::VisitStmt_(tvm::tir::AttrStmtNode const*)
  21: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  20: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7r
  19: tvm::tir::ThreadAllreduceBuilder::VisitStmt_(tvm::tir::AllocateNode const*)
  18: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  17: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7r
  16: tvm::tir::ThreadAllreduceBuilder::VisitStmt_(tvm::tir::AllocateNode const*)
  15: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  14: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7r
  13: tvm::tir::ThreadAllreduceBuilder::VisitStmt_(tvm::tir::AllocateNode const*)
  12: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  11: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7r
  10: tvm::tir::ThreadAllreduceBuilder::VisitStmt_(tvm::tir::AttrStmtNode const*)
  9: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  8: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runt
  7: tvm::runtime::ObjectPtr<tvm::runtime::Object> tvm::runtime::Array<tvm::tir::Stmt, void>::MapHelper<tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1}, tvm::tir::Stmt>(tvm::runtime::ObjectPtr<tvm::runtime::Object>, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1})
  6: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  5: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7r
  4: tvm::tir::ThreadAllreduceBuilder::VisitStmt_(tvm::tir::AttrStmtNode const*)
  3: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  2: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runt
  1: tvm::tir::ThreadAllreduceBuilder::VisitStmt_(tvm::tir::EvaluateNode const*)
  0: tvm::tir::ThreadAllreduceBuilder::MakeAllreduce(tvm::tir::CallNode const*)
  File "/home2/xiachunwei/Software/tvm/src/tir/transforms/lower_thread_allreduce.cc", line 383
InternalError: Check failed: (!load_remap_.count(buffers[i]->data.get())) is false: 

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(64), T.int64(112), T.int64(112)), "float32"), p1: T.Buffer((T.int64(1), T.int64(64), T.int64(1), T.int64(1)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(64), T.int64(56), T.int64(56)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        pool_max_shared = T.alloc_buffer((T.int64(1), T.int64(64), T.int64(56), T.int64(56)), scope="shared")
        for ax0_ax1_ax2_ax3_0_fused in T.thread_binding(T.int64(3584), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 512, "pragma_unroll_explicit": 1}):
            for ax0, ax1, ax2, ax3, ax4_ax5_fused_0 in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(56), T.int64(1)):
                for ax4_ax5_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                    with T.block("pool_max"):
                        v_ax0 = T.axis.spatial(T.int64(1), ax0)
                        v_ax1 = T.axis.spatial(T.int64(64), ax0_ax1_ax2_ax3_0_fused // T.int64(56) + ax1)
                        v_ax2 = T.axis.spatial(T.int64(56), ax0_ax1_ax2_ax3_0_fused % T.int64(56) + ax2)
                        v_ax3 = T.axis.spatial(T.int64(56), ax3)
                        v_rv0 = T.axis.reduce(T.int64(3), (ax4_ax5_fused_0 * T.int64(64) + ax4_ax5_fused_1) // T.int64(3))
                        v_rv1 = T.axis.reduce(T.int64(3), (ax4_ax5_fused_0 * T.int64(64) + ax4_ax5_fused_1) % T.int64(3))
                        T.where(ax4_ax5_fused_0 * T.int64(64) + ax4_ax5_fused_1 < T.int64(9))
                        T.reads(p0[v_ax0, v_ax1, v_ax2 * T.int64(2) + v_rv0 - T.int64(1), v_ax3 * T.int64(2) + v_rv1 - T.int64(1)])
                        T.writes(pool_max_shared[v_ax0, v_ax1, v_ax2, v_ax3])
                        with T.init():
                            pool_max_shared[v_ax0, v_ax1, v_ax2, v_ax3] = T.float32(-3.4028234663852886e+38)
                        pool_max_shared[v_ax0, v_ax1, v_ax2, v_ax3] = T.max(pool_max_shared[v_ax0, v_ax1, v_ax2, v_ax3], T.if_then_else(T.int64(1) <= v_ax2 * T.int64(2) + v_rv0 and v_ax2 * T.int64(2) + v_rv0 < T.int64(113) and T.int64(1) <= v_ax3 * T.int64(2) + v_rv1 and v_ax3 * T.int64(2) + v_rv1 < T.int64(113), p0[v_ax0, v_ax1, v_ax2 * T.int64(2) + v_rv0 - T.int64(1), v_ax3 * T.int64(2) + v_rv1 - T.int64(1)], T.float32(-3.4028234663852886e+38)))
            for ax3_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                with T.block("T_add"):
                    v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                    v_ax1 = T.axis.spatial(T.int64(64), ax0_ax1_ax2_ax3_0_fused // T.int64(56))
                    v_ax2 = T.axis.spatial(T.int64(56), ax0_ax1_ax2_ax3_0_fused % T.int64(56))
                    v_ax3 = T.axis.spatial(T.int64(56), ax3_1)
                    T.where(T.Mul(T.int64(0), T.int64(64)) + ax3_1 < T.int64(56))
                    T.reads(pool_max_shared[v_ax0, v_ax1, v_ax2, v_ax3], p1[v_ax0, v_ax1, T.int64(0), T.int64(0)])
                    T.writes(T_relu[v_ax0, v_ax1, v_ax2, v_ax3])
                    T_relu[v_ax0, v_ax1, v_ax2, v_ax3] = T.max(pool_max_shared[v_ax0, v_ax1, v_ax2, v_ax3] + p1[v_ax0, v_ax1, T.int64(0), T.int64(0)], T.float32(0))
b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="pool_max", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b1, ann_key="schedule_rule")
sch.reverse_compute_inline(block=b3)
sch.compute_inline(block=b0)
b5, = sch.get_consumers(block=b1)
l6, l7, l8, l9 = sch.get_loops(block=b5)
v10 = sch.sample_categorical(candidates=[4, 8, 16, 32, 64, 128, 256, 512], probs=[0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125], decision=4)
l11, l12 = sch.split(loop=l9, factors=[None, v10], preserve_unit_iters=True)
sch.bind(loop=l12, thread_axis="threadIdx.x")
sch.compute_at(block=b1, loop=l11, preserve_unit_loops=True, index=-1)
sch.set_scope(block=b1, buffer_index=0, storage_scope="shared")
l13, l14, l15, l16, l17, l18, l19, l20, l21, l22 = sch.get_loops(block=b1)
l23 = sch.fuse(l21, l22, preserve_unit_iters=True)
l24, l25 = sch.split(loop=l23, factors=[None, v10], preserve_unit_iters=True)
sch.bind(loop=l25, thread_axis="threadIdx.x")
v26 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v26)
l27, l28, l29, l30, l31 = sch.get_loops(block=b2)
l32 = sch.fuse(l27, l28, l29, l30, preserve_unit_iters=True)
sch.bind(loop=l32, thread_axis="blockIdx.x")
sch.enter_postproc()
b33 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b33, ann_key="meta_schedule.unroll_explicit")
b34, b35 = sch.get_child_blocks(b33)
l36, l37, l38, l39, l40, l41, l42 = sch.get_loops(block=b34)
sch.annotate(block_or_loop=l36, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l36, ann_key="pragma_unroll_explicit", ann_val=1)
l43, l44 = sch.get_loops(block=b35)
2023-11-11 00:21:27 [INFO] [task_scheduler.cc:131] [Task #6: fused_nn_max_pool2d_add_nn_relu] Trial #38: GFLOPs: 276.3412. Time: 7.9892 us. Best GFLOPs: 288.9744
2023-11-11 00:21:27 [INFO] [task_scheduler.cc:131] [Task #6: fused_nn_max_pool2d_add_nn_relu] Trial #39: GFLOPs: 274.7210. Time: 8.0363 us. Best GFLOPs: 288.9744
2023-11-11 00:21:27 [INFO] [task_scheduler.cc:131] [Task #6: fused_nn_max_pool2d_add_nn_relu] Trial #40: GFLOPs: 184.9082. Time: 11.9397 us. Best GFLOPs: 288.9744
2023-11-11 00:21:27 [INFO] [task_scheduler.cc:131] [Task #6: fused_nn_max_pool2d_add_nn_relu] Trial #41: GFLOPs: 275.6047. Time: 8.0105 us. Best GFLOPs: 288.9744
2023-11-11 00:21:27 [INFO] [task_scheduler.cc:131] [Task #6: fused_nn_max_pool2d_add_nn_relu] Trial #42: GFLOPs: 284.7091. Time: 7.7544 us. Best GFLOPs: 288.9744
2023-11-11 00:21:27 [INFO] [task_scheduler.cc:131] [Task #6: fused_nn_max_pool2d_add_nn_relu] Trial #43: GFLOPs: 272.2722. Time: 8.1086 us. Best GFLOPs: 288.9744
2023-11-11 00:21:27 [INFO] [task_scheduler.cc:131] [Task #6: fused_nn_max_pool2d_add_nn_relu] Trial #44: GFLOPs: 13.2481. Time: 166.6459 us. Best GFLOPs: 288.9744
2023-11-11 00:21:27 [INFO] [task_scheduler.cc:121] [Task #6: fused_nn_max_pool2d_add_nn_relu] Trial #45: Error in building:
LocalBuilder: An exception occurred
Traceback (most recent call last):
  File "/home2/xiachunwei/Software/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/home2/xiachunwei/Software/tvm/python/tvm/meta_schedule/builder/local_builder.py", line 165, in <lambda>
    lambda x: _worker_func(*x),
  File "/home2/xiachunwei/Software/tvm/python/tvm/meta_schedule/builder/local_builder.py", line 231, in _worker_func
    rt_mod: Module = f_build(mod, target, _deserialize_params(params))
  File "/home2/xiachunwei/Software/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/home2/xiachunwei/Software/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
  File "/home2/xiachunwei/Software/tvm/python/tvm/meta_schedule/builder/local_builder.py", line 261, in default_build
    return tvm_build(mod, target=target)
  File "/home2/xiachunwei/Software/tvm/python/tvm/driver/build_module.py", line 281, in build
    rt_mod_host = _driver_ffi.tir_to_runtime(annotated_mods, target_host)
  File "/home2/xiachunwei/Software/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/home2/xiachunwei/Software/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  33: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::TypedPackedFunc<tvm::runtime::Module (tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)>::AssignTypedLambda<tvm::{lambda(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)#6}>(tvm::{lambda(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)#6}, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}> >::Call(tvm::runtime::PackedFuncObj const*, tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)
  32: tvm::TIRToRuntime(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target const&)
  31: tvm::SplitMixedModule(tvm::IRModule, tvm::Target const&, tvm::Target const&)
  30: tvm::ApplyPasses(tvm::IRModule, tvm::transform::Sequential)
  29: tvm::transform::Pass::operator()(tvm::IRModule) const
  28: tvm::transform::Pass::operator()(tvm::IRModule, tvm::transform::PassContext const&) const
  27: tvm::transform::SequentialNode::operator()(tvm::IRModule, tvm::transform::PassContext const&) const
  26: tvm::transform::Pass::operator()(tvm::IRModule, tvm::transform::PassContext const&) const
  25: tvm::tir::transform::PrimFuncPassNode::operator()(tvm::IRModule, tvm::transform::PassContext const&) const
  24: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::TypedPackedFunc<tvm::tir::PrimFunc (tvm::tir::PrimFunc, tvm::IRModule, tvm::transform::PassContext)>::AssignTypedLambda<tvm::tir::transform::LowerThreadAllreduce()::{lambda(tvm::tir::PrimFunc, tvm::IRModule, tvm::transform::PassContext)#1}>(tvm::tir::transform::LowerThreadAllreduce()::{lambda(tvm::tir::PrimFunc, tvm::IRModule, tvm::transform::PassContext)#1})::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}> >::Call(tvm::runtime::PackedFuncObj const*, tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)
  23: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7r
  22: tvm::tir::ThreadAllreduceBuilder::VisitStmt_(tvm::tir::AttrStmtNode const*)
  21: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  20: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7r
  19: tvm::tir::ThreadAllreduceBuilder::VisitStmt_(tvm::tir::AllocateNode const*)
  18: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  17: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7r
  16: tvm::tir::ThreadAllreduceBuilder::VisitStmt_(tvm::tir::AllocateNode const*)
  15: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  14: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7r
  13: tvm::tir::ThreadAllreduceBuilder::VisitStmt_(tvm::tir::AllocateNode const*)
  12: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  11: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7r
  10: tvm::tir::ThreadAllreduceBuilder::VisitStmt_(tvm::tir::AttrStmtNode const*)
  9: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  8: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runt
  7: tvm::runtime::ObjectPtr<tvm::runtime::Object> tvm::runtime::Array<tvm::tir::Stmt, void>::MapHelper<tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1}, tvm::tir::Stmt>(tvm::runtime::ObjectPtr<tvm::runtime::Object>, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1})
  6: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  5: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7r
  4: tvm::tir::ThreadAllreduceBuilder::VisitStmt_(tvm::tir::AttrStmtNode const*)
  3: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  2: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runt
  1: tvm::tir::ThreadAllreduceBuilder::VisitStmt_(tvm::tir::EvaluateNode const*)
  0: tvm::tir::ThreadAllreduceBuilder::MakeAllreduce(tvm::tir::CallNode const*)
  File "/home2/xiachunwei/Software/tvm/src/tir/transforms/lower_thread_allreduce.cc", line 383
InternalError: Check failed: (!load_remap_.count(buffers[i]->data.get())) is false: 

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(64), T.int64(112), T.int64(112)), "float32"), p1: T.Buffer((T.int64(1), T.int64(64), T.int64(1), T.int64(1)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(64), T.int64(56), T.int64(56)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        pool_max_shared = T.alloc_buffer((T.int64(1), T.int64(64), T.int64(56), T.int64(56)), scope="shared")
        for ax0_ax1_ax2_ax3_0_fused in T.thread_binding(T.int64(3584), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 1024, "pragma_unroll_explicit": 1}):
            for ax0, ax1, ax2, ax3, ax4_ax5_fused_0 in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(56), T.int64(1)):
                for ax4_ax5_fused_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                    with T.block("pool_max"):
                        v_ax0 = T.axis.spatial(T.int64(1), ax0)
                        v_ax1 = T.axis.spatial(T.int64(64), ax0_ax1_ax2_ax3_0_fused // T.int64(56) + ax1)
                        v_ax2 = T.axis.spatial(T.int64(56), ax0_ax1_ax2_ax3_0_fused % T.int64(56) + ax2)
                        v_ax3 = T.axis.spatial(T.int64(56), ax3)
                        v_rv0 = T.axis.reduce(T.int64(3), (ax4_ax5_fused_0 * T.int64(128) + ax4_ax5_fused_1) // T.int64(3))
                        v_rv1 = T.axis.reduce(T.int64(3), (ax4_ax5_fused_0 * T.int64(128) + ax4_ax5_fused_1) % T.int64(3))
                        T.where(ax4_ax5_fused_0 * T.int64(128) + ax4_ax5_fused_1 < T.int64(9))
                        T.reads(p0[v_ax0, v_ax1, v_ax2 * T.int64(2) + v_rv0 - T.int64(1), v_ax3 * T.int64(2) + v_rv1 - T.int64(1)])
                        T.writes(pool_max_shared[v_ax0, v_ax1, v_ax2, v_ax3])
                        with T.init():
                            pool_max_shared[v_ax0, v_ax1, v_ax2, v_ax3] = T.float32(-3.4028234663852886e+38)
                        pool_max_shared[v_ax0, v_ax1, v_ax2, v_ax3] = T.max(pool_max_shared[v_ax0, v_ax1, v_ax2, v_ax3], T.if_then_else(T.int64(1) <= v_ax2 * T.int64(2) + v_rv0 and v_ax2 * T.int64(2) + v_rv0 < T.int64(113) and T.int64(1) <= v_ax3 * T.int64(2) + v_rv1 and v_ax3 * T.int64(2) + v_rv1 < T.int64(113), p0[v_ax0, v_ax1, v_ax2 * T.int64(2) + v_rv0 - T.int64(1), v_ax3 * T.int64(2) + v_rv1 - T.int64(1)], T.float32(-3.4028234663852886e+38)))
            for ax3_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                with T.block("T_add"):
                    v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                    v_ax1 = T.axis.spatial(T.int64(64), ax0_ax1_ax2_ax3_0_fused // T.int64(56))
                    v_ax2 = T.axis.spatial(T.int64(56), ax0_ax1_ax2_ax3_0_fused % T.int64(56))
                    v_ax3 = T.axis.spatial(T.int64(56), ax3_1)
                    T.where(T.Mul(T.int64(0), T.int64(128)) + ax3_1 < T.int64(56))
                    T.reads(pool_max_shared[v_ax0, v_ax1, v_ax2, v_ax3], p1[v_ax0, v_ax1, T.int64(0), T.int64(0)])
                    T.writes(T_relu[v_ax0, v_ax1, v_ax2, v_ax3])
                    T_relu[v_ax0, v_ax1, v_ax2, v_ax3] = T.max(pool_max_shared[v_ax0, v_ax1, v_ax2, v_ax3] + p1[v_ax0, v_ax1, T.int64(0), T.int64(0)], T.float32(0))
b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="pool_max", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b1, ann_key="schedule_rule")
sch.reverse_compute_inline(block=b3)
sch.compute_inline(block=b0)
b5, = sch.get_consumers(block=b1)
l6, l7, l8, l9 = sch.get_loops(block=b5)
v10 = sch.sample_categorical(candidates=[4, 8, 16, 32, 64, 128, 256, 512], probs=[0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125], decision=5)
l11, l12 = sch.split(loop=l9, factors=[None, v10], preserve_unit_iters=True)
sch.bind(loop=l12, thread_axis="threadIdx.x")
sch.compute_at(block=b1, loop=l11, preserve_unit_loops=True, index=-1)
sch.set_scope(block=b1, buffer_index=0, storage_scope="shared")
l13, l14, l15, l16, l17, l18, l19, l20, l21, l22 = sch.get_loops(block=b1)
l23 = sch.fuse(l21, l22, preserve_unit_iters=True)
l24, l25 = sch.split(loop=l23, factors=[None, v10], preserve_unit_iters=True)
sch.bind(loop=l25, thread_axis="threadIdx.x")
v26 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v26)
l27, l28, l29, l30, l31 = sch.get_loops(block=b2)
l32 = sch.fuse(l27, l28, l29, l30, preserve_unit_iters=True)
sch.bind(loop=l32, thread_axis="blockIdx.x")
sch.enter_postproc()
b33 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b33, ann_key="meta_schedule.unroll_explicit")
b34, b35 = sch.get_child_blocks(b33)
l36, l37, l38, l39, l40, l41, l42 = sch.get_loops(block=b34)
sch.annotate(block_or_loop=l36, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l36, ann_key="pragma_unroll_explicit", ann_val=1)
l43, l44 = sch.get_loops(block=b35)
2023-11-11 00:21:27 [INFO] [task_scheduler.cc:131] [Task #6: fused_nn_max_pool2d_add_nn_relu] Trial #46: GFLOPs: 192.9486. Time: 11.4421 us. Best GFLOPs: 288.9744
2023-11-11 00:21:27 [INFO] [task_scheduler.cc:131] [Task #6: fused_nn_max_pool2d_add_nn_relu] Trial #47: GFLOPs: 274.1124. Time: 8.0542 us. Best GFLOPs: 288.9744
2023-11-11 00:21:27 [INFO] [task_scheduler.cc:121] [Task #6: fused_nn_max_pool2d_add_nn_relu] Trial #48: Error in building:
LocalBuilder: An exception occurred
Traceback (most recent call last):
  File "/home2/xiachunwei/Software/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/home2/xiachunwei/Software/tvm/python/tvm/meta_schedule/builder/local_builder.py", line 165, in <lambda>
    lambda x: _worker_func(*x),
  File "/home2/xiachunwei/Software/tvm/python/tvm/meta_schedule/builder/local_builder.py", line 231, in _worker_func
    rt_mod: Module = f_build(mod, target, _deserialize_params(params))
  File "/home2/xiachunwei/Software/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/home2/xiachunwei/Software/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
  File "/home2/xiachunwei/Software/tvm/python/tvm/meta_schedule/builder/local_builder.py", line 261, in default_build
    return tvm_build(mod, target=target)
  File "/home2/xiachunwei/Software/tvm/python/tvm/driver/build_module.py", line 281, in build
    rt_mod_host = _driver_ffi.tir_to_runtime(annotated_mods, target_host)
  File "/home2/xiachunwei/Software/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/home2/xiachunwei/Software/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  33: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::TypedPackedFunc<tvm::runtime::Module (tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)>::AssignTypedLambda<tvm::{lambda(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)#6}>(tvm::{lambda(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)#6}, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}> >::Call(tvm::runtime::PackedFuncObj const*, tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)
  32: tvm::TIRToRuntime(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target const&)
  31: tvm::SplitMixedModule(tvm::IRModule, tvm::Target const&, tvm::Target const&)
  30: tvm::ApplyPasses(tvm::IRModule, tvm::transform::Sequential)
  29: tvm::transform::Pass::operator()(tvm::IRModule) const
  28: tvm::transform::Pass::operator()(tvm::IRModule, tvm::transform::PassContext const&) const
  27: tvm::transform::SequentialNode::operator()(tvm::IRModule, tvm::transform::PassContext const&) const
  26: tvm::transform::Pass::operator()(tvm::IRModule, tvm::transform::PassContext const&) const
  25: tvm::tir::transform::PrimFuncPassNode::operator()(tvm::IRModule, tvm::transform::PassContext const&) const
  24: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::TypedPackedFunc<tvm::tir::PrimFunc (tvm::tir::PrimFunc, tvm::IRModule, tvm::transform::PassContext)>::AssignTypedLambda<tvm::tir::transform::LowerThreadAllreduce()::{lambda(tvm::tir::PrimFunc, tvm::IRModule, tvm::transform::PassContext)#1}>(tvm::tir::transform::LowerThreadAllreduce()::{lambda(tvm::tir::PrimFunc, tvm::IRModule, tvm::transform::PassContext)#1})::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}> >::Call(tvm::runtime::PackedFuncObj const*, tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)
  23: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7r
  22: tvm::tir::ThreadAllreduceBuilder::VisitStmt_(tvm::tir::AttrStmtNode const*)
  21: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  20: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7r
  19: tvm::tir::ThreadAllreduceBuilder::VisitStmt_(tvm::tir::AllocateNode const*)
  18: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  17: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7r
  16: tvm::tir::ThreadAllreduceBuilder::VisitStmt_(tvm::tir::AllocateNode const*)
  15: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  14: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7r
  13: tvm::tir::ThreadAllreduceBuilder::VisitStmt_(tvm::tir::AllocateNode const*)
  12: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  11: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7r
  10: tvm::tir::ThreadAllreduceBuilder::VisitStmt_(tvm::tir::AttrStmtNode const*)
  9: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  8: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runt
  7: tvm::runtime::ObjectPtr<tvm::runtime::Object> tvm::runtime::Array<tvm::tir::Stmt, void>::MapHelper<tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1}, tvm::tir::Stmt>(tvm::runtime::ObjectPtr<tvm::runtime::Object>, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1})
  6: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  5: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7r
  4: tvm::tir::ThreadAllreduceBuilder::VisitStmt_(tvm::tir::AttrStmtNode const*)
  3: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  2: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runt
  1: tvm::tir::ThreadAllreduceBuilder::VisitStmt_(tvm::tir::EvaluateNode const*)
  0: tvm::tir::ThreadAllreduceBuilder::MakeAllreduce(tvm::tir::CallNode const*)
  File "/home2/xiachunwei/Software/tvm/src/tir/transforms/lower_thread_allreduce.cc", line 383
InternalError: Check failed: (!load_remap_.count(buffers[i]->data.get())) is false: 

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(64), T.int64(112), T.int64(112)), "float32"), p1: T.Buffer((T.int64(1), T.int64(64), T.int64(1), T.int64(1)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(64), T.int64(56), T.int64(56)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        pool_max_shared = T.alloc_buffer((T.int64(1), T.int64(64), T.int64(56), T.int64(56)), scope="shared")
        for ax0_ax1_ax2_ax3_0_fused in T.thread_binding(T.int64(3584), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 1024, "pragma_unroll_explicit": 1}):
            for ax0, ax1, ax2, ax3, ax4_ax5_fused_0 in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(56), T.int64(1)):
                for ax4_ax5_fused_1 in T.thread_binding(T.int64(512), thread="threadIdx.x"):
                    with T.block("pool_max"):
                        v_ax0 = T.axis.spatial(T.int64(1), ax0)
                        v_ax1 = T.axis.spatial(T.int64(64), ax0_ax1_ax2_ax3_0_fused // T.int64(56) + ax1)
                        v_ax2 = T.axis.spatial(T.int64(56), ax0_ax1_ax2_ax3_0_fused % T.int64(56) + ax2)
                        v_ax3 = T.axis.spatial(T.int64(56), ax3)
                        v_rv0 = T.axis.reduce(T.int64(3), (ax4_ax5_fused_0 * T.int64(512) + ax4_ax5_fused_1) // T.int64(3))
                        v_rv1 = T.axis.reduce(T.int64(3), (ax4_ax5_fused_0 * T.int64(512) + ax4_ax5_fused_1) % T.int64(3))
                        T.where(ax4_ax5_fused_0 * T.int64(512) + ax4_ax5_fused_1 < T.int64(9))
                        T.reads(p0[v_ax0, v_ax1, v_ax2 * T.int64(2) + v_rv0 - T.int64(1), v_ax3 * T.int64(2) + v_rv1 - T.int64(1)])
                        T.writes(pool_max_shared[v_ax0, v_ax1, v_ax2, v_ax3])
                        with T.init():
                            pool_max_shared[v_ax0, v_ax1, v_ax2, v_ax3] = T.float32(-3.4028234663852886e+38)
                        pool_max_shared[v_ax0, v_ax1, v_ax2, v_ax3] = T.max(pool_max_shared[v_ax0, v_ax1, v_ax2, v_ax3], T.if_then_else(T.int64(1) <= v_ax2 * T.int64(2) + v_rv0 and v_ax2 * T.int64(2) + v_rv0 < T.int64(113) and T.int64(1) <= v_ax3 * T.int64(2) + v_rv1 and v_ax3 * T.int64(2) + v_rv1 < T.int64(113), p0[v_ax0, v_ax1, v_ax2 * T.int64(2) + v_rv0 - T.int64(1), v_ax3 * T.int64(2) + v_rv1 - T.int64(1)], T.float32(-3.4028234663852886e+38)))
            for ax3_1 in T.thread_binding(T.int64(512), thread="threadIdx.x"):
                with T.block("T_add"):
                    v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                    v_ax1 = T.axis.spatial(T.int64(64), ax0_ax1_ax2_ax3_0_fused // T.int64(56))
                    v_ax2 = T.axis.spatial(T.int64(56), ax0_ax1_ax2_ax3_0_fused % T.int64(56))
                    v_ax3 = T.axis.spatial(T.int64(56), ax3_1)
                    T.where(T.Mul(T.int64(0), T.int64(512)) + ax3_1 < T.int64(56))
                    T.reads(pool_max_shared[v_ax0, v_ax1, v_ax2, v_ax3], p1[v_ax0, v_ax1, T.int64(0), T.int64(0)])
                    T.writes(T_relu[v_ax0, v_ax1, v_ax2, v_ax3])
                    T_relu[v_ax0, v_ax1, v_ax2, v_ax3] = T.max(pool_max_shared[v_ax0, v_ax1, v_ax2, v_ax3] + p1[v_ax0, v_ax1, T.int64(0), T.int64(0)], T.float32(0))
b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="pool_max", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b1, ann_key="schedule_rule")
sch.reverse_compute_inline(block=b3)
sch.compute_inline(block=b0)
b5, = sch.get_consumers(block=b1)
l6, l7, l8, l9 = sch.get_loops(block=b5)
v10 = sch.sample_categorical(candidates=[4, 8, 16, 32, 64, 128, 256, 512], probs=[0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125], decision=7)
l11, l12 = sch.split(loop=l9, factors=[None, v10], preserve_unit_iters=True)
sch.bind(loop=l12, thread_axis="threadIdx.x")
sch.compute_at(block=b1, loop=l11, preserve_unit_loops=True, index=-1)
sch.set_scope(block=b1, buffer_index=0, storage_scope="shared")
l13, l14, l15, l16, l17, l18, l19, l20, l21, l22 = sch.get_loops(block=b1)
l23 = sch.fuse(l21, l22, preserve_unit_iters=True)
l24, l25 = sch.split(loop=l23, factors=[None, v10], preserve_unit_iters=True)
sch.bind(loop=l25, thread_axis="threadIdx.x")
v26 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v26)
l27, l28, l29, l30, l31 = sch.get_loops(block=b2)
l32 = sch.fuse(l27, l28, l29, l30, preserve_unit_iters=True)
sch.bind(loop=l32, thread_axis="blockIdx.x")
sch.enter_postproc()
b33 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b33, ann_key="meta_schedule.unroll_explicit")
b34, b35 = sch.get_child_blocks(b33)
l36, l37, l38, l39, l40, l41, l42 = sch.get_loops(block=b34)
sch.annotate(block_or_loop=l36, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l36, ann_key="pragma_unroll_explicit", ann_val=1)
l43, l44 = sch.get_loops(block=b35)
2023-11-11 00:21:27 [INFO] [task_scheduler.cc:131] [Task #6: fused_nn_max_pool2d_add_nn_relu] Trial #49: GFLOPs: 251.5577. Time: 8.7763 us. Best GFLOPs: 288.9744
2023-11-11 00:21:27 [INFO] [task_scheduler.cc:121] [Task #6: fused_nn_max_pool2d_add_nn_relu] Trial #50: Error in building:
LocalBuilder: An exception occurred
Traceback (most recent call last):
  File "/home2/xiachunwei/Software/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/home2/xiachunwei/Software/tvm/python/tvm/meta_schedule/builder/local_builder.py", line 165, in <lambda>
    lambda x: _worker_func(*x),
  File "/home2/xiachunwei/Software/tvm/python/tvm/meta_schedule/builder/local_builder.py", line 231, in _worker_func
    rt_mod: Module = f_build(mod, target, _deserialize_params(params))
  File "/home2/xiachunwei/Software/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/home2/xiachunwei/Software/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
  File "/home2/xiachunwei/Software/tvm/python/tvm/meta_schedule/builder/local_builder.py", line 261, in default_build
    return tvm_build(mod, target=target)
  File "/home2/xiachunwei/Software/tvm/python/tvm/driver/build_module.py", line 281, in build
    rt_mod_host = _driver_ffi.tir_to_runtime(annotated_mods, target_host)
  File "/home2/xiachunwei/Software/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/home2/xiachunwei/Software/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  33: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::TypedPackedFunc<tvm::runtime::Module (tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)>::AssignTypedLambda<tvm::{lambda(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)#6}>(tvm::{lambda(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)#6}, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}> >::Call(tvm::runtime::PackedFuncObj const*, tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)
  32: tvm::TIRToRuntime(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target const&)
  31: tvm::SplitMixedModule(tvm::IRModule, tvm::Target const&, tvm::Target const&)
  30: tvm::ApplyPasses(tvm::IRModule, tvm::transform::Sequential)
  29: tvm::transform::Pass::operator()(tvm::IRModule) const
  28: tvm::transform::Pass::operator()(tvm::IRModule, tvm::transform::PassContext const&) const
  27: tvm::transform::SequentialNode::operator()(tvm::IRModule, tvm::transform::PassContext const&) const
  26: tvm::transform::Pass::operator()(tvm::IRModule, tvm::transform::PassContext const&) const
  25: tvm::tir::transform::PrimFuncPassNode::operator()(tvm::IRModule, tvm::transform::PassContext const&) const
  24: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::TypedPackedFunc<tvm::tir::PrimFunc (tvm::tir::PrimFunc, tvm::IRModule, tvm::transform::PassContext)>::AssignTypedLambda<tvm::tir::transform::LowerThreadAllreduce()::{lambda(tvm::tir::PrimFunc, tvm::IRModule, tvm::transform::PassContext)#1}>(tvm::tir::transform::LowerThreadAllreduce()::{lambda(tvm::tir::PrimFunc, tvm::IRModule, tvm::transform::PassContext)#1})::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}> >::Call(tvm::runtime::PackedFuncObj const*, tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)
  23: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7r
  22: tvm::tir::ThreadAllreduceBuilder::VisitStmt_(tvm::tir::AttrStmtNode const*)
  21: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  20: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7r
  19: tvm::tir::ThreadAllreduceBuilder::VisitStmt_(tvm::tir::AllocateNode const*)
  18: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  17: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7r
  16: tvm::tir::ThreadAllreduceBuilder::VisitStmt_(tvm::tir::AllocateNode const*)
  15: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  14: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7r
  13: tvm::tir::ThreadAllreduceBuilder::VisitStmt_(tvm::tir::AllocateNode const*)
  12: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  11: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7r
  10: tvm::tir::ThreadAllreduceBuilder::VisitStmt_(tvm::tir::AttrStmtNode const*)
  9: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  8: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runt
  7: tvm::runtime::ObjectPtr<tvm::runtime::Object> tvm::runtime::Array<tvm::tir::Stmt, void>::MapHelper<tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1}, tvm::tir::Stmt>(tvm::runtime::ObjectPtr<tvm::runtime::Object>, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1})
  6: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  5: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7r
  4: tvm::tir::ThreadAllreduceBuilder::VisitStmt_(tvm::tir::AttrStmtNode const*)
  3: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  2: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runt
  1: tvm::tir::ThreadAllreduceBuilder::VisitStmt_(tvm::tir::EvaluateNode const*)
  0: tvm::tir::ThreadAllreduceBuilder::MakeAllreduce(tvm::tir::CallNode const*)
  File "/home2/xiachunwei/Software/tvm/src/tir/transforms/lower_thread_allreduce.cc", line 383
InternalError: Check failed: (!load_remap_.count(buffers[i]->data.get())) is false: 

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(64), T.int64(112), T.int64(112)), "float32"), p1: T.Buffer((T.int64(1), T.int64(64), T.int64(1), T.int64(1)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(64), T.int64(56), T.int64(56)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        pool_max_shared = T.alloc_buffer((T.int64(1), T.int64(64), T.int64(56), T.int64(56)), scope="shared")
        for ax0_ax1_ax2_ax3_0_fused in T.thread_binding(T.int64(7168), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 1024, "pragma_unroll_explicit": 1}):
            for ax0, ax1, ax2, ax3, ax4_ax5_fused_0 in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(32), T.int64(1)):
                for ax4_ax5_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                    with T.block("pool_max"):
                        v_ax0 = T.axis.spatial(T.int64(1), ax0)
                        v_ax1 = T.axis.spatial(T.int64(64), ax0_ax1_ax2_ax3_0_fused // T.int64(112) + ax1)
                        v_ax2 = T.axis.spatial(T.int64(56), ax0_ax1_ax2_ax3_0_fused % T.int64(112) // T.int64(2) + ax2)
                        v_ax3 = T.axis.spatial(T.int64(56), ax0_ax1_ax2_ax3_0_fused % T.int64(2) * T.int64(32) + ax3)
                        v_rv0 = T.axis.reduce(T.int64(3), (ax4_ax5_fused_0 * T.int64(32) + ax4_ax5_fused_1) // T.int64(3))
                        v_rv1 = T.axis.reduce(T.int64(3), (ax4_ax5_fused_0 * T.int64(32) + ax4_ax5_fused_1) % T.int64(3))
                        T.where(ax0_ax1_ax2_ax3_0_fused % T.int64(2) * T.int64(32) + ax3 < T.int64(56) and ax4_ax5_fused_0 * T.int64(32) + ax4_ax5_fused_1 < T.int64(9))
                        T.reads(p0[v_ax0, v_ax1, v_ax2 * T.int64(2) + v_rv0 - T.int64(1), v_ax3 * T.int64(2) + v_rv1 - T.int64(1)])
                        T.writes(pool_max_shared[v_ax0, v_ax1, v_ax2, v_ax3])
                        with T.init():
                            pool_max_shared[v_ax0, v_ax1, v_ax2, v_ax3] = T.float32(-3.4028234663852886e+38)
                        pool_max_shared[v_ax0, v_ax1, v_ax2, v_ax3] = T.max(pool_max_shared[v_ax0, v_ax1, v_ax2, v_ax3], T.if_then_else(T.int64(1) <= v_ax2 * T.int64(2) + v_rv0 and v_ax2 * T.int64(2) + v_rv0 < T.int64(113) and T.int64(1) <= v_ax3 * T.int64(2) + v_rv1 and v_ax3 * T.int64(2) + v_rv1 < T.int64(113), p0[v_ax0, v_ax1, v_ax2 * T.int64(2) + v_rv0 - T.int64(1), v_ax3 * T.int64(2) + v_rv1 - T.int64(1)], T.float32(-3.4028234663852886e+38)))
            for ax3_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                with T.block("T_add"):
                    v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                    v_ax1 = T.axis.spatial(T.int64(64), ax0_ax1_ax2_ax3_0_fused // T.int64(112))
                    v_ax2 = T.axis.spatial(T.int64(56), ax0_ax1_ax2_ax3_0_fused % T.int64(112) // T.int64(2))
                    v_ax3 = T.axis.spatial(T.int64(56), ax0_ax1_ax2_ax3_0_fused % T.int64(2) * T.int64(32) + ax3_1)
                    T.where(ax0_ax1_ax2_ax3_0_fused % T.int64(2) * T.int64(32) + ax3_1 < T.int64(56))
                    T.reads(pool_max_shared[v_ax0, v_ax1, v_ax2, v_ax3], p1[v_ax0, v_ax1, T.int64(0), T.int64(0)])
                    T.writes(T_relu[v_ax0, v_ax1, v_ax2, v_ax3])
                    T_relu[v_ax0, v_ax1, v_ax2, v_ax3] = T.max(pool_max_shared[v_ax0, v_ax1, v_ax2, v_ax3] + p1[v_ax0, v_ax1, T.int64(0), T.int64(0)], T.float32(0))
b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="pool_max", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b1, ann_key="schedule_rule")
sch.reverse_compute_inline(block=b3)
sch.compute_inline(block=b0)
b5, = sch.get_consumers(block=b1)
l6, l7, l8, l9 = sch.get_loops(block=b5)
v10 = sch.sample_categorical(candidates=[4, 8, 16, 32, 64, 128, 256, 512], probs=[0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125], decision=3)
l11, l12 = sch.split(loop=l9, factors=[None, v10], preserve_unit_iters=True)
sch.bind(loop=l12, thread_axis="threadIdx.x")
sch.compute_at(block=b1, loop=l11, preserve_unit_loops=True, index=-1)
sch.set_scope(block=b1, buffer_index=0, storage_scope="shared")
l13, l14, l15, l16, l17, l18, l19, l20, l21, l22 = sch.get_loops(block=b1)
l23 = sch.fuse(l21, l22, preserve_unit_iters=True)
l24, l25 = sch.split(loop=l23, factors=[None, v10], preserve_unit_iters=True)
sch.bind(loop=l25, thread_axis="threadIdx.x")
v26 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v26)
l27, l28, l29, l30, l31 = sch.get_loops(block=b2)
l32 = sch.fuse(l27, l28, l29, l30, preserve_unit_iters=True)
sch.bind(loop=l32, thread_axis="blockIdx.x")
sch.enter_postproc()
b33 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b33, ann_key="meta_schedule.unroll_explicit")
b34, b35 = sch.get_child_blocks(b33)
l36, l37, l38, l39, l40, l41, l42 = sch.get_loops(block=b34)
sch.annotate(block_or_loop=l36, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l36, ann_key="pragma_unroll_explicit", ann_val=1)
l43, l44 = sch.get_loops(block=b35)
2023-11-11 00:21:27 [INFO] [task_scheduler.cc:131] [Task #6: fused_nn_max_pool2d_add_nn_relu] Trial #51: GFLOPs: 145.2689. Time: 15.1976 us. Best GFLOPs: 288.9744
2023-11-11 00:21:27 [INFO] [task_scheduler.cc:131] [Task #6: fused_nn_max_pool2d_add_nn_relu] Trial #52: GFLOPs: 173.2966. Time: 12.7397 us. Best GFLOPs: 288.9744
2023-11-11 00:21:27 [INFO] [task_scheduler.cc:131] [Task #6: fused_nn_max_pool2d_add_nn_relu] Trial #53: GFLOPs: 268.3587. Time: 8.2268 us. Best GFLOPs: 288.9744
2023-11-11 00:21:27 [INFO] [task_scheduler.cc:121] [Task #6: fused_nn_max_pool2d_add_nn_relu] Trial #54: Error in building:
LocalBuilder: An exception occurred
Traceback (most recent call last):
  File "/home2/xiachunwei/Software/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/home2/xiachunwei/Software/tvm/python/tvm/meta_schedule/builder/local_builder.py", line 165, in <lambda>
    lambda x: _worker_func(*x),
  File "/home2/xiachunwei/Software/tvm/python/tvm/meta_schedule/builder/local_builder.py", line 231, in _worker_func
    rt_mod: Module = f_build(mod, target, _deserialize_params(params))
  File "/home2/xiachunwei/Software/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/home2/xiachunwei/Software/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
  File "/home2/xiachunwei/Software/tvm/python/tvm/meta_schedule/builder/local_builder.py", line 261, in default_build
    return tvm_build(mod, target=target)
  File "/home2/xiachunwei/Software/tvm/python/tvm/driver/build_module.py", line 281, in build
    rt_mod_host = _driver_ffi.tir_to_runtime(annotated_mods, target_host)
  File "/home2/xiachunwei/Software/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/home2/xiachunwei/Software/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  33: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::TypedPackedFunc<tvm::runtime::Module (tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)>::AssignTypedLambda<tvm::{lambda(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)#6}>(tvm::{lambda(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)#6}, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}> >::Call(tvm::runtime::PackedFuncObj const*, tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)
  32: tvm::TIRToRuntime(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target const&)
  31: tvm::SplitMixedModule(tvm::IRModule, tvm::Target const&, tvm::Target const&)
  30: tvm::ApplyPasses(tvm::IRModule, tvm::transform::Sequential)
  29: tvm::transform::Pass::operator()(tvm::IRModule) const
  28: tvm::transform::Pass::operator()(tvm::IRModule, tvm::transform::PassContext const&) const
  27: tvm::transform::SequentialNode::operator()(tvm::IRModule, tvm::transform::PassContext const&) const
  26: tvm::transform::Pass::operator()(tvm::IRModule, tvm::transform::PassContext const&) const
  25: tvm::tir::transform::PrimFuncPassNode::operator()(tvm::IRModule, tvm::transform::PassContext const&) const
  24: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::TypedPackedFunc<tvm::tir::PrimFunc (tvm::tir::PrimFunc, tvm::IRModule, tvm::transform::PassContext)>::AssignTypedLambda<tvm::tir::transform::LowerThreadAllreduce()::{lambda(tvm::tir::PrimFunc, tvm::IRModule, tvm::transform::PassContext)#1}>(tvm::tir::transform::LowerThreadAllreduce()::{lambda(tvm::tir::PrimFunc, tvm::IRModule, tvm::transform::PassContext)#1})::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}> >::Call(tvm::runtime::PackedFuncObj const*, tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)
  23: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7r
  22: tvm::tir::ThreadAllreduceBuilder::VisitStmt_(tvm::tir::AttrStmtNode const*)
  21: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  20: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7r
  19: tvm::tir::ThreadAllreduceBuilder::VisitStmt_(tvm::tir::AllocateNode const*)
  18: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  17: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7r
  16: tvm::tir::ThreadAllreduceBuilder::VisitStmt_(tvm::tir::AllocateNode const*)
  15: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  14: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7r
  13: tvm::tir::ThreadAllreduceBuilder::VisitStmt_(tvm::tir::AllocateNode const*)
  12: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  11: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7r
  10: tvm::tir::ThreadAllreduceBuilder::VisitStmt_(tvm::tir::AttrStmtNode const*)
  9: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  8: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runt
  7: tvm::runtime::ObjectPtr<tvm::runtime::Object> tvm::runtime::Array<tvm::tir::Stmt, void>::MapHelper<tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1}, tvm::tir::Stmt>(tvm::runtime::ObjectPtr<tvm::runtime::Object>, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1})
  6: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  5: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7r
  4: tvm::tir::ThreadAllreduceBuilder::VisitStmt_(tvm::tir::AttrStmtNode const*)
  3: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  2: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runt
  1: tvm::tir::ThreadAllreduceBuilder::VisitStmt_(tvm::tir::EvaluateNode const*)
  0: tvm::tir::ThreadAllreduceBuilder::MakeAllreduce(tvm::tir::CallNode const*)
  File "/home2/xiachunwei/Software/tvm/src/tir/transforms/lower_thread_allreduce.cc", line 383
InternalError: Check failed: (!load_remap_.count(buffers[i]->data.get())) is false: 

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(64), T.int64(112), T.int64(112)), "float32"), p1: T.Buffer((T.int64(1), T.int64(64), T.int64(1), T.int64(1)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(64), T.int64(56), T.int64(56)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        pool_max_shared = T.alloc_buffer((T.int64(1), T.int64(64), T.int64(56), T.int64(56)), scope="shared")
        for ax0_ax1_ax2_ax3_0_fused in T.thread_binding(T.int64(50176), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 1024, "pragma_unroll_explicit": 1}):
            for ax0, ax1, ax2, ax3, ax4_ax5_fused_0 in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(4), T.int64(3)):
                for ax4_ax5_fused_1 in T.thread_binding(T.int64(4), thread="threadIdx.x"):
                    with T.block("pool_max"):
                        v_ax0 = T.axis.spatial(T.int64(1), ax0)
                        v_ax1 = T.axis.spatial(T.int64(64), ax0_ax1_ax2_ax3_0_fused // T.int64(784) + ax1)
                        v_ax2 = T.axis.spatial(T.int64(56), ax0_ax1_ax2_ax3_0_fused % T.int64(784) // T.int64(14) + ax2)
                        v_ax3 = T.axis.spatial(T.int64(56), ax0_ax1_ax2_ax3_0_fused % T.int64(14) * T.int64(4) + ax3)
                        v_rv0 = T.axis.reduce(T.int64(3), (ax4_ax5_fused_0 * T.int64(4) + ax4_ax5_fused_1) // T.int64(3))
                        v_rv1 = T.axis.reduce(T.int64(3), (ax4_ax5_fused_0 * T.int64(4) + ax4_ax5_fused_1) % T.int64(3))
                        T.where(ax4_ax5_fused_0 * T.int64(4) + ax4_ax5_fused_1 < T.int64(9))
                        T.reads(p0[v_ax0, v_ax1, v_ax2 * T.int64(2) + v_rv0 - T.int64(1), v_ax3 * T.int64(2) + v_rv1 - T.int64(1)])
                        T.writes(pool_max_shared[v_ax0, v_ax1, v_ax2, v_ax3])
                        with T.init():
                            pool_max_shared[v_ax0, v_ax1, v_ax2, v_ax3] = T.float32(-3.4028234663852886e+38)
                        pool_max_shared[v_ax0, v_ax1, v_ax2, v_ax3] = T.max(pool_max_shared[v_ax0, v_ax1, v_ax2, v_ax3], T.if_then_else(T.int64(1) <= v_ax2 * T.int64(2) + v_rv0 and v_ax2 * T.int64(2) + v_rv0 < T.int64(113) and T.int64(1) <= v_ax3 * T.int64(2) + v_rv1 and v_ax3 * T.int64(2) + v_rv1 < T.int64(113), p0[v_ax0, v_ax1, v_ax2 * T.int64(2) + v_rv0 - T.int64(1), v_ax3 * T.int64(2) + v_rv1 - T.int64(1)], T.float32(-3.4028234663852886e+38)))
            for ax3_1 in T.thread_binding(T.int64(4), thread="threadIdx.x"):
                with T.block("T_add"):
                    v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                    v_ax1 = T.axis.spatial(T.int64(64), ax0_ax1_ax2_ax3_0_fused // T.int64(784))
                    v_ax2 = T.axis.spatial(T.int64(56), ax0_ax1_ax2_ax3_0_fused % T.int64(784) // T.int64(14))
                    v_ax3 = T.axis.spatial(T.int64(56), ax0_ax1_ax2_ax3_0_fused % T.int64(14) * T.int64(4) + ax3_1)
                    T.reads(pool_max_shared[v_ax0, v_ax1, v_ax2, v_ax3], p1[v_ax0, v_ax1, T.int64(0), T.int64(0)])
                    T.writes(T_relu[v_ax0, v_ax1, v_ax2, v_ax3])
                    T_relu[v_ax0, v_ax1, v_ax2, v_ax3] = T.max(pool_max_shared[v_ax0, v_ax1, v_ax2, v_ax3] + p1[v_ax0, v_ax1, T.int64(0), T.int64(0)], T.float32(0))
b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="pool_max", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b1, ann_key="schedule_rule")
sch.reverse_compute_inline(block=b3)
sch.compute_inline(block=b0)
b5, = sch.get_consumers(block=b1)
l6, l7, l8, l9 = sch.get_loops(block=b5)
v10 = sch.sample_categorical(candidates=[4, 8, 16, 32, 64, 128, 256, 512], probs=[0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125], decision=0)
l11, l12 = sch.split(loop=l9, factors=[None, v10], preserve_unit_iters=True)
sch.bind(loop=l12, thread_axis="threadIdx.x")
sch.compute_at(block=b1, loop=l11, preserve_unit_loops=True, index=-1)
sch.set_scope(block=b1, buffer_index=0, storage_scope="shared")
l13, l14, l15, l16, l17, l18, l19, l20, l21, l22 = sch.get_loops(block=b1)
l23 = sch.fuse(l21, l22, preserve_unit_iters=True)
l24, l25 = sch.split(loop=l23, factors=[None, v10], preserve_unit_iters=True)
sch.bind(loop=l25, thread_axis="threadIdx.x")
v26 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v26)
l27, l28, l29, l30, l31 = sch.get_loops(block=b2)
l32 = sch.fuse(l27, l28, l29, l30, preserve_unit_iters=True)
sch.bind(loop=l32, thread_axis="blockIdx.x")
sch.enter_postproc()
b33 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b33, ann_key="meta_schedule.unroll_explicit")
b34, b35 = sch.get_child_blocks(b33)
l36, l37, l38, l39, l40, l41, l42 = sch.get_loops(block=b34)
sch.annotate(block_or_loop=l36, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l36, ann_key="pragma_unroll_explicit", ann_val=1)
l43, l44 = sch.get_loops(block=b35)
2023-11-11 00:21:27 [INFO] [task_scheduler.cc:131] [Task #6: fused_nn_max_pool2d_add_nn_relu] Trial #55: GFLOPs: 79.2780. Time: 27.8481 us. Best GFLOPs: 288.9744
2023-11-11 00:21:27 [INFO] [task_scheduler.cc:131] [Task #6: fused_nn_max_pool2d_add_nn_relu] Trial #56: GFLOPs: 192.1463. Time: 11.4899 us. Best GFLOPs: 288.9744
2023-11-11 00:21:27 [INFO] [task_scheduler.cc:121] [Task #6: fused_nn_max_pool2d_add_nn_relu] Trial #57: Error in building:
LocalBuilder: An exception occurred
Traceback (most recent call last):
  File "/home2/xiachunwei/Software/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/home2/xiachunwei/Software/tvm/python/tvm/meta_schedule/builder/local_builder.py", line 165, in <lambda>
    lambda x: _worker_func(*x),
  File "/home2/xiachunwei/Software/tvm/python/tvm/meta_schedule/builder/local_builder.py", line 231, in _worker_func
    rt_mod: Module = f_build(mod, target, _deserialize_params(params))
  File "/home2/xiachunwei/Software/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/home2/xiachunwei/Software/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
  File "/home2/xiachunwei/Software/tvm/python/tvm/meta_schedule/builder/local_builder.py", line 261, in default_build
    return tvm_build(mod, target=target)
  File "/home2/xiachunwei/Software/tvm/python/tvm/driver/build_module.py", line 281, in build
    rt_mod_host = _driver_ffi.tir_to_runtime(annotated_mods, target_host)
  File "/home2/xiachunwei/Software/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/home2/xiachunwei/Software/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  33: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::TypedPackedFunc<tvm::runtime::Module (tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)>::AssignTypedLambda<tvm::{lambda(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)#6}>(tvm::{lambda(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)#6}, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}> >::Call(tvm::runtime::PackedFuncObj const*, tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)
  32: tvm::TIRToRuntime(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target const&)
  31: tvm::SplitMixedModule(tvm::IRModule, tvm::Target const&, tvm::Target const&)
  30: tvm::ApplyPasses(tvm::IRModule, tvm::transform::Sequential)
  29: tvm::transform::Pass::operator()(tvm::IRModule) const
  28: tvm::transform::Pass::operator()(tvm::IRModule, tvm::transform::PassContext const&) const
  27: tvm::transform::SequentialNode::operator()(tvm::IRModule, tvm::transform::PassContext const&) const
  26: tvm::transform::Pass::operator()(tvm::IRModule, tvm::transform::PassContext const&) const
  25: tvm::tir::transform::PrimFuncPassNode::operator()(tvm::IRModule, tvm::transform::PassContext const&) const
  24: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::TypedPackedFunc<tvm::tir::PrimFunc (tvm::tir::PrimFunc, tvm::IRModule, tvm::transform::PassContext)>::AssignTypedLambda<tvm::tir::transform::LowerThreadAllreduce()::{lambda(tvm::tir::PrimFunc, tvm::IRModule, tvm::transform::PassContext)#1}>(tvm::tir::transform::LowerThreadAllreduce()::{lambda(tvm::tir::PrimFunc, tvm::IRModule, tvm::transform::PassContext)#1})::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}> >::Call(tvm::runtime::PackedFuncObj const*, tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)
  23: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7r
  22: tvm::tir::ThreadAllreduceBuilder::VisitStmt_(tvm::tir::AttrStmtNode const*)
  21: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  20: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7r
  19: tvm::tir::ThreadAllreduceBuilder::VisitStmt_(tvm::tir::AllocateNode const*)
  18: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  17: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7r
  16: tvm::tir::ThreadAllreduceBuilder::VisitStmt_(tvm::tir::AllocateNode const*)
  15: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  14: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7r
  13: tvm::tir::ThreadAllreduceBuilder::VisitStmt_(tvm::tir::AllocateNode const*)
  12: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  11: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7r
  10: tvm::tir::ThreadAllreduceBuilder::VisitStmt_(tvm::tir::AttrStmtNode const*)
  9: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  8: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runt
  7: tvm::runtime::ObjectPtr<tvm::runtime::Object> tvm::runtime::Array<tvm::tir::Stmt, void>::MapHelper<tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1}, tvm::tir::Stmt>(tvm::runtime::ObjectPtr<tvm::runtime::Object>, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1})
  6: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  5: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7r
  4: tvm::tir::ThreadAllreduceBuilder::VisitStmt_(tvm::tir::AttrStmtNode const*)
  3: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  2: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runt
  1: tvm::tir::ThreadAllreduceBuilder::VisitStmt_(tvm::tir::EvaluateNode const*)
  0: tvm::tir::ThreadAllreduceBuilder::MakeAllreduce(tvm::tir::CallNode const*)
  File "/home2/xiachunwei/Software/tvm/src/tir/transforms/lower_thread_allreduce.cc", line 383
InternalError: Check failed: (!load_remap_.count(buffers[i]->data.get())) is false: 

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(64), T.int64(112), T.int64(112)), "float32"), p1: T.Buffer((T.int64(1), T.int64(64), T.int64(1), T.int64(1)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(64), T.int64(56), T.int64(56)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        pool_max_shared = T.alloc_buffer((T.int64(1), T.int64(64), T.int64(56), T.int64(56)), scope="shared")
        for ax0_ax1_ax2_ax3_0_fused in T.thread_binding(T.int64(14336), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 1024, "pragma_unroll_explicit": 1}):
            for ax0, ax1, ax2, ax3, ax4_ax5_fused_0 in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(16), T.int64(1)):
                for ax4_ax5_fused_1 in T.thread_binding(T.int64(16), thread="threadIdx.x"):
                    with T.block("pool_max"):
                        v_ax0 = T.axis.spatial(T.int64(1), ax0)
                        v_ax1 = T.axis.spatial(T.int64(64), ax0_ax1_ax2_ax3_0_fused // T.int64(224) + ax1)
                        v_ax2 = T.axis.spatial(T.int64(56), ax0_ax1_ax2_ax3_0_fused % T.int64(224) // T.int64(4) + ax2)
                        v_ax3 = T.axis.spatial(T.int64(56), ax0_ax1_ax2_ax3_0_fused % T.int64(4) * T.int64(16) + ax3)
                        v_rv0 = T.axis.reduce(T.int64(3), (ax4_ax5_fused_0 * T.int64(16) + ax4_ax5_fused_1) // T.int64(3))
                        v_rv1 = T.axis.reduce(T.int64(3), (ax4_ax5_fused_0 * T.int64(16) + ax4_ax5_fused_1) % T.int64(3))
                        T.where(ax0_ax1_ax2_ax3_0_fused % T.int64(4) * T.int64(16) + ax3 < T.int64(56) and ax4_ax5_fused_0 * T.int64(16) + ax4_ax5_fused_1 < T.int64(9))
                        T.reads(p0[v_ax0, v_ax1, v_ax2 * T.int64(2) + v_rv0 - T.int64(1), v_ax3 * T.int64(2) + v_rv1 - T.int64(1)])
                        T.writes(pool_max_shared[v_ax0, v_ax1, v_ax2, v_ax3])
                        with T.init():
                            pool_max_shared[v_ax0, v_ax1, v_ax2, v_ax3] = T.float32(-3.4028234663852886e+38)
                        pool_max_shared[v_ax0, v_ax1, v_ax2, v_ax3] = T.max(pool_max_shared[v_ax0, v_ax1, v_ax2, v_ax3], T.if_then_else(T.int64(1) <= v_ax2 * T.int64(2) + v_rv0 and v_ax2 * T.int64(2) + v_rv0 < T.int64(113) and T.int64(1) <= v_ax3 * T.int64(2) + v_rv1 and v_ax3 * T.int64(2) + v_rv1 < T.int64(113), p0[v_ax0, v_ax1, v_ax2 * T.int64(2) + v_rv0 - T.int64(1), v_ax3 * T.int64(2) + v_rv1 - T.int64(1)], T.float32(-3.4028234663852886e+38)))
            for ax3_1 in T.thread_binding(T.int64(16), thread="threadIdx.x"):
                with T.block("T_add"):
                    v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                    v_ax1 = T.axis.spatial(T.int64(64), ax0_ax1_ax2_ax3_0_fused // T.int64(224))
                    v_ax2 = T.axis.spatial(T.int64(56), ax0_ax1_ax2_ax3_0_fused % T.int64(224) // T.int64(4))
                    v_ax3 = T.axis.spatial(T.int64(56), ax0_ax1_ax2_ax3_0_fused % T.int64(4) * T.int64(16) + ax3_1)
                    T.where(ax0_ax1_ax2_ax3_0_fused % T.int64(4) * T.int64(16) + ax3_1 < T.int64(56))
                    T.reads(pool_max_shared[v_ax0, v_ax1, v_ax2, v_ax3], p1[v_ax0, v_ax1, T.int64(0), T.int64(0)])
                    T.writes(T_relu[v_ax0, v_ax1, v_ax2, v_ax3])
                    T_relu[v_ax0, v_ax1, v_ax2, v_ax3] = T.max(pool_max_shared[v_ax0, v_ax1, v_ax2, v_ax3] + p1[v_ax0, v_ax1, T.int64(0), T.int64(0)], T.float32(0))
b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="pool_max", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b1, ann_key="schedule_rule")
sch.reverse_compute_inline(block=b3)
sch.compute_inline(block=b0)
b5, = sch.get_consumers(block=b1)
l6, l7, l8, l9 = sch.get_loops(block=b5)
v10 = sch.sample_categorical(candidates=[4, 8, 16, 32, 64, 128, 256, 512], probs=[0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125], decision=2)
l11, l12 = sch.split(loop=l9, factors=[None, v10], preserve_unit_iters=True)
sch.bind(loop=l12, thread_axis="threadIdx.x")
sch.compute_at(block=b1, loop=l11, preserve_unit_loops=True, index=-1)
sch.set_scope(block=b1, buffer_index=0, storage_scope="shared")
l13, l14, l15, l16, l17, l18, l19, l20, l21, l22 = sch.get_loops(block=b1)
l23 = sch.fuse(l21, l22, preserve_unit_iters=True)
l24, l25 = sch.split(loop=l23, factors=[None, v10], preserve_unit_iters=True)
sch.bind(loop=l25, thread_axis="threadIdx.x")
v26 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v26)
l27, l28, l29, l30, l31 = sch.get_loops(block=b2)
l32 = sch.fuse(l27, l28, l29, l30, preserve_unit_iters=True)
sch.bind(loop=l32, thread_axis="blockIdx.x")
sch.enter_postproc()
b33 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b33, ann_key="meta_schedule.unroll_explicit")
b34, b35 = sch.get_child_blocks(b33)
l36, l37, l38, l39, l40, l41, l42 = sch.get_loops(block=b34)
sch.annotate(block_or_loop=l36, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l36, ann_key="pragma_unroll_explicit", ann_val=1)
l43, l44 = sch.get_loops(block=b35)
2023-11-11 00:21:27 [INFO] [task_scheduler.cc:131] [Task #6: fused_nn_max_pool2d_add_nn_relu] Trial #58: GFLOPs: 227.0458. Time: 9.7238 us. Best GFLOPs: 288.9744
2023-11-11 00:21:27 [INFO] [task_scheduler.cc:131] [Task #6: fused_nn_max_pool2d_add_nn_relu] Trial #59: GFLOPs: 177.3459. Time: 12.4488 us. Best GFLOPs: 288.9744
2023-11-11 00:21:27 [INFO] [task_scheduler.cc:131] [Task #6: fused_nn_max_pool2d_add_nn_relu] Trial #60: GFLOPs: 233.0906. Time: 9.4716 us. Best GFLOPs: 288.9744
2023-11-11 00:21:27 [INFO] [task_scheduler.cc:131] [Task #6: fused_nn_max_pool2d_add_nn_relu] Trial #61: GFLOPs: 180.8378. Time: 12.2084 us. Best GFLOPs: 288.9744
2023-11-11 00:21:27 [INFO] [task_scheduler.cc:131] [Task #6: fused_nn_max_pool2d_add_nn_relu] Trial #62: GFLOPs: 267.4385. Time: 8.2551 us. Best GFLOPs: 288.9744
2023-11-11 00:21:27 [INFO] [task_scheduler.cc:131] [Task #6: fused_nn_max_pool2d_add_nn_relu] Trial #63: GFLOPs: 284.3877. Time: 7.7631 us. Best GFLOPs: 288.9744
2023-11-11 03:19:12 [INFO] [evolutionary_search.cc:713] Generating candidates......
2023-11-11 03:19:12 [INFO] [evolutionary_search.cc:715] Picked top 51 candidate(s) from database
2023-11-11 03:19:15 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310fb056b8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310e891278)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310e8921e8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310e060038)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56310bbc4548)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310e896198)]: 0 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310e896608)]: 0 failure(s)
2023-11-11 03:19:15 [INFO] [evolutionary_search.cc:723] Sampled 461 candidate(s)
2023-11-11 03:19:19 [INFO] [evolutionary_search.cc:621] Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310fb056b8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310e891278)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310e8921e8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310e060038)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56310bbc4548)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310e896198)]: 0 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310e896608)]: 0 failure(s)
2023-11-11 03:19:22 [INFO] [evolutionary_search.cc:621] Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310fb056b8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310e891278)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310e8921e8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310e060038)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56310bbc4548)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310e896198)]: 0 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310e896608)]: 0 failure(s)
2023-11-11 03:19:26 [INFO] [evolutionary_search.cc:621] Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310fb056b8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310e891278)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310e8921e8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310e060038)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56310bbc4548)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310e896198)]: 0 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310e896608)]: 0 failure(s)
2023-11-11 03:19:30 [INFO] [evolutionary_search.cc:621] Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310fb056b8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310e891278)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310e8921e8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310e060038)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56310bbc4548)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310e896198)]: 0 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310e896608)]: 0 failure(s)
2023-11-11 03:19:32 [INFO] [evolutionary_search.cc:649] Scores of the best 64 candidates:
[1 : 16]:	1.5638  1.5598  1.4749  1.4614  1.4527  1.4391  1.4260  0.9627  0.9623  0.9615  0.9582  0.9570  0.9570  0.9559  0.9557  0.9556
[17 : 32]:	0.9545  0.9537  0.9525  0.9514  0.9494  0.9482  0.9479  0.9469  0.9468  0.9442  0.9430  0.9428  0.9427  0.9420  0.9416  0.9411
[33 : 48]:	0.9410  0.9398  0.9398  0.9389  0.9374  0.9364  0.9360  0.9352  0.9343  0.9330  0.9294  0.9285  0.9239  0.9058  0.9054  0.9040
[49 : 64]:	0.8999  0.8974  0.8954  0.8932  0.8931  0.8926  0.8914  0.8911  0.8874  0.8858  0.8848  0.8844  0.8827  0.8815  0.8803  0.8799
2023-11-11 03:19:32 [INFO] [evolutionary_search.cc:727] Got 64 candidate(s) with evolutionary search
2023-11-11 03:19:32 [INFO] [evolutionary_search.cc:730] Sending 63 candidates(s) for measurement
2023-11-11 03:19:56 [INFO] [task_scheduler.cc:121] [Task #6: fused_nn_max_pool2d_add_nn_relu] Trial #64: Error in building:
LocalBuilder: An exception occurred
Traceback (most recent call last):
  File "/home2/xiachunwei/Software/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/home2/xiachunwei/Software/tvm/python/tvm/meta_schedule/builder/local_builder.py", line 165, in <lambda>
    lambda x: _worker_func(*x),
  File "/home2/xiachunwei/Software/tvm/python/tvm/meta_schedule/builder/local_builder.py", line 231, in _worker_func
    rt_mod: Module = f_build(mod, target, _deserialize_params(params))
  File "/home2/xiachunwei/Software/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/home2/xiachunwei/Software/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
  File "/home2/xiachunwei/Software/tvm/python/tvm/meta_schedule/builder/local_builder.py", line 261, in default_build
    return tvm_build(mod, target=target)
  File "/home2/xiachunwei/Software/tvm/python/tvm/driver/build_module.py", line 281, in build
    rt_mod_host = _driver_ffi.tir_to_runtime(annotated_mods, target_host)
  File "/home2/xiachunwei/Software/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/home2/xiachunwei/Software/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  33: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::TypedPackedFunc<tvm::runtime::Module (tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)>::AssignTypedLambda<tvm::{lambda(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)#6}>(tvm::{lambda(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)#6}, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}> >::Call(tvm::runtime::PackedFuncObj const*, tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)
  32: tvm::TIRToRuntime(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target const&)
  31: tvm::SplitMixedModule(tvm::IRModule, tvm::Target const&, tvm::Target const&)
  30: tvm::ApplyPasses(tvm::IRModule, tvm::transform::Sequential)
  29: tvm::transform::Pass::operator()(tvm::IRModule) const
  28: tvm::transform::Pass::operator()(tvm::IRModule, tvm::transform::PassContext const&) const
  27: tvm::transform::SequentialNode::operator()(tvm::IRModule, tvm::transform::PassContext const&) const
  26: tvm::transform::Pass::operator()(tvm::IRModule, tvm::transform::PassContext const&) const
  25: tvm::tir::transform::PrimFuncPassNode::operator()(tvm::IRModule, tvm::transform::PassContext const&) const
  24: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::TypedPackedFunc<tvm::tir::PrimFunc (tvm::tir::PrimFunc, tvm::IRModule, tvm::transform::PassContext)>::AssignTypedLambda<tvm::tir::transform::LowerThreadAllreduce()::{lambda(tvm::tir::PrimFunc, tvm::IRModule, tvm::transform::PassContext)#1}>(tvm::tir::transform::LowerThreadAllreduce()::{lambda(tvm::tir::PrimFunc, tvm::IRModule, tvm::transform::PassContext)#1})::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}> >::Call(tvm::runtime::PackedFuncObj const*, tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)
  23: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7r
  22: tvm::tir::ThreadAllreduceBuilder::VisitStmt_(tvm::tir::AttrStmtNode const*)
  21: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  20: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7r
  19: tvm::tir::ThreadAllreduceBuilder::VisitStmt_(tvm::tir::AllocateNode const*)
  18: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  17: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7r
  16: tvm::tir::ThreadAllreduceBuilder::VisitStmt_(tvm::tir::AllocateNode const*)
  15: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  14: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7r
  13: tvm::tir::ThreadAllreduceBuilder::VisitStmt_(tvm::tir::AllocateNode const*)
  12: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  11: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7r
  10: tvm::tir::ThreadAllreduceBuilder::VisitStmt_(tvm::tir::AttrStmtNode const*)
  9: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  8: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runt
  7: tvm::runtime::ObjectPtr<tvm::runtime::Object> tvm::runtime::Array<tvm::tir::Stmt, void>::MapHelper<tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1}, tvm::tir::Stmt>(tvm::runtime::ObjectPtr<tvm::runtime::Object>, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1})
  6: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  5: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7r
  4: tvm::tir::ThreadAllreduceBuilder::VisitStmt_(tvm::tir::AttrStmtNode const*)
  3: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  2: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runt
  1: tvm::tir::ThreadAllreduceBuilder::VisitStmt_(tvm::tir::EvaluateNode const*)
  0: tvm::tir::ThreadAllreduceBuilder::MakeAllreduce(tvm::tir::CallNode const*)
  File "/home2/xiachunwei/Software/tvm/src/tir/transforms/lower_thread_allreduce.cc", line 383
InternalError: Check failed: (!load_remap_.count(buffers[i]->data.get())) is false: 

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(64), T.int64(112), T.int64(112)), "float32"), p1: T.Buffer((T.int64(1), T.int64(64), T.int64(1), T.int64(1)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(64), T.int64(56), T.int64(56)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        pool_max_shared = T.alloc_buffer((T.int64(1), T.int64(64), T.int64(56), T.int64(56)), scope="shared")
        for ax0_ax1_ax2_ax3_0_fused in T.thread_binding(T.int64(50176), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 512, "pragma_unroll_explicit": 1}):
            for ax0, ax1, ax2, ax3, ax4_ax5_fused_0 in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(4), T.int64(3)):
                for ax4_ax5_fused_1 in T.thread_binding(T.int64(4), thread="threadIdx.x"):
                    with T.block("pool_max"):
                        v_ax0 = T.axis.spatial(T.int64(1), ax0)
                        v_ax1 = T.axis.spatial(T.int64(64), ax0_ax1_ax2_ax3_0_fused // T.int64(784) + ax1)
                        v_ax2 = T.axis.spatial(T.int64(56), ax0_ax1_ax2_ax3_0_fused % T.int64(784) // T.int64(14) + ax2)
                        v_ax3 = T.axis.spatial(T.int64(56), ax0_ax1_ax2_ax3_0_fused % T.int64(14) * T.int64(4) + ax3)
                        v_rv0 = T.axis.reduce(T.int64(3), (ax4_ax5_fused_0 * T.int64(4) + ax4_ax5_fused_1) // T.int64(3))
                        v_rv1 = T.axis.reduce(T.int64(3), (ax4_ax5_fused_0 * T.int64(4) + ax4_ax5_fused_1) % T.int64(3))
                        T.where(ax4_ax5_fused_0 * T.int64(4) + ax4_ax5_fused_1 < T.int64(9))
                        T.reads(p0[v_ax0, v_ax1, v_ax2 * T.int64(2) + v_rv0 - T.int64(1), v_ax3 * T.int64(2) + v_rv1 - T.int64(1)])
                        T.writes(pool_max_shared[v_ax0, v_ax1, v_ax2, v_ax3])
                        with T.init():
                            pool_max_shared[v_ax0, v_ax1, v_ax2, v_ax3] = T.float32(-3.4028234663852886e+38)
                        pool_max_shared[v_ax0, v_ax1, v_ax2, v_ax3] = T.max(pool_max_shared[v_ax0, v_ax1, v_ax2, v_ax3], T.if_then_else(T.int64(1) <= v_ax2 * T.int64(2) + v_rv0 and v_ax2 * T.int64(2) + v_rv0 < T.int64(113) and T.int64(1) <= v_ax3 * T.int64(2) + v_rv1 and v_ax3 * T.int64(2) + v_rv1 < T.int64(113), p0[v_ax0, v_ax1, v_ax2 * T.int64(2) + v_rv0 - T.int64(1), v_ax3 * T.int64(2) + v_rv1 - T.int64(1)], T.float32(-3.4028234663852886e+38)))
            for ax3_1 in T.thread_binding(T.int64(4), thread="threadIdx.x"):
                with T.block("T_add"):
                    v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                    v_ax1 = T.axis.spatial(T.int64(64), ax0_ax1_ax2_ax3_0_fused // T.int64(784))
                    v_ax2 = T.axis.spatial(T.int64(56), ax0_ax1_ax2_ax3_0_fused % T.int64(784) // T.int64(14))
                    v_ax3 = T.axis.spatial(T.int64(56), ax0_ax1_ax2_ax3_0_fused % T.int64(14) * T.int64(4) + ax3_1)
                    T.reads(pool_max_shared[v_ax0, v_ax1, v_ax2, v_ax3], p1[v_ax0, v_ax1, T.int64(0), T.int64(0)])
                    T.writes(T_relu[v_ax0, v_ax1, v_ax2, v_ax3])
                    T_relu[v_ax0, v_ax1, v_ax2, v_ax3] = T.max(pool_max_shared[v_ax0, v_ax1, v_ax2, v_ax3] + p1[v_ax0, v_ax1, T.int64(0), T.int64(0)], T.float32(0))
b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="pool_max", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b1, ann_key="schedule_rule")
sch.reverse_compute_inline(block=b3)
sch.compute_inline(block=b0)
b5, = sch.get_consumers(block=b1)
l6, l7, l8, l9 = sch.get_loops(block=b5)
v10 = sch.sample_categorical(candidates=[4, 8, 16, 32, 64, 128, 256, 512], probs=[0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125], decision=0)
l11, l12 = sch.split(loop=l9, factors=[None, v10], preserve_unit_iters=True)
sch.bind(loop=l12, thread_axis="threadIdx.x")
sch.compute_at(block=b1, loop=l11, preserve_unit_loops=True, index=-1)
sch.set_scope(block=b1, buffer_index=0, storage_scope="shared")
l13, l14, l15, l16, l17, l18, l19, l20, l21, l22 = sch.get_loops(block=b1)
l23 = sch.fuse(l21, l22, preserve_unit_iters=True)
l24, l25 = sch.split(loop=l23, factors=[None, v10], preserve_unit_iters=True)
sch.bind(loop=l25, thread_axis="threadIdx.x")
v26 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v26)
l27, l28, l29, l30, l31 = sch.get_loops(block=b2)
l32 = sch.fuse(l27, l28, l29, l30, preserve_unit_iters=True)
sch.bind(loop=l32, thread_axis="blockIdx.x")
sch.enter_postproc()
b33 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b33, ann_key="meta_schedule.unroll_explicit")
b34, b35 = sch.get_child_blocks(b33)
l36, l37, l38, l39, l40, l41, l42 = sch.get_loops(block=b34)
sch.annotate(block_or_loop=l36, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l36, ann_key="pragma_unroll_explicit", ann_val=1)
l43, l44 = sch.get_loops(block=b35)
2023-11-11 03:19:56 [INFO] [task_scheduler.cc:121] [Task #6: fused_nn_max_pool2d_add_nn_relu] Trial #65: Error in building:
LocalBuilder: An exception occurred
Traceback (most recent call last):
  File "/home2/xiachunwei/Software/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/home2/xiachunwei/Software/tvm/python/tvm/meta_schedule/builder/local_builder.py", line 165, in <lambda>
    lambda x: _worker_func(*x),
  File "/home2/xiachunwei/Software/tvm/python/tvm/meta_schedule/builder/local_builder.py", line 231, in _worker_func
    rt_mod: Module = f_build(mod, target, _deserialize_params(params))
  File "/home2/xiachunwei/Software/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/home2/xiachunwei/Software/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
  File "/home2/xiachunwei/Software/tvm/python/tvm/meta_schedule/builder/local_builder.py", line 261, in default_build
    return tvm_build(mod, target=target)
  File "/home2/xiachunwei/Software/tvm/python/tvm/driver/build_module.py", line 281, in build
    rt_mod_host = _driver_ffi.tir_to_runtime(annotated_mods, target_host)
  File "/home2/xiachunwei/Software/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/home2/xiachunwei/Software/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  33: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::TypedPackedFunc<tvm::runtime::Module (tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)>::AssignTypedLambda<tvm::{lambda(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)#6}>(tvm::{lambda(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)#6}, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}> >::Call(tvm::runtime::PackedFuncObj const*, tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)
  32: tvm::TIRToRuntime(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target const&)
  31: tvm::SplitMixedModule(tvm::IRModule, tvm::Target const&, tvm::Target const&)
  30: tvm::ApplyPasses(tvm::IRModule, tvm::transform::Sequential)
  29: tvm::transform::Pass::operator()(tvm::IRModule) const
  28: tvm::transform::Pass::operator()(tvm::IRModule, tvm::transform::PassContext const&) const
  27: tvm::transform::SequentialNode::operator()(tvm::IRModule, tvm::transform::PassContext const&) const
  26: tvm::transform::Pass::operator()(tvm::IRModule, tvm::transform::PassContext const&) const
  25: tvm::tir::transform::PrimFuncPassNode::operator()(tvm::IRModule, tvm::transform::PassContext const&) const
  24: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::TypedPackedFunc<tvm::tir::PrimFunc (tvm::tir::PrimFunc, tvm::IRModule, tvm::transform::PassContext)>::AssignTypedLambda<tvm::tir::transform::LowerThreadAllreduce()::{lambda(tvm::tir::PrimFunc, tvm::IRModule, tvm::transform::PassContext)#1}>(tvm::tir::transform::LowerThreadAllreduce()::{lambda(tvm::tir::PrimFunc, tvm::IRModule, tvm::transform::PassContext)#1})::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}> >::Call(tvm::runtime::PackedFuncObj const*, tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)
  23: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7r
  22: tvm::tir::ThreadAllreduceBuilder::VisitStmt_(tvm::tir::AttrStmtNode const*)
  21: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  20: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7r
  19: tvm::tir::ThreadAllreduceBuilder::VisitStmt_(tvm::tir::AllocateNode const*)
  18: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  17: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7r
  16: tvm::tir::ThreadAllreduceBuilder::VisitStmt_(tvm::tir::AllocateNode const*)
  15: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  14: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7r
  13: tvm::tir::ThreadAllreduceBuilder::VisitStmt_(tvm::tir::AllocateNode const*)
  12: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  11: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7r
  10: tvm::tir::ThreadAllreduceBuilder::VisitStmt_(tvm::tir::AttrStmtNode const*)
  9: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  8: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runt
  7: tvm::runtime::ObjectPtr<tvm::runtime::Object> tvm::runtime::Array<tvm::tir::Stmt, void>::MapHelper<tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1}, tvm::tir::Stmt>(tvm::runtime::ObjectPtr<tvm::runtime::Object>, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1})
  6: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  5: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7r
  4: tvm::tir::ThreadAllreduceBuilder::VisitStmt_(tvm::tir::AttrStmtNode const*)
  3: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  2: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runt
  1: tvm::tir::ThreadAllreduceBuilder::VisitStmt_(tvm::tir::EvaluateNode const*)
  0: tvm::tir::ThreadAllreduceBuilder::MakeAllreduce(tvm::tir::CallNode const*)
  File "/home2/xiachunwei/Software/tvm/src/tir/transforms/lower_thread_allreduce.cc", line 383
InternalError: Check failed: (!load_remap_.count(buffers[i]->data.get())) is false: 

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(64), T.int64(112), T.int64(112)), "float32"), p1: T.Buffer((T.int64(1), T.int64(64), T.int64(1), T.int64(1)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(64), T.int64(56), T.int64(56)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        pool_max_shared = T.alloc_buffer((T.int64(1), T.int64(64), T.int64(56), T.int64(56)), scope="shared")
        for ax0_ax1_ax2_ax3_0_fused in T.thread_binding(T.int64(25088), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 1024, "pragma_unroll_explicit": 1}):
            for ax0, ax1, ax2, ax3, ax4_ax5_fused_0 in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(8), T.int64(2)):
                for ax4_ax5_fused_1 in T.thread_binding(T.int64(8), thread="threadIdx.x"):
                    with T.block("pool_max"):
                        v_ax0 = T.axis.spatial(T.int64(1), ax0)
                        v_ax1 = T.axis.spatial(T.int64(64), ax0_ax1_ax2_ax3_0_fused // T.int64(392) + ax1)
                        v_ax2 = T.axis.spatial(T.int64(56), ax0_ax1_ax2_ax3_0_fused % T.int64(392) // T.int64(7) + ax2)
                        v_ax3 = T.axis.spatial(T.int64(56), ax0_ax1_ax2_ax3_0_fused % T.int64(7) * T.int64(8) + ax3)
                        v_rv0 = T.axis.reduce(T.int64(3), (ax4_ax5_fused_0 * T.int64(8) + ax4_ax5_fused_1) // T.int64(3))
                        v_rv1 = T.axis.reduce(T.int64(3), (ax4_ax5_fused_0 * T.int64(8) + ax4_ax5_fused_1) % T.int64(3))
                        T.where(ax4_ax5_fused_0 * T.int64(8) + ax4_ax5_fused_1 < T.int64(9))
                        T.reads(p0[v_ax0, v_ax1, v_ax2 * T.int64(2) + v_rv0 - T.int64(1), v_ax3 * T.int64(2) + v_rv1 - T.int64(1)])
                        T.writes(pool_max_shared[v_ax0, v_ax1, v_ax2, v_ax3])
                        with T.init():
                            pool_max_shared[v_ax0, v_ax1, v_ax2, v_ax3] = T.float32(-3.4028234663852886e+38)
                        pool_max_shared[v_ax0, v_ax1, v_ax2, v_ax3] = T.max(pool_max_shared[v_ax0, v_ax1, v_ax2, v_ax3], T.if_then_else(T.int64(1) <= v_ax2 * T.int64(2) + v_rv0 and v_ax2 * T.int64(2) + v_rv0 < T.int64(113) and T.int64(1) <= v_ax3 * T.int64(2) + v_rv1 and v_ax3 * T.int64(2) + v_rv1 < T.int64(113), p0[v_ax0, v_ax1, v_ax2 * T.int64(2) + v_rv0 - T.int64(1), v_ax3 * T.int64(2) + v_rv1 - T.int64(1)], T.float32(-3.4028234663852886e+38)))
            for ax3_1 in T.thread_binding(T.int64(8), thread="threadIdx.x"):
                with T.block("T_add"):
                    v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                    v_ax1 = T.axis.spatial(T.int64(64), ax0_ax1_ax2_ax3_0_fused // T.int64(392))
                    v_ax2 = T.axis.spatial(T.int64(56), ax0_ax1_ax2_ax3_0_fused % T.int64(392) // T.int64(7))
                    v_ax3 = T.axis.spatial(T.int64(56), ax0_ax1_ax2_ax3_0_fused % T.int64(7) * T.int64(8) + ax3_1)
                    T.reads(pool_max_shared[v_ax0, v_ax1, v_ax2, v_ax3], p1[v_ax0, v_ax1, T.int64(0), T.int64(0)])
                    T.writes(T_relu[v_ax0, v_ax1, v_ax2, v_ax3])
                    T_relu[v_ax0, v_ax1, v_ax2, v_ax3] = T.max(pool_max_shared[v_ax0, v_ax1, v_ax2, v_ax3] + p1[v_ax0, v_ax1, T.int64(0), T.int64(0)], T.float32(0))
b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="pool_max", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b1, ann_key="schedule_rule")
sch.reverse_compute_inline(block=b3)
sch.compute_inline(block=b0)
b5, = sch.get_consumers(block=b1)
l6, l7, l8, l9 = sch.get_loops(block=b5)
v10 = sch.sample_categorical(candidates=[4, 8, 16, 32, 64, 128, 256, 512], probs=[0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125], decision=1)
l11, l12 = sch.split(loop=l9, factors=[None, v10], preserve_unit_iters=True)
sch.bind(loop=l12, thread_axis="threadIdx.x")
sch.compute_at(block=b1, loop=l11, preserve_unit_loops=True, index=-1)
sch.set_scope(block=b1, buffer_index=0, storage_scope="shared")
l13, l14, l15, l16, l17, l18, l19, l20, l21, l22 = sch.get_loops(block=b1)
l23 = sch.fuse(l21, l22, preserve_unit_iters=True)
l24, l25 = sch.split(loop=l23, factors=[None, v10], preserve_unit_iters=True)
sch.bind(loop=l25, thread_axis="threadIdx.x")
v26 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v26)
l27, l28, l29, l30, l31 = sch.get_loops(block=b2)
l32 = sch.fuse(l27, l28, l29, l30, preserve_unit_iters=True)
sch.bind(loop=l32, thread_axis="blockIdx.x")
sch.enter_postproc()
b33 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b33, ann_key="meta_schedule.unroll_explicit")
b34, b35 = sch.get_child_blocks(b33)
l36, l37, l38, l39, l40, l41, l42 = sch.get_loops(block=b34)
sch.annotate(block_or_loop=l36, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l36, ann_key="pragma_unroll_explicit", ann_val=1)
l43, l44 = sch.get_loops(block=b35)
2023-11-11 03:19:56 [INFO] [task_scheduler.cc:131] [Task #6: fused_nn_max_pool2d_add_nn_relu] Trial #66: GFLOPs: 65.1847. Time: 33.8690 us. Best GFLOPs: 288.9744
2023-11-11 03:19:56 [INFO] [task_scheduler.cc:131] [Task #6: fused_nn_max_pool2d_add_nn_relu] Trial #67: GFLOPs: 47.1011. Time: 46.8725 us. Best GFLOPs: 288.9744
2023-11-11 03:19:56 [INFO] [task_scheduler.cc:131] [Task #6: fused_nn_max_pool2d_add_nn_relu] Trial #68: GFLOPs: 99.2605. Time: 22.2419 us. Best GFLOPs: 288.9744
2023-11-11 03:19:56 [INFO] [task_scheduler.cc:131] [Task #6: fused_nn_max_pool2d_add_nn_relu] Trial #69: GFLOPs: 54.3729. Time: 40.6038 us. Best GFLOPs: 288.9744
2023-11-11 03:19:56 [INFO] [task_scheduler.cc:121] [Task #6: fused_nn_max_pool2d_add_nn_relu] Trial #70: Error in building:
LocalBuilder: An exception occurred
Traceback (most recent call last):
  File "/home2/xiachunwei/Software/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/home2/xiachunwei/Software/tvm/python/tvm/meta_schedule/builder/local_builder.py", line 165, in <lambda>
    lambda x: _worker_func(*x),
  File "/home2/xiachunwei/Software/tvm/python/tvm/meta_schedule/builder/local_builder.py", line 231, in _worker_func
    rt_mod: Module = f_build(mod, target, _deserialize_params(params))
  File "/home2/xiachunwei/Software/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/home2/xiachunwei/Software/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
  File "/home2/xiachunwei/Software/tvm/python/tvm/meta_schedule/builder/local_builder.py", line 261, in default_build
    return tvm_build(mod, target=target)
  File "/home2/xiachunwei/Software/tvm/python/tvm/driver/build_module.py", line 281, in build
    rt_mod_host = _driver_ffi.tir_to_runtime(annotated_mods, target_host)
  File "/home2/xiachunwei/Software/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/home2/xiachunwei/Software/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  33: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::TypedPackedFunc<tvm::runtime::Module (tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)>::AssignTypedLambda<tvm::{lambda(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)#6}>(tvm::{lambda(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)#6}, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}> >::Call(tvm::runtime::PackedFuncObj const*, tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)
  32: tvm::TIRToRuntime(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target const&)
  31: tvm::SplitMixedModule(tvm::IRModule, tvm::Target const&, tvm::Target const&)
  30: tvm::ApplyPasses(tvm::IRModule, tvm::transform::Sequential)
  29: tvm::transform::Pass::operator()(tvm::IRModule) const
  28: tvm::transform::Pass::operator()(tvm::IRModule, tvm::transform::PassContext const&) const
  27: tvm::transform::SequentialNode::operator()(tvm::IRModule, tvm::transform::PassContext const&) const
  26: tvm::transform::Pass::operator()(tvm::IRModule, tvm::transform::PassContext const&) const
  25: tvm::tir::transform::PrimFuncPassNode::operator()(tvm::IRModule, tvm::transform::PassContext const&) const
  24: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::TypedPackedFunc<tvm::tir::PrimFunc (tvm::tir::PrimFunc, tvm::IRModule, tvm::transform::PassContext)>::AssignTypedLambda<tvm::tir::transform::LowerThreadAllreduce()::{lambda(tvm::tir::PrimFunc, tvm::IRModule, tvm::transform::PassContext)#1}>(tvm::tir::transform::LowerThreadAllreduce()::{lambda(tvm::tir::PrimFunc, tvm::IRModule, tvm::transform::PassContext)#1})::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}> >::Call(tvm::runtime::PackedFuncObj const*, tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)
  23: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7r
  22: tvm::tir::ThreadAllreduceBuilder::VisitStmt_(tvm::tir::AttrStmtNode const*)
  21: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  20: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7r
  19: tvm::tir::ThreadAllreduceBuilder::VisitStmt_(tvm::tir::AllocateNode const*)
  18: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  17: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7r
  16: tvm::tir::ThreadAllreduceBuilder::VisitStmt_(tvm::tir::AllocateNode const*)
  15: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  14: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7r
  13: tvm::tir::ThreadAllreduceBuilder::VisitStmt_(tvm::tir::AllocateNode const*)
  12: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  11: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7r
  10: tvm::tir::ThreadAllreduceBuilder::VisitStmt_(tvm::tir::AttrStmtNode const*)
  9: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  8: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runt
  7: tvm::runtime::ObjectPtr<tvm::runtime::Object> tvm::runtime::Array<tvm::tir::Stmt, void>::MapHelper<tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1}, tvm::tir::Stmt>(tvm::runtime::ObjectPtr<tvm::runtime::Object>, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1})
  6: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  5: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7r
  4: tvm::tir::ThreadAllreduceBuilder::VisitStmt_(tvm::tir::AttrStmtNode const*)
  3: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  2: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runt
  1: tvm::tir::ThreadAllreduceBuilder::VisitStmt_(tvm::tir::EvaluateNode const*)
  0: tvm::tir::ThreadAllreduceBuilder::MakeAllreduce(tvm::tir::CallNode const*)
  File "/home2/xiachunwei/Software/tvm/src/tir/transforms/lower_thread_allreduce.cc", line 383
InternalError: Check failed: (!load_remap_.count(buffers[i]->data.get())) is false: 

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(64), T.int64(112), T.int64(112)), "float32"), p1: T.Buffer((T.int64(1), T.int64(64), T.int64(1), T.int64(1)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(64), T.int64(56), T.int64(56)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        pool_max_shared = T.alloc_buffer((T.int64(1), T.int64(64), T.int64(56), T.int64(56)), scope="shared")
        for ax0_ax1_ax2_ax3_0_fused in T.thread_binding(T.int64(50176), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 64, "pragma_unroll_explicit": 1}):
            for ax0, ax1, ax2, ax3, ax4_ax5_fused_0 in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(4), T.int64(3)):
                for ax4_ax5_fused_1 in T.thread_binding(T.int64(4), thread="threadIdx.x"):
                    with T.block("pool_max"):
                        v_ax0 = T.axis.spatial(T.int64(1), ax0)
                        v_ax1 = T.axis.spatial(T.int64(64), ax0_ax1_ax2_ax3_0_fused // T.int64(784) + ax1)
                        v_ax2 = T.axis.spatial(T.int64(56), ax0_ax1_ax2_ax3_0_fused % T.int64(784) // T.int64(14) + ax2)
                        v_ax3 = T.axis.spatial(T.int64(56), ax0_ax1_ax2_ax3_0_fused % T.int64(14) * T.int64(4) + ax3)
                        v_rv0 = T.axis.reduce(T.int64(3), (ax4_ax5_fused_0 * T.int64(4) + ax4_ax5_fused_1) // T.int64(3))
                        v_rv1 = T.axis.reduce(T.int64(3), (ax4_ax5_fused_0 * T.int64(4) + ax4_ax5_fused_1) % T.int64(3))
                        T.where(ax4_ax5_fused_0 * T.int64(4) + ax4_ax5_fused_1 < T.int64(9))
                        T.reads(p0[v_ax0, v_ax1, v_ax2 * T.int64(2) + v_rv0 - T.int64(1), v_ax3 * T.int64(2) + v_rv1 - T.int64(1)])
                        T.writes(pool_max_shared[v_ax0, v_ax1, v_ax2, v_ax3])
                        with T.init():
                            pool_max_shared[v_ax0, v_ax1, v_ax2, v_ax3] = T.float32(-3.4028234663852886e+38)
                        pool_max_shared[v_ax0, v_ax1, v_ax2, v_ax3] = T.max(pool_max_shared[v_ax0, v_ax1, v_ax2, v_ax3], T.if_then_else(T.int64(1) <= v_ax2 * T.int64(2) + v_rv0 and v_ax2 * T.int64(2) + v_rv0 < T.int64(113) and T.int64(1) <= v_ax3 * T.int64(2) + v_rv1 and v_ax3 * T.int64(2) + v_rv1 < T.int64(113), p0[v_ax0, v_ax1, v_ax2 * T.int64(2) + v_rv0 - T.int64(1), v_ax3 * T.int64(2) + v_rv1 - T.int64(1)], T.float32(-3.4028234663852886e+38)))
            for ax3_1 in T.thread_binding(T.int64(4), thread="threadIdx.x"):
                with T.block("T_add"):
                    v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                    v_ax1 = T.axis.spatial(T.int64(64), ax0_ax1_ax2_ax3_0_fused // T.int64(784))
                    v_ax2 = T.axis.spatial(T.int64(56), ax0_ax1_ax2_ax3_0_fused % T.int64(784) // T.int64(14))
                    v_ax3 = T.axis.spatial(T.int64(56), ax0_ax1_ax2_ax3_0_fused % T.int64(14) * T.int64(4) + ax3_1)
                    T.reads(pool_max_shared[v_ax0, v_ax1, v_ax2, v_ax3], p1[v_ax0, v_ax1, T.int64(0), T.int64(0)])
                    T.writes(T_relu[v_ax0, v_ax1, v_ax2, v_ax3])
                    T_relu[v_ax0, v_ax1, v_ax2, v_ax3] = T.max(pool_max_shared[v_ax0, v_ax1, v_ax2, v_ax3] + p1[v_ax0, v_ax1, T.int64(0), T.int64(0)], T.float32(0))
b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="pool_max", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b1, ann_key="schedule_rule")
sch.reverse_compute_inline(block=b3)
sch.compute_inline(block=b0)
b5, = sch.get_consumers(block=b1)
l6, l7, l8, l9 = sch.get_loops(block=b5)
v10 = sch.sample_categorical(candidates=[4, 8, 16, 32, 64, 128, 256, 512], probs=[0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125], decision=0)
l11, l12 = sch.split(loop=l9, factors=[None, v10], preserve_unit_iters=True)
sch.bind(loop=l12, thread_axis="threadIdx.x")
sch.compute_at(block=b1, loop=l11, preserve_unit_loops=True, index=-1)
sch.set_scope(block=b1, buffer_index=0, storage_scope="shared")
l13, l14, l15, l16, l17, l18, l19, l20, l21, l22 = sch.get_loops(block=b1)
l23 = sch.fuse(l21, l22, preserve_unit_iters=True)
l24, l25 = sch.split(loop=l23, factors=[None, v10], preserve_unit_iters=True)
sch.bind(loop=l25, thread_axis="threadIdx.x")
v26 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=2)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v26)
l27, l28, l29, l30, l31 = sch.get_loops(block=b2)
l32 = sch.fuse(l27, l28, l29, l30, preserve_unit_iters=True)
sch.bind(loop=l32, thread_axis="blockIdx.x")
sch.enter_postproc()
b33 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b33, ann_key="meta_schedule.unroll_explicit")
b34, b35 = sch.get_child_blocks(b33)
l36, l37, l38, l39, l40, l41, l42 = sch.get_loops(block=b34)
sch.annotate(block_or_loop=l36, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l36, ann_key="pragma_unroll_explicit", ann_val=1)
l43, l44 = sch.get_loops(block=b35)
2023-11-11 03:19:56 [INFO] [task_scheduler.cc:131] [Task #6: fused_nn_max_pool2d_add_nn_relu] Trial #71: GFLOPs: 292.9171. Time: 7.5371 us. Best GFLOPs: 292.9171
2023-11-11 03:19:56 [INFO] [task_scheduler.cc:131] [Task #6: fused_nn_max_pool2d_add_nn_relu] Trial #72: GFLOPs: 287.6305. Time: 7.6756 us. Best GFLOPs: 292.9171
2023-11-11 03:19:56 [INFO] [task_scheduler.cc:131] [Task #6: fused_nn_max_pool2d_add_nn_relu] Trial #73: GFLOPs: 295.7219. Time: 7.4656 us. Best GFLOPs: 295.7219
2023-11-11 03:19:56 [INFO] [task_scheduler.cc:131] [Task #6: fused_nn_max_pool2d_add_nn_relu] Trial #74: GFLOPs: 277.5306. Time: 7.9550 us. Best GFLOPs: 295.7219
2023-11-11 03:19:56 [INFO] [task_scheduler.cc:131] [Task #6: fused_nn_max_pool2d_add_nn_relu] Trial #75: GFLOPs: 277.5515. Time: 7.9544 us. Best GFLOPs: 295.7219
2023-11-11 03:19:56 [INFO] [task_scheduler.cc:131] [Task #6: fused_nn_max_pool2d_add_nn_relu] Trial #76: GFLOPs: 293.8936. Time: 7.5121 us. Best GFLOPs: 295.7219
2023-11-11 03:19:56 [INFO] [task_scheduler.cc:131] [Task #6: fused_nn_max_pool2d_add_nn_relu] Trial #77: GFLOPs: 286.4052. Time: 7.7085 us. Best GFLOPs: 295.7219
2023-11-11 03:19:56 [INFO] [task_scheduler.cc:131] [Task #6: fused_nn_max_pool2d_add_nn_relu] Trial #78: GFLOPs: 283.6621. Time: 7.7830 us. Best GFLOPs: 295.7219
2023-11-11 03:19:56 [INFO] [task_scheduler.cc:131] [Task #6: fused_nn_max_pool2d_add_nn_relu] Trial #79: GFLOPs: 288.7282. Time: 7.6464 us. Best GFLOPs: 295.7219
2023-11-11 03:19:56 [INFO] [task_scheduler.cc:131] [Task #6: fused_nn_max_pool2d_add_nn_relu] Trial #80: GFLOPs: 288.0135. Time: 7.6654 us. Best GFLOPs: 295.7219
2023-11-11 03:19:56 [INFO] [task_scheduler.cc:131] [Task #6: fused_nn_max_pool2d_add_nn_relu] Trial #81: GFLOPs: 283.6855. Time: 7.7824 us. Best GFLOPs: 295.7219
2023-11-11 03:19:56 [INFO] [task_scheduler.cc:131] [Task #6: fused_nn_max_pool2d_add_nn_relu] Trial #82: GFLOPs: 288.0859. Time: 7.6635 us. Best GFLOPs: 295.7219
2023-11-11 03:19:56 [INFO] [task_scheduler.cc:131] [Task #6: fused_nn_max_pool2d_add_nn_relu] Trial #83: GFLOPs: 270.7956. Time: 8.1528 us. Best GFLOPs: 295.7219
2023-11-11 03:19:56 [INFO] [task_scheduler.cc:131] [Task #6: fused_nn_max_pool2d_add_nn_relu] Trial #84: GFLOPs: 279.1043. Time: 7.9101 us. Best GFLOPs: 295.7219
2023-11-11 03:19:56 [INFO] [task_scheduler.cc:131] [Task #6: fused_nn_max_pool2d_add_nn_relu] Trial #85: GFLOPs: 281.2696. Time: 7.8492 us. Best GFLOPs: 295.7219
2023-11-11 03:19:56 [INFO] [task_scheduler.cc:131] [Task #6: fused_nn_max_pool2d_add_nn_relu] Trial #86: GFLOPs: 277.6793. Time: 7.9507 us. Best GFLOPs: 295.7219
2023-11-11 03:19:56 [INFO] [task_scheduler.cc:131] [Task #6: fused_nn_max_pool2d_add_nn_relu] Trial #87: GFLOPs: 285.5711. Time: 7.7310 us. Best GFLOPs: 295.7219
2023-11-11 03:19:56 [INFO] [task_scheduler.cc:131] [Task #6: fused_nn_max_pool2d_add_nn_relu] Trial #88: GFLOPs: 277.7479. Time: 7.9487 us. Best GFLOPs: 295.7219
2023-11-11 03:19:56 [INFO] [task_scheduler.cc:131] [Task #6: fused_nn_max_pool2d_add_nn_relu] Trial #89: GFLOPs: 277.7057. Time: 7.9499 us. Best GFLOPs: 295.7219
2023-11-11 03:19:56 [INFO] [task_scheduler.cc:131] [Task #6: fused_nn_max_pool2d_add_nn_relu] Trial #90: GFLOPs: 284.7890. Time: 7.7522 us. Best GFLOPs: 295.7219
2023-11-11 03:19:56 [INFO] [task_scheduler.cc:131] [Task #6: fused_nn_max_pool2d_add_nn_relu] Trial #91: GFLOPs: 279.0961. Time: 7.9103 us. Best GFLOPs: 295.7219
2023-11-11 03:19:56 [INFO] [task_scheduler.cc:131] [Task #6: fused_nn_max_pool2d_add_nn_relu] Trial #92: GFLOPs: 273.8718. Time: 8.0612 us. Best GFLOPs: 295.7219
2023-11-11 03:19:56 [INFO] [task_scheduler.cc:131] [Task #6: fused_nn_max_pool2d_add_nn_relu] Trial #93: GFLOPs: 277.5768. Time: 7.9536 us. Best GFLOPs: 295.7219
2023-11-11 03:19:56 [INFO] [task_scheduler.cc:131] [Task #6: fused_nn_max_pool2d_add_nn_relu] Trial #94: GFLOPs: 281.1088. Time: 7.8537 us. Best GFLOPs: 295.7219
2023-11-11 03:19:56 [INFO] [task_scheduler.cc:131] [Task #6: fused_nn_max_pool2d_add_nn_relu] Trial #95: GFLOPs: 278.1343. Time: 7.9377 us. Best GFLOPs: 295.7219
2023-11-11 03:19:56 [INFO] [task_scheduler.cc:131] [Task #6: fused_nn_max_pool2d_add_nn_relu] Trial #96: GFLOPs: 283.6940. Time: 7.7821 us. Best GFLOPs: 295.7219
2023-11-11 03:19:56 [INFO] [task_scheduler.cc:131] [Task #6: fused_nn_max_pool2d_add_nn_relu] Trial #97: GFLOPs: 288.0407. Time: 7.6647 us. Best GFLOPs: 295.7219
2023-11-11 03:19:56 [INFO] [task_scheduler.cc:131] [Task #6: fused_nn_max_pool2d_add_nn_relu] Trial #98: GFLOPs: 279.1648. Time: 7.9084 us. Best GFLOPs: 295.7219
2023-11-11 03:19:56 [INFO] [task_scheduler.cc:131] [Task #6: fused_nn_max_pool2d_add_nn_relu] Trial #99: GFLOPs: 273.8560. Time: 8.0617 us. Best GFLOPs: 295.7219
2023-11-11 03:19:56 [INFO] [task_scheduler.cc:131] [Task #6: fused_nn_max_pool2d_add_nn_relu] Trial #100: GFLOPs: 278.1501. Time: 7.9372 us. Best GFLOPs: 295.7219
2023-11-11 03:19:56 [INFO] [task_scheduler.cc:131] [Task #6: fused_nn_max_pool2d_add_nn_relu] Trial #101: GFLOPs: 270.7655. Time: 8.1537 us. Best GFLOPs: 295.7219
2023-11-11 03:19:56 [INFO] [task_scheduler.cc:131] [Task #6: fused_nn_max_pool2d_add_nn_relu] Trial #102: GFLOPs: 273.9190. Time: 8.0598 us. Best GFLOPs: 295.7219
2023-11-11 03:19:56 [INFO] [task_scheduler.cc:131] [Task #6: fused_nn_max_pool2d_add_nn_relu] Trial #103: GFLOPs: 277.4899. Time: 7.9561 us. Best GFLOPs: 295.7219
2023-11-11 03:19:56 [INFO] [task_scheduler.cc:131] [Task #6: fused_nn_max_pool2d_add_nn_relu] Trial #104: GFLOPs: 285.5694. Time: 7.7310 us. Best GFLOPs: 295.7219
2023-11-11 03:19:56 [INFO] [task_scheduler.cc:131] [Task #6: fused_nn_max_pool2d_add_nn_relu] Trial #105: GFLOPs: 273.8560. Time: 8.0617 us. Best GFLOPs: 295.7219
2023-11-11 03:19:56 [INFO] [task_scheduler.cc:131] [Task #6: fused_nn_max_pool2d_add_nn_relu] Trial #106: GFLOPs: 277.5103. Time: 7.9555 us. Best GFLOPs: 295.7219
2023-11-11 03:19:56 [INFO] [task_scheduler.cc:131] [Task #6: fused_nn_max_pool2d_add_nn_relu] Trial #107: GFLOPs: 270.7896. Time: 8.1530 us. Best GFLOPs: 295.7219
2023-11-11 03:19:56 [INFO] [task_scheduler.cc:131] [Task #6: fused_nn_max_pool2d_add_nn_relu] Trial #108: GFLOPs: 270.7333. Time: 8.1547 us. Best GFLOPs: 295.7219
2023-11-11 03:19:56 [INFO] [task_scheduler.cc:131] [Task #6: fused_nn_max_pool2d_add_nn_relu] Trial #109: GFLOPs: 270.8463. Time: 8.1513 us. Best GFLOPs: 295.7219
2023-11-11 03:19:56 [INFO] [task_scheduler.cc:131] [Task #6: fused_nn_max_pool2d_add_nn_relu] Trial #110: GFLOPs: 277.5957. Time: 7.9531 us. Best GFLOPs: 295.7219
2023-11-11 03:19:56 [INFO] [task_scheduler.cc:131] [Task #6: fused_nn_max_pool2d_add_nn_relu] Trial #111: GFLOPs: 272.0510. Time: 8.1152 us. Best GFLOPs: 295.7219
2023-11-11 03:19:56 [INFO] [task_scheduler.cc:131] [Task #6: fused_nn_max_pool2d_add_nn_relu] Trial #112: GFLOPs: 256.6926. Time: 8.6007 us. Best GFLOPs: 295.7219
2023-11-11 03:19:56 [INFO] [task_scheduler.cc:131] [Task #6: fused_nn_max_pool2d_add_nn_relu] Trial #113: GFLOPs: 270.8358. Time: 8.1516 us. Best GFLOPs: 295.7219
2023-11-11 03:19:56 [INFO] [task_scheduler.cc:131] [Task #6: fused_nn_max_pool2d_add_nn_relu] Trial #114: GFLOPs: 270.8177. Time: 8.1521 us. Best GFLOPs: 295.7219
2023-11-11 03:19:56 [INFO] [task_scheduler.cc:131] [Task #6: fused_nn_max_pool2d_add_nn_relu] Trial #115: GFLOPs: 270.8157. Time: 8.1522 us. Best GFLOPs: 295.7219
2023-11-11 03:19:56 [INFO] [task_scheduler.cc:131] [Task #6: fused_nn_max_pool2d_add_nn_relu] Trial #116: GFLOPs: 263.4481. Time: 8.3802 us. Best GFLOPs: 295.7219
2023-11-11 03:19:56 [INFO] [task_scheduler.cc:131] [Task #6: fused_nn_max_pool2d_add_nn_relu] Trial #117: GFLOPs: 257.3703. Time: 8.5781 us. Best GFLOPs: 295.7219
2023-11-11 03:19:56 [INFO] [task_scheduler.cc:131] [Task #6: fused_nn_max_pool2d_add_nn_relu] Trial #118: GFLOPs: 260.3501. Time: 8.4799 us. Best GFLOPs: 295.7219
2023-11-11 03:19:56 [INFO] [task_scheduler.cc:131] [Task #6: fused_nn_max_pool2d_add_nn_relu] Trial #119: GFLOPs: 258.6871. Time: 8.5344 us. Best GFLOPs: 295.7219
2023-11-11 03:19:56 [INFO] [task_scheduler.cc:131] [Task #6: fused_nn_max_pool2d_add_nn_relu] Trial #120: GFLOPs: 258.7384. Time: 8.5327 us. Best GFLOPs: 295.7219
2023-11-11 03:19:56 [INFO] [task_scheduler.cc:131] [Task #6: fused_nn_max_pool2d_add_nn_relu] Trial #121: GFLOPs: 256.8882. Time: 8.5942 us. Best GFLOPs: 295.7219
2023-11-11 03:19:56 [INFO] [task_scheduler.cc:131] [Task #6: fused_nn_max_pool2d_add_nn_relu] Trial #122: GFLOPs: 256.7011. Time: 8.6004 us. Best GFLOPs: 295.7219
2023-11-11 03:19:56 [INFO] [task_scheduler.cc:131] [Task #6: fused_nn_max_pool2d_add_nn_relu] Trial #123: GFLOPs: 258.6472. Time: 8.5357 us. Best GFLOPs: 295.7219
2023-11-11 03:19:56 [INFO] [task_scheduler.cc:131] [Task #6: fused_nn_max_pool2d_add_nn_relu] Trial #124: GFLOPs: 270.8092. Time: 8.1524 us. Best GFLOPs: 295.7219
2023-11-11 03:19:56 [INFO] [task_scheduler.cc:131] [Task #6: fused_nn_max_pool2d_add_nn_relu] Trial #125: GFLOPs: 239.0657. Time: 9.2349 us. Best GFLOPs: 295.7219
2023-11-11 03:19:56 [INFO] [task_scheduler.cc:131] [Task #6: fused_nn_max_pool2d_add_nn_relu] Trial #126: GFLOPs: 97.5841. Time: 22.6240 us. Best GFLOPs: 295.7219
2023-11-11 05:52:12 [INFO] [evolutionary_search.cc:713] Generating candidates......
2023-11-11 05:52:13 [INFO] [evolutionary_search.cc:715] Picked top 102 candidate(s) from database
2023-11-11 05:52:15 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310fb056b8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310e891278)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310e8921e8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310e060038)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56310bbc4548)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310e896198)]: 0 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310e896608)]: 0 failure(s)
2023-11-11 05:52:15 [INFO] [evolutionary_search.cc:723] Sampled 410 candidate(s)
2023-11-11 05:52:19 [INFO] [evolutionary_search.cc:621] Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310fb056b8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310e891278)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310e8921e8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310e060038)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56310bbc4548)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310e896198)]: 0 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310e896608)]: 0 failure(s)
2023-11-11 05:52:23 [INFO] [evolutionary_search.cc:621] Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310fb056b8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310e891278)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310e8921e8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310e060038)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56310bbc4548)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310e896198)]: 0 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310e896608)]: 0 failure(s)
2023-11-11 05:52:26 [INFO] [evolutionary_search.cc:621] Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310fb056b8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310e891278)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310e8921e8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310e060038)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56310bbc4548)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310e896198)]: 0 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310e896608)]: 0 failure(s)
2023-11-11 05:52:29 [INFO] [evolutionary_search.cc:621] Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310fb056b8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310e891278)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310e8921e8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310e060038)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56310bbc4548)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310e896198)]: 0 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310e896608)]: 0 failure(s)
2023-11-11 05:52:31 [INFO] [evolutionary_search.cc:649] Scores of the best 64 candidates:
[1 : 16]:	0.8925  0.8838  0.8827  0.8821  0.8816  0.8792  0.8760  0.8594  0.8548  0.8381  0.8371  0.8365  0.8349  0.8325  0.8307  0.8297
[17 : 32]:	0.8224  0.8197  0.8192  0.8179  0.8082  0.8064  0.8030  0.8026  0.7977  0.7963  0.7942  0.7931  0.7758  0.7652  0.7486  0.7475
[33 : 48]:	0.7430  0.7283  0.7022  0.6994  0.6992  0.6990  0.6989  0.6965  0.6964  0.6961  0.6960  0.6933  0.6910  0.6905  0.6894  0.6878
[49 : 64]:	0.6873  0.6842  0.6705  0.6628  0.6609  0.6575  0.6572  0.6564  0.6555  0.6522  0.6522  0.6494  0.6476  0.6439  0.6031  0.5975
2023-11-11 05:52:31 [INFO] [evolutionary_search.cc:727] Got 64 candidate(s) with evolutionary search
2023-11-11 05:52:31 [INFO] [evolutionary_search.cc:730] Sending 61 candidates(s) for measurement
2023-11-11 05:52:57 [INFO] [task_scheduler.cc:131] [Task #6: fused_nn_max_pool2d_add_nn_relu] Trial #127: GFLOPs: 266.0172. Time: 8.2993 us. Best GFLOPs: 295.7219
2023-11-11 05:52:57 [INFO] [task_scheduler.cc:131] [Task #6: fused_nn_max_pool2d_add_nn_relu] Trial #128: GFLOPs: 257.3947. Time: 8.5773 us. Best GFLOPs: 295.7219
2023-11-11 05:52:57 [INFO] [task_scheduler.cc:131] [Task #6: fused_nn_max_pool2d_add_nn_relu] Trial #129: GFLOPs: 256.3165. Time: 8.6134 us. Best GFLOPs: 295.7219
2023-11-11 05:52:57 [INFO] [task_scheduler.cc:131] [Task #6: fused_nn_max_pool2d_add_nn_relu] Trial #130: GFLOPs: 258.2296. Time: 8.5495 us. Best GFLOPs: 295.7219
2023-11-11 05:52:57 [INFO] [task_scheduler.cc:131] [Task #6: fused_nn_max_pool2d_add_nn_relu] Trial #131: GFLOPs: 254.9578. Time: 8.6593 us. Best GFLOPs: 295.7219
2023-11-11 05:52:57 [INFO] [task_scheduler.cc:131] [Task #6: fused_nn_max_pool2d_add_nn_relu] Trial #132: GFLOPs: 255.7176. Time: 8.6335 us. Best GFLOPs: 295.7219
2023-11-11 05:52:57 [INFO] [task_scheduler.cc:131] [Task #6: fused_nn_max_pool2d_add_nn_relu] Trial #133: GFLOPs: 256.5609. Time: 8.6051 us. Best GFLOPs: 295.7219
2023-11-11 05:52:57 [INFO] [task_scheduler.cc:131] [Task #6: fused_nn_max_pool2d_add_nn_relu] Trial #134: GFLOPs: 250.3653. Time: 8.8181 us. Best GFLOPs: 295.7219
2023-11-11 05:52:57 [INFO] [task_scheduler.cc:131] [Task #6: fused_nn_max_pool2d_add_nn_relu] Trial #135: GFLOPs: 256.4588. Time: 8.6086 us. Best GFLOPs: 295.7219
2023-11-11 05:52:57 [INFO] [task_scheduler.cc:131] [Task #6: fused_nn_max_pool2d_add_nn_relu] Trial #136: GFLOPs: 241.1782. Time: 9.1540 us. Best GFLOPs: 295.7219
2023-11-11 05:52:57 [INFO] [task_scheduler.cc:131] [Task #6: fused_nn_max_pool2d_add_nn_relu] Trial #137: GFLOPs: 241.4213. Time: 9.1448 us. Best GFLOPs: 295.7219
2023-11-11 05:52:57 [INFO] [task_scheduler.cc:131] [Task #6: fused_nn_max_pool2d_add_nn_relu] Trial #138: GFLOPs: 234.0902. Time: 9.4312 us. Best GFLOPs: 295.7219
2023-11-11 05:52:57 [INFO] [task_scheduler.cc:131] [Task #6: fused_nn_max_pool2d_add_nn_relu] Trial #139: GFLOPs: 234.1332. Time: 9.4294 us. Best GFLOPs: 295.7219
2023-11-11 05:52:57 [INFO] [task_scheduler.cc:131] [Task #6: fused_nn_max_pool2d_add_nn_relu] Trial #140: GFLOPs: 241.1678. Time: 9.1544 us. Best GFLOPs: 295.7219
2023-11-11 05:52:57 [INFO] [task_scheduler.cc:131] [Task #6: fused_nn_max_pool2d_add_nn_relu] Trial #141: GFLOPs: 234.4387. Time: 9.4172 us. Best GFLOPs: 295.7219
2023-11-11 05:52:57 [INFO] [task_scheduler.cc:131] [Task #6: fused_nn_max_pool2d_add_nn_relu] Trial #142: GFLOPs: 234.0692. Time: 9.4320 us. Best GFLOPs: 295.7219
2023-11-11 05:52:57 [INFO] [task_scheduler.cc:131] [Task #6: fused_nn_max_pool2d_add_nn_relu] Trial #143: GFLOPs: 258.1092. Time: 8.5535 us. Best GFLOPs: 295.7219
2023-11-11 05:52:57 [INFO] [task_scheduler.cc:131] [Task #6: fused_nn_max_pool2d_add_nn_relu] Trial #144: GFLOPs: 258.3041. Time: 8.5471 us. Best GFLOPs: 295.7219
2023-11-11 05:52:57 [INFO] [task_scheduler.cc:131] [Task #6: fused_nn_max_pool2d_add_nn_relu] Trial #145: GFLOPs: 256.0390. Time: 8.6227 us. Best GFLOPs: 295.7219
2023-11-11 05:52:57 [INFO] [task_scheduler.cc:131] [Task #6: fused_nn_max_pool2d_add_nn_relu] Trial #146: GFLOPs: 241.2049. Time: 9.1530 us. Best GFLOPs: 295.7219
2023-11-11 05:52:57 [INFO] [task_scheduler.cc:131] [Task #6: fused_nn_max_pool2d_add_nn_relu] Trial #147: GFLOPs: 223.4013. Time: 9.8824 us. Best GFLOPs: 295.7219
2023-11-11 05:52:57 [INFO] [task_scheduler.cc:131] [Task #6: fused_nn_max_pool2d_add_nn_relu] Trial #148: GFLOPs: 228.7505. Time: 9.6513 us. Best GFLOPs: 295.7219
2023-11-11 05:52:57 [INFO] [task_scheduler.cc:131] [Task #6: fused_nn_max_pool2d_add_nn_relu] Trial #149: GFLOPs: 228.7565. Time: 9.6511 us. Best GFLOPs: 295.7219
2023-11-11 05:52:57 [INFO] [task_scheduler.cc:131] [Task #6: fused_nn_max_pool2d_add_nn_relu] Trial #150: GFLOPs: 223.1249. Time: 9.8947 us. Best GFLOPs: 295.7219
2023-11-11 05:52:57 [INFO] [task_scheduler.cc:131] [Task #6: fused_nn_max_pool2d_add_nn_relu] Trial #151: GFLOPs: 222.8868. Time: 9.9052 us. Best GFLOPs: 295.7219
2023-11-11 05:52:57 [INFO] [task_scheduler.cc:131] [Task #6: fused_nn_max_pool2d_add_nn_relu] Trial #152: GFLOPs: 222.8884. Time: 9.9052 us. Best GFLOPs: 295.7219
2023-11-11 05:52:57 [INFO] [task_scheduler.cc:131] [Task #6: fused_nn_max_pool2d_add_nn_relu] Trial #153: GFLOPs: 225.2214. Time: 9.8025 us. Best GFLOPs: 295.7219
2023-11-11 05:52:57 [INFO] [task_scheduler.cc:131] [Task #6: fused_nn_max_pool2d_add_nn_relu] Trial #154: GFLOPs: 223.0634. Time: 9.8974 us. Best GFLOPs: 295.7219
2023-11-11 05:52:57 [INFO] [task_scheduler.cc:131] [Task #6: fused_nn_max_pool2d_add_nn_relu] Trial #155: GFLOPs: 241.4076. Time: 9.1453 us. Best GFLOPs: 295.7219
2023-11-11 05:52:57 [INFO] [task_scheduler.cc:131] [Task #6: fused_nn_max_pool2d_add_nn_relu] Trial #156: GFLOPs: 222.9578. Time: 9.9021 us. Best GFLOPs: 295.7219
2023-11-11 05:52:57 [INFO] [task_scheduler.cc:131] [Task #6: fused_nn_max_pool2d_add_nn_relu] Trial #157: GFLOPs: 212.4673. Time: 10.3910 us. Best GFLOPs: 295.7219
2023-11-11 05:52:57 [INFO] [task_scheduler.cc:131] [Task #6: fused_nn_max_pool2d_add_nn_relu] Trial #158: GFLOPs: 212.7835. Time: 10.3755 us. Best GFLOPs: 295.7219
2023-11-11 05:52:57 [INFO] [task_scheduler.cc:131] [Task #6: fused_nn_max_pool2d_add_nn_relu] Trial #159: GFLOPs: 212.0122. Time: 10.4133 us. Best GFLOPs: 295.7219
2023-11-11 05:52:57 [INFO] [task_scheduler.cc:131] [Task #6: fused_nn_max_pool2d_add_nn_relu] Trial #160: GFLOPs: 213.1445. Time: 10.3580 us. Best GFLOPs: 295.7219
2023-11-11 05:52:57 [INFO] [task_scheduler.cc:131] [Task #6: fused_nn_max_pool2d_add_nn_relu] Trial #161: GFLOPs: 205.1427. Time: 10.7620 us. Best GFLOPs: 295.7219
2023-11-11 05:52:57 [INFO] [task_scheduler.cc:131] [Task #6: fused_nn_max_pool2d_add_nn_relu] Trial #162: GFLOPs: 205.1538. Time: 10.7614 us. Best GFLOPs: 295.7219
2023-11-11 05:52:57 [INFO] [task_scheduler.cc:131] [Task #6: fused_nn_max_pool2d_add_nn_relu] Trial #163: GFLOPs: 205.1360. Time: 10.7623 us. Best GFLOPs: 295.7219
2023-11-11 05:52:57 [INFO] [task_scheduler.cc:131] [Task #6: fused_nn_max_pool2d_add_nn_relu] Trial #164: GFLOPs: 200.5185. Time: 11.0102 us. Best GFLOPs: 295.7219
2023-11-11 05:52:57 [INFO] [task_scheduler.cc:131] [Task #6: fused_nn_max_pool2d_add_nn_relu] Trial #165: GFLOPs: 205.1686. Time: 10.7606 us. Best GFLOPs: 295.7219
2023-11-11 05:52:57 [INFO] [task_scheduler.cc:131] [Task #6: fused_nn_max_pool2d_add_nn_relu] Trial #166: GFLOPs: 205.1244. Time: 10.7629 us. Best GFLOPs: 295.7219
2023-11-11 05:52:57 [INFO] [task_scheduler.cc:131] [Task #6: fused_nn_max_pool2d_add_nn_relu] Trial #167: GFLOPs: 202.1950. Time: 10.9189 us. Best GFLOPs: 295.7219
2023-11-11 05:52:57 [INFO] [task_scheduler.cc:131] [Task #6: fused_nn_max_pool2d_add_nn_relu] Trial #168: GFLOPs: 205.1595. Time: 10.7611 us. Best GFLOPs: 295.7219
2023-11-11 05:52:57 [INFO] [task_scheduler.cc:131] [Task #6: fused_nn_max_pool2d_add_nn_relu] Trial #169: GFLOPs: 204.6351. Time: 10.7887 us. Best GFLOPs: 295.7219
2023-11-11 05:52:57 [INFO] [task_scheduler.cc:131] [Task #6: fused_nn_max_pool2d_add_nn_relu] Trial #170: GFLOPs: 200.3908. Time: 11.0172 us. Best GFLOPs: 295.7219
2023-11-11 05:52:57 [INFO] [task_scheduler.cc:131] [Task #6: fused_nn_max_pool2d_add_nn_relu] Trial #171: GFLOPs: 186.1033. Time: 11.8630 us. Best GFLOPs: 295.7219
2023-11-11 05:52:57 [INFO] [task_scheduler.cc:131] [Task #6: fused_nn_max_pool2d_add_nn_relu] Trial #172: GFLOPs: 204.8518. Time: 10.7773 us. Best GFLOPs: 295.7219
2023-11-11 05:52:57 [INFO] [task_scheduler.cc:131] [Task #6: fused_nn_max_pool2d_add_nn_relu] Trial #173: GFLOPs: 186.0502. Time: 11.8664 us. Best GFLOPs: 295.7219
2023-11-11 05:52:57 [INFO] [task_scheduler.cc:131] [Task #6: fused_nn_max_pool2d_add_nn_relu] Trial #174: GFLOPs: 205.1532. Time: 10.7614 us. Best GFLOPs: 295.7219
2023-11-11 05:52:57 [INFO] [task_scheduler.cc:131] [Task #6: fused_nn_max_pool2d_add_nn_relu] Trial #175: GFLOPs: 204.5397. Time: 10.7937 us. Best GFLOPs: 295.7219
2023-11-11 05:52:57 [INFO] [task_scheduler.cc:131] [Task #6: fused_nn_max_pool2d_add_nn_relu] Trial #176: GFLOPs: 186.0591. Time: 11.8658 us. Best GFLOPs: 295.7219
2023-11-11 05:52:57 [INFO] [task_scheduler.cc:131] [Task #6: fused_nn_max_pool2d_add_nn_relu] Trial #177: GFLOPs: 186.3171. Time: 11.8494 us. Best GFLOPs: 295.7219
2023-11-11 05:52:57 [INFO] [task_scheduler.cc:131] [Task #6: fused_nn_max_pool2d_add_nn_relu] Trial #178: GFLOPs: 180.0389. Time: 12.2626 us. Best GFLOPs: 295.7219
2023-11-11 05:52:57 [INFO] [task_scheduler.cc:131] [Task #6: fused_nn_max_pool2d_add_nn_relu] Trial #179: GFLOPs: 183.2661. Time: 12.0467 us. Best GFLOPs: 295.7219
2023-11-11 05:52:57 [INFO] [task_scheduler.cc:131] [Task #6: fused_nn_max_pool2d_add_nn_relu] Trial #180: GFLOPs: 183.4585. Time: 12.0340 us. Best GFLOPs: 295.7219
2023-11-11 05:52:57 [INFO] [task_scheduler.cc:131] [Task #6: fused_nn_max_pool2d_add_nn_relu] Trial #181: GFLOPs: 180.0604. Time: 12.2611 us. Best GFLOPs: 295.7219
2023-11-11 05:52:57 [INFO] [task_scheduler.cc:131] [Task #6: fused_nn_max_pool2d_add_nn_relu] Trial #182: GFLOPs: 180.0677. Time: 12.2606 us. Best GFLOPs: 295.7219
2023-11-11 05:52:57 [INFO] [task_scheduler.cc:131] [Task #6: fused_nn_max_pool2d_add_nn_relu] Trial #183: GFLOPs: 193.7733. Time: 11.3934 us. Best GFLOPs: 295.7219
2023-11-11 05:52:57 [INFO] [task_scheduler.cc:131] [Task #6: fused_nn_max_pool2d_add_nn_relu] Trial #184: GFLOPs: 192.3016. Time: 11.4806 us. Best GFLOPs: 295.7219
2023-11-11 05:52:57 [INFO] [task_scheduler.cc:131] [Task #6: fused_nn_max_pool2d_add_nn_relu] Trial #185: GFLOPs: 179.5442. Time: 12.2964 us. Best GFLOPs: 295.7219
2023-11-11 05:52:57 [INFO] [task_scheduler.cc:131] [Task #6: fused_nn_max_pool2d_add_nn_relu] Trial #186: GFLOPs: 183.4344. Time: 12.0356 us. Best GFLOPs: 295.7219
2023-11-11 05:52:57 [INFO] [task_scheduler.cc:131] [Task #6: fused_nn_max_pool2d_add_nn_relu] Trial #187: GFLOPs: 180.0882. Time: 12.2592 us. Best GFLOPs: 295.7219
2023-11-11 08:12:50 [INFO] [evolutionary_search.cc:713] Generating candidates......
2023-11-11 08:12:51 [INFO] [evolutionary_search.cc:715] Picked top 102 candidate(s) from database
2023-11-11 08:12:54 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310fb056b8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310e891278)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310e8921e8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310e060038)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56310bbc4548)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310e896198)]: 0 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310e896608)]: 0 failure(s)
2023-11-11 08:12:54 [INFO] [evolutionary_search.cc:723] Sampled 410 candidate(s)
2023-11-11 08:12:58 [INFO] [evolutionary_search.cc:621] Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310fb056b8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310e891278)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310e8921e8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310e060038)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56310bbc4548)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310e896198)]: 0 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310e896608)]: 0 failure(s)
2023-11-11 08:13:01 [INFO] [evolutionary_search.cc:621] Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310fb056b8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310e891278)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310e8921e8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310e060038)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56310bbc4548)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310e896198)]: 0 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310e896608)]: 0 failure(s)
2023-11-11 08:13:04 [INFO] [evolutionary_search.cc:621] Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310fb056b8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310e891278)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310e8921e8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310e060038)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56310bbc4548)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310e896198)]: 0 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310e896608)]: 0 failure(s)
2023-11-11 08:13:08 [INFO] [evolutionary_search.cc:621] Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310fb056b8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310e891278)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310e8921e8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310e060038)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56310bbc4548)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310e896198)]: 0 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310e896608)]: 0 failure(s)
2023-11-11 08:13:10 [INFO] [evolutionary_search.cc:649] Scores of the best 31 candidates:
[1 : 16]:	0.6584  0.6210  0.5644  0.5613  0.5597  0.5596  0.5578  0.5457  0.5430  0.3961  0.3917  0.3916  0.3916  0.3820  0.3731  0.3393
[17 : 31]:	0.3319  0.3299  0.3247  0.3235  0.3026  0.2491  0.2428  0.2331  0.2303  0.2018  0.1560  0.1436  0.1433  0.0800  0.0000
2023-11-11 08:13:10 [INFO] [evolutionary_search.cc:727] Got 31 candidate(s) with evolutionary search
2023-11-11 08:13:10 [INFO] [evolutionary_search.cc:730] Sending 31 candidates(s) for measurement
2023-11-11 08:13:22 [INFO] [task_scheduler.cc:131] [Task #6: fused_nn_max_pool2d_add_nn_relu] Trial #188: GFLOPs: 190.8156. Time: 11.5700 us. Best GFLOPs: 295.7219
2023-11-11 08:13:22 [INFO] [task_scheduler.cc:131] [Task #6: fused_nn_max_pool2d_add_nn_relu] Trial #189: GFLOPs: 175.7648. Time: 12.5608 us. Best GFLOPs: 295.7219
2023-11-11 08:13:22 [INFO] [task_scheduler.cc:131] [Task #6: fused_nn_max_pool2d_add_nn_relu] Trial #190: GFLOPs: 171.8339. Time: 12.8481 us. Best GFLOPs: 295.7219
2023-11-11 08:13:22 [INFO] [task_scheduler.cc:131] [Task #6: fused_nn_max_pool2d_add_nn_relu] Trial #191: GFLOPs: 171.8363. Time: 12.8479 us. Best GFLOPs: 295.7219
2023-11-11 08:13:22 [INFO] [task_scheduler.cc:131] [Task #6: fused_nn_max_pool2d_add_nn_relu] Trial #192: GFLOPs: 171.8036. Time: 12.8504 us. Best GFLOPs: 295.7219
2023-11-11 08:13:22 [INFO] [task_scheduler.cc:131] [Task #6: fused_nn_max_pool2d_add_nn_relu] Trial #193: GFLOPs: 171.8368. Time: 12.8479 us. Best GFLOPs: 295.7219
2023-11-11 08:13:22 [INFO] [task_scheduler.cc:131] [Task #6: fused_nn_max_pool2d_add_nn_relu] Trial #194: GFLOPs: 171.8133. Time: 12.8497 us. Best GFLOPs: 295.7219
2023-11-11 08:13:22 [INFO] [task_scheduler.cc:131] [Task #6: fused_nn_max_pool2d_add_nn_relu] Trial #195: GFLOPs: 172.9487. Time: 12.7653 us. Best GFLOPs: 295.7219
2023-11-11 08:13:22 [INFO] [task_scheduler.cc:131] [Task #6: fused_nn_max_pool2d_add_nn_relu] Trial #196: GFLOPs: 172.2657. Time: 12.8159 us. Best GFLOPs: 295.7219
2023-11-11 08:13:22 [INFO] [task_scheduler.cc:131] [Task #6: fused_nn_max_pool2d_add_nn_relu] Trial #197: GFLOPs: 145.7471. Time: 15.1478 us. Best GFLOPs: 295.7219
2023-11-11 08:13:22 [INFO] [task_scheduler.cc:131] [Task #6: fused_nn_max_pool2d_add_nn_relu] Trial #198: GFLOPs: 145.8335. Time: 15.1388 us. Best GFLOPs: 295.7219
2023-11-11 08:13:22 [INFO] [task_scheduler.cc:131] [Task #6: fused_nn_max_pool2d_add_nn_relu] Trial #199: GFLOPs: 145.7669. Time: 15.1457 us. Best GFLOPs: 295.7219
2023-11-11 08:13:22 [INFO] [task_scheduler.cc:131] [Task #6: fused_nn_max_pool2d_add_nn_relu] Trial #200: GFLOPs: 145.7663. Time: 15.1458 us. Best GFLOPs: 295.7219
2023-11-11 08:13:22 [INFO] [task_scheduler.cc:131] [Task #6: fused_nn_max_pool2d_add_nn_relu] Trial #201: GFLOPs: 38.0266. Time: 58.0579 us. Best GFLOPs: 295.7219
2023-11-11 08:13:22 [INFO] [task_scheduler.cc:121] [Task #6: fused_nn_max_pool2d_add_nn_relu] Trial #202: Error in building:
LocalBuilder: An exception occurred
Traceback (most recent call last):
  File "/home2/xiachunwei/Software/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/home2/xiachunwei/Software/tvm/python/tvm/meta_schedule/builder/local_builder.py", line 165, in <lambda>
    lambda x: _worker_func(*x),
  File "/home2/xiachunwei/Software/tvm/python/tvm/meta_schedule/builder/local_builder.py", line 231, in _worker_func
    rt_mod: Module = f_build(mod, target, _deserialize_params(params))
  File "/home2/xiachunwei/Software/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/home2/xiachunwei/Software/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
  File "/home2/xiachunwei/Software/tvm/python/tvm/meta_schedule/builder/local_builder.py", line 261, in default_build
    return tvm_build(mod, target=target)
  File "/home2/xiachunwei/Software/tvm/python/tvm/driver/build_module.py", line 281, in build
    rt_mod_host = _driver_ffi.tir_to_runtime(annotated_mods, target_host)
  File "/home2/xiachunwei/Software/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/home2/xiachunwei/Software/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  33: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::TypedPackedFunc<tvm::runtime::Module (tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)>::AssignTypedLambda<tvm::{lambda(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)#6}>(tvm::{lambda(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)#6}, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}> >::Call(tvm::runtime::PackedFuncObj const*, tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)
  32: tvm::TIRToRuntime(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target const&)
  31: tvm::SplitMixedModule(tvm::IRModule, tvm::Target const&, tvm::Target const&)
  30: tvm::ApplyPasses(tvm::IRModule, tvm::transform::Sequential)
  29: tvm::transform::Pass::operator()(tvm::IRModule) const
  28: tvm::transform::Pass::operator()(tvm::IRModule, tvm::transform::PassContext const&) const
  27: tvm::transform::SequentialNode::operator()(tvm::IRModule, tvm::transform::PassContext const&) const
  26: tvm::transform::Pass::operator()(tvm::IRModule, tvm::transform::PassContext const&) const
  25: tvm::tir::transform::PrimFuncPassNode::operator()(tvm::IRModule, tvm::transform::PassContext const&) const
  24: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::TypedPackedFunc<tvm::tir::PrimFunc (tvm::tir::PrimFunc, tvm::IRModule, tvm::transform::PassContext)>::AssignTypedLambda<tvm::tir::transform::LowerThreadAllreduce()::{lambda(tvm::tir::PrimFunc, tvm::IRModule, tvm::transform::PassContext)#1}>(tvm::tir::transform::LowerThreadAllreduce()::{lambda(tvm::tir::PrimFunc, tvm::IRModule, tvm::transform::PassContext)#1})::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}> >::Call(tvm::runtime::PackedFuncObj const*, tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)
  23: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7r
  22: tvm::tir::ThreadAllreduceBuilder::VisitStmt_(tvm::tir::AttrStmtNode const*)
  21: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  20: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7r
  19: tvm::tir::ThreadAllreduceBuilder::VisitStmt_(tvm::tir::AllocateNode const*)
  18: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  17: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7r
  16: tvm::tir::ThreadAllreduceBuilder::VisitStmt_(tvm::tir::AllocateNode const*)
  15: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  14: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7r
  13: tvm::tir::ThreadAllreduceBuilder::VisitStmt_(tvm::tir::AllocateNode const*)
  12: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  11: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7r
  10: tvm::tir::ThreadAllreduceBuilder::VisitStmt_(tvm::tir::AttrStmtNode const*)
  9: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  8: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runt
  7: tvm::runtime::ObjectPtr<tvm::runtime::Object> tvm::runtime::Array<tvm::tir::Stmt, void>::MapHelper<tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1}, tvm::tir::Stmt>(tvm::runtime::ObjectPtr<tvm::runtime::Object>, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1})
  6: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  5: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7r
  4: tvm::tir::ThreadAllreduceBuilder::VisitStmt_(tvm::tir::AttrStmtNode const*)
  3: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  2: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runt
  1: tvm::tir::ThreadAllreduceBuilder::VisitStmt_(tvm::tir::EvaluateNode const*)
  0: tvm::tir::ThreadAllreduceBuilder::MakeAllreduce(tvm::tir::CallNode const*)
  File "/home2/xiachunwei/Software/tvm/src/tir/transforms/lower_thread_allreduce.cc", line 383
InternalError: Check failed: (!load_remap_.count(buffers[i]->data.get())) is false: 

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(64), T.int64(112), T.int64(112)), "float32"), p1: T.Buffer((T.int64(1), T.int64(64), T.int64(1), T.int64(1)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(64), T.int64(56), T.int64(56)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        pool_max_shared = T.alloc_buffer((T.int64(1), T.int64(64), T.int64(56), T.int64(56)), scope="shared")
        for ax0_ax1_ax2_ax3_0_fused in T.thread_binding(T.int64(14336), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 64, "pragma_unroll_explicit": 1}):
            for ax0, ax1, ax2, ax3, ax4_ax5_fused_0 in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(16), T.int64(1)):
                for ax4_ax5_fused_1 in T.thread_binding(T.int64(16), thread="threadIdx.x"):
                    with T.block("pool_max"):
                        v_ax0 = T.axis.spatial(T.int64(1), ax0)
                        v_ax1 = T.axis.spatial(T.int64(64), ax0_ax1_ax2_ax3_0_fused // T.int64(224) + ax1)
                        v_ax2 = T.axis.spatial(T.int64(56), ax0_ax1_ax2_ax3_0_fused % T.int64(224) // T.int64(4) + ax2)
                        v_ax3 = T.axis.spatial(T.int64(56), ax0_ax1_ax2_ax3_0_fused % T.int64(4) * T.int64(16) + ax3)
                        v_rv0 = T.axis.reduce(T.int64(3), (ax4_ax5_fused_0 * T.int64(16) + ax4_ax5_fused_1) // T.int64(3))
                        v_rv1 = T.axis.reduce(T.int64(3), (ax4_ax5_fused_0 * T.int64(16) + ax4_ax5_fused_1) % T.int64(3))
                        T.where(ax0_ax1_ax2_ax3_0_fused % T.int64(4) * T.int64(16) + ax3 < T.int64(56) and ax4_ax5_fused_0 * T.int64(16) + ax4_ax5_fused_1 < T.int64(9))
                        T.reads(p0[v_ax0, v_ax1, v_ax2 * T.int64(2) + v_rv0 - T.int64(1), v_ax3 * T.int64(2) + v_rv1 - T.int64(1)])
                        T.writes(pool_max_shared[v_ax0, v_ax1, v_ax2, v_ax3])
                        with T.init():
                            pool_max_shared[v_ax0, v_ax1, v_ax2, v_ax3] = T.float32(-3.4028234663852886e+38)
                        pool_max_shared[v_ax0, v_ax1, v_ax2, v_ax3] = T.max(pool_max_shared[v_ax0, v_ax1, v_ax2, v_ax3], T.if_then_else(T.int64(1) <= v_ax2 * T.int64(2) + v_rv0 and v_ax2 * T.int64(2) + v_rv0 < T.int64(113) and T.int64(1) <= v_ax3 * T.int64(2) + v_rv1 and v_ax3 * T.int64(2) + v_rv1 < T.int64(113), p0[v_ax0, v_ax1, v_ax2 * T.int64(2) + v_rv0 - T.int64(1), v_ax3 * T.int64(2) + v_rv1 - T.int64(1)], T.float32(-3.4028234663852886e+38)))
            for ax3_1 in T.thread_binding(T.int64(16), thread="threadIdx.x"):
                with T.block("T_add"):
                    v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                    v_ax1 = T.axis.spatial(T.int64(64), ax0_ax1_ax2_ax3_0_fused // T.int64(224))
                    v_ax2 = T.axis.spatial(T.int64(56), ax0_ax1_ax2_ax3_0_fused % T.int64(224) // T.int64(4))
                    v_ax3 = T.axis.spatial(T.int64(56), ax0_ax1_ax2_ax3_0_fused % T.int64(4) * T.int64(16) + ax3_1)
                    T.where(ax0_ax1_ax2_ax3_0_fused % T.int64(4) * T.int64(16) + ax3_1 < T.int64(56))
                    T.reads(pool_max_shared[v_ax0, v_ax1, v_ax2, v_ax3], p1[v_ax0, v_ax1, T.int64(0), T.int64(0)])
                    T.writes(T_relu[v_ax0, v_ax1, v_ax2, v_ax3])
                    T_relu[v_ax0, v_ax1, v_ax2, v_ax3] = T.max(pool_max_shared[v_ax0, v_ax1, v_ax2, v_ax3] + p1[v_ax0, v_ax1, T.int64(0), T.int64(0)], T.float32(0))
b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="pool_max", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b1, ann_key="schedule_rule")
sch.reverse_compute_inline(block=b3)
sch.compute_inline(block=b0)
b5, = sch.get_consumers(block=b1)
l6, l7, l8, l9 = sch.get_loops(block=b5)
v10 = sch.sample_categorical(candidates=[4, 8, 16, 32, 64, 128, 256, 512], probs=[0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125], decision=2)
l11, l12 = sch.split(loop=l9, factors=[None, v10], preserve_unit_iters=True)
sch.bind(loop=l12, thread_axis="threadIdx.x")
sch.compute_at(block=b1, loop=l11, preserve_unit_loops=True, index=-1)
sch.set_scope(block=b1, buffer_index=0, storage_scope="shared")
l13, l14, l15, l16, l17, l18, l19, l20, l21, l22 = sch.get_loops(block=b1)
l23 = sch.fuse(l21, l22, preserve_unit_iters=True)
l24, l25 = sch.split(loop=l23, factors=[None, v10], preserve_unit_iters=True)
sch.bind(loop=l25, thread_axis="threadIdx.x")
v26 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=2)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v26)
l27, l28, l29, l30, l31 = sch.get_loops(block=b2)
l32 = sch.fuse(l27, l28, l29, l30, preserve_unit_iters=True)
sch.bind(loop=l32, thread_axis="blockIdx.x")
sch.enter_postproc()
b33 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b33, ann_key="meta_schedule.unroll_explicit")
b34, b35 = sch.get_child_blocks(b33)
l36, l37, l38, l39, l40, l41, l42 = sch.get_loops(block=b34)
sch.annotate(block_or_loop=l36, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l36, ann_key="pragma_unroll_explicit", ann_val=1)
l43, l44 = sch.get_loops(block=b35)
2023-11-11 08:13:22 [INFO] [task_scheduler.cc:131] [Task #6: fused_nn_max_pool2d_add_nn_relu] Trial #203: GFLOPs: 96.9434. Time: 22.7735 us. Best GFLOPs: 295.7219
2023-11-11 08:13:22 [INFO] [task_scheduler.cc:131] [Task #6: fused_nn_max_pool2d_add_nn_relu] Trial #204: GFLOPs: 78.8658. Time: 27.9937 us. Best GFLOPs: 295.7219
2023-11-11 08:13:22 [INFO] [task_scheduler.cc:131] [Task #6: fused_nn_max_pool2d_add_nn_relu] Trial #205: GFLOPs: 38.0287. Time: 58.0547 us. Best GFLOPs: 295.7219
2023-11-11 08:13:22 [INFO] [task_scheduler.cc:131] [Task #6: fused_nn_max_pool2d_add_nn_relu] Trial #206: GFLOPs: 24.1370. Time: 91.4673 us. Best GFLOPs: 295.7219
2023-11-11 08:13:22 [INFO] [task_scheduler.cc:131] [Task #6: fused_nn_max_pool2d_add_nn_relu] Trial #207: GFLOPs: 24.1351. Time: 91.4744 us. Best GFLOPs: 295.7219
2023-11-11 08:13:22 [INFO] [task_scheduler.cc:131] [Task #6: fused_nn_max_pool2d_add_nn_relu] Trial #208: GFLOPs: 79.1418. Time: 27.8961 us. Best GFLOPs: 295.7219
2023-11-11 08:13:22 [INFO] [task_scheduler.cc:131] [Task #6: fused_nn_max_pool2d_add_nn_relu] Trial #209: GFLOPs: 6.4999. Time: 339.6591 us. Best GFLOPs: 295.7219
2023-11-11 08:13:22 [INFO] [task_scheduler.cc:131] [Task #6: fused_nn_max_pool2d_add_nn_relu] Trial #210: GFLOPs: 6.4994. Time: 339.6828 us. Best GFLOPs: 295.7219
2023-11-11 08:13:22 [INFO] [task_scheduler.cc:131] [Task #6: fused_nn_max_pool2d_add_nn_relu] Trial #211: GFLOPs: 6.4998. Time: 339.6612 us. Best GFLOPs: 295.7219
2023-11-11 08:13:22 [INFO] [task_scheduler.cc:121] [Task #6: fused_nn_max_pool2d_add_nn_relu] Trial #212: Error in building:
LocalBuilder: An exception occurred
Traceback (most recent call last):
  File "/home2/xiachunwei/Software/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/home2/xiachunwei/Software/tvm/python/tvm/meta_schedule/builder/local_builder.py", line 165, in <lambda>
    lambda x: _worker_func(*x),
  File "/home2/xiachunwei/Software/tvm/python/tvm/meta_schedule/builder/local_builder.py", line 231, in _worker_func
    rt_mod: Module = f_build(mod, target, _deserialize_params(params))
  File "/home2/xiachunwei/Software/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/home2/xiachunwei/Software/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
  File "/home2/xiachunwei/Software/tvm/python/tvm/meta_schedule/builder/local_builder.py", line 261, in default_build
    return tvm_build(mod, target=target)
  File "/home2/xiachunwei/Software/tvm/python/tvm/driver/build_module.py", line 281, in build
    rt_mod_host = _driver_ffi.tir_to_runtime(annotated_mods, target_host)
  File "/home2/xiachunwei/Software/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/home2/xiachunwei/Software/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  33: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::TypedPackedFunc<tvm::runtime::Module (tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)>::AssignTypedLambda<tvm::{lambda(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)#6}>(tvm::{lambda(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)#6}, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}> >::Call(tvm::runtime::PackedFuncObj const*, tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)
  32: tvm::TIRToRuntime(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target const&)
  31: tvm::SplitMixedModule(tvm::IRModule, tvm::Target const&, tvm::Target const&)
  30: tvm::ApplyPasses(tvm::IRModule, tvm::transform::Sequential)
  29: tvm::transform::Pass::operator()(tvm::IRModule) const
  28: tvm::transform::Pass::operator()(tvm::IRModule, tvm::transform::PassContext const&) const
  27: tvm::transform::SequentialNode::operator()(tvm::IRModule, tvm::transform::PassContext const&) const
  26: tvm::transform::Pass::operator()(tvm::IRModule, tvm::transform::PassContext const&) const
  25: tvm::tir::transform::PrimFuncPassNode::operator()(tvm::IRModule, tvm::transform::PassContext const&) const
  24: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::TypedPackedFunc<tvm::tir::PrimFunc (tvm::tir::PrimFunc, tvm::IRModule, tvm::transform::PassContext)>::AssignTypedLambda<tvm::tir::transform::LowerThreadAllreduce()::{lambda(tvm::tir::PrimFunc, tvm::IRModule, tvm::transform::PassContext)#1}>(tvm::tir::transform::LowerThreadAllreduce()::{lambda(tvm::tir::PrimFunc, tvm::IRModule, tvm::transform::PassContext)#1})::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}> >::Call(tvm::runtime::PackedFuncObj const*, tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)
  23: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7r
  22: tvm::tir::ThreadAllreduceBuilder::VisitStmt_(tvm::tir::AttrStmtNode const*)
  21: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  20: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7r
  19: tvm::tir::ThreadAllreduceBuilder::VisitStmt_(tvm::tir::AllocateNode const*)
  18: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  17: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7r
  16: tvm::tir::ThreadAllreduceBuilder::VisitStmt_(tvm::tir::AllocateNode const*)
  15: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  14: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7r
  13: tvm::tir::ThreadAllreduceBuilder::VisitStmt_(tvm::tir::AllocateNode const*)
  12: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  11: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7r
  10: tvm::tir::ThreadAllreduceBuilder::VisitStmt_(tvm::tir::AttrStmtNode const*)
  9: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  8: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runt
  7: tvm::runtime::ObjectPtr<tvm::runtime::Object> tvm::runtime::Array<tvm::tir::Stmt, void>::MapHelper<tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1}, tvm::tir::Stmt>(tvm::runtime::ObjectPtr<tvm::runtime::Object>, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1})
  6: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  5: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7r
  4: tvm::tir::ThreadAllreduceBuilder::VisitStmt_(tvm::tir::AttrStmtNode const*)
  3: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  2: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runt
  1: tvm::tir::ThreadAllreduceBuilder::VisitStmt_(tvm::tir::EvaluateNode const*)
  0: tvm::tir::ThreadAllreduceBuilder::MakeAllreduce(tvm::tir::CallNode const*)
  File "/home2/xiachunwei/Software/tvm/src/tir/transforms/lower_thread_allreduce.cc", line 383
InternalError: Check failed: (!load_remap_.count(buffers[i]->data.get())) is false: 

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(64), T.int64(112), T.int64(112)), "float32"), p1: T.Buffer((T.int64(1), T.int64(64), T.int64(1), T.int64(1)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(64), T.int64(56), T.int64(56)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        pool_max_shared = T.alloc_buffer((T.int64(1), T.int64(64), T.int64(56), T.int64(56)), scope="shared")
        for ax0_ax1_ax2_ax3_0_fused in T.thread_binding(T.int64(3584), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 1024, "pragma_unroll_explicit": 1}):
            for ax0, ax1, ax2, ax3, ax4_ax5_fused_0 in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(56), T.int64(1)):
                for ax4_ax5_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                    with T.block("pool_max"):
                        v_ax0 = T.axis.spatial(T.int64(1), ax0)
                        v_ax1 = T.axis.spatial(T.int64(64), ax0_ax1_ax2_ax3_0_fused // T.int64(56) + ax1)
                        v_ax2 = T.axis.spatial(T.int64(56), ax0_ax1_ax2_ax3_0_fused % T.int64(56) + ax2)
                        v_ax3 = T.axis.spatial(T.int64(56), ax3)
                        v_rv0 = T.axis.reduce(T.int64(3), (ax4_ax5_fused_0 * T.int64(64) + ax4_ax5_fused_1) // T.int64(3))
                        v_rv1 = T.axis.reduce(T.int64(3), (ax4_ax5_fused_0 * T.int64(64) + ax4_ax5_fused_1) % T.int64(3))
                        T.where(ax4_ax5_fused_0 * T.int64(64) + ax4_ax5_fused_1 < T.int64(9))
                        T.reads(p0[v_ax0, v_ax1, v_ax2 * T.int64(2) + v_rv0 - T.int64(1), v_ax3 * T.int64(2) + v_rv1 - T.int64(1)])
                        T.writes(pool_max_shared[v_ax0, v_ax1, v_ax2, v_ax3])
                        with T.init():
                            pool_max_shared[v_ax0, v_ax1, v_ax2, v_ax3] = T.float32(-3.4028234663852886e+38)
                        pool_max_shared[v_ax0, v_ax1, v_ax2, v_ax3] = T.max(pool_max_shared[v_ax0, v_ax1, v_ax2, v_ax3], T.if_then_else(T.int64(1) <= v_ax2 * T.int64(2) + v_rv0 and v_ax2 * T.int64(2) + v_rv0 < T.int64(113) and T.int64(1) <= v_ax3 * T.int64(2) + v_rv1 and v_ax3 * T.int64(2) + v_rv1 < T.int64(113), p0[v_ax0, v_ax1, v_ax2 * T.int64(2) + v_rv0 - T.int64(1), v_ax3 * T.int64(2) + v_rv1 - T.int64(1)], T.float32(-3.4028234663852886e+38)))
            for ax3_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                with T.block("T_add"):
                    v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                    v_ax1 = T.axis.spatial(T.int64(64), ax0_ax1_ax2_ax3_0_fused // T.int64(56))
                    v_ax2 = T.axis.spatial(T.int64(56), ax0_ax1_ax2_ax3_0_fused % T.int64(56))
                    v_ax3 = T.axis.spatial(T.int64(56), ax3_1)
                    T.where(T.Mul(T.int64(0), T.int64(64)) + ax3_1 < T.int64(56))
                    T.reads(pool_max_shared[v_ax0, v_ax1, v_ax2, v_ax3], p1[v_ax0, v_ax1, T.int64(0), T.int64(0)])
                    T.writes(T_relu[v_ax0, v_ax1, v_ax2, v_ax3])
                    T_relu[v_ax0, v_ax1, v_ax2, v_ax3] = T.max(pool_max_shared[v_ax0, v_ax1, v_ax2, v_ax3] + p1[v_ax0, v_ax1, T.int64(0), T.int64(0)], T.float32(0))
b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="pool_max", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b1, ann_key="schedule_rule")
sch.reverse_compute_inline(block=b3)
sch.compute_inline(block=b0)
b5, = sch.get_consumers(block=b1)
l6, l7, l8, l9 = sch.get_loops(block=b5)
v10 = sch.sample_categorical(candidates=[4, 8, 16, 32, 64, 128, 256, 512], probs=[0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125], decision=4)
l11, l12 = sch.split(loop=l9, factors=[None, v10], preserve_unit_iters=True)
sch.bind(loop=l12, thread_axis="threadIdx.x")
sch.compute_at(block=b1, loop=l11, preserve_unit_loops=True, index=-1)
sch.set_scope(block=b1, buffer_index=0, storage_scope="shared")
l13, l14, l15, l16, l17, l18, l19, l20, l21, l22 = sch.get_loops(block=b1)
l23 = sch.fuse(l21, l22, preserve_unit_iters=True)
l24, l25 = sch.split(loop=l23, factors=[None, v10], preserve_unit_iters=True)
sch.bind(loop=l25, thread_axis="threadIdx.x")
v26 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v26)
l27, l28, l29, l30, l31 = sch.get_loops(block=b2)
l32 = sch.fuse(l27, l28, l29, l30, preserve_unit_iters=True)
sch.bind(loop=l32, thread_axis="blockIdx.x")
sch.enter_postproc()
b33 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b33, ann_key="meta_schedule.unroll_explicit")
b34, b35 = sch.get_child_blocks(b33)
l36, l37, l38, l39, l40, l41, l42 = sch.get_loops(block=b34)
sch.annotate(block_or_loop=l36, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l36, ann_key="pragma_unroll_explicit", ann_val=1)
l43, l44 = sch.get_loops(block=b35)
2023-11-11 08:13:22 [INFO] [task_scheduler.cc:121] [Task #6: fused_nn_max_pool2d_add_nn_relu] Trial #213: Error in building:
LocalBuilder: An exception occurred
Traceback (most recent call last):
  File "/home2/xiachunwei/Software/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/home2/xiachunwei/Software/tvm/python/tvm/meta_schedule/builder/local_builder.py", line 165, in <lambda>
    lambda x: _worker_func(*x),
  File "/home2/xiachunwei/Software/tvm/python/tvm/meta_schedule/builder/local_builder.py", line 231, in _worker_func
    rt_mod: Module = f_build(mod, target, _deserialize_params(params))
  File "/home2/xiachunwei/Software/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/home2/xiachunwei/Software/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
  File "/home2/xiachunwei/Software/tvm/python/tvm/meta_schedule/builder/local_builder.py", line 261, in default_build
    return tvm_build(mod, target=target)
  File "/home2/xiachunwei/Software/tvm/python/tvm/driver/build_module.py", line 281, in build
    rt_mod_host = _driver_ffi.tir_to_runtime(annotated_mods, target_host)
  File "/home2/xiachunwei/Software/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/home2/xiachunwei/Software/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  33: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::TypedPackedFunc<tvm::runtime::Module (tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)>::AssignTypedLambda<tvm::{lambda(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)#6}>(tvm::{lambda(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)#6}, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}> >::Call(tvm::runtime::PackedFuncObj const*, tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)
  32: tvm::TIRToRuntime(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target const&)
  31: tvm::SplitMixedModule(tvm::IRModule, tvm::Target const&, tvm::Target const&)
  30: tvm::ApplyPasses(tvm::IRModule, tvm::transform::Sequential)
  29: tvm::transform::Pass::operator()(tvm::IRModule) const
  28: tvm::transform::Pass::operator()(tvm::IRModule, tvm::transform::PassContext const&) const
  27: tvm::transform::SequentialNode::operator()(tvm::IRModule, tvm::transform::PassContext const&) const
  26: tvm::transform::Pass::operator()(tvm::IRModule, tvm::transform::PassContext const&) const
  25: tvm::tir::transform::PrimFuncPassNode::operator()(tvm::IRModule, tvm::transform::PassContext const&) const
  24: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::TypedPackedFunc<tvm::tir::PrimFunc (tvm::tir::PrimFunc, tvm::IRModule, tvm::transform::PassContext)>::AssignTypedLambda<tvm::tir::transform::LowerThreadAllreduce()::{lambda(tvm::tir::PrimFunc, tvm::IRModule, tvm::transform::PassContext)#1}>(tvm::tir::transform::LowerThreadAllreduce()::{lambda(tvm::tir::PrimFunc, tvm::IRModule, tvm::transform::PassContext)#1})::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}> >::Call(tvm::runtime::PackedFuncObj const*, tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)
  23: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7r
  22: tvm::tir::ThreadAllreduceBuilder::VisitStmt_(tvm::tir::AttrStmtNode const*)
  21: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  20: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7r
  19: tvm::tir::ThreadAllreduceBuilder::VisitStmt_(tvm::tir::AllocateNode const*)
  18: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  17: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7r
  16: tvm::tir::ThreadAllreduceBuilder::VisitStmt_(tvm::tir::AllocateNode const*)
  15: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  14: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7r
  13: tvm::tir::ThreadAllreduceBuilder::VisitStmt_(tvm::tir::AllocateNode const*)
  12: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  11: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7r
  10: tvm::tir::ThreadAllreduceBuilder::VisitStmt_(tvm::tir::AttrStmtNode const*)
  9: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  8: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runt
  7: tvm::runtime::ObjectPtr<tvm::runtime::Object> tvm::runtime::Array<tvm::tir::Stmt, void>::MapHelper<tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1}, tvm::tir::Stmt>(tvm::runtime::ObjectPtr<tvm::runtime::Object>, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1})
  6: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  5: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7r
  4: tvm::tir::ThreadAllreduceBuilder::VisitStmt_(tvm::tir::AttrStmtNode const*)
  3: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  2: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runt
  1: tvm::tir::ThreadAllreduceBuilder::VisitStmt_(tvm::tir::EvaluateNode const*)
  0: tvm::tir::ThreadAllreduceBuilder::MakeAllreduce(tvm::tir::CallNode const*)
  File "/home2/xiachunwei/Software/tvm/src/tir/transforms/lower_thread_allreduce.cc", line 383
InternalError: Check failed: (!load_remap_.count(buffers[i]->data.get())) is false: 

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(64), T.int64(112), T.int64(112)), "float32"), p1: T.Buffer((T.int64(1), T.int64(64), T.int64(1), T.int64(1)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(64), T.int64(56), T.int64(56)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        pool_max_shared = T.alloc_buffer((T.int64(1), T.int64(64), T.int64(56), T.int64(56)), scope="shared")
        for ax0_ax1_ax2_ax3_0_fused in T.thread_binding(T.int64(14336), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 512, "pragma_unroll_explicit": 1}):
            for ax0, ax1, ax2, ax3, ax4_ax5_fused_0 in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(16), T.int64(1)):
                for ax4_ax5_fused_1 in T.thread_binding(T.int64(16), thread="threadIdx.x"):
                    with T.block("pool_max"):
                        v_ax0 = T.axis.spatial(T.int64(1), ax0)
                        v_ax1 = T.axis.spatial(T.int64(64), ax0_ax1_ax2_ax3_0_fused // T.int64(224) + ax1)
                        v_ax2 = T.axis.spatial(T.int64(56), ax0_ax1_ax2_ax3_0_fused % T.int64(224) // T.int64(4) + ax2)
                        v_ax3 = T.axis.spatial(T.int64(56), ax0_ax1_ax2_ax3_0_fused % T.int64(4) * T.int64(16) + ax3)
                        v_rv0 = T.axis.reduce(T.int64(3), (ax4_ax5_fused_0 * T.int64(16) + ax4_ax5_fused_1) // T.int64(3))
                        v_rv1 = T.axis.reduce(T.int64(3), (ax4_ax5_fused_0 * T.int64(16) + ax4_ax5_fused_1) % T.int64(3))
                        T.where(ax0_ax1_ax2_ax3_0_fused % T.int64(4) * T.int64(16) + ax3 < T.int64(56) and ax4_ax5_fused_0 * T.int64(16) + ax4_ax5_fused_1 < T.int64(9))
                        T.reads(p0[v_ax0, v_ax1, v_ax2 * T.int64(2) + v_rv0 - T.int64(1), v_ax3 * T.int64(2) + v_rv1 - T.int64(1)])
                        T.writes(pool_max_shared[v_ax0, v_ax1, v_ax2, v_ax3])
                        with T.init():
                            pool_max_shared[v_ax0, v_ax1, v_ax2, v_ax3] = T.float32(-3.4028234663852886e+38)
                        pool_max_shared[v_ax0, v_ax1, v_ax2, v_ax3] = T.max(pool_max_shared[v_ax0, v_ax1, v_ax2, v_ax3], T.if_then_else(T.int64(1) <= v_ax2 * T.int64(2) + v_rv0 and v_ax2 * T.int64(2) + v_rv0 < T.int64(113) and T.int64(1) <= v_ax3 * T.int64(2) + v_rv1 and v_ax3 * T.int64(2) + v_rv1 < T.int64(113), p0[v_ax0, v_ax1, v_ax2 * T.int64(2) + v_rv0 - T.int64(1), v_ax3 * T.int64(2) + v_rv1 - T.int64(1)], T.float32(-3.4028234663852886e+38)))
            for ax3_1 in T.thread_binding(T.int64(16), thread="threadIdx.x"):
                with T.block("T_add"):
                    v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                    v_ax1 = T.axis.spatial(T.int64(64), ax0_ax1_ax2_ax3_0_fused // T.int64(224))
                    v_ax2 = T.axis.spatial(T.int64(56), ax0_ax1_ax2_ax3_0_fused % T.int64(224) // T.int64(4))
                    v_ax3 = T.axis.spatial(T.int64(56), ax0_ax1_ax2_ax3_0_fused % T.int64(4) * T.int64(16) + ax3_1)
                    T.where(ax0_ax1_ax2_ax3_0_fused % T.int64(4) * T.int64(16) + ax3_1 < T.int64(56))
                    T.reads(pool_max_shared[v_ax0, v_ax1, v_ax2, v_ax3], p1[v_ax0, v_ax1, T.int64(0), T.int64(0)])
                    T.writes(T_relu[v_ax0, v_ax1, v_ax2, v_ax3])
                    T_relu[v_ax0, v_ax1, v_ax2, v_ax3] = T.max(pool_max_shared[v_ax0, v_ax1, v_ax2, v_ax3] + p1[v_ax0, v_ax1, T.int64(0), T.int64(0)], T.float32(0))
b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="pool_max", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b1, ann_key="schedule_rule")
sch.reverse_compute_inline(block=b3)
sch.compute_inline(block=b0)
b5, = sch.get_consumers(block=b1)
l6, l7, l8, l9 = sch.get_loops(block=b5)
v10 = sch.sample_categorical(candidates=[4, 8, 16, 32, 64, 128, 256, 512], probs=[0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125], decision=2)
l11, l12 = sch.split(loop=l9, factors=[None, v10], preserve_unit_iters=True)
sch.bind(loop=l12, thread_axis="threadIdx.x")
sch.compute_at(block=b1, loop=l11, preserve_unit_loops=True, index=-1)
sch.set_scope(block=b1, buffer_index=0, storage_scope="shared")
l13, l14, l15, l16, l17, l18, l19, l20, l21, l22 = sch.get_loops(block=b1)
l23 = sch.fuse(l21, l22, preserve_unit_iters=True)
l24, l25 = sch.split(loop=l23, factors=[None, v10], preserve_unit_iters=True)
sch.bind(loop=l25, thread_axis="threadIdx.x")
v26 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v26)
l27, l28, l29, l30, l31 = sch.get_loops(block=b2)
l32 = sch.fuse(l27, l28, l29, l30, preserve_unit_iters=True)
sch.bind(loop=l32, thread_axis="blockIdx.x")
sch.enter_postproc()
b33 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b33, ann_key="meta_schedule.unroll_explicit")
b34, b35 = sch.get_child_blocks(b33)
l36, l37, l38, l39, l40, l41, l42 = sch.get_loops(block=b34)
sch.annotate(block_or_loop=l36, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l36, ann_key="pragma_unroll_explicit", ann_val=1)
l43, l44 = sch.get_loops(block=b35)
2023-11-11 08:13:22 [INFO] [task_scheduler.cc:121] [Task #6: fused_nn_max_pool2d_add_nn_relu] Trial #214: Error in building:
LocalBuilder: An exception occurred
Traceback (most recent call last):
  File "/home2/xiachunwei/Software/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/home2/xiachunwei/Software/tvm/python/tvm/meta_schedule/builder/local_builder.py", line 165, in <lambda>
    lambda x: _worker_func(*x),
  File "/home2/xiachunwei/Software/tvm/python/tvm/meta_schedule/builder/local_builder.py", line 231, in _worker_func
    rt_mod: Module = f_build(mod, target, _deserialize_params(params))
  File "/home2/xiachunwei/Software/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/home2/xiachunwei/Software/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
  File "/home2/xiachunwei/Software/tvm/python/tvm/meta_schedule/builder/local_builder.py", line 261, in default_build
    return tvm_build(mod, target=target)
  File "/home2/xiachunwei/Software/tvm/python/tvm/driver/build_module.py", line 281, in build
    rt_mod_host = _driver_ffi.tir_to_runtime(annotated_mods, target_host)
  File "/home2/xiachunwei/Software/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/home2/xiachunwei/Software/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  33: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::TypedPackedFunc<tvm::runtime::Module (tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)>::AssignTypedLambda<tvm::{lambda(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)#6}>(tvm::{lambda(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)#6}, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}> >::Call(tvm::runtime::PackedFuncObj const*, tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)
  32: tvm::TIRToRuntime(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target const&)
  31: tvm::SplitMixedModule(tvm::IRModule, tvm::Target const&, tvm::Target const&)
  30: tvm::ApplyPasses(tvm::IRModule, tvm::transform::Sequential)
  29: tvm::transform::Pass::operator()(tvm::IRModule) const
  28: tvm::transform::Pass::operator()(tvm::IRModule, tvm::transform::PassContext const&) const
  27: tvm::transform::SequentialNode::operator()(tvm::IRModule, tvm::transform::PassContext const&) const
  26: tvm::transform::Pass::operator()(tvm::IRModule, tvm::transform::PassContext const&) const
  25: tvm::tir::transform::PrimFuncPassNode::operator()(tvm::IRModule, tvm::transform::PassContext const&) const
  24: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::TypedPackedFunc<tvm::tir::PrimFunc (tvm::tir::PrimFunc, tvm::IRModule, tvm::transform::PassContext)>::AssignTypedLambda<tvm::tir::transform::LowerThreadAllreduce()::{lambda(tvm::tir::PrimFunc, tvm::IRModule, tvm::transform::PassContext)#1}>(tvm::tir::transform::LowerThreadAllreduce()::{lambda(tvm::tir::PrimFunc, tvm::IRModule, tvm::transform::PassContext)#1})::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}> >::Call(tvm::runtime::PackedFuncObj const*, tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)
  23: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7r
  22: tvm::tir::ThreadAllreduceBuilder::VisitStmt_(tvm::tir::AttrStmtNode const*)
  21: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  20: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7r
  19: tvm::tir::ThreadAllreduceBuilder::VisitStmt_(tvm::tir::AllocateNode const*)
  18: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  17: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7r
  16: tvm::tir::ThreadAllreduceBuilder::VisitStmt_(tvm::tir::AllocateNode const*)
  15: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  14: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7r
  13: tvm::tir::ThreadAllreduceBuilder::VisitStmt_(tvm::tir::AllocateNode const*)
  12: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  11: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7r
  10: tvm::tir::ThreadAllreduceBuilder::VisitStmt_(tvm::tir::AttrStmtNode const*)
  9: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  8: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runt
  7: tvm::runtime::ObjectPtr<tvm::runtime::Object> tvm::runtime::Array<tvm::tir::Stmt, void>::MapHelper<tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1}, tvm::tir::Stmt>(tvm::runtime::ObjectPtr<tvm::runtime::Object>, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1})
  6: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  5: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7r
  4: tvm::tir::ThreadAllreduceBuilder::VisitStmt_(tvm::tir::AttrStmtNode const*)
  3: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  2: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runt
  1: tvm::tir::ThreadAllreduceBuilder::VisitStmt_(tvm::tir::EvaluateNode const*)
  0: tvm::tir::ThreadAllreduceBuilder::MakeAllreduce(tvm::tir::CallNode const*)
  File "/home2/xiachunwei/Software/tvm/src/tir/transforms/lower_thread_allreduce.cc", line 383
InternalError: Check failed: (!load_remap_.count(buffers[i]->data.get())) is false: 

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(64), T.int64(112), T.int64(112)), "float32"), p1: T.Buffer((T.int64(1), T.int64(64), T.int64(1), T.int64(1)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(64), T.int64(56), T.int64(56)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        pool_max_shared = T.alloc_buffer((T.int64(1), T.int64(64), T.int64(56), T.int64(56)), scope="shared")
        for ax0_ax1_ax2_ax3_0_fused in T.thread_binding(T.int64(7168), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 512, "pragma_unroll_explicit": 1}):
            for ax0, ax1, ax2, ax3, ax4_ax5_fused_0 in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(32), T.int64(1)):
                for ax4_ax5_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                    with T.block("pool_max"):
                        v_ax0 = T.axis.spatial(T.int64(1), ax0)
                        v_ax1 = T.axis.spatial(T.int64(64), ax0_ax1_ax2_ax3_0_fused // T.int64(112) + ax1)
                        v_ax2 = T.axis.spatial(T.int64(56), ax0_ax1_ax2_ax3_0_fused % T.int64(112) // T.int64(2) + ax2)
                        v_ax3 = T.axis.spatial(T.int64(56), ax0_ax1_ax2_ax3_0_fused % T.int64(2) * T.int64(32) + ax3)
                        v_rv0 = T.axis.reduce(T.int64(3), (ax4_ax5_fused_0 * T.int64(32) + ax4_ax5_fused_1) // T.int64(3))
                        v_rv1 = T.axis.reduce(T.int64(3), (ax4_ax5_fused_0 * T.int64(32) + ax4_ax5_fused_1) % T.int64(3))
                        T.where(ax0_ax1_ax2_ax3_0_fused % T.int64(2) * T.int64(32) + ax3 < T.int64(56) and ax4_ax5_fused_0 * T.int64(32) + ax4_ax5_fused_1 < T.int64(9))
                        T.reads(p0[v_ax0, v_ax1, v_ax2 * T.int64(2) + v_rv0 - T.int64(1), v_ax3 * T.int64(2) + v_rv1 - T.int64(1)])
                        T.writes(pool_max_shared[v_ax0, v_ax1, v_ax2, v_ax3])
                        with T.init():
                            pool_max_shared[v_ax0, v_ax1, v_ax2, v_ax3] = T.float32(-3.4028234663852886e+38)
                        pool_max_shared[v_ax0, v_ax1, v_ax2, v_ax3] = T.max(pool_max_shared[v_ax0, v_ax1, v_ax2, v_ax3], T.if_then_else(T.int64(1) <= v_ax2 * T.int64(2) + v_rv0 and v_ax2 * T.int64(2) + v_rv0 < T.int64(113) and T.int64(1) <= v_ax3 * T.int64(2) + v_rv1 and v_ax3 * T.int64(2) + v_rv1 < T.int64(113), p0[v_ax0, v_ax1, v_ax2 * T.int64(2) + v_rv0 - T.int64(1), v_ax3 * T.int64(2) + v_rv1 - T.int64(1)], T.float32(-3.4028234663852886e+38)))
            for ax3_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                with T.block("T_add"):
                    v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                    v_ax1 = T.axis.spatial(T.int64(64), ax0_ax1_ax2_ax3_0_fused // T.int64(112))
                    v_ax2 = T.axis.spatial(T.int64(56), ax0_ax1_ax2_ax3_0_fused % T.int64(112) // T.int64(2))
                    v_ax3 = T.axis.spatial(T.int64(56), ax0_ax1_ax2_ax3_0_fused % T.int64(2) * T.int64(32) + ax3_1)
                    T.where(ax0_ax1_ax2_ax3_0_fused % T.int64(2) * T.int64(32) + ax3_1 < T.int64(56))
                    T.reads(pool_max_shared[v_ax0, v_ax1, v_ax2, v_ax3], p1[v_ax0, v_ax1, T.int64(0), T.int64(0)])
                    T.writes(T_relu[v_ax0, v_ax1, v_ax2, v_ax3])
                    T_relu[v_ax0, v_ax1, v_ax2, v_ax3] = T.max(pool_max_shared[v_ax0, v_ax1, v_ax2, v_ax3] + p1[v_ax0, v_ax1, T.int64(0), T.int64(0)], T.float32(0))
b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="pool_max", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b1, ann_key="schedule_rule")
sch.reverse_compute_inline(block=b3)
sch.compute_inline(block=b0)
b5, = sch.get_consumers(block=b1)
l6, l7, l8, l9 = sch.get_loops(block=b5)
v10 = sch.sample_categorical(candidates=[4, 8, 16, 32, 64, 128, 256, 512], probs=[0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125], decision=3)
l11, l12 = sch.split(loop=l9, factors=[None, v10], preserve_unit_iters=True)
sch.bind(loop=l12, thread_axis="threadIdx.x")
sch.compute_at(block=b1, loop=l11, preserve_unit_loops=True, index=-1)
sch.set_scope(block=b1, buffer_index=0, storage_scope="shared")
l13, l14, l15, l16, l17, l18, l19, l20, l21, l22 = sch.get_loops(block=b1)
l23 = sch.fuse(l21, l22, preserve_unit_iters=True)
l24, l25 = sch.split(loop=l23, factors=[None, v10], preserve_unit_iters=True)
sch.bind(loop=l25, thread_axis="threadIdx.x")
v26 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v26)
l27, l28, l29, l30, l31 = sch.get_loops(block=b2)
l32 = sch.fuse(l27, l28, l29, l30, preserve_unit_iters=True)
sch.bind(loop=l32, thread_axis="blockIdx.x")
sch.enter_postproc()
b33 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b33, ann_key="meta_schedule.unroll_explicit")
b34, b35 = sch.get_child_blocks(b33)
l36, l37, l38, l39, l40, l41, l42 = sch.get_loops(block=b34)
sch.annotate(block_or_loop=l36, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l36, ann_key="pragma_unroll_explicit", ann_val=1)
l43, l44 = sch.get_loops(block=b35)
2023-11-11 08:13:22 [INFO] [task_scheduler.cc:131] [Task #6: fused_nn_max_pool2d_add_nn_relu] Trial #215: GFLOPs: 13.2633. Time: 166.4554 us. Best GFLOPs: 295.7219
2023-11-11 08:13:22 [INFO] [task_scheduler.cc:131] [Task #6: fused_nn_max_pool2d_add_nn_relu] Trial #216: GFLOPs: 13.2640. Time: 166.4468 us. Best GFLOPs: 295.7219
2023-11-11 08:13:22 [INFO] [task_scheduler.cc:121] [Task #6: fused_nn_max_pool2d_add_nn_relu] Trial #217: Error in building:
LocalBuilder: An exception occurred
Traceback (most recent call last):
  File "/home2/xiachunwei/Software/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/home2/xiachunwei/Software/tvm/python/tvm/meta_schedule/builder/local_builder.py", line 165, in <lambda>
    lambda x: _worker_func(*x),
  File "/home2/xiachunwei/Software/tvm/python/tvm/meta_schedule/builder/local_builder.py", line 231, in _worker_func
    rt_mod: Module = f_build(mod, target, _deserialize_params(params))
  File "/home2/xiachunwei/Software/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/home2/xiachunwei/Software/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
  File "/home2/xiachunwei/Software/tvm/python/tvm/meta_schedule/builder/local_builder.py", line 261, in default_build
    return tvm_build(mod, target=target)
  File "/home2/xiachunwei/Software/tvm/python/tvm/driver/build_module.py", line 281, in build
    rt_mod_host = _driver_ffi.tir_to_runtime(annotated_mods, target_host)
  File "/home2/xiachunwei/Software/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/home2/xiachunwei/Software/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  33: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::TypedPackedFunc<tvm::runtime::Module (tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)>::AssignTypedLambda<tvm::{lambda(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)#6}>(tvm::{lambda(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)#6}, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}> >::Call(tvm::runtime::PackedFuncObj const*, tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)
  32: tvm::TIRToRuntime(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target const&)
  31: tvm::SplitMixedModule(tvm::IRModule, tvm::Target const&, tvm::Target const&)
  30: tvm::ApplyPasses(tvm::IRModule, tvm::transform::Sequential)
  29: tvm::transform::Pass::operator()(tvm::IRModule) const
  28: tvm::transform::Pass::operator()(tvm::IRModule, tvm::transform::PassContext const&) const
  27: tvm::transform::SequentialNode::operator()(tvm::IRModule, tvm::transform::PassContext const&) const
  26: tvm::transform::Pass::operator()(tvm::IRModule, tvm::transform::PassContext const&) const
  25: tvm::tir::transform::PrimFuncPassNode::operator()(tvm::IRModule, tvm::transform::PassContext const&) const
  24: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::TypedPackedFunc<tvm::tir::PrimFunc (tvm::tir::PrimFunc, tvm::IRModule, tvm::transform::PassContext)>::AssignTypedLambda<tvm::tir::transform::LowerThreadAllreduce()::{lambda(tvm::tir::PrimFunc, tvm::IRModule, tvm::transform::PassContext)#1}>(tvm::tir::transform::LowerThreadAllreduce()::{lambda(tvm::tir::PrimFunc, tvm::IRModule, tvm::transform::PassContext)#1})::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}> >::Call(tvm::runtime::PackedFuncObj const*, tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)
  23: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7r
  22: tvm::tir::ThreadAllreduceBuilder::VisitStmt_(tvm::tir::AttrStmtNode const*)
  21: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  20: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7r
  19: tvm::tir::ThreadAllreduceBuilder::VisitStmt_(tvm::tir::AllocateNode const*)
  18: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  17: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7r
  16: tvm::tir::ThreadAllreduceBuilder::VisitStmt_(tvm::tir::AllocateNode const*)
  15: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  14: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7r
  13: tvm::tir::ThreadAllreduceBuilder::VisitStmt_(tvm::tir::AllocateNode const*)
  12: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  11: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7r
  10: tvm::tir::ThreadAllreduceBuilder::VisitStmt_(tvm::tir::AttrStmtNode const*)
  9: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  8: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runt
  7: tvm::runtime::ObjectPtr<tvm::runtime::Object> tvm::runtime::Array<tvm::tir::Stmt, void>::MapHelper<tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1}, tvm::tir::Stmt>(tvm::runtime::ObjectPtr<tvm::runtime::Object>, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1})
  6: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  5: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7r
  4: tvm::tir::ThreadAllreduceBuilder::VisitStmt_(tvm::tir::AttrStmtNode const*)
  3: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  2: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runt
  1: tvm::tir::ThreadAllreduceBuilder::VisitStmt_(tvm::tir::EvaluateNode const*)
  0: tvm::tir::ThreadAllreduceBuilder::MakeAllreduce(tvm::tir::CallNode const*)
  File "/home2/xiachunwei/Software/tvm/src/tir/transforms/lower_thread_allreduce.cc", line 383
InternalError: Check failed: (!load_remap_.count(buffers[i]->data.get())) is false: 

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(64), T.int64(112), T.int64(112)), "float32"), p1: T.Buffer((T.int64(1), T.int64(64), T.int64(1), T.int64(1)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(64), T.int64(56), T.int64(56)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        pool_max_shared = T.alloc_buffer((T.int64(1), T.int64(64), T.int64(56), T.int64(56)), scope="shared")
        for ax0_ax1_ax2_ax3_0_fused in T.thread_binding(T.int64(3584), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 512, "pragma_unroll_explicit": 1}):
            for ax0, ax1, ax2, ax3, ax4_ax5_fused_0 in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(56), T.int64(1)):
                for ax4_ax5_fused_1 in T.thread_binding(T.int64(512), thread="threadIdx.x"):
                    with T.block("pool_max"):
                        v_ax0 = T.axis.spatial(T.int64(1), ax0)
                        v_ax1 = T.axis.spatial(T.int64(64), ax0_ax1_ax2_ax3_0_fused // T.int64(56) + ax1)
                        v_ax2 = T.axis.spatial(T.int64(56), ax0_ax1_ax2_ax3_0_fused % T.int64(56) + ax2)
                        v_ax3 = T.axis.spatial(T.int64(56), ax3)
                        v_rv0 = T.axis.reduce(T.int64(3), (ax4_ax5_fused_0 * T.int64(512) + ax4_ax5_fused_1) // T.int64(3))
                        v_rv1 = T.axis.reduce(T.int64(3), (ax4_ax5_fused_0 * T.int64(512) + ax4_ax5_fused_1) % T.int64(3))
                        T.where(ax4_ax5_fused_0 * T.int64(512) + ax4_ax5_fused_1 < T.int64(9))
                        T.reads(p0[v_ax0, v_ax1, v_ax2 * T.int64(2) + v_rv0 - T.int64(1), v_ax3 * T.int64(2) + v_rv1 - T.int64(1)])
                        T.writes(pool_max_shared[v_ax0, v_ax1, v_ax2, v_ax3])
                        with T.init():
                            pool_max_shared[v_ax0, v_ax1, v_ax2, v_ax3] = T.float32(-3.4028234663852886e+38)
                        pool_max_shared[v_ax0, v_ax1, v_ax2, v_ax3] = T.max(pool_max_shared[v_ax0, v_ax1, v_ax2, v_ax3], T.if_then_else(T.int64(1) <= v_ax2 * T.int64(2) + v_rv0 and v_ax2 * T.int64(2) + v_rv0 < T.int64(113) and T.int64(1) <= v_ax3 * T.int64(2) + v_rv1 and v_ax3 * T.int64(2) + v_rv1 < T.int64(113), p0[v_ax0, v_ax1, v_ax2 * T.int64(2) + v_rv0 - T.int64(1), v_ax3 * T.int64(2) + v_rv1 - T.int64(1)], T.float32(-3.4028234663852886e+38)))
            for ax3_1 in T.thread_binding(T.int64(512), thread="threadIdx.x"):
                with T.block("T_add"):
                    v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                    v_ax1 = T.axis.spatial(T.int64(64), ax0_ax1_ax2_ax3_0_fused // T.int64(56))
                    v_ax2 = T.axis.spatial(T.int64(56), ax0_ax1_ax2_ax3_0_fused % T.int64(56))
                    v_ax3 = T.axis.spatial(T.int64(56), ax3_1)
                    T.where(T.Mul(T.int64(0), T.int64(512)) + ax3_1 < T.int64(56))
                    T.reads(pool_max_shared[v_ax0, v_ax1, v_ax2, v_ax3], p1[v_ax0, v_ax1, T.int64(0), T.int64(0)])
                    T.writes(T_relu[v_ax0, v_ax1, v_ax2, v_ax3])
                    T_relu[v_ax0, v_ax1, v_ax2, v_ax3] = T.max(pool_max_shared[v_ax0, v_ax1, v_ax2, v_ax3] + p1[v_ax0, v_ax1, T.int64(0), T.int64(0)], T.float32(0))
b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="pool_max", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b1, ann_key="schedule_rule")
sch.reverse_compute_inline(block=b3)
sch.compute_inline(block=b0)
b5, = sch.get_consumers(block=b1)
l6, l7, l8, l9 = sch.get_loops(block=b5)
v10 = sch.sample_categorical(candidates=[4, 8, 16, 32, 64, 128, 256, 512], probs=[0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125], decision=7)
l11, l12 = sch.split(loop=l9, factors=[None, v10], preserve_unit_iters=True)
sch.bind(loop=l12, thread_axis="threadIdx.x")
sch.compute_at(block=b1, loop=l11, preserve_unit_loops=True, index=-1)
sch.set_scope(block=b1, buffer_index=0, storage_scope="shared")
l13, l14, l15, l16, l17, l18, l19, l20, l21, l22 = sch.get_loops(block=b1)
l23 = sch.fuse(l21, l22, preserve_unit_iters=True)
l24, l25 = sch.split(loop=l23, factors=[None, v10], preserve_unit_iters=True)
sch.bind(loop=l25, thread_axis="threadIdx.x")
v26 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v26)
l27, l28, l29, l30, l31 = sch.get_loops(block=b2)
l32 = sch.fuse(l27, l28, l29, l30, preserve_unit_iters=True)
sch.bind(loop=l32, thread_axis="blockIdx.x")
sch.enter_postproc()
b33 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b33, ann_key="meta_schedule.unroll_explicit")
b34, b35 = sch.get_child_blocks(b33)
l36, l37, l38, l39, l40, l41, l42 = sch.get_loops(block=b34)
sch.annotate(block_or_loop=l36, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l36, ann_key="pragma_unroll_explicit", ann_val=1)
l43, l44 = sch.get_loops(block=b35)
2023-11-11 08:13:22 [INFO] [task_scheduler.cc:121] [Task #6: fused_nn_max_pool2d_add_nn_relu] Trial #218: Error in building:
LocalBuilder: An exception occurred
Traceback (most recent call last):
  File "/home2/xiachunwei/Software/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/home2/xiachunwei/Software/tvm/python/tvm/meta_schedule/builder/local_builder.py", line 165, in <lambda>
    lambda x: _worker_func(*x),
  File "/home2/xiachunwei/Software/tvm/python/tvm/meta_schedule/builder/local_builder.py", line 231, in _worker_func
    rt_mod: Module = f_build(mod, target, _deserialize_params(params))
  File "/home2/xiachunwei/Software/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/home2/xiachunwei/Software/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
  File "/home2/xiachunwei/Software/tvm/python/tvm/meta_schedule/builder/local_builder.py", line 261, in default_build
    return tvm_build(mod, target=target)
  File "/home2/xiachunwei/Software/tvm/python/tvm/driver/build_module.py", line 281, in build
    rt_mod_host = _driver_ffi.tir_to_runtime(annotated_mods, target_host)
  File "/home2/xiachunwei/Software/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/home2/xiachunwei/Software/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  33: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::TypedPackedFunc<tvm::runtime::Module (tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)>::AssignTypedLambda<tvm::{lambda(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)#6}>(tvm::{lambda(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)#6}, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}> >::Call(tvm::runtime::PackedFuncObj const*, tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)
  32: tvm::TIRToRuntime(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target const&)
  31: tvm::SplitMixedModule(tvm::IRModule, tvm::Target const&, tvm::Target const&)
  30: tvm::ApplyPasses(tvm::IRModule, tvm::transform::Sequential)
  29: tvm::transform::Pass::operator()(tvm::IRModule) const
  28: tvm::transform::Pass::operator()(tvm::IRModule, tvm::transform::PassContext const&) const
  27: tvm::transform::SequentialNode::operator()(tvm::IRModule, tvm::transform::PassContext const&) const
  26: tvm::transform::Pass::operator()(tvm::IRModule, tvm::transform::PassContext const&) const
  25: tvm::tir::transform::PrimFuncPassNode::operator()(tvm::IRModule, tvm::transform::PassContext const&) const
  24: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::TypedPackedFunc<tvm::tir::PrimFunc (tvm::tir::PrimFunc, tvm::IRModule, tvm::transform::PassContext)>::AssignTypedLambda<tvm::tir::transform::LowerThreadAllreduce()::{lambda(tvm::tir::PrimFunc, tvm::IRModule, tvm::transform::PassContext)#1}>(tvm::tir::transform::LowerThreadAllreduce()::{lambda(tvm::tir::PrimFunc, tvm::IRModule, tvm::transform::PassContext)#1})::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}> >::Call(tvm::runtime::PackedFuncObj const*, tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)
  23: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7r
  22: tvm::tir::ThreadAllreduceBuilder::VisitStmt_(tvm::tir::AttrStmtNode const*)
  21: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  20: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7r
  19: tvm::tir::ThreadAllreduceBuilder::VisitStmt_(tvm::tir::AllocateNode const*)
  18: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  17: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7r
  16: tvm::tir::ThreadAllreduceBuilder::VisitStmt_(tvm::tir::AllocateNode const*)
  15: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  14: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7r
  13: tvm::tir::ThreadAllreduceBuilder::VisitStmt_(tvm::tir::AllocateNode const*)
  12: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  11: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7r
  10: tvm::tir::ThreadAllreduceBuilder::VisitStmt_(tvm::tir::AttrStmtNode const*)
  9: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  8: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runt
  7: tvm::runtime::ObjectPtr<tvm::runtime::Object> tvm::runtime::Array<tvm::tir::Stmt, void>::MapHelper<tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1}, tvm::tir::Stmt>(tvm::runtime::ObjectPtr<tvm::runtime::Object>, tvm::tir::StmtMutator::Internal::Mutate(tvm::tir::StmtMutator*, tvm::runtime::Array<tvm::tir::Stmt, void> const&)::{lambda(tvm::tir::Stmt const&)#1})
  6: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  5: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7r
  4: tvm::tir::ThreadAllreduceBuilder::VisitStmt_(tvm::tir::AttrStmtNode const*)
  3: tvm::tir::StmtMutator::VisitStmt(tvm::tir::Stmt const&)
  2: _ZZN3tvm3tir11StmtFunctorIFNS0_4StmtERKS2_EE10InitVTableEvENUlRKNS_7runt
  1: tvm::tir::ThreadAllreduceBuilder::VisitStmt_(tvm::tir::EvaluateNode const*)
  0: tvm::tir::ThreadAllreduceBuilder::MakeAllreduce(tvm::tir::CallNode const*)
  File "/home2/xiachunwei/Software/tvm/src/tir/transforms/lower_thread_allreduce.cc", line 383
InternalError: Check failed: (!load_remap_.count(buffers[i]->data.get())) is false: 

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(64), T.int64(112), T.int64(112)), "float32"), p1: T.Buffer((T.int64(1), T.int64(64), T.int64(1), T.int64(1)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(64), T.int64(56), T.int64(56)), "float32")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        pool_max_shared = T.alloc_buffer((T.int64(1), T.int64(64), T.int64(56), T.int64(56)), scope="shared")
        for ax0_ax1_ax2_ax3_0_fused in T.thread_binding(T.int64(3584), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 512, "pragma_unroll_explicit": 1}):
            for ax0, ax1, ax2, ax3, ax4_ax5_fused_0 in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(56), T.int64(1)):
                for ax4_ax5_fused_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                    with T.block("pool_max"):
                        v_ax0 = T.axis.spatial(T.int64(1), ax0)
                        v_ax1 = T.axis.spatial(T.int64(64), ax0_ax1_ax2_ax3_0_fused // T.int64(56) + ax1)
                        v_ax2 = T.axis.spatial(T.int64(56), ax0_ax1_ax2_ax3_0_fused % T.int64(56) + ax2)
                        v_ax3 = T.axis.spatial(T.int64(56), ax3)
                        v_rv0 = T.axis.reduce(T.int64(3), (ax4_ax5_fused_0 * T.int64(256) + ax4_ax5_fused_1) // T.int64(3))
                        v_rv1 = T.axis.reduce(T.int64(3), (ax4_ax5_fused_0 * T.int64(256) + ax4_ax5_fused_1) % T.int64(3))
                        T.where(ax4_ax5_fused_0 * T.int64(256) + ax4_ax5_fused_1 < T.int64(9))
                        T.reads(p0[v_ax0, v_ax1, v_ax2 * T.int64(2) + v_rv0 - T.int64(1), v_ax3 * T.int64(2) + v_rv1 - T.int64(1)])
                        T.writes(pool_max_shared[v_ax0, v_ax1, v_ax2, v_ax3])
                        with T.init():
                            pool_max_shared[v_ax0, v_ax1, v_ax2, v_ax3] = T.float32(-3.4028234663852886e+38)
                        pool_max_shared[v_ax0, v_ax1, v_ax2, v_ax3] = T.max(pool_max_shared[v_ax0, v_ax1, v_ax2, v_ax3], T.if_then_else(T.int64(1) <= v_ax2 * T.int64(2) + v_rv0 and v_ax2 * T.int64(2) + v_rv0 < T.int64(113) and T.int64(1) <= v_ax3 * T.int64(2) + v_rv1 and v_ax3 * T.int64(2) + v_rv1 < T.int64(113), p0[v_ax0, v_ax1, v_ax2 * T.int64(2) + v_rv0 - T.int64(1), v_ax3 * T.int64(2) + v_rv1 - T.int64(1)], T.float32(-3.4028234663852886e+38)))
            for ax3_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                with T.block("T_add"):
                    v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))
                    v_ax1 = T.axis.spatial(T.int64(64), ax0_ax1_ax2_ax3_0_fused // T.int64(56))
                    v_ax2 = T.axis.spatial(T.int64(56), ax0_ax1_ax2_ax3_0_fused % T.int64(56))
                    v_ax3 = T.axis.spatial(T.int64(56), ax3_1)
                    T.where(T.Mul(T.int64(0), T.int64(256)) + ax3_1 < T.int64(56))
                    T.reads(pool_max_shared[v_ax0, v_ax1, v_ax2, v_ax3], p1[v_ax0, v_ax1, T.int64(0), T.int64(0)])
                    T.writes(T_relu[v_ax0, v_ax1, v_ax2, v_ax3])
                    T_relu[v_ax0, v_ax1, v_ax2, v_ax3] = T.max(pool_max_shared[v_ax0, v_ax1, v_ax2, v_ax3] + p1[v_ax0, v_ax1, T.int64(0), T.int64(0)], T.float32(0))
b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="pool_max", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b1, ann_key="schedule_rule")
sch.reverse_compute_inline(block=b3)
sch.compute_inline(block=b0)
b5, = sch.get_consumers(block=b1)
l6, l7, l8, l9 = sch.get_loops(block=b5)
v10 = sch.sample_categorical(candidates=[4, 8, 16, 32, 64, 128, 256, 512], probs=[0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125], decision=6)
l11, l12 = sch.split(loop=l9, factors=[None, v10], preserve_unit_iters=True)
sch.bind(loop=l12, thread_axis="threadIdx.x")
sch.compute_at(block=b1, loop=l11, preserve_unit_loops=True, index=-1)
sch.set_scope(block=b1, buffer_index=0, storage_scope="shared")
l13, l14, l15, l16, l17, l18, l19, l20, l21, l22 = sch.get_loops(block=b1)
l23 = sch.fuse(l21, l22, preserve_unit_iters=True)
l24, l25 = sch.split(loop=l23, factors=[None, v10], preserve_unit_iters=True)
sch.bind(loop=l25, thread_axis="threadIdx.x")
v26 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v26)
l27, l28, l29, l30, l31 = sch.get_loops(block=b2)
l32 = sch.fuse(l27, l28, l29, l30, preserve_unit_iters=True)
sch.bind(loop=l32, thread_axis="blockIdx.x")
sch.enter_postproc()
b33 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b33, ann_key="meta_schedule.unroll_explicit")
b34, b35 = sch.get_child_blocks(b33)
l36, l37, l38, l39, l40, l41, l42 = sch.get_loops(block=b34)
sch.annotate(block_or_loop=l36, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l36, ann_key="pragma_unroll_explicit", ann_val=1)
l43, l44 = sch.get_loops(block=b35)
2023-11-11 09:59:55 [INFO] [evolutionary_search.cc:713] Generating candidates......
2023-11-11 09:59:56 [INFO] [evolutionary_search.cc:715] Picked top 102 candidate(s) from database
2023-11-11 09:59:58 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310fb056b8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310e891278)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310e8921e8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310e060038)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56310bbc4548)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310e896198)]: 0 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310e896608)]: 0 failure(s)
2023-11-11 09:59:58 [INFO] [evolutionary_search.cc:723] Sampled 410 candidate(s)
2023-11-11 10:00:02 [INFO] [evolutionary_search.cc:621] Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310fb056b8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310e891278)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310e8921e8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310e060038)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56310bbc4548)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310e896198)]: 0 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310e896608)]: 0 failure(s)
2023-11-11 10:00:06 [INFO] [evolutionary_search.cc:621] Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310fb056b8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310e891278)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310e8921e8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310e060038)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56310bbc4548)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310e896198)]: 0 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310e896608)]: 0 failure(s)
2023-11-11 10:00:09 [INFO] [evolutionary_search.cc:621] Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310fb056b8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310e891278)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310e8921e8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310e060038)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56310bbc4548)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310e896198)]: 0 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310e896608)]: 0 failure(s)
2023-11-11 10:00:12 [INFO] [evolutionary_search.cc:621] Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310fb056b8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310e891278)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310e8921e8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310e060038)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56310bbc4548)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310e896198)]: 0 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310e896608)]: 0 failure(s)
2023-11-11 10:00:15 [INFO] [evolutionary_search.cc:649] Scores of the best 2 candidates:
[1 : 2]:	0.5600  0.1350
2023-11-11 10:00:15 [INFO] [evolutionary_search.cc:727] Got 2 candidate(s) with evolutionary search
2023-11-11 10:00:15 [INFO] [evolutionary_search.cc:730] Sending 2 candidates(s) for measurement
2023-11-11 10:00:17 [INFO] [task_scheduler.cc:131] [Task #6: fused_nn_max_pool2d_add_nn_relu] Trial #219: GFLOPs: 172.1244. Time: 12.8264 us. Best GFLOPs: 295.7219
2023-11-11 10:00:17 [INFO] [task_scheduler.cc:131] [Task #6: fused_nn_max_pool2d_add_nn_relu] Trial #220: GFLOPs: 38.0554. Time: 58.0139 us. Best GFLOPs: 295.7219
2023-11-11 11:09:08 [INFO] [evolutionary_search.cc:713] Generating candidates......
2023-11-11 11:09:09 [INFO] [evolutionary_search.cc:715] Picked top 102 candidate(s) from database
2023-11-11 11:09:11 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310fb056b8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310e891278)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310e8921e8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310e060038)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56310bbc4548)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310e896198)]: 0 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310e896608)]: 0 failure(s)
2023-11-11 11:09:11 [INFO] [evolutionary_search.cc:723] Sampled 410 candidate(s)
2023-11-11 11:09:15 [INFO] [evolutionary_search.cc:621] Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310fb056b8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310e891278)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310e8921e8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310e060038)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56310bbc4548)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310e896198)]: 0 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310e896608)]: 0 failure(s)
2023-11-11 11:09:18 [INFO] [evolutionary_search.cc:621] Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310fb056b8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310e891278)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310e8921e8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310e060038)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56310bbc4548)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310e896198)]: 0 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310e896608)]: 0 failure(s)
2023-11-11 11:09:22 [INFO] [evolutionary_search.cc:621] Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310fb056b8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310e891278)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310e8921e8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310e060038)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56310bbc4548)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310e896198)]: 0 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310e896608)]: 0 failure(s)
2023-11-11 11:09:25 [INFO] [evolutionary_search.cc:621] Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310fb056b8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310e891278)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310e8921e8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310e060038)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56310bbc4548)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310e896198)]: 0 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310e896608)]: 0 failure(s)
2023-11-11 11:09:27 [INFO] [evolutionary_search.cc:649] Scores of the best 0 candidates:
2023-11-11 11:09:27 [INFO] [evolutionary_search.cc:727] Got 0 candidate(s) with evolutionary search
2023-11-11 11:09:27 [INFO] [evolutionary_search.cc:730] Sending 0 candidates(s) for measurement
2023-11-11 11:39:11 [INFO] [evolutionary_search.cc:713] Generating candidates......
2023-11-11 11:39:12 [INFO] [evolutionary_search.cc:715] Picked top 102 candidate(s) from database
2023-11-11 11:39:14 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310fb056b8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310e891278)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310e8921e8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310e060038)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56310bbc4548)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310e896198)]: 0 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310e896608)]: 0 failure(s)
2023-11-11 11:39:14 [INFO] [evolutionary_search.cc:723] Sampled 410 candidate(s)
2023-11-11 11:39:18 [INFO] [evolutionary_search.cc:621] Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310fb056b8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310e891278)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310e8921e8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310e060038)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56310bbc4548)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310e896198)]: 0 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310e896608)]: 0 failure(s)
2023-11-11 11:39:22 [INFO] [evolutionary_search.cc:621] Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310fb056b8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310e891278)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310e8921e8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310e060038)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56310bbc4548)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310e896198)]: 0 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310e896608)]: 0 failure(s)
2023-11-11 11:39:25 [INFO] [evolutionary_search.cc:621] Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310fb056b8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310e891278)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310e8921e8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310e060038)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56310bbc4548)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310e896198)]: 0 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310e896608)]: 0 failure(s)
2023-11-11 11:39:28 [INFO] [evolutionary_search.cc:621] Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310fb056b8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310e891278)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310e8921e8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310e060038)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56310bbc4548)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310e896198)]: 0 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310e896608)]: 0 failure(s)
2023-11-11 11:39:30 [INFO] [evolutionary_search.cc:649] Scores of the best 0 candidates:
2023-11-11 11:39:30 [INFO] [evolutionary_search.cc:727] Got 0 candidate(s) with evolutionary search
2023-11-11 11:39:30 [INFO] [evolutionary_search.cc:730] Sending 0 candidates(s) for measurement
2023-11-11 11:45:50 [INFO] [evolutionary_search.cc:713] Generating candidates......
2023-11-11 11:45:51 [INFO] [evolutionary_search.cc:715] Picked top 102 candidate(s) from database
2023-11-11 11:45:54 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310fb056b8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310e891278)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310e8921e8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310e060038)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56310bbc4548)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310e896198)]: 0 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310e896608)]: 0 failure(s)
2023-11-11 11:45:54 [INFO] [evolutionary_search.cc:723] Sampled 410 candidate(s)
2023-11-11 11:45:57 [INFO] [evolutionary_search.cc:621] Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310fb056b8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310e891278)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310e8921e8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310e060038)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56310bbc4548)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310e896198)]: 0 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310e896608)]: 0 failure(s)
2023-11-11 11:46:00 [INFO] [evolutionary_search.cc:621] Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310fb056b8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310e891278)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310e8921e8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310e060038)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56310bbc4548)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310e896198)]: 0 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310e896608)]: 0 failure(s)
2023-11-11 11:46:04 [INFO] [evolutionary_search.cc:621] Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310fb056b8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310e891278)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310e8921e8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310e060038)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56310bbc4548)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310e896198)]: 0 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310e896608)]: 0 failure(s)
2023-11-11 11:46:07 [INFO] [evolutionary_search.cc:621] Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310fb056b8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310e891278)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310e8921e8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310e060038)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56310bbc4548)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310e896198)]: 0 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310e896608)]: 0 failure(s)
2023-11-11 11:46:09 [INFO] [evolutionary_search.cc:649] Scores of the best 0 candidates:
2023-11-11 11:46:09 [INFO] [evolutionary_search.cc:727] Got 0 candidate(s) with evolutionary search
2023-11-11 11:46:09 [INFO] [evolutionary_search.cc:730] Sending 0 candidates(s) for measurement
2023-11-11 11:55:21 [INFO] [evolutionary_search.cc:713] Generating candidates......
2023-11-11 11:55:22 [INFO] [evolutionary_search.cc:715] Picked top 102 candidate(s) from database
2023-11-11 11:55:25 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310fb056b8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310e891278)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310e8921e8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310e060038)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56310bbc4548)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310e896198)]: 0 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310e896608)]: 0 failure(s)
2023-11-11 11:55:25 [INFO] [evolutionary_search.cc:723] Sampled 410 candidate(s)
2023-11-11 11:55:28 [INFO] [evolutionary_search.cc:621] Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310fb056b8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310e891278)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310e8921e8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310e060038)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56310bbc4548)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310e896198)]: 0 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310e896608)]: 0 failure(s)
2023-11-11 11:55:32 [INFO] [evolutionary_search.cc:621] Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310fb056b8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310e891278)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310e8921e8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310e060038)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56310bbc4548)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310e896198)]: 0 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310e896608)]: 0 failure(s)
2023-11-11 11:55:36 [INFO] [evolutionary_search.cc:621] Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310fb056b8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310e891278)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310e8921e8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310e060038)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56310bbc4548)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310e896198)]: 0 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310e896608)]: 0 failure(s)
2023-11-11 11:55:39 [INFO] [evolutionary_search.cc:621] Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310fb056b8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310e891278)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310e8921e8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310e060038)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56310bbc4548)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310e896198)]: 0 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310e896608)]: 0 failure(s)
2023-11-11 11:55:41 [INFO] [evolutionary_search.cc:649] Scores of the best 0 candidates:
2023-11-11 11:55:41 [INFO] [evolutionary_search.cc:727] Got 0 candidate(s) with evolutionary search
2023-11-11 11:55:41 [INFO] [evolutionary_search.cc:730] Sending 0 candidates(s) for measurement
2023-11-11 11:59:10 [INFO] [evolutionary_search.cc:713] Generating candidates......
2023-11-11 11:59:11 [INFO] [evolutionary_search.cc:715] Picked top 102 candidate(s) from database
2023-11-11 11:59:13 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310fb056b8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310e891278)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310e8921e8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310e060038)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56310bbc4548)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310e896198)]: 0 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310e896608)]: 0 failure(s)
2023-11-11 11:59:13 [INFO] [evolutionary_search.cc:723] Sampled 410 candidate(s)
2023-11-11 11:59:17 [INFO] [evolutionary_search.cc:621] Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310fb056b8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310e891278)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310e8921e8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310e060038)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56310bbc4548)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310e896198)]: 0 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310e896608)]: 0 failure(s)
2023-11-11 11:59:20 [INFO] [evolutionary_search.cc:621] Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310fb056b8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310e891278)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310e8921e8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310e060038)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56310bbc4548)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310e896198)]: 0 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310e896608)]: 0 failure(s)
2023-11-11 11:59:24 [INFO] [evolutionary_search.cc:621] Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310fb056b8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310e891278)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310e8921e8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310e060038)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56310bbc4548)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310e896198)]: 0 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310e896608)]: 0 failure(s)
2023-11-11 11:59:27 [INFO] [evolutionary_search.cc:621] Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310fb056b8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310e891278)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310e8921e8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310e060038)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56310bbc4548)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310e896198)]: 0 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310e896608)]: 0 failure(s)
2023-11-11 11:59:29 [INFO] [evolutionary_search.cc:649] Scores of the best 0 candidates:
2023-11-11 11:59:29 [INFO] [evolutionary_search.cc:727] Got 0 candidate(s) with evolutionary search
2023-11-11 11:59:29 [INFO] [evolutionary_search.cc:730] Sending 0 candidates(s) for measurement
