2023-11-10 23:34:13 [INFO] [task_scheduler.cc:160] Initializing Task #15: "fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1"
2023-11-10 23:34:13 [INFO] [task_scheduler.cc:35] 
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(128), T.int64(28), T.int64(28)), "float32"), p1: T.Buffer((T.int64(4), T.int64(4), T.int64(128), T.int64(128)), "float32"), p2: T.Buffer((T.int64(1), T.int64(128), T.int64(28), T.int64(28)), "float32"), p3: T.Buffer((T.int64(1), T.int64(128), T.int64(1), T.int64(1)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(128), T.int64(28), T.int64(28)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        data_pad = T.alloc_buffer((T.int64(1), T.int64(128), T.int64(30), T.int64(30)))
        input_tile = T.alloc_buffer((T.int64(128), T.int64(196), T.int64(4), T.int64(4)))
        B = T.alloc_buffer((T.int64(4), T.int64(4)))
        data_pack = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(128), T.int64(196)))
        bgemm = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(128), T.int64(196)))
        A = T.alloc_buffer((T.int64(4), T.int64(2)))
        inverse = T.alloc_buffer((T.int64(128), T.int64(196), T.int64(2), T.int64(2)))
        conv2d_winograd = T.alloc_buffer((T.int64(1), T.int64(128), T.int64(28), T.int64(28)))
        T_add = T.alloc_buffer((T.int64(1), T.int64(128), T.int64(28), T.int64(28)))
        T_add_1 = T.alloc_buffer((T.int64(1), T.int64(128), T.int64(28), T.int64(28)))
        for i0, i1, i2, i3 in T.grid(T.int64(1), T.int64(128), T.int64(30), T.int64(30)):
            with T.block("data_pad"):
                v_i0, v_i1, v_i2, v_i3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(p0[v_i0, v_i1, v_i2 - T.int64(1), v_i3 - T.int64(1)])
                T.writes(data_pad[v_i0, v_i1, v_i2, v_i3])
                data_pad[v_i0, v_i1, v_i2, v_i3] = T.if_then_else(T.int64(1) <= v_i2 and v_i2 < T.int64(29) and T.int64(1) <= v_i3 and v_i3 < T.int64(29), p0[v_i0, v_i1, v_i2 - T.int64(1), v_i3 - T.int64(1)], T.float32(0))
        for ci, p, eps, nu in T.grid(T.int64(128), T.int64(196), T.int64(4), T.int64(4)):
            with T.block("input_tile"):
                v_ci, v_p, v_eps, v_nu = T.axis.remap("SSSS", [ci, p, eps, nu])
                T.reads(data_pad[v_p // T.int64(196), v_ci, v_p % T.int64(196) // T.int64(14) * T.int64(2) + v_eps, v_p % T.int64(14) * T.int64(2) + v_nu])
                T.writes(input_tile[v_ci, v_p, v_eps, v_nu])
                T.block_attr({"schedule_rule": "None"})
                input_tile[v_ci, v_p, v_eps, v_nu] = data_pad[v_p // T.int64(196), v_ci, v_p % T.int64(196) // T.int64(14) * T.int64(2) + v_eps, v_p % T.int64(14) * T.int64(2) + v_nu]
        for i, j in T.grid(T.int64(4), T.int64(4)):
            with T.block("B"):
                v_i, v_j = T.axis.remap("SS", [i, j])
                T.reads()
                T.writes(B[v_i, v_j])
                T.block_attr({"schedule_rule": "None"})
                B[v_i, v_j] = T.Select(v_i % T.int64(4) == T.int64(3) and v_j % T.int64(4) == T.int64(3), T.float32(1), T.Select(v_i % T.int64(4) == T.int64(3) and v_j % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_i % T.int64(4) == T.int64(3) and v_j % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_i % T.int64(4) == T.int64(3) and v_j % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_i % T.int64(4) == T.int64(2) and v_j % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_i % T.int64(4) == T.int64(2) and v_j % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_i % T.int64(4) == T.int64(2) and v_j % T.int64(4) == T.int64(1), T.float32(1), T.Select(v_i % T.int64(4) == T.int64(2) and v_j % T.int64(4) == T.int64(0), T.float32(-1), T.Select(v_i % T.int64(4) == T.int64(1) and v_j % T.int64(4) == T.int64(3), T.float32(-1), T.Select(v_i % T.int64(4) == T.int64(1) and v_j % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_i % T.int64(4) == T.int64(1) and v_j % T.int64(4) == T.int64(1), T.float32(-1), T.Select(v_i % T.int64(4) == T.int64(1) and v_j % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_i % T.int64(4) == T.int64(0) and v_j % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_i % T.int64(4) == T.int64(0) and v_j % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_i % T.int64(4) == T.int64(0) and v_j % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_i % T.int64(4) == T.int64(0) and v_j % T.int64(4) == T.int64(0), T.float32(1), T.float32(0)))))))))))))))))
        for eps, nu, ci, p, r_a, r_b in T.grid(T.int64(4), T.int64(4), T.int64(128), T.int64(196), T.int64(4), T.int64(4)):
            with T.block("data_pack"):
                v_eps, v_nu, v_ci, v_p, v_r_a, v_r_b = T.axis.remap("SSSSRR", [eps, nu, ci, p, r_a, r_b])
                T.reads(input_tile[v_ci, v_p, v_r_a, v_r_b], B[T.min(v_r_a, v_r_b):T.min(v_r_a, v_r_b) + (T.max(v_r_a, v_r_b) + T.int64(1) - T.min(v_r_a, v_r_b)), T.min(v_eps, v_nu):T.min(v_eps, v_nu) + (T.max(v_eps, v_nu) + T.int64(1) - T.min(v_eps, v_nu))])
                T.writes(data_pack[v_eps, v_nu, v_ci, v_p])
                T.block_attr({"schedule_rule": "conv2d_nchw_winograd_data_pack"})
                with T.init():
                    data_pack[v_eps, v_nu, v_ci, v_p] = T.float32(0)
                data_pack[v_eps, v_nu, v_ci, v_p] = data_pack[v_eps, v_nu, v_ci, v_p] + input_tile[v_ci, v_p, v_r_a, v_r_b] * B[v_r_a, v_eps] * B[v_r_b, v_nu]
        for eps, nu, co, p, ci in T.grid(T.int64(4), T.int64(4), T.int64(128), T.int64(196), T.int64(128)):
            with T.block("bgemm"):
                v_eps, v_nu, v_co, v_p, v_ci = T.axis.remap("SSSSR", [eps, nu, co, p, ci])
                T.reads(data_pack[v_eps, v_nu, v_ci, v_p], p1[v_eps, v_nu, v_ci, v_co])
                T.writes(bgemm[v_eps, v_nu, v_co, v_p])
                with T.init():
                    bgemm[v_eps, v_nu, v_co, v_p] = T.float32(0)
                bgemm[v_eps, v_nu, v_co, v_p] = bgemm[v_eps, v_nu, v_co, v_p] + data_pack[v_eps, v_nu, v_ci, v_p] * p1[v_eps, v_nu, v_ci, v_co]
        for i, j in T.grid(T.int64(4), T.int64(2)):
            with T.block("A"):
                v_i, v_j = T.axis.remap("SS", [i, j])
                T.reads()
                T.writes(A[v_i, v_j])
                T.block_attr({"schedule_rule": "None"})
                A[v_i, v_j] = T.Select(v_i % T.int64(4) == T.int64(3) and v_j % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_i % T.int64(4) == T.int64(3) and v_j % T.int64(2) == T.int64(0), T.float32(0), T.Select(v_i % T.int64(4) == T.int64(2) and v_j % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_i % T.int64(4) == T.int64(2) and v_j % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_i % T.int64(4) == T.int64(1) and v_j % T.int64(2) == T.int64(1), T.float32(-1), T.Select(v_i % T.int64(4) == T.int64(1) and v_j % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_i % T.int64(4) == T.int64(0) and v_j % T.int64(2) == T.int64(1), T.float32(0), T.Select(v_i % T.int64(4) == T.int64(0) and v_j % T.int64(2) == T.int64(0), T.float32(1), T.float32(0)))))))))
        for co, p, vh, vw, r_a, r_b in T.grid(T.int64(128), T.int64(196), T.int64(2), T.int64(2), T.int64(4), T.int64(4)):
            with T.block("inverse"):
                v_co, v_p, v_vh, v_vw, v_r_a, v_r_b = T.axis.remap("SSSSRR", [co, p, vh, vw, r_a, r_b])
                T.reads(bgemm[v_r_a, v_r_b, v_co, v_p], A[T.min(v_r_a, v_r_b):T.min(v_r_a, v_r_b) + (T.max(v_r_a, v_r_b) + T.int64(1) - T.min(v_r_a, v_r_b)), T.min(v_vh, v_vw):T.min(v_vh, v_vw) + (T.max(v_vh, v_vw) + T.int64(1) - T.min(v_vh, v_vw))])
                T.writes(inverse[v_co, v_p, v_vh, v_vw])
                T.block_attr({"schedule_rule": "conv2d_nchw_winograd_inverse"})
                with T.init():
                    inverse[v_co, v_p, v_vh, v_vw] = T.float32(0)
                inverse[v_co, v_p, v_vh, v_vw] = inverse[v_co, v_p, v_vh, v_vw] + bgemm[v_r_a, v_r_b, v_co, v_p] * A[v_r_a, v_vh] * A[v_r_b, v_vw]
        for n, co, h, w in T.grid(T.int64(1), T.int64(128), T.int64(28), T.int64(28)):
            with T.block("conv2d_winograd"):
                v_n, v_co, v_h, v_w = T.axis.remap("SSSS", [n, co, h, w])
                T.reads(inverse[v_co, v_n * T.int64(196) + v_h // T.int64(2) * T.int64(14) + v_w // T.int64(2), v_h % T.int64(2), v_w % T.int64(2)])
                T.writes(conv2d_winograd[v_n, v_co, v_h, v_w])
                conv2d_winograd[v_n, v_co, v_h, v_w] = inverse[v_co, v_n * T.int64(196) + v_h // T.int64(2) * T.int64(14) + v_w // T.int64(2), v_h % T.int64(2), v_w % T.int64(2)]
        for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(128), T.int64(28), T.int64(28)):
            with T.block("T_add"):
                v_ax0, v_ax1, v_ax2, v_ax3 = T.axis.remap("SSSS", [ax0, ax1, ax2, ax3])
                T.reads(conv2d_winograd[v_ax0, v_ax1, v_ax2, v_ax3], p2[v_ax0, v_ax1, v_ax2, v_ax3])
                T.writes(T_add[v_ax0, v_ax1, v_ax2, v_ax3])
                T_add[v_ax0, v_ax1, v_ax2, v_ax3] = conv2d_winograd[v_ax0, v_ax1, v_ax2, v_ax3] + p2[v_ax0, v_ax1, v_ax2, v_ax3]
        for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(128), T.int64(28), T.int64(28)):
            with T.block("T_add_1"):
                v_ax0, v_ax1, v_ax2, v_ax3 = T.axis.remap("SSSS", [ax0, ax1, ax2, ax3])
                T.reads(T_add[v_ax0, v_ax1, v_ax2, v_ax3], p3[v_ax0, v_ax1, T.int64(0), T.int64(0)])
                T.writes(T_add_1[v_ax0, v_ax1, v_ax2, v_ax3])
                T_add_1[v_ax0, v_ax1, v_ax2, v_ax3] = T_add[v_ax0, v_ax1, v_ax2, v_ax3] + p3[v_ax0, v_ax1, T.int64(0), T.int64(0)]
        for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(128), T.int64(28), T.int64(28)):
            with T.block("T_relu"):
                v_ax0, v_ax1, v_ax2, v_ax3 = T.axis.remap("SSSS", [ax0, ax1, ax2, ax3])
                T.reads(T_add_1[v_ax0, v_ax1, v_ax2, v_ax3])
                T.writes(T_relu[v_ax0, v_ax1, v_ax2, v_ax3])
                T_relu[v_ax0, v_ax1, v_ax2, v_ax3] = T.max(T_add_1[v_ax0, v_ax1, v_ax2, v_ax3], T.float32(0))
2023-11-10 23:34:13 [INFO] [task_scheduler.cc:164] Total 3 design space(s) generated
2023-11-10 23:34:13 [INFO] [task_scheduler.cc:170] Design space #0:
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(128), T.int64(28), T.int64(28)), "float32"), p1: T.Buffer((T.int64(4), T.int64(4), T.int64(128), T.int64(128)), "float32"), p2: T.Buffer((T.int64(1), T.int64(128), T.int64(28), T.int64(28)), "float32"), p3: T.Buffer((T.int64(1), T.int64(128), T.int64(1), T.int64(1)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(128), T.int64(28), T.int64(28)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit": 512})
            input_tile_local = T.alloc_buffer((T.int64(128), T.int64(196), T.int64(4), T.int64(4)), scope="local")
            data_pack = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(128), T.int64(196)))
            bgemm = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(128), T.int64(196)))
            inverse_local = T.alloc_buffer((T.int64(128), T.int64(196), T.int64(2), T.int64(2)), scope="local")
            data_pack_local = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(128), T.int64(196)), scope="local")
            bgemm_local = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(128), T.int64(196)), scope="local")
            data_pack_shared = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(128), T.int64(196)), scope="shared")
            p1_shared = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(128), T.int64(128)), scope="shared")
            for ci_p_fused_0 in T.thread_binding(T.int64(49), thread="blockIdx.x"):
                for ci_p_fused_1 in T.thread_binding(T.int64(512), thread="threadIdx.x"):
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(1), T.int64(4), T.int64(4)):
                        with T.block("input_tile"):
                            v_ci = T.axis.spatial(T.int64(128), (ci_p_fused_0 * T.int64(512) + ci_p_fused_1) // T.int64(196) + ax0)
                            v_p = T.axis.spatial(T.int64(196), (ci_p_fused_0 * T.int64(120) + ci_p_fused_1) % T.int64(196) + ax1)
                            v_eps, v_nu = T.axis.remap("SS", [ax2, ax3])
                            T.reads(p0[v_p // T.int64(196), v_ci, v_p % T.int64(196) // T.int64(14) * T.int64(2) + v_eps - T.int64(1), v_p % T.int64(14) * T.int64(2) + v_nu - T.int64(1)])
                            T.writes(input_tile_local[v_ci, v_p, v_eps, v_nu])
                            T.block_attr({"schedule_rule": "None"})
                            input_tile_local[v_ci, v_p, v_eps, v_nu] = T.if_then_else(T.int64(1) <= v_p % T.int64(196) // T.int64(14) * T.int64(2) + v_eps and v_p % T.int64(196) // T.int64(14) * T.int64(2) + v_eps < T.int64(29) and T.int64(1) <= v_p % T.int64(14) * T.int64(2) + v_nu and v_p % T.int64(14) * T.int64(2) + v_nu < T.int64(29), p0[v_p // T.int64(196), v_ci, v_p % T.int64(196) // T.int64(14) * T.int64(2) + v_eps - T.int64(1), v_p % T.int64(14) * T.int64(2) + v_nu - T.int64(1)], T.float32(0))
                    for eps in T.unroll(T.int64(4)):
                        for nu in T.unroll(T.int64(4)):
                            for r_a in T.unroll(T.int64(4)):
                                for r_b in T.unroll(T.int64(4)):
                                    with T.block("data_pack"):
                                        v_eps, v_nu = T.axis.remap("SS", [eps, nu])
                                        v_ci = T.axis.spatial(T.int64(128), (ci_p_fused_0 * T.int64(512) + ci_p_fused_1) // T.int64(196))
                                        v_p = T.axis.spatial(T.int64(196), (ci_p_fused_0 * T.int64(512) + ci_p_fused_1) % T.int64(196))
                                        v_r_a, v_r_b = T.axis.remap("RR", [r_a, r_b])
                                        T.reads(input_tile_local[v_ci, v_p, v_r_a, v_r_b])
                                        T.writes(data_pack_local[v_eps, v_nu, v_ci, v_p])
                                        T.block_attr({"schedule_rule": "conv2d_nchw_winograd_data_pack"})
                                        with T.init():
                                            data_pack_local[v_eps, v_nu, v_ci, v_p] = T.float32(0)
                                        data_pack_local[v_eps, v_nu, v_ci, v_p] = data_pack_local[v_eps, v_nu, v_ci, v_p] + input_tile_local[v_ci, v_p, v_r_a, v_r_b] * T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(3), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(1), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(0), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(3), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(1), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(0), T.float32(1), T.float32(0))))))))))))))))) * T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(3), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(1), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(0), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(3), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(1), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(0), T.float32(1), T.float32(0)))))))))))))))))
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(4), T.int64(4), T.int64(1), T.int64(1)):
                        with T.block("data_pack_local"):
                            v0, v1 = T.axis.remap("SS", [ax0, ax1])
                            v2 = T.axis.spatial(T.int64(128), (ci_p_fused_0 * T.int64(512) + ci_p_fused_1) // T.int64(196) + ax2)
                            v3 = T.axis.spatial(T.int64(196), (ci_p_fused_0 * T.int64(120) + ci_p_fused_1) % T.int64(196) + ax3)
                            T.reads(data_pack_local[v0, v1, v2, v3])
                            T.writes(data_pack[v0, v1, v2, v3])
                            data_pack[v0, v1, v2, v3] = data_pack_local[v0, v1, v2, v3]
            for eps_0_nu_0_co_0_p_0_fused in T.thread_binding(T.int64(896), thread="blockIdx.x"):
                for eps_1_nu_1_co_1_p_1_fused in T.thread_binding(T.int64(8), thread="vthread.x"):
                    for eps_2_nu_2_co_2_p_2_fused in T.thread_binding(T.int64(1), thread="threadIdx.x"):
                        for ci_0 in range(T.int64(2)):
                            for ax0_ax1_ax2_ax3_fused in range(T.int64(1792)):
                                with T.block("data_pack_shared"):
                                    v0 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(224))
                                    v1 = T.axis.spatial(T.int64(4), ax0_ax1_ax2_ax3_fused // T.int64(448))
                                    v2 = T.axis.spatial(T.int64(128), ci_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused % T.int64(448) // T.int64(7))
                                    v3 = T.axis.spatial(T.int64(196), eps_0_nu_0_co_0_p_0_fused % T.int64(28) * T.int64(7) + ax0_ax1_ax2_ax3_fused % T.int64(7))
                                    T.reads(data_pack[v0, v1, v2, v3])
                                    T.writes(data_pack_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch": 4})
                                    data_pack_shared[v0, v1, v2, v3] = data_pack[v0, v1, v2, v3]
                            for ax0_ax1_ax2_ax3_fused in range(T.int64(4096)):
                                with T.block("p1_shared"):
                                    v0 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(224))
                                    v1 = T.axis.spatial(T.int64(4), ax0_ax1_ax2_ax3_fused // T.int64(1024))
                                    v2 = T.axis.spatial(T.int64(128), ci_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused % T.int64(1024) // T.int64(16))
                                    v3 = T.axis.spatial(T.int64(128), eps_0_nu_0_co_0_p_0_fused % T.int64(224) // T.int64(28) * T.int64(16) + ax0_ax1_ax2_ax3_fused % T.int64(16))
                                    T.reads(p1[v0, v1, v2, v3])
                                    T.writes(p1_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch": 4})
                                    p1_shared[v0, v1, v2, v3] = p1[v0, v1, v2, v3]
                            for ci_1, eps_3, nu_3, co_3, p_3, ci_2, eps_4, nu_4, co_4, p_4 in T.grid(T.int64(16), T.int64(1), T.int64(2), T.int64(4), T.int64(7), T.int64(4), T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                                with T.block("bgemm"):
                                    v_eps = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(224) + eps_3 + eps_4)
                                    v_nu = T.axis.spatial(T.int64(4), eps_1_nu_1_co_1_p_1_fused // T.int64(4) * T.int64(2) + nu_3 + nu_4)
                                    v_co = T.axis.spatial(T.int64(128), eps_0_nu_0_co_0_p_0_fused % T.int64(224) // T.int64(28) * T.int64(16) + eps_1_nu_1_co_1_p_1_fused % T.int64(4) * T.int64(4) + co_3 + co_4)
                                    v_p = T.axis.spatial(T.int64(196), eps_0_nu_0_co_0_p_0_fused % T.int64(28) * T.int64(7) + p_3 + p_4)
                                    v_ci = T.axis.reduce(T.int64(128), ci_0 * T.int64(64) + ci_1 * T.int64(4) + ci_2)
                                    T.reads(data_pack_shared[v_eps, v_nu, v_ci, v_p], p1_shared[v_eps, v_nu, v_ci, v_co])
                                    T.writes(bgemm_local[v_eps, v_nu, v_co, v_p])
                                    T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                    with T.init():
                                        bgemm_local[v_eps, v_nu, v_co, v_p] = T.float32(0)
                                    bgemm_local[v_eps, v_nu, v_co, v_p] = bgemm_local[v_eps, v_nu, v_co, v_p] + data_pack_shared[v_eps, v_nu, v_ci, v_p] * p1_shared[v_eps, v_nu, v_ci, v_co]
                        for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(2), T.int64(4), T.int64(7)):
                            with T.block("bgemm_local"):
                                v0 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(224) + ax0)
                                v1 = T.axis.spatial(T.int64(4), eps_1_nu_1_co_1_p_1_fused // T.int64(4) * T.int64(2) + ax1)
                                v2 = T.axis.spatial(T.int64(128), eps_0_nu_0_co_0_p_0_fused % T.int64(224) // T.int64(28) * T.int64(16) + eps_1_nu_1_co_1_p_1_fused % T.int64(4) * T.int64(4) + ax2)
                                v3 = T.axis.spatial(T.int64(196), eps_0_nu_0_co_0_p_0_fused % T.int64(28) * T.int64(7) + ax3)
                                T.reads(bgemm_local[v0, v1, v2, v3])
                                T.writes(bgemm[v0, v1, v2, v3])
                                bgemm[v0, v1, v2, v3] = bgemm_local[v0, v1, v2, v3]
            for n_co_h_0_w_0_fused_0 in T.thread_binding(T.int64(98), thread="blockIdx.x"):
                for n_co_h_0_w_0_fused_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                    for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                        for ax2 in T.unroll(T.int64(2)):
                            for ax3 in T.unroll(T.int64(2)):
                                for ax4 in T.unroll(T.int64(4)):
                                    for ax5 in T.unroll(T.int64(4)):
                                        with T.block("inverse"):
                                            v_co = T.axis.spatial(T.int64(128), (n_co_h_0_w_0_fused_0 * T.int64(256) + n_co_h_0_w_0_fused_1) // T.int64(196) + ax0)
                                            v_p = T.axis.spatial(T.int64(196), (n_co_h_0_w_0_fused_0 * T.int64(256) + n_co_h_0_w_0_fused_1) % T.int64(196) + ax1)
                                            v_vh, v_vw, v_r_a, v_r_b = T.axis.remap("SSRR", [ax2, ax3, ax4, ax5])
                                            T.reads(bgemm[v_r_a, v_r_b, v_co, v_p])
                                            T.writes(inverse_local[v_co, v_p, v_vh, v_vw])
                                            T.block_attr({"schedule_rule": "conv2d_nchw_winograd_inverse"})
                                            with T.init():
                                                inverse_local[v_co, v_p, v_vh, v_vw] = T.float32(0)
                                            inverse_local[v_co, v_p, v_vh, v_vw] = inverse_local[v_co, v_p, v_vh, v_vw] + bgemm[v_r_a, v_r_b, v_co, v_p] * T.Select(v_r_a % T.int64(4) == T.int64(3) and v_vh % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_vh % T.int64(2) == T.int64(0), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_vh % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_vh % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_vh % T.int64(2) == T.int64(1), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_vh % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_vh % T.int64(2) == T.int64(1), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_vh % T.int64(2) == T.int64(0), T.float32(1), T.float32(0))))))))) * T.Select(v_r_b % T.int64(4) == T.int64(3) and v_vw % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_vw % T.int64(2) == T.int64(0), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_vw % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_vw % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_vw % T.int64(2) == T.int64(1), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_vw % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_vw % T.int64(2) == T.int64(1), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_vw % T.int64(2) == T.int64(0), T.float32(1), T.float32(0)))))))))
                    for h_1, w_1 in T.grid(T.int64(2), T.int64(2)):
                        with T.block("conv2d_winograd"):
                            v_n = T.axis.spatial(T.int64(1), T.int64(0))
                            v_co = T.axis.spatial(T.int64(128), (n_co_h_0_w_0_fused_0 * T.int64(256) + n_co_h_0_w_0_fused_1) // T.int64(196))
                            v_h = T.axis.spatial(T.int64(28), (n_co_h_0_w_0_fused_0 * T.int64(256) + n_co_h_0_w_0_fused_1) % T.int64(196) // T.int64(14) * T.int64(2) + h_1)
                            v_w = T.axis.spatial(T.int64(28), (n_co_h_0_w_0_fused_0 * T.int64(256) + n_co_h_0_w_0_fused_1) % T.int64(14) * T.int64(2) + w_1)
                            T.reads(inverse_local[v_co, v_n * T.int64(196) + v_h // T.int64(2) * T.int64(14) + v_w // T.int64(2), v_h % T.int64(2), v_w % T.int64(2)], p2[v_n, v_co, v_h, v_w], p3[v_n, v_co, T.int64(0), T.int64(0)])
                            T.writes(T_relu[v_n, v_co, v_h, v_w])
                            T_relu[v_n, v_co, v_h, v_w] = T.max(inverse_local[v_co, v_n * T.int64(196) + v_h // T.int64(2) * T.int64(14) + v_w // T.int64(2), v_h % T.int64(2), v_w % T.int64(2)] + p2[v_n, v_co, v_h, v_w] + p3[v_n, v_co, T.int64(0), T.int64(0)], T.float32(0))
b0 = sch.get_block(name="data_pack", func_name="main")
b1 = sch.get_block(name="bgemm", func_name="main")
b2 = sch.get_block(name="inverse", func_name="main")
b3 = sch.get_block(name="conv2d_winograd", func_name="main")
b4 = sch.get_block(name="T_add", func_name="main")
b5 = sch.get_block(name="T_add_1", func_name="main")
b6 = sch.get_block(name="T_relu", func_name="main")
b7 = sch.get_block(name="root", func_name="main")
b8, b9 = sch.get_producers(block=b2)
sch.compute_inline(block=b9)
b10, = sch.get_consumers(block=b2)
l11, l12, l13, l14 = sch.get_loops(block=b10)
l15, l16 = sch.split(loop=l13, factors=[None, 2], preserve_unit_iters=True)
l17, l18 = sch.split(loop=l14, factors=[None, 2], preserve_unit_iters=True)
sch.reorder(l15, l17, l16, l18)
sch.compute_at(block=b2, loop=l17, preserve_unit_loops=True, index=-1)
sch.set_scope(block=b2, buffer_index=0, storage_scope="local")
l19, l20, l21, l22, l23, l24, l25, l26, l27, l28 = sch.get_loops(block=b2)
sch.unroll(loop=l25)
sch.unroll(loop=l26)
sch.unroll(loop=l27)
sch.unroll(loop=l28)
b29, b30 = sch.get_producers(block=b0)
sch.compute_inline(block=b30)
b31, = sch.get_producers(block=b29)
l32, l33, l34, l35, l36, l37 = sch.get_loops(block=b0)
sch.reorder(l34, l35, l32, l33, l36, l37)
sch.unroll(loop=l32)
sch.unroll(loop=l33)
sch.unroll(loop=l36)
sch.unroll(loop=l37)
l38 = sch.fuse(l34, l35, preserve_unit_iters=True)
v39 = sch.sample_categorical(candidates=[32, 64, 128, 256, 512, 1024], probs=[0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666], decision=4)
l40, l41 = sch.split(loop=l38, factors=[None, v39], preserve_unit_iters=True)
sch.bind(loop=l40, thread_axis="blockIdx.x")
sch.bind(loop=l41, thread_axis="threadIdx.x")
b42 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b42, loop=l41, preserve_unit_loops=True, index=-1)
sch.compute_at(block=b29, loop=l41, preserve_unit_loops=True, index=-1)
sch.set_scope(block=b29, buffer_index=0, storage_scope="local")
sch.compute_inline(block=b31)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l43, l44, l45, l46, l47 = sch.get_loops(block=b1)
v48, v49, v50, v51, v52 = sch.sample_perfect_tile(loop=l43, n=5, max_innermost_factor=64, decision=[4, 1, 1, 1, 1])
l53, l54, l55, l56, l57 = sch.split(loop=l43, factors=[v48, v49, v50, v51, v52], preserve_unit_iters=True)
v58, v59, v60, v61, v62 = sch.sample_perfect_tile(loop=l44, n=5, max_innermost_factor=64, decision=[1, 2, 1, 2, 1])
l63, l64, l65, l66, l67 = sch.split(loop=l44, factors=[v58, v59, v60, v61, v62], preserve_unit_iters=True)
v68, v69, v70, v71, v72 = sch.sample_perfect_tile(loop=l45, n=5, max_innermost_factor=64, decision=[8, 4, 1, 4, 1])
l73, l74, l75, l76, l77 = sch.split(loop=l45, factors=[v68, v69, v70, v71, v72], preserve_unit_iters=True)
v78, v79, v80, v81, v82 = sch.sample_perfect_tile(loop=l46, n=5, max_innermost_factor=64, decision=[28, 1, 1, 7, 1])
l83, l84, l85, l86, l87 = sch.split(loop=l46, factors=[v78, v79, v80, v81, v82], preserve_unit_iters=True)
v88, v89, v90 = sch.sample_perfect_tile(loop=l47, n=3, max_innermost_factor=64, decision=[2, 16, 4])
l91, l92, l93 = sch.split(loop=l47, factors=[v88, v89, v90], preserve_unit_iters=True)
sch.reorder(l53, l63, l73, l83, l54, l64, l74, l84, l55, l65, l75, l85, l91, l92, l56, l66, l76, l86, l93, l57, l67, l77, l87)
l94 = sch.fuse(l53, l63, l73, l83, preserve_unit_iters=True)
sch.bind(loop=l94, thread_axis="blockIdx.x")
l95 = sch.fuse(l54, l64, l74, l84, preserve_unit_iters=True)
sch.bind(loop=l95, thread_axis="vthread.x")
l96 = sch.fuse(l55, l65, l75, l85, preserve_unit_iters=True)
sch.bind(loop=l96, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b97 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b97, loop=l96, preserve_unit_loops=True, index=-1)
b98 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b98, loop=l91, preserve_unit_loops=True, index=-1)
l99, l100, l101, l102, l103, l104, l105, l106 = sch.get_loops(block=b98)
l107 = sch.fuse(l103, l104, l105, l106, preserve_unit_iters=True)
v108 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b98, ann_key="meta_schedule.cooperative_fetch", ann_val=v108)
b109 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b109, loop=l91, preserve_unit_loops=True, index=-1)
l110, l111, l112, l113, l114, l115, l116, l117 = sch.get_loops(block=b109)
l118 = sch.fuse(l114, l115, l116, l117, preserve_unit_iters=True)
v119 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b109, ann_key="meta_schedule.cooperative_fetch", ann_val=v119)
sch.reverse_compute_inline(block=b6)
sch.reverse_compute_inline(block=b5)
sch.reverse_compute_inline(block=b4)
v120 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b7, ann_key="meta_schedule.unroll_explicit", ann_val=v120)
l121, l122, l123, l124, l125, l126 = sch.get_loops(block=b3)
l127 = sch.fuse(l121, l122, l123, l124, preserve_unit_iters=True)
v128 = sch.sample_categorical(candidates=[32, 64, 128, 256, 512, 1024], probs=[0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666], decision=3)
l129, l130 = sch.split(loop=l127, factors=[None, v128], preserve_unit_iters=True)
sch.bind(loop=l129, thread_axis="blockIdx.x")
sch.bind(loop=l130, thread_axis="threadIdx.x")
2023-11-10 23:34:13 [INFO] [task_scheduler.cc:170] Design space #1:
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(128), T.int64(28), T.int64(28)), "float32"), p1: T.Buffer((T.int64(4), T.int64(4), T.int64(128), T.int64(128)), "float32"), p2: T.Buffer((T.int64(1), T.int64(128), T.int64(28), T.int64(28)), "float32"), p3: T.Buffer((T.int64(1), T.int64(128), T.int64(1), T.int64(1)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(128), T.int64(28), T.int64(28)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit": 1024})
            input_tile_local = T.alloc_buffer((T.int64(128), T.int64(196), T.int64(4), T.int64(4)), scope="local")
            data_pack = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(128), T.int64(196)))
            bgemm = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(128), T.int64(196)))
            inverse_local = T.alloc_buffer((T.int64(128), T.int64(196), T.int64(2), T.int64(2)), scope="local")
            data_pack_local = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(128), T.int64(196)), scope="local")
            bgemm_local = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(128), T.int64(196)), scope="local")
            data_pack_shared = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(128), T.int64(196)), scope="shared")
            p1_shared = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(128), T.int64(128)), scope="shared")
            for ci_p_fused_0 in T.thread_binding(T.int64(49), thread="blockIdx.x"):
                for ci_p_fused_1 in T.thread_binding(T.int64(512), thread="threadIdx.x"):
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(1), T.int64(4), T.int64(4)):
                        with T.block("input_tile"):
                            v_ci = T.axis.spatial(T.int64(128), (ci_p_fused_0 * T.int64(512) + ci_p_fused_1) // T.int64(196) + ax0)
                            v_p = T.axis.spatial(T.int64(196), (ci_p_fused_0 * T.int64(120) + ci_p_fused_1) % T.int64(196) + ax1)
                            v_eps, v_nu = T.axis.remap("SS", [ax2, ax3])
                            T.reads(p0[v_p // T.int64(196), v_ci, v_p % T.int64(196) // T.int64(14) * T.int64(2) + v_eps - T.int64(1), v_p % T.int64(14) * T.int64(2) + v_nu - T.int64(1)])
                            T.writes(input_tile_local[v_ci, v_p, v_eps, v_nu])
                            T.block_attr({"schedule_rule": "None"})
                            input_tile_local[v_ci, v_p, v_eps, v_nu] = T.if_then_else(T.int64(1) <= v_p % T.int64(196) // T.int64(14) * T.int64(2) + v_eps and v_p % T.int64(196) // T.int64(14) * T.int64(2) + v_eps < T.int64(29) and T.int64(1) <= v_p % T.int64(14) * T.int64(2) + v_nu and v_p % T.int64(14) * T.int64(2) + v_nu < T.int64(29), p0[v_p // T.int64(196), v_ci, v_p % T.int64(196) // T.int64(14) * T.int64(2) + v_eps - T.int64(1), v_p % T.int64(14) * T.int64(2) + v_nu - T.int64(1)], T.float32(0))
                    for eps in T.unroll(T.int64(4)):
                        for nu in T.unroll(T.int64(4)):
                            for r_a in T.unroll(T.int64(4)):
                                for r_b in T.unroll(T.int64(4)):
                                    with T.block("data_pack"):
                                        v_eps, v_nu = T.axis.remap("SS", [eps, nu])
                                        v_ci = T.axis.spatial(T.int64(128), (ci_p_fused_0 * T.int64(512) + ci_p_fused_1) // T.int64(196))
                                        v_p = T.axis.spatial(T.int64(196), (ci_p_fused_0 * T.int64(512) + ci_p_fused_1) % T.int64(196))
                                        v_r_a, v_r_b = T.axis.remap("RR", [r_a, r_b])
                                        T.reads(input_tile_local[v_ci, v_p, v_r_a, v_r_b])
                                        T.writes(data_pack_local[v_eps, v_nu, v_ci, v_p])
                                        T.block_attr({"schedule_rule": "conv2d_nchw_winograd_data_pack"})
                                        with T.init():
                                            data_pack_local[v_eps, v_nu, v_ci, v_p] = T.float32(0)
                                        data_pack_local[v_eps, v_nu, v_ci, v_p] = data_pack_local[v_eps, v_nu, v_ci, v_p] + input_tile_local[v_ci, v_p, v_r_a, v_r_b] * T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(3), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(1), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(0), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(3), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(1), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(0), T.float32(1), T.float32(0))))))))))))))))) * T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(3), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(1), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(0), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(3), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(1), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(0), T.float32(1), T.float32(0)))))))))))))))))
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(4), T.int64(4), T.int64(1), T.int64(1)):
                        with T.block("data_pack_local"):
                            v0, v1 = T.axis.remap("SS", [ax0, ax1])
                            v2 = T.axis.spatial(T.int64(128), (ci_p_fused_0 * T.int64(512) + ci_p_fused_1) // T.int64(196) + ax2)
                            v3 = T.axis.spatial(T.int64(196), (ci_p_fused_0 * T.int64(120) + ci_p_fused_1) % T.int64(196) + ax3)
                            T.reads(data_pack_local[v0, v1, v2, v3])
                            T.writes(data_pack[v0, v1, v2, v3])
                            data_pack[v0, v1, v2, v3] = data_pack_local[v0, v1, v2, v3]
            for eps_0_nu_0_co_0_p_0_fused in T.thread_binding(T.int64(896), thread="blockIdx.x"):
                for eps_1_nu_1_co_1_p_1_fused in T.thread_binding(T.int64(8), thread="vthread.x"):
                    for eps_2_nu_2_co_2_p_2_fused in T.thread_binding(T.int64(1), thread="threadIdx.x"):
                        for ci_0_fused in T.serial(T.int64(2), annotations={"software_pipeline_async_stages": [0], "software_pipeline_order": [0, 1, 2], "software_pipeline_stage": [0, 0, 2]}):
                            for ax0_ax1_ax2_ax3_fused in range(T.int64(1792)):
                                with T.block("data_pack_shared"):
                                    v0 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(224))
                                    v1 = T.axis.spatial(T.int64(4), ax0_ax1_ax2_ax3_fused // T.int64(448))
                                    v2 = T.axis.spatial(T.int64(128), ci_0_fused * T.int64(64) + ax0_ax1_ax2_ax3_fused % T.int64(448) // T.int64(7))
                                    v3 = T.axis.spatial(T.int64(196), eps_0_nu_0_co_0_p_0_fused % T.int64(28) * T.int64(7) + ax0_ax1_ax2_ax3_fused % T.int64(7))
                                    T.reads(data_pack[v0, v1, v2, v3])
                                    T.writes(data_pack_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch": 4})
                                    data_pack_shared[v0, v1, v2, v3] = data_pack[v0, v1, v2, v3]
                            for ax0_ax1_ax2_ax3_fused in range(T.int64(4096)):
                                with T.block("p1_shared"):
                                    v0 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(224))
                                    v1 = T.axis.spatial(T.int64(4), ax0_ax1_ax2_ax3_fused // T.int64(1024))
                                    v2 = T.axis.spatial(T.int64(128), ci_0_fused * T.int64(64) + ax0_ax1_ax2_ax3_fused % T.int64(1024) // T.int64(16))
                                    v3 = T.axis.spatial(T.int64(128), eps_0_nu_0_co_0_p_0_fused % T.int64(224) // T.int64(28) * T.int64(16) + ax0_ax1_ax2_ax3_fused % T.int64(16))
                                    T.reads(p1[v0, v1, v2, v3])
                                    T.writes(p1_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch": 4})
                                    p1_shared[v0, v1, v2, v3] = p1[v0, v1, v2, v3]
                            for ci_1, eps_3, nu_3, co_3, p_3, ci_2, eps_4, nu_4, co_4, p_4 in T.grid(T.int64(16), T.int64(1), T.int64(2), T.int64(4), T.int64(7), T.int64(4), T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                                with T.block("bgemm"):
                                    v_eps = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(224) + eps_3 + eps_4)
                                    v_nu = T.axis.spatial(T.int64(4), eps_1_nu_1_co_1_p_1_fused // T.int64(4) * T.int64(2) + nu_3 + nu_4)
                                    v_co = T.axis.spatial(T.int64(128), eps_0_nu_0_co_0_p_0_fused % T.int64(224) // T.int64(28) * T.int64(16) + eps_1_nu_1_co_1_p_1_fused % T.int64(4) * T.int64(4) + co_3 + co_4)
                                    v_p = T.axis.spatial(T.int64(196), eps_0_nu_0_co_0_p_0_fused % T.int64(28) * T.int64(7) + p_3 + p_4)
                                    v_ci = T.axis.reduce(T.int64(128), ci_0_fused * T.int64(64) + ci_1 * T.int64(4) + ci_2)
                                    T.reads(data_pack_shared[v_eps, v_nu, v_ci, v_p], p1_shared[v_eps, v_nu, v_ci, v_co])
                                    T.writes(bgemm_local[v_eps, v_nu, v_co, v_p])
                                    T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                    with T.init():
                                        bgemm_local[v_eps, v_nu, v_co, v_p] = T.float32(0)
                                    bgemm_local[v_eps, v_nu, v_co, v_p] = bgemm_local[v_eps, v_nu, v_co, v_p] + data_pack_shared[v_eps, v_nu, v_ci, v_p] * p1_shared[v_eps, v_nu, v_ci, v_co]
                        for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(2), T.int64(4), T.int64(7)):
                            with T.block("bgemm_local"):
                                v0 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(224) + ax0)
                                v1 = T.axis.spatial(T.int64(4), eps_1_nu_1_co_1_p_1_fused // T.int64(4) * T.int64(2) + ax1)
                                v2 = T.axis.spatial(T.int64(128), eps_0_nu_0_co_0_p_0_fused % T.int64(224) // T.int64(28) * T.int64(16) + eps_1_nu_1_co_1_p_1_fused % T.int64(4) * T.int64(4) + ax2)
                                v3 = T.axis.spatial(T.int64(196), eps_0_nu_0_co_0_p_0_fused % T.int64(28) * T.int64(7) + ax3)
                                T.reads(bgemm_local[v0, v1, v2, v3])
                                T.writes(bgemm[v0, v1, v2, v3])
                                bgemm[v0, v1, v2, v3] = bgemm_local[v0, v1, v2, v3]
            for n_co_h_0_w_0_fused_0 in T.thread_binding(T.int64(98), thread="blockIdx.x"):
                for n_co_h_0_w_0_fused_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                    for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                        for ax2 in T.unroll(T.int64(2)):
                            for ax3 in T.unroll(T.int64(2)):
                                for ax4 in T.unroll(T.int64(4)):
                                    for ax5 in T.unroll(T.int64(4)):
                                        with T.block("inverse"):
                                            v_co = T.axis.spatial(T.int64(128), (n_co_h_0_w_0_fused_0 * T.int64(256) + n_co_h_0_w_0_fused_1) // T.int64(196) + ax0)
                                            v_p = T.axis.spatial(T.int64(196), (n_co_h_0_w_0_fused_0 * T.int64(256) + n_co_h_0_w_0_fused_1) % T.int64(196) + ax1)
                                            v_vh, v_vw, v_r_a, v_r_b = T.axis.remap("SSRR", [ax2, ax3, ax4, ax5])
                                            T.reads(bgemm[v_r_a, v_r_b, v_co, v_p])
                                            T.writes(inverse_local[v_co, v_p, v_vh, v_vw])
                                            T.block_attr({"schedule_rule": "conv2d_nchw_winograd_inverse"})
                                            with T.init():
                                                inverse_local[v_co, v_p, v_vh, v_vw] = T.float32(0)
                                            inverse_local[v_co, v_p, v_vh, v_vw] = inverse_local[v_co, v_p, v_vh, v_vw] + bgemm[v_r_a, v_r_b, v_co, v_p] * T.Select(v_r_a % T.int64(4) == T.int64(3) and v_vh % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_vh % T.int64(2) == T.int64(0), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_vh % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_vh % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_vh % T.int64(2) == T.int64(1), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_vh % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_vh % T.int64(2) == T.int64(1), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_vh % T.int64(2) == T.int64(0), T.float32(1), T.float32(0))))))))) * T.Select(v_r_b % T.int64(4) == T.int64(3) and v_vw % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_vw % T.int64(2) == T.int64(0), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_vw % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_vw % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_vw % T.int64(2) == T.int64(1), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_vw % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_vw % T.int64(2) == T.int64(1), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_vw % T.int64(2) == T.int64(0), T.float32(1), T.float32(0)))))))))
                    for h_1, w_1 in T.grid(T.int64(2), T.int64(2)):
                        with T.block("conv2d_winograd"):
                            v_n = T.axis.spatial(T.int64(1), T.int64(0))
                            v_co = T.axis.spatial(T.int64(128), (n_co_h_0_w_0_fused_0 * T.int64(256) + n_co_h_0_w_0_fused_1) // T.int64(196))
                            v_h = T.axis.spatial(T.int64(28), (n_co_h_0_w_0_fused_0 * T.int64(256) + n_co_h_0_w_0_fused_1) % T.int64(196) // T.int64(14) * T.int64(2) + h_1)
                            v_w = T.axis.spatial(T.int64(28), (n_co_h_0_w_0_fused_0 * T.int64(256) + n_co_h_0_w_0_fused_1) % T.int64(14) * T.int64(2) + w_1)
                            T.reads(inverse_local[v_co, v_n * T.int64(196) + v_h // T.int64(2) * T.int64(14) + v_w // T.int64(2), v_h % T.int64(2), v_w % T.int64(2)], p2[v_n, v_co, v_h, v_w], p3[v_n, v_co, T.int64(0), T.int64(0)])
                            T.writes(T_relu[v_n, v_co, v_h, v_w])
                            T_relu[v_n, v_co, v_h, v_w] = T.max(inverse_local[v_co, v_n * T.int64(196) + v_h // T.int64(2) * T.int64(14) + v_w // T.int64(2), v_h % T.int64(2), v_w % T.int64(2)] + p2[v_n, v_co, v_h, v_w] + p3[v_n, v_co, T.int64(0), T.int64(0)], T.float32(0))
b0 = sch.get_block(name="data_pack", func_name="main")
b1 = sch.get_block(name="bgemm", func_name="main")
b2 = sch.get_block(name="inverse", func_name="main")
b3 = sch.get_block(name="conv2d_winograd", func_name="main")
b4 = sch.get_block(name="T_add", func_name="main")
b5 = sch.get_block(name="T_add_1", func_name="main")
b6 = sch.get_block(name="T_relu", func_name="main")
b7 = sch.get_block(name="root", func_name="main")
b8, b9 = sch.get_producers(block=b2)
sch.compute_inline(block=b9)
b10, = sch.get_consumers(block=b2)
l11, l12, l13, l14 = sch.get_loops(block=b10)
l15, l16 = sch.split(loop=l13, factors=[None, 2], preserve_unit_iters=True)
l17, l18 = sch.split(loop=l14, factors=[None, 2], preserve_unit_iters=True)
sch.reorder(l15, l17, l16, l18)
sch.compute_at(block=b2, loop=l17, preserve_unit_loops=True, index=-1)
sch.set_scope(block=b2, buffer_index=0, storage_scope="local")
l19, l20, l21, l22, l23, l24, l25, l26, l27, l28 = sch.get_loops(block=b2)
sch.unroll(loop=l25)
sch.unroll(loop=l26)
sch.unroll(loop=l27)
sch.unroll(loop=l28)
b29, b30 = sch.get_producers(block=b0)
sch.compute_inline(block=b30)
b31, = sch.get_producers(block=b29)
l32, l33, l34, l35, l36, l37 = sch.get_loops(block=b0)
sch.reorder(l34, l35, l32, l33, l36, l37)
sch.unroll(loop=l32)
sch.unroll(loop=l33)
sch.unroll(loop=l36)
sch.unroll(loop=l37)
l38 = sch.fuse(l34, l35, preserve_unit_iters=True)
v39 = sch.sample_categorical(candidates=[32, 64, 128, 256, 512, 1024], probs=[0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666], decision=4)
l40, l41 = sch.split(loop=l38, factors=[None, v39], preserve_unit_iters=True)
sch.bind(loop=l40, thread_axis="blockIdx.x")
sch.bind(loop=l41, thread_axis="threadIdx.x")
b42 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b42, loop=l41, preserve_unit_loops=True, index=-1)
sch.compute_at(block=b29, loop=l41, preserve_unit_loops=True, index=-1)
sch.set_scope(block=b29, buffer_index=0, storage_scope="local")
sch.compute_inline(block=b31)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l43, l44, l45, l46, l47 = sch.get_loops(block=b1)
v48, v49, v50, v51, v52 = sch.sample_perfect_tile(loop=l43, n=5, max_innermost_factor=64, decision=[4, 1, 1, 1, 1])
l53, l54, l55, l56, l57 = sch.split(loop=l43, factors=[v48, v49, v50, v51, v52], preserve_unit_iters=True)
v58, v59, v60, v61, v62 = sch.sample_perfect_tile(loop=l44, n=5, max_innermost_factor=64, decision=[1, 2, 1, 2, 1])
l63, l64, l65, l66, l67 = sch.split(loop=l44, factors=[v58, v59, v60, v61, v62], preserve_unit_iters=True)
v68, v69, v70, v71, v72 = sch.sample_perfect_tile(loop=l45, n=5, max_innermost_factor=64, decision=[8, 4, 1, 4, 1])
l73, l74, l75, l76, l77 = sch.split(loop=l45, factors=[v68, v69, v70, v71, v72], preserve_unit_iters=True)
v78, v79, v80, v81, v82 = sch.sample_perfect_tile(loop=l46, n=5, max_innermost_factor=64, decision=[28, 1, 1, 7, 1])
l83, l84, l85, l86, l87 = sch.split(loop=l46, factors=[v78, v79, v80, v81, v82], preserve_unit_iters=True)
v88, v89, v90 = sch.sample_perfect_tile(loop=l47, n=3, max_innermost_factor=64, decision=[2, 16, 4])
l91, l92, l93 = sch.split(loop=l47, factors=[v88, v89, v90], preserve_unit_iters=True)
sch.reorder(l53, l63, l73, l83, l54, l64, l74, l84, l55, l65, l75, l85, l91, l92, l56, l66, l76, l86, l93, l57, l67, l77, l87)
l94 = sch.fuse(l53, l63, l73, l83, preserve_unit_iters=True)
sch.bind(loop=l94, thread_axis="blockIdx.x")
l95 = sch.fuse(l54, l64, l74, l84, preserve_unit_iters=True)
sch.bind(loop=l95, thread_axis="vthread.x")
l96 = sch.fuse(l55, l65, l75, l85, preserve_unit_iters=True)
sch.bind(loop=l96, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b97 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b97, loop=l96, preserve_unit_loops=True, index=-1)
b98 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b98, loop=l91, preserve_unit_loops=True, index=-1)
l99, l100, l101, l102, l103, l104, l105, l106 = sch.get_loops(block=b98)
l107 = sch.fuse(l103, l104, l105, l106, preserve_unit_iters=True)
v108 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b98, ann_key="meta_schedule.cooperative_fetch", ann_val=v108)
b109 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b109, loop=l91, preserve_unit_loops=True, index=-1)
l110, l111, l112, l113, l114, l115, l116, l117 = sch.get_loops(block=b109)
l118 = sch.fuse(l114, l115, l116, l117, preserve_unit_iters=True)
v119 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b109, ann_key="meta_schedule.cooperative_fetch", ann_val=v119)
l120 = sch.fuse(l91, preserve_unit_iters=True)
sch.annotate(block_or_loop=l120, ann_key="software_pipeline_stage", ann_val=[0, 0, 2])
sch.annotate(block_or_loop=l120, ann_key="software_pipeline_order", ann_val=[0, 1, 2])
sch.annotate(block_or_loop=l120, ann_key="software_pipeline_async_stages", ann_val=[0])
sch.reverse_compute_inline(block=b6)
sch.reverse_compute_inline(block=b5)
sch.reverse_compute_inline(block=b4)
v121 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b7, ann_key="meta_schedule.unroll_explicit", ann_val=v121)
l122, l123, l124, l125, l126, l127 = sch.get_loops(block=b3)
l128 = sch.fuse(l122, l123, l124, l125, preserve_unit_iters=True)
v129 = sch.sample_categorical(candidates=[32, 64, 128, 256, 512, 1024], probs=[0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666], decision=3)
l130, l131 = sch.split(loop=l128, factors=[None, v129], preserve_unit_iters=True)
sch.bind(loop=l130, thread_axis="blockIdx.x")
sch.bind(loop=l131, thread_axis="threadIdx.x")
2023-11-10 23:34:13 [INFO] [task_scheduler.cc:170] Design space #2:
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(128), T.int64(28), T.int64(28)), "float32"), p1: T.Buffer((T.int64(4), T.int64(4), T.int64(128), T.int64(128)), "float32"), p2: T.Buffer((T.int64(1), T.int64(128), T.int64(28), T.int64(28)), "float32"), p3: T.Buffer((T.int64(1), T.int64(128), T.int64(1), T.int64(1)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(128), T.int64(28), T.int64(28)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit": 512})
            input_tile_local = T.alloc_buffer((T.int64(128), T.int64(196), T.int64(4), T.int64(4)), scope="local")
            data_pack = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(128), T.int64(196)))
            bgemm = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(128), T.int64(196)))
            inverse_local = T.alloc_buffer((T.int64(128), T.int64(196), T.int64(2), T.int64(2)), scope="local")
            data_pack_local = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(128), T.int64(196)), scope="local")
            bgemm_local = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(128), T.int64(196)), scope="local")
            data_pack_shared = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(128), T.int64(196)), scope="shared")
            p1_shared = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(128), T.int64(128)), scope="shared")
            for ci_p_fused_0 in T.thread_binding(T.int64(49), thread="blockIdx.x"):
                for ci_p_fused_1 in T.thread_binding(T.int64(512), thread="threadIdx.x"):
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(1), T.int64(4), T.int64(4)):
                        with T.block("input_tile"):
                            v_ci = T.axis.spatial(T.int64(128), (ci_p_fused_0 * T.int64(512) + ci_p_fused_1) // T.int64(196) + ax0)
                            v_p = T.axis.spatial(T.int64(196), (ci_p_fused_0 * T.int64(120) + ci_p_fused_1) % T.int64(196) + ax1)
                            v_eps, v_nu = T.axis.remap("SS", [ax2, ax3])
                            T.reads(p0[v_p // T.int64(196), v_ci, v_p % T.int64(196) // T.int64(14) * T.int64(2) + v_eps - T.int64(1), v_p % T.int64(14) * T.int64(2) + v_nu - T.int64(1)])
                            T.writes(input_tile_local[v_ci, v_p, v_eps, v_nu])
                            T.block_attr({"schedule_rule": "None"})
                            input_tile_local[v_ci, v_p, v_eps, v_nu] = T.if_then_else(T.int64(1) <= v_p % T.int64(196) // T.int64(14) * T.int64(2) + v_eps and v_p % T.int64(196) // T.int64(14) * T.int64(2) + v_eps < T.int64(29) and T.int64(1) <= v_p % T.int64(14) * T.int64(2) + v_nu and v_p % T.int64(14) * T.int64(2) + v_nu < T.int64(29), p0[v_p // T.int64(196), v_ci, v_p % T.int64(196) // T.int64(14) * T.int64(2) + v_eps - T.int64(1), v_p % T.int64(14) * T.int64(2) + v_nu - T.int64(1)], T.float32(0))
                    for eps in T.unroll(T.int64(4)):
                        for nu in T.unroll(T.int64(4)):
                            for r_a in T.unroll(T.int64(4)):
                                for r_b in T.unroll(T.int64(4)):
                                    with T.block("data_pack"):
                                        v_eps, v_nu = T.axis.remap("SS", [eps, nu])
                                        v_ci = T.axis.spatial(T.int64(128), (ci_p_fused_0 * T.int64(512) + ci_p_fused_1) // T.int64(196))
                                        v_p = T.axis.spatial(T.int64(196), (ci_p_fused_0 * T.int64(512) + ci_p_fused_1) % T.int64(196))
                                        v_r_a, v_r_b = T.axis.remap("RR", [r_a, r_b])
                                        T.reads(input_tile_local[v_ci, v_p, v_r_a, v_r_b])
                                        T.writes(data_pack_local[v_eps, v_nu, v_ci, v_p])
                                        T.block_attr({"schedule_rule": "conv2d_nchw_winograd_data_pack"})
                                        with T.init():
                                            data_pack_local[v_eps, v_nu, v_ci, v_p] = T.float32(0)
                                        data_pack_local[v_eps, v_nu, v_ci, v_p] = data_pack_local[v_eps, v_nu, v_ci, v_p] + input_tile_local[v_ci, v_p, v_r_a, v_r_b] * T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(3), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(1), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(0), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(3), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(1), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(0), T.float32(1), T.float32(0))))))))))))))))) * T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(3), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(1), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(0), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(3), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(1), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(0), T.float32(1), T.float32(0)))))))))))))))))
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(4), T.int64(4), T.int64(1), T.int64(1)):
                        with T.block("data_pack_local"):
                            v0, v1 = T.axis.remap("SS", [ax0, ax1])
                            v2 = T.axis.spatial(T.int64(128), (ci_p_fused_0 * T.int64(512) + ci_p_fused_1) // T.int64(196) + ax2)
                            v3 = T.axis.spatial(T.int64(196), (ci_p_fused_0 * T.int64(120) + ci_p_fused_1) % T.int64(196) + ax3)
                            T.reads(data_pack_local[v0, v1, v2, v3])
                            T.writes(data_pack[v0, v1, v2, v3])
                            data_pack[v0, v1, v2, v3] = data_pack_local[v0, v1, v2, v3]
            for eps_0_nu_0_co_0_p_0_fused in T.thread_binding(T.int64(896), thread="blockIdx.x"):
                for eps_1_nu_1_co_1_p_1_fused in T.thread_binding(T.int64(8), thread="vthread.x"):
                    for eps_2_nu_2_co_2_p_2_fused in T.thread_binding(T.int64(1), thread="threadIdx.x"):
                        for ci_0_fused in T.serial(T.int64(2), annotations={"software_pipeline_async_stages": [0], "software_pipeline_order": [0, 1, 2], "software_pipeline_stage": [0, 0, 3]}):
                            for ax0_ax1_ax2_ax3_fused in range(T.int64(1792)):
                                with T.block("data_pack_shared"):
                                    v0 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(224))
                                    v1 = T.axis.spatial(T.int64(4), ax0_ax1_ax2_ax3_fused // T.int64(448))
                                    v2 = T.axis.spatial(T.int64(128), ci_0_fused * T.int64(64) + ax0_ax1_ax2_ax3_fused % T.int64(448) // T.int64(7))
                                    v3 = T.axis.spatial(T.int64(196), eps_0_nu_0_co_0_p_0_fused % T.int64(28) * T.int64(7) + ax0_ax1_ax2_ax3_fused % T.int64(7))
                                    T.reads(data_pack[v0, v1, v2, v3])
                                    T.writes(data_pack_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch": 4})
                                    data_pack_shared[v0, v1, v2, v3] = data_pack[v0, v1, v2, v3]
                            for ax0_ax1_ax2_ax3_fused in range(T.int64(4096)):
                                with T.block("p1_shared"):
                                    v0 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(224))
                                    v1 = T.axis.spatial(T.int64(4), ax0_ax1_ax2_ax3_fused // T.int64(1024))
                                    v2 = T.axis.spatial(T.int64(128), ci_0_fused * T.int64(64) + ax0_ax1_ax2_ax3_fused % T.int64(1024) // T.int64(16))
                                    v3 = T.axis.spatial(T.int64(128), eps_0_nu_0_co_0_p_0_fused % T.int64(224) // T.int64(28) * T.int64(16) + ax0_ax1_ax2_ax3_fused % T.int64(16))
                                    T.reads(p1[v0, v1, v2, v3])
                                    T.writes(p1_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch": 4})
                                    p1_shared[v0, v1, v2, v3] = p1[v0, v1, v2, v3]
                            for ci_1, eps_3, nu_3, co_3, p_3, ci_2, eps_4, nu_4, co_4, p_4 in T.grid(T.int64(16), T.int64(1), T.int64(2), T.int64(4), T.int64(7), T.int64(4), T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                                with T.block("bgemm"):
                                    v_eps = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(224) + eps_3 + eps_4)
                                    v_nu = T.axis.spatial(T.int64(4), eps_1_nu_1_co_1_p_1_fused // T.int64(4) * T.int64(2) + nu_3 + nu_4)
                                    v_co = T.axis.spatial(T.int64(128), eps_0_nu_0_co_0_p_0_fused % T.int64(224) // T.int64(28) * T.int64(16) + eps_1_nu_1_co_1_p_1_fused % T.int64(4) * T.int64(4) + co_3 + co_4)
                                    v_p = T.axis.spatial(T.int64(196), eps_0_nu_0_co_0_p_0_fused % T.int64(28) * T.int64(7) + p_3 + p_4)
                                    v_ci = T.axis.reduce(T.int64(128), ci_0_fused * T.int64(64) + ci_1 * T.int64(4) + ci_2)
                                    T.reads(data_pack_shared[v_eps, v_nu, v_ci, v_p], p1_shared[v_eps, v_nu, v_ci, v_co])
                                    T.writes(bgemm_local[v_eps, v_nu, v_co, v_p])
                                    T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                    with T.init():
                                        bgemm_local[v_eps, v_nu, v_co, v_p] = T.float32(0)
                                    bgemm_local[v_eps, v_nu, v_co, v_p] = bgemm_local[v_eps, v_nu, v_co, v_p] + data_pack_shared[v_eps, v_nu, v_ci, v_p] * p1_shared[v_eps, v_nu, v_ci, v_co]
                        for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(2), T.int64(4), T.int64(7)):
                            with T.block("bgemm_local"):
                                v0 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(224) + ax0)
                                v1 = T.axis.spatial(T.int64(4), eps_1_nu_1_co_1_p_1_fused // T.int64(4) * T.int64(2) + ax1)
                                v2 = T.axis.spatial(T.int64(128), eps_0_nu_0_co_0_p_0_fused % T.int64(224) // T.int64(28) * T.int64(16) + eps_1_nu_1_co_1_p_1_fused % T.int64(4) * T.int64(4) + ax2)
                                v3 = T.axis.spatial(T.int64(196), eps_0_nu_0_co_0_p_0_fused % T.int64(28) * T.int64(7) + ax3)
                                T.reads(bgemm_local[v0, v1, v2, v3])
                                T.writes(bgemm[v0, v1, v2, v3])
                                bgemm[v0, v1, v2, v3] = bgemm_local[v0, v1, v2, v3]
            for n_co_h_0_w_0_fused_0 in T.thread_binding(T.int64(196), thread="blockIdx.x"):
                for n_co_h_0_w_0_fused_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                    for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                        for ax2 in T.unroll(T.int64(2)):
                            for ax3 in T.unroll(T.int64(2)):
                                for ax4 in T.unroll(T.int64(4)):
                                    for ax5 in T.unroll(T.int64(4)):
                                        with T.block("inverse"):
                                            v_co = T.axis.spatial(T.int64(128), (n_co_h_0_w_0_fused_0 * T.int64(128) + n_co_h_0_w_0_fused_1) // T.int64(196) + ax0)
                                            v_p = T.axis.spatial(T.int64(196), (n_co_h_0_w_0_fused_0 * T.int64(128) + n_co_h_0_w_0_fused_1) % T.int64(196) + ax1)
                                            v_vh, v_vw, v_r_a, v_r_b = T.axis.remap("SSRR", [ax2, ax3, ax4, ax5])
                                            T.reads(bgemm[v_r_a, v_r_b, v_co, v_p])
                                            T.writes(inverse_local[v_co, v_p, v_vh, v_vw])
                                            T.block_attr({"schedule_rule": "conv2d_nchw_winograd_inverse"})
                                            with T.init():
                                                inverse_local[v_co, v_p, v_vh, v_vw] = T.float32(0)
                                            inverse_local[v_co, v_p, v_vh, v_vw] = inverse_local[v_co, v_p, v_vh, v_vw] + bgemm[v_r_a, v_r_b, v_co, v_p] * T.Select(v_r_a % T.int64(4) == T.int64(3) and v_vh % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_vh % T.int64(2) == T.int64(0), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_vh % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_vh % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_vh % T.int64(2) == T.int64(1), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_vh % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_vh % T.int64(2) == T.int64(1), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_vh % T.int64(2) == T.int64(0), T.float32(1), T.float32(0))))))))) * T.Select(v_r_b % T.int64(4) == T.int64(3) and v_vw % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_vw % T.int64(2) == T.int64(0), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_vw % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_vw % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_vw % T.int64(2) == T.int64(1), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_vw % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_vw % T.int64(2) == T.int64(1), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_vw % T.int64(2) == T.int64(0), T.float32(1), T.float32(0)))))))))
                    for h_1, w_1 in T.grid(T.int64(2), T.int64(2)):
                        with T.block("conv2d_winograd"):
                            v_n = T.axis.spatial(T.int64(1), T.int64(0))
                            v_co = T.axis.spatial(T.int64(128), (n_co_h_0_w_0_fused_0 * T.int64(128) + n_co_h_0_w_0_fused_1) // T.int64(196))
                            v_h = T.axis.spatial(T.int64(28), (n_co_h_0_w_0_fused_0 * T.int64(128) + n_co_h_0_w_0_fused_1) % T.int64(196) // T.int64(14) * T.int64(2) + h_1)
                            v_w = T.axis.spatial(T.int64(28), (n_co_h_0_w_0_fused_0 * T.int64(128) + n_co_h_0_w_0_fused_1) % T.int64(14) * T.int64(2) + w_1)
                            T.reads(inverse_local[v_co, v_n * T.int64(196) + v_h // T.int64(2) * T.int64(14) + v_w // T.int64(2), v_h % T.int64(2), v_w % T.int64(2)], p2[v_n, v_co, v_h, v_w], p3[v_n, v_co, T.int64(0), T.int64(0)])
                            T.writes(T_relu[v_n, v_co, v_h, v_w])
                            T_relu[v_n, v_co, v_h, v_w] = T.max(inverse_local[v_co, v_n * T.int64(196) + v_h // T.int64(2) * T.int64(14) + v_w // T.int64(2), v_h % T.int64(2), v_w % T.int64(2)] + p2[v_n, v_co, v_h, v_w] + p3[v_n, v_co, T.int64(0), T.int64(0)], T.float32(0))
b0 = sch.get_block(name="data_pack", func_name="main")
b1 = sch.get_block(name="bgemm", func_name="main")
b2 = sch.get_block(name="inverse", func_name="main")
b3 = sch.get_block(name="conv2d_winograd", func_name="main")
b4 = sch.get_block(name="T_add", func_name="main")
b5 = sch.get_block(name="T_add_1", func_name="main")
b6 = sch.get_block(name="T_relu", func_name="main")
b7 = sch.get_block(name="root", func_name="main")
b8, b9 = sch.get_producers(block=b2)
sch.compute_inline(block=b9)
b10, = sch.get_consumers(block=b2)
l11, l12, l13, l14 = sch.get_loops(block=b10)
l15, l16 = sch.split(loop=l13, factors=[None, 2], preserve_unit_iters=True)
l17, l18 = sch.split(loop=l14, factors=[None, 2], preserve_unit_iters=True)
sch.reorder(l15, l17, l16, l18)
sch.compute_at(block=b2, loop=l17, preserve_unit_loops=True, index=-1)
sch.set_scope(block=b2, buffer_index=0, storage_scope="local")
l19, l20, l21, l22, l23, l24, l25, l26, l27, l28 = sch.get_loops(block=b2)
sch.unroll(loop=l25)
sch.unroll(loop=l26)
sch.unroll(loop=l27)
sch.unroll(loop=l28)
b29, b30 = sch.get_producers(block=b0)
sch.compute_inline(block=b30)
b31, = sch.get_producers(block=b29)
l32, l33, l34, l35, l36, l37 = sch.get_loops(block=b0)
sch.reorder(l34, l35, l32, l33, l36, l37)
sch.unroll(loop=l32)
sch.unroll(loop=l33)
sch.unroll(loop=l36)
sch.unroll(loop=l37)
l38 = sch.fuse(l34, l35, preserve_unit_iters=True)
v39 = sch.sample_categorical(candidates=[32, 64, 128, 256, 512, 1024], probs=[0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666], decision=4)
l40, l41 = sch.split(loop=l38, factors=[None, v39], preserve_unit_iters=True)
sch.bind(loop=l40, thread_axis="blockIdx.x")
sch.bind(loop=l41, thread_axis="threadIdx.x")
b42 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b42, loop=l41, preserve_unit_loops=True, index=-1)
sch.compute_at(block=b29, loop=l41, preserve_unit_loops=True, index=-1)
sch.set_scope(block=b29, buffer_index=0, storage_scope="local")
sch.compute_inline(block=b31)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l43, l44, l45, l46, l47 = sch.get_loops(block=b1)
v48, v49, v50, v51, v52 = sch.sample_perfect_tile(loop=l43, n=5, max_innermost_factor=64, decision=[4, 1, 1, 1, 1])
l53, l54, l55, l56, l57 = sch.split(loop=l43, factors=[v48, v49, v50, v51, v52], preserve_unit_iters=True)
v58, v59, v60, v61, v62 = sch.sample_perfect_tile(loop=l44, n=5, max_innermost_factor=64, decision=[1, 2, 1, 2, 1])
l63, l64, l65, l66, l67 = sch.split(loop=l44, factors=[v58, v59, v60, v61, v62], preserve_unit_iters=True)
v68, v69, v70, v71, v72 = sch.sample_perfect_tile(loop=l45, n=5, max_innermost_factor=64, decision=[8, 4, 1, 4, 1])
l73, l74, l75, l76, l77 = sch.split(loop=l45, factors=[v68, v69, v70, v71, v72], preserve_unit_iters=True)
v78, v79, v80, v81, v82 = sch.sample_perfect_tile(loop=l46, n=5, max_innermost_factor=64, decision=[28, 1, 1, 7, 1])
l83, l84, l85, l86, l87 = sch.split(loop=l46, factors=[v78, v79, v80, v81, v82], preserve_unit_iters=True)
v88, v89, v90 = sch.sample_perfect_tile(loop=l47, n=3, max_innermost_factor=64, decision=[2, 16, 4])
l91, l92, l93 = sch.split(loop=l47, factors=[v88, v89, v90], preserve_unit_iters=True)
sch.reorder(l53, l63, l73, l83, l54, l64, l74, l84, l55, l65, l75, l85, l91, l92, l56, l66, l76, l86, l93, l57, l67, l77, l87)
l94 = sch.fuse(l53, l63, l73, l83, preserve_unit_iters=True)
sch.bind(loop=l94, thread_axis="blockIdx.x")
l95 = sch.fuse(l54, l64, l74, l84, preserve_unit_iters=True)
sch.bind(loop=l95, thread_axis="vthread.x")
l96 = sch.fuse(l55, l65, l75, l85, preserve_unit_iters=True)
sch.bind(loop=l96, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b97 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b97, loop=l96, preserve_unit_loops=True, index=-1)
b98 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b98, loop=l91, preserve_unit_loops=True, index=-1)
l99, l100, l101, l102, l103, l104, l105, l106 = sch.get_loops(block=b98)
l107 = sch.fuse(l103, l104, l105, l106, preserve_unit_iters=True)
v108 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b98, ann_key="meta_schedule.cooperative_fetch", ann_val=v108)
b109 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b109, loop=l91, preserve_unit_loops=True, index=-1)
l110, l111, l112, l113, l114, l115, l116, l117 = sch.get_loops(block=b109)
l118 = sch.fuse(l114, l115, l116, l117, preserve_unit_iters=True)
v119 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b109, ann_key="meta_schedule.cooperative_fetch", ann_val=v119)
l120 = sch.fuse(l91, preserve_unit_iters=True)
sch.annotate(block_or_loop=l120, ann_key="software_pipeline_stage", ann_val=[0, 0, 3])
sch.annotate(block_or_loop=l120, ann_key="software_pipeline_order", ann_val=[0, 1, 2])
sch.annotate(block_or_loop=l120, ann_key="software_pipeline_async_stages", ann_val=[0])
sch.reverse_compute_inline(block=b6)
sch.reverse_compute_inline(block=b5)
sch.reverse_compute_inline(block=b4)
v121 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b7, ann_key="meta_schedule.unroll_explicit", ann_val=v121)
l122, l123, l124, l125, l126, l127 = sch.get_loops(block=b3)
l128 = sch.fuse(l122, l123, l124, l125, preserve_unit_iters=True)
v129 = sch.sample_categorical(candidates=[32, 64, 128, 256, 512, 1024], probs=[0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666], decision=2)
l130, l131 = sch.split(loop=l128, factors=[None, v129], preserve_unit_iters=True)
sch.bind(loop=l130, thread_axis="blockIdx.x")
sch.bind(loop=l131, thread_axis="threadIdx.x")
2023-11-10 23:55:05 [INFO] [evolutionary_search.cc:713] Generating candidates......
2023-11-10 23:55:05 [INFO] [evolutionary_search.cc:715] Picked top 0 candidate(s) from database
2023-11-10 23:55:13 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310fb046d8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310f554558)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310dc5ddb8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310bbcd2d8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56310b1dbf38)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310dc5d048)]: 496 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310dc26e38)]: 0 failure(s)
2023-11-10 23:55:22 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310fb046d8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310f554558)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310dc5ddb8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310bbcd2d8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56310b1dbf38)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310dc5d048)]: 992 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310dc26e38)]: 0 failure(s)
2023-11-10 23:55:30 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310fb046d8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310f554558)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310dc5ddb8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310bbcd2d8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56310b1dbf38)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310dc5d048)]: 1485 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310dc26e38)]: 0 failure(s)
2023-11-10 23:55:30 [INFO] [evolutionary_search.cc:723] Sampled 51 candidate(s)
2023-11-10 23:55:49 [INFO] [evolutionary_search.cc:621] Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310fb046d8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310f554558)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310dc5ddb8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310bbcd2d8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56310b1dbf38)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310dc5d048)]: 102 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310dc26e38)]: 0 failure(s)
2023-11-10 23:56:06 [INFO] [evolutionary_search.cc:621] Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310fb046d8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310f554558)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310dc5ddb8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310bbcd2d8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56310b1dbf38)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310dc5d048)]: 92 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310dc26e38)]: 0 failure(s)
2023-11-10 23:56:23 [INFO] [evolutionary_search.cc:621] Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310fb046d8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310f554558)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310dc5ddb8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310bbcd2d8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56310b1dbf38)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310dc5d048)]: 95 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310dc26e38)]: 0 failure(s)
2023-11-10 23:56:41 [INFO] [evolutionary_search.cc:621] Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310fb046d8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310f554558)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310dc5ddb8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310bbcd2d8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56310b1dbf38)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310dc5d048)]: 107 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310dc26e38)]: 0 failure(s)
2023-11-10 23:56:42 [INFO] [evolutionary_search.cc:649] Scores of the best 64 candidates:
[1 : 16]:	0.9997  0.9993  0.9980  0.9975  0.9963  0.9945  0.9940  0.9927  0.9927  0.9924  0.9910  0.9908  0.9907  0.9898  0.9894  0.9881
[17 : 32]:	0.9875  0.9868  0.9865  0.9856  0.9856  0.9837  0.9835  0.9833  0.9825  0.9822  0.9818  0.9817  0.9809  0.9808  0.9795  0.9790
[33 : 48]:	0.9764  0.9762  0.9758  0.9754  0.9749  0.9738  0.9728  0.9724  0.9718  0.9708  0.9704  0.9692  0.9691  0.9686  0.9682  0.9682
[49 : 64]:	0.9682  0.9677  0.9672  0.9668  0.9666  0.9657  0.9653  0.9650  0.9648  0.9643  0.9628  0.9625  0.9623  0.9618  0.9604  0.9588
2023-11-10 23:56:42 [INFO] [evolutionary_search.cc:727] Got 64 candidate(s) with evolutionary search
2023-11-10 23:56:42 [INFO] [evolutionary_search.cc:730] Sending 64 candidates(s) for measurement
2023-11-11 00:25:35 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #1: GFLOPs: 32.6083. Time: 3899.1952 us. Best GFLOPs: 32.6083
2023-11-11 00:25:35 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #2: GFLOPs: 1112.3663. Time: 114.3023 us. Best GFLOPs: 1112.3663
2023-11-11 00:25:35 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #3: GFLOPs: 2709.9146. Time: 46.9188 us. Best GFLOPs: 2709.9146
2023-11-11 00:25:35 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #4: GFLOPs: 11.5918. Time: 10968.5760 us. Best GFLOPs: 2709.9146
2023-11-11 00:25:35 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #5: GFLOPs: 2597.6558. Time: 48.9464 us. Best GFLOPs: 2709.9146
2023-11-11 00:25:35 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #6: GFLOPs: 1918.4817. Time: 66.2743 us. Best GFLOPs: 2709.9146
2023-11-11 00:25:35 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #7: GFLOPs: 74.9373. Time: 1696.6986 us. Best GFLOPs: 2709.9146
2023-11-11 00:25:35 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #8: GFLOPs: 1860.5575. Time: 68.3376 us. Best GFLOPs: 2709.9146
2023-11-11 00:25:35 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #9: GFLOPs: 2749.6080. Time: 46.2415 us. Best GFLOPs: 2749.6080
2023-11-11 00:25:35 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #10: GFLOPs: 73.0351. Time: 1740.8883 us. Best GFLOPs: 2749.6080
2023-11-11 00:25:35 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #11: GFLOPs: 73.2304. Time: 1736.2450 us. Best GFLOPs: 2749.6080
2023-11-11 00:25:35 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #12: GFLOPs: 1323.9469. Time: 96.0356 us. Best GFLOPs: 2749.6080
2023-11-11 00:25:35 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #13: GFLOPs: 2448.1062. Time: 51.9365 us. Best GFLOPs: 2749.6080
2023-11-11 00:25:35 [INFO] [task_scheduler.cc:121] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #14: Error in building:
LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(128), T.int64(28), T.int64(28)), "float32"), p1: T.Buffer((T.int64(4), T.int64(4), T.int64(128), T.int64(128)), "float32"), p2: T.Buffer((T.int64(1), T.int64(128), T.int64(28), T.int64(28)), "float32"), p3: T.Buffer((T.int64(1), T.int64(128), T.int64(1), T.int64(1)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(128), T.int64(28), T.int64(28)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        input_tile_local = T.alloc_buffer((T.int64(128), T.int64(196), T.int64(4), T.int64(4)), scope="local")
        data_pack = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(128), T.int64(196)))
        bgemm = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(128), T.int64(196)))
        inverse_local = T.alloc_buffer((T.int64(128), T.int64(196), T.int64(2), T.int64(2)), scope="local")
        data_pack_local = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(128), T.int64(196)), scope="local")
        bgemm_local = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(128), T.int64(196)), scope="local")
        data_pack_shared = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(128), T.int64(196)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(128), T.int64(128)), scope="shared")
        for ci_p_fused_0 in T.thread_binding(T.int64(784), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 512, "pragma_unroll_explicit": 1}):
            for ci_p_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(1), T.int64(4), T.int64(4)):
                    with T.block("input_tile"):
                        v_ci = T.axis.spatial(T.int64(128), (ci_p_fused_0 * T.int64(32) + ci_p_fused_1) // T.int64(196) + ax0)
                        v_p = T.axis.spatial(T.int64(196), (ci_p_fused_0 * T.int64(32) + ci_p_fused_1) % T.int64(196) + ax1)
                        v_eps, v_nu = T.axis.remap("SS", [ax2, ax3])
                        T.reads(p0[v_p // T.int64(196), v_ci, v_p % T.int64(196) // T.int64(14) * T.int64(2) + v_eps - T.int64(1), v_p % T.int64(14) * T.int64(2) + v_nu - T.int64(1)])
                        T.writes(input_tile_local[v_ci, v_p, v_eps, v_nu])
                        T.block_attr({"schedule_rule": "None"})
                        input_tile_local[v_ci, v_p, v_eps, v_nu] = T.if_then_else(T.int64(1) <= v_p % T.int64(196) // T.int64(14) * T.int64(2) + v_eps and v_p % T.int64(196) // T.int64(14) * T.int64(2) + v_eps < T.int64(29) and T.int64(1) <= v_p % T.int64(14) * T.int64(2) + v_nu and v_p % T.int64(14) * T.int64(2) + v_nu < T.int64(29), p0[v_p // T.int64(196), v_ci, v_p % T.int64(196) // T.int64(14) * T.int64(2) + v_eps - T.int64(1), v_p % T.int64(14) * T.int64(2) + v_nu - T.int64(1)], T.float32(0))
                for eps in T.unroll(T.int64(4)):
                    for nu in T.unroll(T.int64(4)):
                        with T.block("data_pack_init"):
                            v_eps, v_nu = T.axis.remap("SS", [eps, nu])
                            v_ci = T.axis.spatial(T.int64(128), (ci_p_fused_0 * T.int64(32) + ci_p_fused_1) // T.int64(196))
                            v_p = T.axis.spatial(T.int64(196), (ci_p_fused_0 * T.int64(32) + ci_p_fused_1) % T.int64(196))
                            T.reads()
                            T.writes(data_pack_local[v_eps, v_nu, v_ci, v_p])
                            T.block_attr({"schedule_rule": "conv2d_nchw_winograd_data_pack"})
                            data_pack_local[v_eps, v_nu, v_ci, v_p] = T.float32(0)
                        for r_a in T.unroll(T.int64(4)):
                            for r_b in T.unroll(T.int64(4)):
                                with T.block("data_pack_update"):
                                    v_eps, v_nu = T.axis.remap("SS", [eps, nu])
                                    v_ci = T.axis.spatial(T.int64(128), (ci_p_fused_0 * T.int64(32) + ci_p_fused_1) // T.int64(196))
                                    v_p = T.axis.spatial(T.int64(196), (ci_p_fused_0 * T.int64(32) + ci_p_fused_1) % T.int64(196))
                                    v_r_a, v_r_b = T.axis.remap("RR", [r_a, r_b])
                                    T.reads(data_pack_local[v_eps, v_nu, v_ci, v_p], input_tile_local[v_ci, v_p, v_r_a, v_r_b])
                                    T.writes(data_pack_local[v_eps, v_nu, v_ci, v_p])
                                    T.block_attr({"schedule_rule": "conv2d_nchw_winograd_data_pack"})
                                    data_pack_local[v_eps, v_nu, v_ci, v_p] = data_pack_local[v_eps, v_nu, v_ci, v_p] + input_tile_local[v_ci, v_p, v_r_a, v_r_b] * T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(3), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(1), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(0), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(3), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(1), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(0), T.float32(1), T.float32(0))))))))))))))))) * T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(3), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(1), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(0), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(3), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(1), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(0), T.float32(1), T.float32(0)))))))))))))))))
                for ax0, ax1, ax2, ax3 in T.grid(T.int64(4), T.int64(4), T.int64(1), T.int64(1)):
                    with T.block("data_pack_local"):
                        v0, v1 = T.axis.remap("SS", [ax0, ax1])
                        v2 = T.axis.spatial(T.int64(128), (ci_p_fused_0 * T.int64(32) + ci_p_fused_1) // T.int64(196) + ax2)
                        v3 = T.axis.spatial(T.int64(196), (ci_p_fused_0 * T.int64(32) + ci_p_fused_1) % T.int64(196) + ax3)
                        T.reads(data_pack_local[v0, v1, v2, v3])
                        T.writes(data_pack[v0, v1, v2, v3])
                        data_pack[v0, v1, v2, v3] = data_pack_local[v0, v1, v2, v3]
        for eps_0_nu_0_co_0_p_0_fused in T.thread_binding(T.int64(8), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 512, "pragma_unroll_explicit": 1}):
            for eps_1_nu_1_co_1_p_1_fused in T.thread_binding(T.int64(56), thread="vthread.x"):
                for eps_2_nu_2_co_2_p_2_fused in T.thread_binding(T.int64(56), thread="threadIdx.x"):
                    for eps_3_init, nu_3_init, co_3_init, p_3_init, eps_4_init, nu_4_init, co_4_init, p_4_init in T.grid(T.int64(1), T.int64(1), T.int64(2), T.int64(1), T.int64(1), T.int64(2), T.int64(4), T.int64(1)):
                        with T.block("bgemm_init"):
                            v_eps = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(2) + eps_3_init + eps_4_init)
                            v_nu = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused % T.int64(2) * T.int64(2) + nu_3_init * T.int64(2) + nu_4_init)
                            v_co = T.axis.spatial(T.int64(128), eps_1_nu_1_co_1_p_1_fused // T.int64(28) * T.int64(64) + eps_2_nu_2_co_2_p_2_fused // T.int64(7) * T.int64(8) + co_3_init * T.int64(4) + co_4_init)
                            v_p = T.axis.spatial(T.int64(196), eps_1_nu_1_co_1_p_1_fused % T.int64(28) * T.int64(7) + eps_2_nu_2_co_2_p_2_fused % T.int64(7) + p_3_init + p_4_init)
                            T.reads()
                            T.writes(bgemm_local[v_eps, v_nu, v_co, v_p])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            bgemm_local[v_eps, v_nu, v_co, v_p] = T.float32(0)
                    for ci_0_fused in T.serial(T.int64(32), annotations={"software_pipeline_async_stages": [0], "software_pipeline_order": [0, 1, 2], "software_pipeline_stage": [0, 0, 3]}):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(14)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(56), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("data_pack_shared"):
                                        v0 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(2))
                                        v1 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused % T.int64(2) * T.int64(2) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(112) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) // T.int64(784))
                                        v2 = T.axis.spatial(T.int64(128), ci_0_fused * T.int64(4) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(112) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) % T.int64(784) // T.int64(196))
                                        v3 = T.axis.spatial(T.int64(196), (ax0_ax1_ax2_ax3_fused_0 * T.int64(112) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) % T.int64(196))
                                        T.reads(data_pack[v0, v1, v2, v3])
                                        T.writes(data_pack_shared[v0, v1, v2, v3])
                                        data_pack_shared[v0, v1, v2, v3] = data_pack[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(19)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(56), thread="threadIdx.x"):
                                with T.block("p1_shared"):
                                    v0 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(2))
                                    v1 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused % T.int64(2) * T.int64(2) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(56) + ax0_ax1_ax2_ax3_fused_1) // T.int64(512))
                                    v2 = T.axis.spatial(T.int64(128), ci_0_fused * T.int64(4) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(56) + ax0_ax1_ax2_ax3_fused_1) % T.int64(512) // T.int64(128))
                                    v3 = T.axis.spatial(T.int64(128), (ax0_ax1_ax2_ax3_fused_0 * T.int64(56) + ax0_ax1_ax2_ax3_fused_1) % T.int64(128))
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * T.int64(56) + ax0_ax1_ax2_ax3_fused_1 < T.int64(1024))
                                    T.reads(p1[v0, v1, v2, v3])
                                    T.writes(p1_shared[v0, v1, v2, v3])
                                    p1_shared[v0, v1, v2, v3] = p1[v0, v1, v2, v3]
                        for ci_1, eps_3, nu_3, co_3, p_3, ci_2, eps_4, nu_4, co_4, p_4 in T.grid(T.int64(4), T.int64(1), T.int64(1), T.int64(2), T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(4), T.int64(1)):
                            with T.block("bgemm_update"):
                                v_eps = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(2) + eps_3 + eps_4)
                                v_nu = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused % T.int64(2) * T.int64(2) + nu_3 * T.int64(2) + nu_4)
                                v_co = T.axis.spatial(T.int64(128), eps_1_nu_1_co_1_p_1_fused // T.int64(28) * T.int64(64) + eps_2_nu_2_co_2_p_2_fused // T.int64(7) * T.int64(8) + co_3 * T.int64(4) + co_4)
                                v_p = T.axis.spatial(T.int64(196), eps_1_nu_1_co_1_p_1_fused % T.int64(28) * T.int64(7) + eps_2_nu_2_co_2_p_2_fused % T.int64(7) + p_3 + p_4)
                                v_ci = T.axis.reduce(T.int64(128), ci_0_fused * T.int64(4) + ci_1 + ci_2)
                                T.reads(bgemm_local[v_eps, v_nu, v_co, v_p], data_pack_shared[v_eps, v_nu, v_ci, v_p], p1_shared[v_eps, v_nu, v_ci, v_co])
                                T.writes(bgemm_local[v_eps, v_nu, v_co, v_p])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                bgemm_local[v_eps, v_nu, v_co, v_p] = bgemm_local[v_eps, v_nu, v_co, v_p] + data_pack_shared[v_eps, v_nu, v_ci, v_p] * p1_shared[v_eps, v_nu, v_ci, v_co]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(2), T.int64(8), T.int64(1)):
                        with T.block("bgemm_local"):
                            v0 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(2) + ax0)
                            v1 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused % T.int64(2) * T.int64(2) + ax1)
                            v2 = T.axis.spatial(T.int64(128), eps_1_nu_1_co_1_p_1_fused // T.int64(28) * T.int64(64) + eps_2_nu_2_co_2_p_2_fused // T.int64(7) * T.int64(8) + ax2)
                            v3 = T.axis.spatial(T.int64(196), eps_1_nu_1_co_1_p_1_fused % T.int64(28) * T.int64(7) + eps_2_nu_2_co_2_p_2_fused % T.int64(7) + ax3)
                            T.reads(bgemm_local[v0, v1, v2, v3])
                            T.writes(bgemm[v0, v1, v2, v3])
                            bgemm[v0, v1, v2, v3] = bgemm_local[v0, v1, v2, v3]
        for n_co_h_0_w_0_fused_0 in T.thread_binding(T.int64(392), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 512, "pragma_unroll_explicit": 1}):
            for n_co_h_0_w_0_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                    for ax2 in T.unroll(T.int64(2)):
                        for ax3 in T.unroll(T.int64(2)):
                            with T.block("inverse_init"):
                                v_co = T.axis.spatial(T.int64(128), (n_co_h_0_w_0_fused_0 * T.int64(64) + n_co_h_0_w_0_fused_1) // T.int64(196) + ax0)
                                v_p = T.axis.spatial(T.int64(196), (n_co_h_0_w_0_fused_0 * T.int64(64) + n_co_h_0_w_0_fused_1) % T.int64(196) + ax1)
                                v_vh, v_vw = T.axis.remap("SS", [ax2, ax3])
                                T.reads()
                                T.writes(inverse_local[v_co, v_p, v_vh, v_vw])
                                T.block_attr({"schedule_rule": "conv2d_nchw_winograd_inverse"})
                                inverse_local[v_co, v_p, v_vh, v_vw] = T.float32(0)
                            for ax4 in T.unroll(T.int64(4)):
                                for ax5 in T.unroll(T.int64(4)):
                                    with T.block("inverse_update"):
                                        v_co = T.axis.spatial(T.int64(128), (n_co_h_0_w_0_fused_0 * T.int64(64) + n_co_h_0_w_0_fused_1) // T.int64(196) + ax0)
                                        v_p = T.axis.spatial(T.int64(196), (n_co_h_0_w_0_fused_0 * T.int64(64) + n_co_h_0_w_0_fused_1) % T.int64(196) + ax1)
                                        v_vh, v_vw, v_r_a, v_r_b = T.axis.remap("SSRR", [ax2, ax3, ax4, ax5])
                                        T.reads(inverse_local[v_co, v_p, v_vh, v_vw], bgemm[v_r_a, v_r_b, v_co, v_p])
                                        T.writes(inverse_local[v_co, v_p, v_vh, v_vw])
                                        T.block_attr({"schedule_rule": "conv2d_nchw_winograd_inverse"})
                                        inverse_local[v_co, v_p, v_vh, v_vw] = inverse_local[v_co, v_p, v_vh, v_vw] + bgemm[v_r_a, v_r_b, v_co, v_p] * T.Select(v_r_a % T.int64(4) == T.int64(3) and v_vh % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_vh % T.int64(2) == T.int64(0), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_vh % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_vh % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_vh % T.int64(2) == T.int64(1), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_vh % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_vh % T.int64(2) == T.int64(1), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_vh % T.int64(2) == T.int64(0), T.float32(1), T.float32(0))))))))) * T.Select(v_r_b % T.int64(4) == T.int64(3) and v_vw % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_vw % T.int64(2) == T.int64(0), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_vw % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_vw % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_vw % T.int64(2) == T.int64(1), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_vw % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_vw % T.int64(2) == T.int64(1), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_vw % T.int64(2) == T.int64(0), T.float32(1), T.float32(0)))))))))
                for h_1, w_1 in T.grid(T.int64(2), T.int64(2)):
                    with T.block("conv2d_winograd"):
                        v_n = T.axis.spatial(T.int64(1), T.int64(0))
                        v_co = T.axis.spatial(T.int64(128), (n_co_h_0_w_0_fused_0 * T.int64(64) + n_co_h_0_w_0_fused_1) // T.int64(196))
                        v_h = T.axis.spatial(T.int64(28), (n_co_h_0_w_0_fused_0 * T.int64(64) + n_co_h_0_w_0_fused_1) % T.int64(196) // T.int64(14) * T.int64(2) + h_1)
                        v_w = T.axis.spatial(T.int64(28), (n_co_h_0_w_0_fused_0 * T.int64(64) + n_co_h_0_w_0_fused_1) % T.int64(14) * T.int64(2) + w_1)
                        T.reads(inverse_local[v_co, v_n * T.int64(196) + v_h // T.int64(2) * T.int64(14) + v_w // T.int64(2), v_h % T.int64(2), v_w % T.int64(2)], p2[v_n, v_co, v_h, v_w], p3[v_n, v_co, T.int64(0), T.int64(0)])
                        T.writes(T_relu[v_n, v_co, v_h, v_w])
                        T_relu[v_n, v_co, v_h, v_w] = T.max(inverse_local[v_co, v_n * T.int64(196) + v_h // T.int64(2) * T.int64(14) + v_w // T.int64(2), v_h % T.int64(2), v_w % T.int64(2)] + p2[v_n, v_co, v_h, v_w] + p3[v_n, v_co, T.int64(0), T.int64(0)], T.float32(0))
b0 = sch.get_block(name="data_pack", func_name="main")
b1 = sch.get_block(name="bgemm", func_name="main")
b2 = sch.get_block(name="inverse", func_name="main")
b3 = sch.get_block(name="conv2d_winograd", func_name="main")
b4 = sch.get_block(name="T_add", func_name="main")
b5 = sch.get_block(name="T_add_1", func_name="main")
b6 = sch.get_block(name="T_relu", func_name="main")
b7 = sch.get_block(name="root", func_name="main")
b8, b9 = sch.get_producers(block=b2)
sch.compute_inline(block=b9)
b10, = sch.get_consumers(block=b2)
l11, l12, l13, l14 = sch.get_loops(block=b10)
l15, l16 = sch.split(loop=l13, factors=[None, 2], preserve_unit_iters=True)
l17, l18 = sch.split(loop=l14, factors=[None, 2], preserve_unit_iters=True)
sch.reorder(l15, l17, l16, l18)
sch.compute_at(block=b2, loop=l17, preserve_unit_loops=True, index=-1)
sch.set_scope(block=b2, buffer_index=0, storage_scope="local")
l19, l20, l21, l22, l23, l24, l25, l26, l27, l28 = sch.get_loops(block=b2)
sch.unroll(loop=l25)
sch.unroll(loop=l26)
sch.unroll(loop=l27)
sch.unroll(loop=l28)
b29, b30 = sch.get_producers(block=b0)
sch.compute_inline(block=b30)
b31, = sch.get_producers(block=b29)
l32, l33, l34, l35, l36, l37 = sch.get_loops(block=b0)
sch.reorder(l34, l35, l32, l33, l36, l37)
sch.unroll(loop=l32)
sch.unroll(loop=l33)
sch.unroll(loop=l36)
sch.unroll(loop=l37)
l38 = sch.fuse(l34, l35, preserve_unit_iters=True)
v39 = sch.sample_categorical(candidates=[32, 64, 128, 256, 512, 1024], probs=[0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666], decision=0)
l40, l41 = sch.split(loop=l38, factors=[None, v39], preserve_unit_iters=True)
sch.bind(loop=l40, thread_axis="blockIdx.x")
sch.bind(loop=l41, thread_axis="threadIdx.x")
b42 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b42, loop=l41, preserve_unit_loops=True, index=-1)
sch.compute_at(block=b29, loop=l41, preserve_unit_loops=True, index=-1)
sch.set_scope(block=b29, buffer_index=0, storage_scope="local")
sch.compute_inline(block=b31)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l43, l44, l45, l46, l47 = sch.get_loops(block=b1)
v48, v49, v50, v51, v52 = sch.sample_perfect_tile(loop=l43, n=5, max_innermost_factor=64, decision=[4, 1, 1, 1, 1])
l53, l54, l55, l56, l57 = sch.split(loop=l43, factors=[v48, v49, v50, v51, v52], preserve_unit_iters=True)
v58, v59, v60, v61, v62 = sch.sample_perfect_tile(loop=l44, n=5, max_innermost_factor=64, decision=[2, 1, 1, 1, 2])
l63, l64, l65, l66, l67 = sch.split(loop=l44, factors=[v58, v59, v60, v61, v62], preserve_unit_iters=True)
v68, v69, v70, v71, v72 = sch.sample_perfect_tile(loop=l45, n=5, max_innermost_factor=64, decision=[1, 2, 8, 2, 4])
l73, l74, l75, l76, l77 = sch.split(loop=l45, factors=[v68, v69, v70, v71, v72], preserve_unit_iters=True)
v78, v79, v80, v81, v82 = sch.sample_perfect_tile(loop=l46, n=5, max_innermost_factor=64, decision=[1, 28, 7, 1, 1])
l83, l84, l85, l86, l87 = sch.split(loop=l46, factors=[v78, v79, v80, v81, v82], preserve_unit_iters=True)
v88, v89, v90 = sch.sample_perfect_tile(loop=l47, n=3, max_innermost_factor=64, decision=[32, 4, 1])
l91, l92, l93 = sch.split(loop=l47, factors=[v88, v89, v90], preserve_unit_iters=True)
sch.reorder(l53, l63, l73, l83, l54, l64, l74, l84, l55, l65, l75, l85, l91, l92, l56, l66, l76, l86, l93, l57, l67, l77, l87)
l94 = sch.fuse(l53, l63, l73, l83, preserve_unit_iters=True)
sch.bind(loop=l94, thread_axis="blockIdx.x")
l95 = sch.fuse(l54, l64, l74, l84, preserve_unit_iters=True)
sch.bind(loop=l95, thread_axis="vthread.x")
l96 = sch.fuse(l55, l65, l75, l85, preserve_unit_iters=True)
sch.bind(loop=l96, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b97 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b97, loop=l96, preserve_unit_loops=True, index=-1)
b98 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b98, loop=l91, preserve_unit_loops=True, index=-1)
l99, l100, l101, l102, l103, l104, l105, l106 = sch.get_loops(block=b98)
l107 = sch.fuse(l103, l104, l105, l106, preserve_unit_iters=True)
v108 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b98, ann_key="meta_schedule.cooperative_fetch", ann_val=v108)
b109 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b109, loop=l91, preserve_unit_loops=True, index=-1)
l110, l111, l112, l113, l114, l115, l116, l117 = sch.get_loops(block=b109)
l118 = sch.fuse(l114, l115, l116, l117, preserve_unit_iters=True)
v119 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b109, ann_key="meta_schedule.cooperative_fetch", ann_val=v119)
l120 = sch.fuse(l91, preserve_unit_iters=True)
sch.annotate(block_or_loop=l120, ann_key="software_pipeline_stage", ann_val=[0, 0, 3])
sch.annotate(block_or_loop=l120, ann_key="software_pipeline_order", ann_val=[0, 1, 2])
sch.annotate(block_or_loop=l120, ann_key="software_pipeline_async_stages", ann_val=[0])
sch.reverse_compute_inline(block=b6)
sch.reverse_compute_inline(block=b5)
sch.reverse_compute_inline(block=b4)
v121 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b7, ann_key="meta_schedule.unroll_explicit", ann_val=v121)
l122, l123, l124, l125, l126, l127 = sch.get_loops(block=b3)
l128 = sch.fuse(l122, l123, l124, l125, preserve_unit_iters=True)
v129 = sch.sample_categorical(candidates=[32, 64, 128, 256, 512, 1024], probs=[0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666], decision=1)
l130, l131 = sch.split(loop=l128, factors=[None, v129], preserve_unit_iters=True)
sch.bind(loop=l130, thread_axis="blockIdx.x")
sch.bind(loop=l131, thread_axis="threadIdx.x")
sch.enter_postproc()
sch.unannotate(block_or_loop=b98, ann_key="meta_schedule.cooperative_fetch")
l132, l133, l134, l135, l136 = sch.get_loops(block=b98)
l137, l138, l139 = sch.split(loop=l136, factors=[None, 56, 2], preserve_unit_iters=True)
sch.vectorize(loop=l139)
sch.bind(loop=l138, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b109, ann_key="meta_schedule.cooperative_fetch")
l140, l141, l142, l143, l144 = sch.get_loops(block=b109)
l145, l146 = sch.split(loop=l144, factors=[None, 56], preserve_unit_iters=True)
sch.bind(loop=l146, thread_axis="threadIdx.x")
b147 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b147, ann_key="meta_schedule.unroll_explicit")
b148, b149, b150, b151, b152, b153, b154, b155, b156 = sch.get_child_blocks(b147)
l157, l158, l159, l160, l161, l162 = sch.get_loops(block=b148)
l163, l164, l165, l166, l167, l168 = sch.get_loops(block=b149)
sch.annotate(block_or_loop=l163, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l163, ann_key="pragma_unroll_explicit", ann_val=1)
l169, l170, l171, l172, l173, l174 = sch.get_loops(block=b150)
l175, l176, l177, l178, l179, l180, l181 = sch.get_loops(block=b151)
l182, l183, l184, l185, l186, l187 = sch.get_loops(block=b152)
l188, l189, l190, l191, l192, l193, l194, l195, l196, l197, l198, l199, l200, l201 = sch.get_loops(block=b153)
sch.annotate(block_or_loop=l188, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l188, ann_key="pragma_unroll_explicit", ann_val=1)
l202, l203, l204, l205, l206, l207, l208 = sch.get_loops(block=b154)
l209, l210, l211, l212, l213, l214, l215, l216 = sch.get_loops(block=b155)
sch.annotate(block_or_loop=l209, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l209, ann_key="pragma_unroll_explicit", ann_val=1)
l217, l218, l219, l220 = sch.get_loops(block=b156)
b221 = sch.get_block(name="data_pack", func_name="main")
l222, l223, l224, l225, l226, l227 = sch.get_loops(block=b221)
b228 = sch.decompose_reduction(block=b221, loop=l226)
b229 = sch.get_block(name="bgemm", func_name="main")
l230, l231, l232, l233, l234, l235, l236, l237, l238, l239, l240, l241, l242, l243 = sch.get_loops(block=b229)
b244 = sch.decompose_reduction(block=b229, loop=l233)
b245 = sch.get_block(name="inverse", func_name="main")
l246, l247, l248, l249, l250, l251, l252, l253 = sch.get_loops(block=b245)
b254 = sch.decompose_reduction(block=b245, loop=l252)
2023-11-11 00:25:35 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #15: GFLOPs: 63.0752. Time: 2015.7849 us. Best GFLOPs: 2749.6080
2023-11-11 00:25:35 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #16: GFLOPs: 2425.1840. Time: 52.4274 us. Best GFLOPs: 2749.6080
2023-11-11 00:25:35 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #17: GFLOPs: 322.1674. Time: 394.6581 us. Best GFLOPs: 2749.6080
2023-11-11 00:25:35 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #18: GFLOPs: 71.5784. Time: 1776.3166 us. Best GFLOPs: 2749.6080
2023-11-11 00:25:35 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #19: GFLOPs: 2205.2351. Time: 57.6564 us. Best GFLOPs: 2749.6080
2023-11-11 00:25:35 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #20: GFLOPs: 84.4607. Time: 1505.3870 us. Best GFLOPs: 2749.6080
2023-11-11 00:25:35 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #21: GFLOPs: 46.0284. Time: 2762.3369 us. Best GFLOPs: 2749.6080
2023-11-11 00:25:35 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #22: GFLOPs: 63.2835. Time: 2009.1495 us. Best GFLOPs: 2749.6080
2023-11-11 00:25:35 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #23: GFLOPs: 1298.9443. Time: 97.8841 us. Best GFLOPs: 2749.6080
2023-11-11 00:25:35 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #24: GFLOPs: 2300.9822. Time: 55.2573 us. Best GFLOPs: 2749.6080
2023-11-11 00:25:35 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #25: GFLOPs: 68.0058. Time: 1869.6344 us. Best GFLOPs: 2749.6080
2023-11-11 00:25:35 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #26: GFLOPs: 39.8351. Time: 3191.8080 us. Best GFLOPs: 2749.6080
2023-11-11 00:25:35 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #27: GFLOPs: 2465.4636. Time: 51.5708 us. Best GFLOPs: 2749.6080
2023-11-11 00:25:35 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #28: GFLOPs: 639.8760. Time: 198.7041 us. Best GFLOPs: 2749.6080
2023-11-11 00:25:35 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #29: GFLOPs: 12.2382. Time: 10389.2990 us. Best GFLOPs: 2749.6080
2023-11-11 00:25:35 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #30: GFLOPs: 2146.7557. Time: 59.2270 us. Best GFLOPs: 2749.6080
2023-11-11 00:25:35 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #31: GFLOPs: 1037.7271. Time: 122.5235 us. Best GFLOPs: 2749.6080
2023-11-11 00:25:35 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #32: GFLOPs: 12.2576. Time: 10372.8126 us. Best GFLOPs: 2749.6080
2023-11-11 00:25:35 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #33: GFLOPs: 59.7718. Time: 2127.1893 us. Best GFLOPs: 2749.6080
2023-11-11 00:25:35 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #34: GFLOPs: 188.0417. Time: 676.1583 us. Best GFLOPs: 2749.6080
2023-11-11 00:25:35 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #35: GFLOPs: 234.8888. Time: 541.3030 us. Best GFLOPs: 2749.6080
2023-11-11 00:25:35 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #36: GFLOPs: 95.3412. Time: 1333.5894 us. Best GFLOPs: 2749.6080
2023-11-11 00:25:35 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #37: GFLOPs: 12.2575. Time: 10372.9148 us. Best GFLOPs: 2749.6080
2023-11-11 00:25:35 [INFO] [task_scheduler.cc:121] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #38: Error in building:
LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(128), T.int64(28), T.int64(28)), "float32"), p1: T.Buffer((T.int64(4), T.int64(4), T.int64(128), T.int64(128)), "float32"), p2: T.Buffer((T.int64(1), T.int64(128), T.int64(28), T.int64(28)), "float32"), p3: T.Buffer((T.int64(1), T.int64(128), T.int64(1), T.int64(1)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(128), T.int64(28), T.int64(28)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        input_tile_local = T.alloc_buffer((T.int64(128), T.int64(196), T.int64(4), T.int64(4)), scope="local")
        data_pack = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(128), T.int64(196)))
        bgemm = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(128), T.int64(196)))
        inverse_local = T.alloc_buffer((T.int64(128), T.int64(196), T.int64(2), T.int64(2)), scope="local")
        data_pack_local = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(128), T.int64(196)), scope="local")
        bgemm_local = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(128), T.int64(196)), scope="local")
        data_pack_shared = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(128), T.int64(196)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(128), T.int64(128)), scope="shared")
        for ci_p_fused_0 in T.thread_binding(T.int64(784), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 1024, "pragma_unroll_explicit": 1}):
            for ci_p_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(1), T.int64(4), T.int64(4)):
                    with T.block("input_tile"):
                        v_ci = T.axis.spatial(T.int64(128), (ci_p_fused_0 * T.int64(32) + ci_p_fused_1) // T.int64(196) + ax0)
                        v_p = T.axis.spatial(T.int64(196), (ci_p_fused_0 * T.int64(32) + ci_p_fused_1) % T.int64(196) + ax1)
                        v_eps, v_nu = T.axis.remap("SS", [ax2, ax3])
                        T.reads(p0[v_p // T.int64(196), v_ci, v_p % T.int64(196) // T.int64(14) * T.int64(2) + v_eps - T.int64(1), v_p % T.int64(14) * T.int64(2) + v_nu - T.int64(1)])
                        T.writes(input_tile_local[v_ci, v_p, v_eps, v_nu])
                        T.block_attr({"schedule_rule": "None"})
                        input_tile_local[v_ci, v_p, v_eps, v_nu] = T.if_then_else(T.int64(1) <= v_p % T.int64(196) // T.int64(14) * T.int64(2) + v_eps and v_p % T.int64(196) // T.int64(14) * T.int64(2) + v_eps < T.int64(29) and T.int64(1) <= v_p % T.int64(14) * T.int64(2) + v_nu and v_p % T.int64(14) * T.int64(2) + v_nu < T.int64(29), p0[v_p // T.int64(196), v_ci, v_p % T.int64(196) // T.int64(14) * T.int64(2) + v_eps - T.int64(1), v_p % T.int64(14) * T.int64(2) + v_nu - T.int64(1)], T.float32(0))
                for eps in T.unroll(T.int64(4)):
                    for nu in T.unroll(T.int64(4)):
                        with T.block("data_pack_init"):
                            v_eps, v_nu = T.axis.remap("SS", [eps, nu])
                            v_ci = T.axis.spatial(T.int64(128), (ci_p_fused_0 * T.int64(32) + ci_p_fused_1) // T.int64(196))
                            v_p = T.axis.spatial(T.int64(196), (ci_p_fused_0 * T.int64(32) + ci_p_fused_1) % T.int64(196))
                            T.reads()
                            T.writes(data_pack_local[v_eps, v_nu, v_ci, v_p])
                            T.block_attr({"schedule_rule": "conv2d_nchw_winograd_data_pack"})
                            data_pack_local[v_eps, v_nu, v_ci, v_p] = T.float32(0)
                        for r_a in T.unroll(T.int64(4)):
                            for r_b in T.unroll(T.int64(4)):
                                with T.block("data_pack_update"):
                                    v_eps, v_nu = T.axis.remap("SS", [eps, nu])
                                    v_ci = T.axis.spatial(T.int64(128), (ci_p_fused_0 * T.int64(32) + ci_p_fused_1) // T.int64(196))
                                    v_p = T.axis.spatial(T.int64(196), (ci_p_fused_0 * T.int64(32) + ci_p_fused_1) % T.int64(196))
                                    v_r_a, v_r_b = T.axis.remap("RR", [r_a, r_b])
                                    T.reads(data_pack_local[v_eps, v_nu, v_ci, v_p], input_tile_local[v_ci, v_p, v_r_a, v_r_b])
                                    T.writes(data_pack_local[v_eps, v_nu, v_ci, v_p])
                                    T.block_attr({"schedule_rule": "conv2d_nchw_winograd_data_pack"})
                                    data_pack_local[v_eps, v_nu, v_ci, v_p] = data_pack_local[v_eps, v_nu, v_ci, v_p] + input_tile_local[v_ci, v_p, v_r_a, v_r_b] * T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(3), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(1), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(0), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(3), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(1), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(0), T.float32(1), T.float32(0))))))))))))))))) * T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(3), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(1), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(0), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(3), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(1), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(0), T.float32(1), T.float32(0)))))))))))))))))
                for ax0, ax1, ax2, ax3 in T.grid(T.int64(4), T.int64(4), T.int64(1), T.int64(1)):
                    with T.block("data_pack_local"):
                        v0, v1 = T.axis.remap("SS", [ax0, ax1])
                        v2 = T.axis.spatial(T.int64(128), (ci_p_fused_0 * T.int64(32) + ci_p_fused_1) // T.int64(196) + ax2)
                        v3 = T.axis.spatial(T.int64(196), (ci_p_fused_0 * T.int64(32) + ci_p_fused_1) % T.int64(196) + ax3)
                        T.reads(data_pack_local[v0, v1, v2, v3])
                        T.writes(data_pack[v0, v1, v2, v3])
                        data_pack[v0, v1, v2, v3] = data_pack_local[v0, v1, v2, v3]
        for eps_0_nu_0_co_0_p_0_fused in T.thread_binding(T.int64(2), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 1024, "pragma_unroll_explicit": 1}):
            for eps_1_nu_1_co_1_p_1_fused in T.thread_binding(T.int64(49), thread="vthread.x"):
                for eps_2_nu_2_co_2_p_2_fused in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                    for eps_3_init, nu_3_init, co_3_init, p_3_init, eps_4_init, nu_4_init, co_4_init, p_4_init in T.grid(T.int64(1), T.int64(1), T.int64(4), T.int64(2), T.int64(2), T.int64(2), T.int64(1), T.int64(1)):
                        with T.block("bgemm_init"):
                            v_eps = T.axis.spatial(T.int64(4), eps_2_nu_2_co_2_p_2_fused // T.int64(64) * T.int64(2) + eps_3_init * T.int64(2) + eps_4_init)
                            v_nu = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused * T.int64(2) + nu_3_init * T.int64(2) + nu_4_init)
                            v_co = T.axis.spatial(T.int64(128), eps_2_nu_2_co_2_p_2_fused % T.int64(64) // T.int64(2) * T.int64(4) + co_3_init + co_4_init)
                            v_p = T.axis.spatial(T.int64(196), eps_1_nu_1_co_1_p_1_fused * T.int64(4) + eps_2_nu_2_co_2_p_2_fused % T.int64(2) * T.int64(2) + p_3_init + p_4_init)
                            T.reads()
                            T.writes(bgemm_local[v_eps, v_nu, v_co, v_p])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            bgemm_local[v_eps, v_nu, v_co, v_p] = T.float32(0)
                    for ci_0_fused in T.serial(T.int64(128), annotations={"software_pipeline_async_stages": [0], "software_pipeline_order": [0, 1, 2], "software_pipeline_stage": [0, 0, 3]}):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(13)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                                with T.block("data_pack_shared"):
                                    v0 = T.axis.spatial(T.int64(4), (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1) // T.int64(392))
                                    v1 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused * T.int64(2) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1) % T.int64(392) // T.int64(196))
                                    v2 = T.axis.spatial(T.int64(128), ci_0_fused)
                                    v3 = T.axis.spatial(T.int64(196), (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1) % T.int64(196))
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1 < T.int64(1568))
                                    T.reads(data_pack[v0, v1, v2, v3])
                                    T.writes(data_pack_shared[v0, v1, v2, v3])
                                    data_pack_shared[v0, v1, v2, v3] = data_pack[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(8)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                                with T.block("p1_shared"):
                                    v0 = T.axis.spatial(T.int64(4), (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1) // T.int64(256))
                                    v1 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused * T.int64(2) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1) % T.int64(256) // T.int64(128))
                                    v2 = T.axis.spatial(T.int64(128), ci_0_fused)
                                    v3 = T.axis.spatial(T.int64(128), (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1) % T.int64(128))
                                    T.reads(p1[v0, v1, v2, v3])
                                    T.writes(p1_shared[v0, v1, v2, v3])
                                    p1_shared[v0, v1, v2, v3] = p1[v0, v1, v2, v3]
                        for ci_1, eps_3, nu_3, co_3, p_3, ci_2, eps_4, nu_4, co_4, p_4 in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(4), T.int64(2), T.int64(1), T.int64(2), T.int64(2), T.int64(1), T.int64(1)):
                            with T.block("bgemm_update"):
                                v_eps = T.axis.spatial(T.int64(4), eps_2_nu_2_co_2_p_2_fused // T.int64(64) * T.int64(2) + eps_3 * T.int64(2) + eps_4)
                                v_nu = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused * T.int64(2) + nu_3 * T.int64(2) + nu_4)
                                v_co = T.axis.spatial(T.int64(128), eps_2_nu_2_co_2_p_2_fused % T.int64(64) // T.int64(2) * T.int64(4) + co_3 + co_4)
                                v_p = T.axis.spatial(T.int64(196), eps_1_nu_1_co_1_p_1_fused * T.int64(4) + eps_2_nu_2_co_2_p_2_fused % T.int64(2) * T.int64(2) + p_3 + p_4)
                                v_ci = T.axis.reduce(T.int64(128), ci_0_fused + ci_1 + ci_2)
                                T.reads(bgemm_local[v_eps, v_nu, v_co, v_p], data_pack_shared[v_eps, v_nu, v_ci, v_p], p1_shared[v_eps, v_nu, v_ci, v_co])
                                T.writes(bgemm_local[v_eps, v_nu, v_co, v_p])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                bgemm_local[v_eps, v_nu, v_co, v_p] = bgemm_local[v_eps, v_nu, v_co, v_p] + data_pack_shared[v_eps, v_nu, v_ci, v_p] * p1_shared[v_eps, v_nu, v_ci, v_co]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(2), T.int64(2), T.int64(4), T.int64(2)):
                        with T.block("bgemm_local"):
                            v0 = T.axis.spatial(T.int64(4), eps_2_nu_2_co_2_p_2_fused // T.int64(64) * T.int64(2) + ax0)
                            v1 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused * T.int64(2) + ax1)
                            v2 = T.axis.spatial(T.int64(128), eps_2_nu_2_co_2_p_2_fused % T.int64(64) // T.int64(2) * T.int64(4) + ax2)
                            v3 = T.axis.spatial(T.int64(196), eps_1_nu_1_co_1_p_1_fused * T.int64(4) + eps_2_nu_2_co_2_p_2_fused % T.int64(2) * T.int64(2) + ax3)
                            T.reads(bgemm_local[v0, v1, v2, v3])
                            T.writes(bgemm[v0, v1, v2, v3])
                            bgemm[v0, v1, v2, v3] = bgemm_local[v0, v1, v2, v3]
        for n_co_h_0_w_0_fused_0 in T.thread_binding(T.int64(784), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 1024, "pragma_unroll_explicit": 1}):
            for n_co_h_0_w_0_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                    for ax2 in T.unroll(T.int64(2)):
                        for ax3 in T.unroll(T.int64(2)):
                            with T.block("inverse_init"):
                                v_co = T.axis.spatial(T.int64(128), (n_co_h_0_w_0_fused_0 * T.int64(32) + n_co_h_0_w_0_fused_1) // T.int64(196) + ax0)
                                v_p = T.axis.spatial(T.int64(196), (n_co_h_0_w_0_fused_0 * T.int64(32) + n_co_h_0_w_0_fused_1) % T.int64(196) + ax1)
                                v_vh, v_vw = T.axis.remap("SS", [ax2, ax3])
                                T.reads()
                                T.writes(inverse_local[v_co, v_p, v_vh, v_vw])
                                T.block_attr({"schedule_rule": "conv2d_nchw_winograd_inverse"})
                                inverse_local[v_co, v_p, v_vh, v_vw] = T.float32(0)
                            for ax4 in T.unroll(T.int64(4)):
                                for ax5 in T.unroll(T.int64(4)):
                                    with T.block("inverse_update"):
                                        v_co = T.axis.spatial(T.int64(128), (n_co_h_0_w_0_fused_0 * T.int64(32) + n_co_h_0_w_0_fused_1) // T.int64(196) + ax0)
                                        v_p = T.axis.spatial(T.int64(196), (n_co_h_0_w_0_fused_0 * T.int64(32) + n_co_h_0_w_0_fused_1) % T.int64(196) + ax1)
                                        v_vh, v_vw, v_r_a, v_r_b = T.axis.remap("SSRR", [ax2, ax3, ax4, ax5])
                                        T.reads(inverse_local[v_co, v_p, v_vh, v_vw], bgemm[v_r_a, v_r_b, v_co, v_p])
                                        T.writes(inverse_local[v_co, v_p, v_vh, v_vw])
                                        T.block_attr({"schedule_rule": "conv2d_nchw_winograd_inverse"})
                                        inverse_local[v_co, v_p, v_vh, v_vw] = inverse_local[v_co, v_p, v_vh, v_vw] + bgemm[v_r_a, v_r_b, v_co, v_p] * T.Select(v_r_a % T.int64(4) == T.int64(3) and v_vh % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_vh % T.int64(2) == T.int64(0), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_vh % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_vh % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_vh % T.int64(2) == T.int64(1), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_vh % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_vh % T.int64(2) == T.int64(1), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_vh % T.int64(2) == T.int64(0), T.float32(1), T.float32(0))))))))) * T.Select(v_r_b % T.int64(4) == T.int64(3) and v_vw % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_vw % T.int64(2) == T.int64(0), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_vw % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_vw % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_vw % T.int64(2) == T.int64(1), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_vw % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_vw % T.int64(2) == T.int64(1), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_vw % T.int64(2) == T.int64(0), T.float32(1), T.float32(0)))))))))
                for h_1, w_1 in T.grid(T.int64(2), T.int64(2)):
                    with T.block("conv2d_winograd"):
                        v_n = T.axis.spatial(T.int64(1), T.int64(0))
                        v_co = T.axis.spatial(T.int64(128), (n_co_h_0_w_0_fused_0 * T.int64(32) + n_co_h_0_w_0_fused_1) // T.int64(196))
                        v_h = T.axis.spatial(T.int64(28), (n_co_h_0_w_0_fused_0 * T.int64(32) + n_co_h_0_w_0_fused_1) % T.int64(196) // T.int64(14) * T.int64(2) + h_1)
                        v_w = T.axis.spatial(T.int64(28), (n_co_h_0_w_0_fused_0 * T.int64(32) + n_co_h_0_w_0_fused_1) % T.int64(14) * T.int64(2) + w_1)
                        T.reads(inverse_local[v_co, v_n * T.int64(196) + v_h // T.int64(2) * T.int64(14) + v_w // T.int64(2), v_h % T.int64(2), v_w % T.int64(2)], p2[v_n, v_co, v_h, v_w], p3[v_n, v_co, T.int64(0), T.int64(0)])
                        T.writes(T_relu[v_n, v_co, v_h, v_w])
                        T_relu[v_n, v_co, v_h, v_w] = T.max(inverse_local[v_co, v_n * T.int64(196) + v_h // T.int64(2) * T.int64(14) + v_w // T.int64(2), v_h % T.int64(2), v_w % T.int64(2)] + p2[v_n, v_co, v_h, v_w] + p3[v_n, v_co, T.int64(0), T.int64(0)], T.float32(0))
b0 = sch.get_block(name="data_pack", func_name="main")
b1 = sch.get_block(name="bgemm", func_name="main")
b2 = sch.get_block(name="inverse", func_name="main")
b3 = sch.get_block(name="conv2d_winograd", func_name="main")
b4 = sch.get_block(name="T_add", func_name="main")
b5 = sch.get_block(name="T_add_1", func_name="main")
b6 = sch.get_block(name="T_relu", func_name="main")
b7 = sch.get_block(name="root", func_name="main")
b8, b9 = sch.get_producers(block=b2)
sch.compute_inline(block=b9)
b10, = sch.get_consumers(block=b2)
l11, l12, l13, l14 = sch.get_loops(block=b10)
l15, l16 = sch.split(loop=l13, factors=[None, 2], preserve_unit_iters=True)
l17, l18 = sch.split(loop=l14, factors=[None, 2], preserve_unit_iters=True)
sch.reorder(l15, l17, l16, l18)
sch.compute_at(block=b2, loop=l17, preserve_unit_loops=True, index=-1)
sch.set_scope(block=b2, buffer_index=0, storage_scope="local")
l19, l20, l21, l22, l23, l24, l25, l26, l27, l28 = sch.get_loops(block=b2)
sch.unroll(loop=l25)
sch.unroll(loop=l26)
sch.unroll(loop=l27)
sch.unroll(loop=l28)
b29, b30 = sch.get_producers(block=b0)
sch.compute_inline(block=b30)
b31, = sch.get_producers(block=b29)
l32, l33, l34, l35, l36, l37 = sch.get_loops(block=b0)
sch.reorder(l34, l35, l32, l33, l36, l37)
sch.unroll(loop=l32)
sch.unroll(loop=l33)
sch.unroll(loop=l36)
sch.unroll(loop=l37)
l38 = sch.fuse(l34, l35, preserve_unit_iters=True)
v39 = sch.sample_categorical(candidates=[32, 64, 128, 256, 512, 1024], probs=[0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666], decision=0)
l40, l41 = sch.split(loop=l38, factors=[None, v39], preserve_unit_iters=True)
sch.bind(loop=l40, thread_axis="blockIdx.x")
sch.bind(loop=l41, thread_axis="threadIdx.x")
b42 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b42, loop=l41, preserve_unit_loops=True, index=-1)
sch.compute_at(block=b29, loop=l41, preserve_unit_loops=True, index=-1)
sch.set_scope(block=b29, buffer_index=0, storage_scope="local")
sch.compute_inline(block=b31)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l43, l44, l45, l46, l47 = sch.get_loops(block=b1)
v48, v49, v50, v51, v52 = sch.sample_perfect_tile(loop=l43, n=5, max_innermost_factor=64, decision=[1, 1, 2, 1, 2])
l53, l54, l55, l56, l57 = sch.split(loop=l43, factors=[v48, v49, v50, v51, v52], preserve_unit_iters=True)
v58, v59, v60, v61, v62 = sch.sample_perfect_tile(loop=l44, n=5, max_innermost_factor=64, decision=[2, 1, 1, 1, 2])
l63, l64, l65, l66, l67 = sch.split(loop=l44, factors=[v58, v59, v60, v61, v62], preserve_unit_iters=True)
v68, v69, v70, v71, v72 = sch.sample_perfect_tile(loop=l45, n=5, max_innermost_factor=64, decision=[1, 1, 32, 4, 1])
l73, l74, l75, l76, l77 = sch.split(loop=l45, factors=[v68, v69, v70, v71, v72], preserve_unit_iters=True)
v78, v79, v80, v81, v82 = sch.sample_perfect_tile(loop=l46, n=5, max_innermost_factor=64, decision=[1, 49, 2, 2, 1])
l83, l84, l85, l86, l87 = sch.split(loop=l46, factors=[v78, v79, v80, v81, v82], preserve_unit_iters=True)
v88, v89, v90 = sch.sample_perfect_tile(loop=l47, n=3, max_innermost_factor=64, decision=[128, 1, 1])
l91, l92, l93 = sch.split(loop=l47, factors=[v88, v89, v90], preserve_unit_iters=True)
sch.reorder(l53, l63, l73, l83, l54, l64, l74, l84, l55, l65, l75, l85, l91, l92, l56, l66, l76, l86, l93, l57, l67, l77, l87)
l94 = sch.fuse(l53, l63, l73, l83, preserve_unit_iters=True)
sch.bind(loop=l94, thread_axis="blockIdx.x")
l95 = sch.fuse(l54, l64, l74, l84, preserve_unit_iters=True)
sch.bind(loop=l95, thread_axis="vthread.x")
l96 = sch.fuse(l55, l65, l75, l85, preserve_unit_iters=True)
sch.bind(loop=l96, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b97 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b97, loop=l96, preserve_unit_loops=True, index=-1)
b98 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b98, loop=l91, preserve_unit_loops=True, index=-1)
l99, l100, l101, l102, l103, l104, l105, l106 = sch.get_loops(block=b98)
l107 = sch.fuse(l103, l104, l105, l106, preserve_unit_iters=True)
v108 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b98, ann_key="meta_schedule.cooperative_fetch", ann_val=v108)
b109 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b109, loop=l91, preserve_unit_loops=True, index=-1)
l110, l111, l112, l113, l114, l115, l116, l117 = sch.get_loops(block=b109)
l118 = sch.fuse(l114, l115, l116, l117, preserve_unit_iters=True)
v119 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b109, ann_key="meta_schedule.cooperative_fetch", ann_val=v119)
l120 = sch.fuse(l91, preserve_unit_iters=True)
sch.annotate(block_or_loop=l120, ann_key="software_pipeline_stage", ann_val=[0, 0, 3])
sch.annotate(block_or_loop=l120, ann_key="software_pipeline_order", ann_val=[0, 1, 2])
sch.annotate(block_or_loop=l120, ann_key="software_pipeline_async_stages", ann_val=[0])
sch.reverse_compute_inline(block=b6)
sch.reverse_compute_inline(block=b5)
sch.reverse_compute_inline(block=b4)
v121 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b7, ann_key="meta_schedule.unroll_explicit", ann_val=v121)
l122, l123, l124, l125, l126, l127 = sch.get_loops(block=b3)
l128 = sch.fuse(l122, l123, l124, l125, preserve_unit_iters=True)
v129 = sch.sample_categorical(candidates=[32, 64, 128, 256, 512, 1024], probs=[0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666], decision=0)
l130, l131 = sch.split(loop=l128, factors=[None, v129], preserve_unit_iters=True)
sch.bind(loop=l130, thread_axis="blockIdx.x")
sch.bind(loop=l131, thread_axis="threadIdx.x")
sch.enter_postproc()
sch.unannotate(block_or_loop=b98, ann_key="meta_schedule.cooperative_fetch")
l132, l133, l134, l135, l136 = sch.get_loops(block=b98)
l137, l138 = sch.split(loop=l136, factors=[None, 128], preserve_unit_iters=True)
sch.bind(loop=l138, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b109, ann_key="meta_schedule.cooperative_fetch")
l139, l140, l141, l142, l143 = sch.get_loops(block=b109)
l144, l145 = sch.split(loop=l143, factors=[None, 128], preserve_unit_iters=True)
sch.bind(loop=l145, thread_axis="threadIdx.x")
b146 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b146, ann_key="meta_schedule.unroll_explicit")
b147, b148, b149, b150, b151, b152, b153, b154, b155 = sch.get_child_blocks(b146)
l156, l157, l158, l159, l160, l161 = sch.get_loops(block=b147)
l162, l163, l164, l165, l166, l167 = sch.get_loops(block=b148)
sch.annotate(block_or_loop=l162, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l162, ann_key="pragma_unroll_explicit", ann_val=1)
l168, l169, l170, l171, l172, l173 = sch.get_loops(block=b149)
l174, l175, l176, l177, l178, l179 = sch.get_loops(block=b150)
l180, l181, l182, l183, l184, l185 = sch.get_loops(block=b151)
l186, l187, l188, l189, l190, l191, l192, l193, l194, l195, l196, l197, l198, l199 = sch.get_loops(block=b152)
sch.annotate(block_or_loop=l186, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l186, ann_key="pragma_unroll_explicit", ann_val=1)
l200, l201, l202, l203, l204, l205, l206 = sch.get_loops(block=b153)
l207, l208, l209, l210, l211, l212, l213, l214 = sch.get_loops(block=b154)
sch.annotate(block_or_loop=l207, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l207, ann_key="pragma_unroll_explicit", ann_val=1)
l215, l216, l217, l218 = sch.get_loops(block=b155)
b219 = sch.get_block(name="data_pack", func_name="main")
l220, l221, l222, l223, l224, l225 = sch.get_loops(block=b219)
b226 = sch.decompose_reduction(block=b219, loop=l224)
b227 = sch.get_block(name="bgemm", func_name="main")
l228, l229, l230, l231, l232, l233, l234, l235, l236, l237, l238, l239, l240, l241 = sch.get_loops(block=b227)
b242 = sch.decompose_reduction(block=b227, loop=l231)
b243 = sch.get_block(name="inverse", func_name="main")
l244, l245, l246, l247, l248, l249, l250, l251 = sch.get_loops(block=b243)
b252 = sch.decompose_reduction(block=b243, loop=l250)
2023-11-11 00:25:35 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #39: GFLOPs: 2523.9078. Time: 50.3766 us. Best GFLOPs: 2749.6080
2023-11-11 00:25:35 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #40: GFLOPs: 52.8832. Time: 2404.2789 us. Best GFLOPs: 2749.6080
2023-11-11 00:25:35 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #41: GFLOPs: 434.5909. Time: 292.5648 us. Best GFLOPs: 2749.6080
2023-11-11 00:25:35 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #42: GFLOPs: 2696.7673. Time: 47.1476 us. Best GFLOPs: 2749.6080
2023-11-11 00:25:35 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #43: GFLOPs: 1352.5866. Time: 94.0021 us. Best GFLOPs: 2749.6080
2023-11-11 00:25:35 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #44: GFLOPs: 1718.2418. Time: 73.9977 us. Best GFLOPs: 2749.6080
2023-11-11 00:25:35 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #45: GFLOPs: 170.8968. Time: 743.9929 us. Best GFLOPs: 2749.6080
2023-11-11 00:25:35 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #46: GFLOPs: 1723.2998. Time: 73.7805 us. Best GFLOPs: 2749.6080
2023-11-11 00:25:35 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #47: GFLOPs: 71.2188. Time: 1785.2871 us. Best GFLOPs: 2749.6080
2023-11-11 00:25:35 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #48: GFLOPs: 73.5280. Time: 1729.2182 us. Best GFLOPs: 2749.6080
2023-11-11 00:25:35 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #49: GFLOPs: 1878.8414. Time: 67.6725 us. Best GFLOPs: 2749.6080
2023-11-11 00:25:35 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #50: GFLOPs: 588.0374. Time: 216.2209 us. Best GFLOPs: 2749.6080
2023-11-11 00:25:35 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #51: GFLOPs: 834.8705. Time: 152.2943 us. Best GFLOPs: 2749.6080
2023-11-11 00:25:35 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #52: GFLOPs: 32.6968. Time: 3888.6399 us. Best GFLOPs: 2749.6080
2023-11-11 00:25:35 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #53: GFLOPs: 1503.5970. Time: 84.5612 us. Best GFLOPs: 2749.6080
2023-11-11 00:25:35 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #54: GFLOPs: 2979.9968. Time: 42.6665 us. Best GFLOPs: 2979.9968
2023-11-11 00:25:35 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #55: GFLOPs: 134.8739. Time: 942.7028 us. Best GFLOPs: 2979.9968
2023-11-11 00:25:35 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #56: GFLOPs: 386.9108. Time: 328.6183 us. Best GFLOPs: 2979.9968
2023-11-11 00:25:35 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #57: GFLOPs: 37.6820. Time: 3374.1824 us. Best GFLOPs: 2979.9968
2023-11-11 00:25:35 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #58: GFLOPs: 287.9619. Time: 441.5376 us. Best GFLOPs: 2979.9968
2023-11-11 00:25:35 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #59: GFLOPs: 893.4064. Time: 142.3160 us. Best GFLOPs: 2979.9968
2023-11-11 00:25:35 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #60: GFLOPs: 1405.6589. Time: 90.4529 us. Best GFLOPs: 2979.9968
2023-11-11 00:25:35 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #61: GFLOPs: 3218.4128. Time: 39.5058 us. Best GFLOPs: 3218.4128
2023-11-11 00:25:35 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #62: GFLOPs: 634.2038. Time: 200.4813 us. Best GFLOPs: 3218.4128
2023-11-11 00:25:35 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #63: GFLOPs: 1812.6248. Time: 70.1447 us. Best GFLOPs: 3218.4128
2023-11-11 00:25:35 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #64: GFLOPs: 66.6977. Time: 1906.3015 us. Best GFLOPs: 3218.4128
2023-11-11 00:58:55 [INFO] [evolutionary_search.cc:713] Generating candidates......
2023-11-11 00:58:57 [INFO] [evolutionary_search.cc:715] Picked top 62 candidate(s) from database
2023-11-11 00:59:04 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310fb046d8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310f554558)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310dc5ddb8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310bbcd2d8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56310b1dbf38)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310dc5d048)]: 434 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310dc26e38)]: 0 failure(s)
2023-11-11 00:59:11 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310fb046d8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310f554558)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310dc5ddb8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310bbcd2d8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56310b1dbf38)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310dc5d048)]: 868 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310dc26e38)]: 0 failure(s)
2023-11-11 00:59:19 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310fb046d8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310f554558)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310dc5ddb8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310bbcd2d8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56310b1dbf38)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310dc5d048)]: 1296 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310dc26e38)]: 0 failure(s)
2023-11-11 00:59:19 [INFO] [evolutionary_search.cc:723] Sampled 54 candidate(s)
2023-11-11 00:59:38 [INFO] [evolutionary_search.cc:621] Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310fb046d8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310f554558)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310dc5ddb8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310bbcd2d8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56310b1dbf38)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310dc5d048)]: 126 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310dc26e38)]: 0 failure(s)
2023-11-11 01:00:00 [INFO] [evolutionary_search.cc:621] Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310fb046d8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310f554558)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310dc5ddb8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310bbcd2d8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56310b1dbf38)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310dc5d048)]: 98 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310dc26e38)]: 0 failure(s)
2023-11-11 01:00:22 [INFO] [evolutionary_search.cc:621] Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310fb046d8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310f554558)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310dc5ddb8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310bbcd2d8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56310b1dbf38)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310dc5d048)]: 87 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310dc26e38)]: 0 failure(s)
2023-11-11 01:00:46 [INFO] [evolutionary_search.cc:621] Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310fb046d8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310f554558)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310dc5ddb8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310bbcd2d8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56310b1dbf38)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310dc5d048)]: 104 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310dc26e38)]: 0 failure(s)
2023-11-11 01:00:53 [INFO] [evolutionary_search.cc:649] Scores of the best 64 candidates:
[1 : 16]:	1.4333  1.3653  1.3564  1.3528  1.3528  1.3475  1.3097  1.3043  1.2791  1.1281  1.1242  1.1192  1.1091  1.0346  1.0293  1.0104
[17 : 32]:	1.0076  0.9993  0.9910  0.9857  0.9794  0.9616  0.9608  0.9605  0.9581  0.9576  0.9555  0.9533  0.9489  0.9463  0.9446  0.9441
[33 : 48]:	0.9427  0.9425  0.9419  0.9401  0.9392  0.9364  0.9361  0.9338  0.9319  0.9292  0.9255  0.9247  0.9243  0.9231  0.9225  0.9211
[49 : 64]:	0.9207  0.9205  0.9204  0.9196  0.9194  0.9176  0.9166  0.9165  0.9161  0.9149  0.9132  0.9117  0.9111  0.9089  0.9087  0.9072
2023-11-11 01:00:53 [INFO] [evolutionary_search.cc:727] Got 64 candidate(s) with evolutionary search
2023-11-11 01:00:53 [INFO] [evolutionary_search.cc:730] Sending 64 candidates(s) for measurement
2023-11-11 01:01:40 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #65: GFLOPs: 3519.9723. Time: 36.1213 us. Best GFLOPs: 3519.9723
2023-11-11 01:01:40 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #66: GFLOPs: 2523.5271. Time: 50.3842 us. Best GFLOPs: 3519.9723
2023-11-11 01:01:40 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #67: GFLOPs: 2877.5001. Time: 44.1863 us. Best GFLOPs: 3519.9723
2023-11-11 01:01:40 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #68: GFLOPs: 3565.5653. Time: 35.6594 us. Best GFLOPs: 3565.5653
2023-11-11 01:01:40 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #69: GFLOPs: 3565.1454. Time: 35.6636 us. Best GFLOPs: 3565.5653
2023-11-11 01:01:40 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #70: GFLOPs: 4339.3315. Time: 29.3008 us. Best GFLOPs: 4339.3315
2023-11-11 01:01:40 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #71: GFLOPs: 2655.2231. Time: 47.8852 us. Best GFLOPs: 4339.3315
2023-11-11 01:01:40 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #72: GFLOPs: 2551.9921. Time: 49.8222 us. Best GFLOPs: 4339.3315
2023-11-11 01:01:40 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #73: GFLOPs: 2145.6122. Time: 59.2586 us. Best GFLOPs: 4339.3315
2023-11-11 01:01:40 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #74: GFLOPs: 110.7968. Time: 1147.5602 us. Best GFLOPs: 4339.3315
2023-11-11 01:01:40 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #75: GFLOPs: 2576.1488. Time: 49.3551 us. Best GFLOPs: 4339.3315
2023-11-11 01:01:40 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #76: GFLOPs: 277.6140. Time: 457.9956 us. Best GFLOPs: 4339.3315
2023-11-11 01:01:40 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #77: GFLOPs: 110.5245. Time: 1150.3875 us. Best GFLOPs: 4339.3315
2023-11-11 01:01:40 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #78: GFLOPs: 2643.2749. Time: 48.1017 us. Best GFLOPs: 4339.3315
2023-11-11 01:01:40 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #79: GFLOPs: 970.8345. Time: 130.9657 us. Best GFLOPs: 4339.3315
2023-11-11 01:01:40 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #80: GFLOPs: 1080.1813. Time: 117.7080 us. Best GFLOPs: 4339.3315
2023-11-11 01:01:40 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #81: GFLOPs: 1925.9909. Time: 66.0159 us. Best GFLOPs: 4339.3315
2023-11-11 01:01:40 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #82: GFLOPs: 1908.9275. Time: 66.6060 us. Best GFLOPs: 4339.3315
2023-11-11 01:01:40 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #83: GFLOPs: 1077.7527. Time: 117.9732 us. Best GFLOPs: 4339.3315
2023-11-11 01:01:40 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #84: GFLOPs: 2822.5108. Time: 45.0471 us. Best GFLOPs: 4339.3315
2023-11-11 01:01:40 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #85: GFLOPs: 587.7718. Time: 216.3186 us. Best GFLOPs: 4339.3315
2023-11-11 01:01:40 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #86: GFLOPs: 47.5061. Time: 2676.4127 us. Best GFLOPs: 4339.3315
2023-11-11 01:01:40 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #87: GFLOPs: 342.7845. Time: 370.9211 us. Best GFLOPs: 4339.3315
2023-11-11 01:01:40 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #88: GFLOPs: 267.7413. Time: 474.8836 us. Best GFLOPs: 4339.3315
2023-11-11 01:01:40 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #89: GFLOPs: 3161.9333. Time: 40.2115 us. Best GFLOPs: 4339.3315
2023-11-11 01:01:40 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #90: GFLOPs: 3189.0004. Time: 39.8702 us. Best GFLOPs: 4339.3315
2023-11-11 01:01:40 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #91: GFLOPs: 1791.6839. Time: 70.9645 us. Best GFLOPs: 4339.3315
2023-11-11 01:01:40 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #92: GFLOPs: 2921.1255. Time: 43.5264 us. Best GFLOPs: 4339.3315
2023-11-11 01:01:40 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #93: GFLOPs: 3605.5359. Time: 35.2641 us. Best GFLOPs: 4339.3315
2023-11-11 01:01:40 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #94: GFLOPs: 1801.1801. Time: 70.5904 us. Best GFLOPs: 4339.3315
2023-11-11 01:01:40 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #95: GFLOPs: 2867.2303. Time: 44.3445 us. Best GFLOPs: 4339.3315
2023-11-11 01:01:40 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #96: GFLOPs: 3311.7796. Time: 38.3920 us. Best GFLOPs: 4339.3315
2023-11-11 01:01:40 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #97: GFLOPs: 1789.4775. Time: 71.0520 us. Best GFLOPs: 4339.3315
2023-11-11 01:01:40 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #98: GFLOPs: 2448.5647. Time: 51.9267 us. Best GFLOPs: 4339.3315
2023-11-11 01:01:40 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #99: GFLOPs: 2233.6766. Time: 56.9223 us. Best GFLOPs: 4339.3315
2023-11-11 01:01:40 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #100: GFLOPs: 2838.0336. Time: 44.8007 us. Best GFLOPs: 4339.3315
2023-11-11 01:01:40 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #101: GFLOPs: 2701.1124. Time: 47.0717 us. Best GFLOPs: 4339.3315
2023-11-11 01:01:40 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #102: GFLOPs: 45.1791. Time: 2814.2649 us. Best GFLOPs: 4339.3315
2023-11-11 01:01:40 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #103: GFLOPs: 2995.3922. Time: 42.4472 us. Best GFLOPs: 4339.3315
2023-11-11 01:01:40 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #104: GFLOPs: 2981.8523. Time: 42.6399 us. Best GFLOPs: 4339.3315
2023-11-11 01:01:40 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #105: GFLOPs: 3185.9538. Time: 39.9083 us. Best GFLOPs: 4339.3315
2023-11-11 01:01:40 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #106: GFLOPs: 3228.4551. Time: 39.3829 us. Best GFLOPs: 4339.3315
2023-11-11 01:01:40 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #107: GFLOPs: 2780.4156. Time: 45.7291 us. Best GFLOPs: 4339.3315
2023-11-11 01:01:40 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #108: GFLOPs: 2970.8419. Time: 42.7980 us. Best GFLOPs: 4339.3315
2023-11-11 01:01:40 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #109: GFLOPs: 4292.9535. Time: 29.6174 us. Best GFLOPs: 4339.3315
2023-11-11 01:01:40 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #110: GFLOPs: 1792.0618. Time: 70.9496 us. Best GFLOPs: 4339.3315
2023-11-11 01:01:40 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #111: GFLOPs: 45.6283. Time: 2786.5601 us. Best GFLOPs: 4339.3315
2023-11-11 01:01:40 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #112: GFLOPs: 3151.6205. Time: 40.3431 us. Best GFLOPs: 4339.3315
2023-11-11 01:01:40 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #113: GFLOPs: 2975.6645. Time: 42.7286 us. Best GFLOPs: 4339.3315
2023-11-11 01:01:40 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #114: GFLOPs: 3093.0859. Time: 41.1065 us. Best GFLOPs: 4339.3315
2023-11-11 01:01:40 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #115: GFLOPs: 3224.9106. Time: 39.4262 us. Best GFLOPs: 4339.3315
2023-11-11 01:01:40 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #116: GFLOPs: 3184.8274. Time: 39.9224 us. Best GFLOPs: 4339.3315
2023-11-11 01:01:40 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #117: GFLOPs: 2898.1670. Time: 43.8712 us. Best GFLOPs: 4339.3315
2023-11-11 01:01:40 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #118: GFLOPs: 4387.5143. Time: 28.9790 us. Best GFLOPs: 4387.5143
2023-11-11 01:01:40 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #119: GFLOPs: 2521.1697. Time: 50.4313 us. Best GFLOPs: 4387.5143
2023-11-11 01:01:40 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #120: GFLOPs: 2654.0092. Time: 47.9071 us. Best GFLOPs: 4387.5143
2023-11-11 01:01:40 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #121: GFLOPs: 2585.8944. Time: 49.1691 us. Best GFLOPs: 4387.5143
2023-11-11 01:01:40 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #122: GFLOPs: 2939.1261. Time: 43.2598 us. Best GFLOPs: 4387.5143
2023-11-11 01:01:40 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #123: GFLOPs: 3599.9920. Time: 35.3184 us. Best GFLOPs: 4387.5143
2023-11-11 01:01:40 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #124: GFLOPs: 3058.1517. Time: 41.5761 us. Best GFLOPs: 4387.5143
2023-11-11 01:01:40 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #125: GFLOPs: 3533.1856. Time: 35.9862 us. Best GFLOPs: 4387.5143
2023-11-11 01:01:40 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #126: GFLOPs: 81.4902. Time: 1560.2609 us. Best GFLOPs: 4387.5143
2023-11-11 01:01:40 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #127: GFLOPs: 3621.3398. Time: 35.1102 us. Best GFLOPs: 4387.5143
2023-11-11 01:01:40 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #128: GFLOPs: 916.9259. Time: 138.6655 us. Best GFLOPs: 4387.5143
2023-11-11 01:58:29 [INFO] [evolutionary_search.cc:713] Generating candidates......
2023-11-11 01:58:33 [INFO] [evolutionary_search.cc:715] Picked top 102 candidate(s) from database
2023-11-11 01:58:40 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310fb046d8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310f554558)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310dc5ddb8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310bbcd2d8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56310b1dbf38)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310dc5d048)]: 397 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310dc26e38)]: 0 failure(s)
2023-11-11 01:58:46 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310fb046d8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310f554558)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310dc5ddb8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310bbcd2d8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56310b1dbf38)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310dc5d048)]: 792 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310dc26e38)]: 0 failure(s)
2023-11-11 01:58:53 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310fb046d8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310f554558)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310dc5ddb8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310bbcd2d8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56310b1dbf38)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310dc5d048)]: 1190 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310dc26e38)]: 0 failure(s)
2023-11-11 01:58:59 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310fb046d8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310f554558)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310dc5ddb8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310bbcd2d8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56310b1dbf38)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310dc5d048)]: 1587 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310dc26e38)]: 0 failure(s)
2023-11-11 01:58:59 [INFO] [evolutionary_search.cc:723] Sampled 53 candidate(s)
2023-11-11 01:59:19 [INFO] [evolutionary_search.cc:621] Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310fb046d8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310f554558)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310dc5ddb8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310bbcd2d8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56310b1dbf38)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310dc5d048)]: 104 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310dc26e38)]: 0 failure(s)
2023-11-11 01:59:42 [INFO] [evolutionary_search.cc:621] Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310fb046d8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310f554558)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310dc5ddb8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310bbcd2d8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56310b1dbf38)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310dc5d048)]: 90 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310dc26e38)]: 0 failure(s)
2023-11-11 02:00:06 [INFO] [evolutionary_search.cc:621] Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310fb046d8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310f554558)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310dc5ddb8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310bbcd2d8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56310b1dbf38)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310dc5d048)]: 99 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310dc26e38)]: 0 failure(s)
2023-11-11 02:00:31 [INFO] [evolutionary_search.cc:621] Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310fb046d8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310f554558)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310dc5ddb8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310bbcd2d8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56310b1dbf38)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310dc5d048)]: 121 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310dc26e38)]: 0 failure(s)
2023-11-11 02:00:37 [INFO] [evolutionary_search.cc:649] Scores of the best 64 candidates:
[1 : 16]:	1.0823  1.0687  1.0225  0.9367  0.9335  0.9332  0.9315  0.9228  0.9131  0.9127  0.9112  0.9097  0.9082  0.9046  0.8929  0.8913
[17 : 32]:	0.8887  0.8861  0.8858  0.8834  0.8825  0.8769  0.8768  0.8757  0.8680  0.8645  0.8633  0.8613  0.8592  0.8550  0.8535  0.8511
[33 : 48]:	0.8506  0.8470  0.8463  0.8455  0.8435  0.8435  0.8435  0.8416  0.8410  0.8405  0.8402  0.8356  0.8353  0.8353  0.8353  0.8353
[49 : 64]:	0.8351  0.8338  0.8329  0.8326  0.8320  0.8313  0.8312  0.8312  0.8305  0.8274  0.8271  0.8262  0.8262  0.8245  0.8242  0.8231
2023-11-11 02:00:38 [INFO] [evolutionary_search.cc:727] Got 64 candidate(s) with evolutionary search
2023-11-11 02:00:38 [INFO] [evolutionary_search.cc:730] Sending 64 candidates(s) for measurement
2023-11-11 02:01:15 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #129: GFLOPs: 3281.9749. Time: 38.7407 us. Best GFLOPs: 4387.5143
2023-11-11 02:01:15 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #130: GFLOPs: 3306.3149. Time: 38.4555 us. Best GFLOPs: 4387.5143
2023-11-11 02:01:15 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #131: GFLOPs: 3204.3310. Time: 39.6794 us. Best GFLOPs: 4387.5143
2023-11-11 02:01:15 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #132: GFLOPs: 4425.7563. Time: 28.7286 us. Best GFLOPs: 4425.7563
2023-11-11 02:01:15 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #133: GFLOPs: 4470.0325. Time: 28.4441 us. Best GFLOPs: 4470.0325
2023-11-11 02:01:15 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #134: GFLOPs: 4219.5385. Time: 30.1327 us. Best GFLOPs: 4470.0325
2023-11-11 02:01:15 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #135: GFLOPs: 4231.5506. Time: 30.0471 us. Best GFLOPs: 4470.0325
2023-11-11 02:01:15 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #136: GFLOPs: 3689.4097. Time: 34.4624 us. Best GFLOPs: 4470.0325
2023-11-11 02:01:15 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #137: GFLOPs: 4067.1541. Time: 31.2617 us. Best GFLOPs: 4470.0325
2023-11-11 02:01:15 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #138: GFLOPs: 4256.6906. Time: 29.8697 us. Best GFLOPs: 4470.0325
2023-11-11 02:01:15 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #139: GFLOPs: 4264.6346. Time: 29.8140 us. Best GFLOPs: 4470.0325
2023-11-11 02:01:15 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #140: GFLOPs: 4193.4431. Time: 30.3202 us. Best GFLOPs: 4470.0325
2023-11-11 02:01:15 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #141: GFLOPs: 4351.4112. Time: 29.2195 us. Best GFLOPs: 4470.0325
2023-11-11 02:01:15 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #142: GFLOPs: 4442.0113. Time: 28.6235 us. Best GFLOPs: 4470.0325
2023-11-11 02:01:15 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #143: GFLOPs: 4494.3872. Time: 28.2899 us. Best GFLOPs: 4494.3872
2023-11-11 02:01:15 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #144: GFLOPs: 4264.9189. Time: 29.8121 us. Best GFLOPs: 4494.3872
2023-11-11 02:01:15 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #145: GFLOPs: 4061.9005. Time: 31.3021 us. Best GFLOPs: 4494.3872
2023-11-11 02:01:15 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #146: GFLOPs: 3704.1237. Time: 34.3255 us. Best GFLOPs: 4494.3872
2023-11-11 02:01:15 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #147: GFLOPs: 4214.6035. Time: 30.1680 us. Best GFLOPs: 4494.3872
2023-11-11 02:01:15 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #148: GFLOPs: 4027.0454. Time: 31.5730 us. Best GFLOPs: 4494.3872
2023-11-11 02:01:15 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #149: GFLOPs: 4366.7360. Time: 29.1169 us. Best GFLOPs: 4494.3872
2023-11-11 02:01:15 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #150: GFLOPs: 4240.4129. Time: 29.9843 us. Best GFLOPs: 4494.3872
2023-11-11 02:01:15 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #151: GFLOPs: 4314.0038. Time: 29.4728 us. Best GFLOPs: 4494.3872
2023-11-11 02:01:15 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #152: GFLOPs: 4127.5874. Time: 30.8039 us. Best GFLOPs: 4494.3872
2023-11-11 02:01:15 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #153: GFLOPs: 4552.6036. Time: 27.9282 us. Best GFLOPs: 4552.6036
2023-11-11 02:01:15 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #154: GFLOPs: 4821.3725. Time: 26.3713 us. Best GFLOPs: 4821.3725
2023-11-11 02:01:15 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #155: GFLOPs: 3496.4225. Time: 36.3646 us. Best GFLOPs: 4821.3725
2023-11-11 02:01:15 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #156: GFLOPs: 3621.2223. Time: 35.1113 us. Best GFLOPs: 4821.3725
2023-11-11 02:01:15 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #157: GFLOPs: 2500.4601. Time: 50.8490 us. Best GFLOPs: 4821.3725
2023-11-11 02:01:15 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #158: GFLOPs: 4250.2987. Time: 29.9146 us. Best GFLOPs: 4821.3725
2023-11-11 02:01:15 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #159: GFLOPs: 4146.0505. Time: 30.6668 us. Best GFLOPs: 4821.3725
2023-11-11 02:01:15 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #160: GFLOPs: 3665.5277. Time: 34.6870 us. Best GFLOPs: 4821.3725
2023-11-11 02:01:15 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #161: GFLOPs: 3621.1474. Time: 35.1121 us. Best GFLOPs: 4821.3725
2023-11-11 02:01:15 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #162: GFLOPs: 4126.0479. Time: 30.8154 us. Best GFLOPs: 4821.3725
2023-11-11 02:01:15 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #163: GFLOPs: 4465.3181. Time: 28.4741 us. Best GFLOPs: 4821.3725
2023-11-11 02:01:15 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #164: GFLOPs: 4220.8047. Time: 30.1236 us. Best GFLOPs: 4821.3725
2023-11-11 02:01:15 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #165: GFLOPs: 3954.4302. Time: 32.1528 us. Best GFLOPs: 4821.3725
2023-11-11 02:01:15 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #166: GFLOPs: 3659.3298. Time: 34.7457 us. Best GFLOPs: 4821.3725
2023-11-11 02:01:15 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #167: GFLOPs: 4824.8571. Time: 26.3523 us. Best GFLOPs: 4824.8571
2023-11-11 02:01:15 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #168: GFLOPs: 3339.6058. Time: 38.0722 us. Best GFLOPs: 4824.8571
2023-11-11 02:01:15 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #169: GFLOPs: 3970.2718. Time: 32.0245 us. Best GFLOPs: 4824.8571
2023-11-11 02:01:15 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #170: GFLOPs: 3677.2369. Time: 34.5765 us. Best GFLOPs: 4824.8571
2023-11-11 02:01:15 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #171: GFLOPs: 3985.9378. Time: 31.8986 us. Best GFLOPs: 4824.8571
2023-11-11 02:01:15 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #172: GFLOPs: 3556.7598. Time: 35.7477 us. Best GFLOPs: 4824.8571
2023-11-11 02:01:15 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #173: GFLOPs: 3853.8836. Time: 32.9917 us. Best GFLOPs: 4824.8571
2023-11-11 02:01:15 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #174: GFLOPs: 3945.1069. Time: 32.2288 us. Best GFLOPs: 4824.8571
2023-11-11 02:01:15 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #175: GFLOPs: 3855.3115. Time: 32.9794 us. Best GFLOPs: 4824.8571
2023-11-11 02:01:15 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #176: GFLOPs: 3946.5826. Time: 32.2167 us. Best GFLOPs: 4824.8571
2023-11-11 02:01:15 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #177: GFLOPs: 3848.6907. Time: 33.0362 us. Best GFLOPs: 4824.8571
2023-11-11 02:01:15 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #178: GFLOPs: 3510.0306. Time: 36.2236 us. Best GFLOPs: 4824.8571
2023-11-11 02:01:15 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #179: GFLOPs: 3939.9626. Time: 32.2709 us. Best GFLOPs: 4824.8571
2023-11-11 02:01:15 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #180: GFLOPs: 3684.4632. Time: 34.5087 us. Best GFLOPs: 4824.8571
2023-11-11 02:01:15 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #181: GFLOPs: 4782.3288. Time: 26.5866 us. Best GFLOPs: 4824.8571
2023-11-11 02:01:15 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #182: GFLOPs: 3079.4221. Time: 41.2889 us. Best GFLOPs: 4824.8571
2023-11-11 02:01:15 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #183: GFLOPs: 2974.8104. Time: 42.7409 us. Best GFLOPs: 4824.8571
2023-11-11 02:01:15 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #184: GFLOPs: 2974.9469. Time: 42.7389 us. Best GFLOPs: 4824.8571
2023-11-11 02:01:15 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #185: GFLOPs: 4153.0142. Time: 30.6154 us. Best GFLOPs: 4824.8571
2023-11-11 02:01:15 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #186: GFLOPs: 4146.9528. Time: 30.6601 us. Best GFLOPs: 4824.8571
2023-11-11 02:01:15 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #187: GFLOPs: 3288.0200. Time: 38.6695 us. Best GFLOPs: 4824.8571
2023-11-11 02:01:15 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #188: GFLOPs: 2279.7560. Time: 55.7717 us. Best GFLOPs: 4824.8571
2023-11-11 02:01:15 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #189: GFLOPs: 3025.9594. Time: 42.0184 us. Best GFLOPs: 4824.8571
2023-11-11 02:01:15 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #190: GFLOPs: 134.9982. Time: 941.8345 us. Best GFLOPs: 4824.8571
2023-11-11 02:01:15 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #191: GFLOPs: 155.6472. Time: 816.8856 us. Best GFLOPs: 4824.8571
2023-11-11 02:01:15 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #192: GFLOPs: 2694.6819. Time: 47.1840 us. Best GFLOPs: 4824.8571
2023-11-11 02:58:43 [INFO] [evolutionary_search.cc:713] Generating candidates......
2023-11-11 02:58:47 [INFO] [evolutionary_search.cc:715] Picked top 102 candidate(s) from database
2023-11-11 02:58:53 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310fb046d8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310f554558)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310dc5ddb8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310bbcd2d8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56310b1dbf38)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310dc5d048)]: 396 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310dc26e38)]: 0 failure(s)
2023-11-11 02:59:00 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310fb046d8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310f554558)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310dc5ddb8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310bbcd2d8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56310b1dbf38)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310dc5d048)]: 790 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310dc26e38)]: 0 failure(s)
2023-11-11 02:59:06 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310fb046d8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310f554558)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310dc5ddb8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310bbcd2d8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56310b1dbf38)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310dc5d048)]: 1181 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310dc26e38)]: 0 failure(s)
2023-11-11 02:59:13 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310fb046d8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310f554558)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310dc5ddb8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310bbcd2d8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56310b1dbf38)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310dc5d048)]: 1578 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310dc26e38)]: 0 failure(s)
2023-11-11 02:59:13 [INFO] [evolutionary_search.cc:723] Sampled 62 candidate(s)
2023-11-11 02:59:33 [INFO] [evolutionary_search.cc:621] Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310fb046d8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310f554558)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310dc5ddb8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310bbcd2d8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56310b1dbf38)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310dc5d048)]: 122 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310dc26e38)]: 0 failure(s)
2023-11-11 02:59:57 [INFO] [evolutionary_search.cc:621] Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310fb046d8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310f554558)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310dc5ddb8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310bbcd2d8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56310b1dbf38)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310dc5d048)]: 111 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310dc26e38)]: 0 failure(s)
2023-11-11 03:00:21 [INFO] [evolutionary_search.cc:621] Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310fb046d8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310f554558)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310dc5ddb8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310bbcd2d8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56310b1dbf38)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310dc5d048)]: 113 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310dc26e38)]: 0 failure(s)
2023-11-11 03:00:45 [INFO] [evolutionary_search.cc:621] Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310fb046d8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310f554558)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310dc5ddb8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310bbcd2d8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56310b1dbf38)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310dc5d048)]: 138 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310dc26e38)]: 0 failure(s)
2023-11-11 03:00:52 [INFO] [evolutionary_search.cc:649] Scores of the best 64 candidates:
[1 : 16]:	1.0032  0.9552  0.9406  0.9388  0.9327  0.9324  0.9293  0.9289  0.9281  0.9222  0.9212  0.9192  0.9163  0.9157  0.9154  0.9152
[17 : 32]:	0.9149  0.9148  0.9147  0.9137  0.9106  0.9097  0.9096  0.9088  0.9082  0.9061  0.9059  0.9056  0.9049  0.9044  0.9034  0.9031
[33 : 48]:	0.8997  0.8997  0.8984  0.8982  0.8973  0.8958  0.8952  0.8950  0.8949  0.8934  0.8924  0.8921  0.8906  0.8905  0.8896  0.8896
[49 : 64]:	0.8886  0.8884  0.8882  0.8865  0.8860  0.8856  0.8850  0.8847  0.8846  0.8845  0.8844  0.8840  0.8835  0.8835  0.8831  0.8828
2023-11-11 03:00:52 [INFO] [evolutionary_search.cc:727] Got 64 candidate(s) with evolutionary search
2023-11-11 03:00:52 [INFO] [evolutionary_search.cc:730] Sending 64 candidates(s) for measurement
2023-11-11 03:01:25 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #193: GFLOPs: 3831.4322. Time: 33.1850 us. Best GFLOPs: 4824.8571
2023-11-11 03:01:25 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #194: GFLOPs: 4759.3084. Time: 26.7152 us. Best GFLOPs: 4824.8571
2023-11-11 03:01:25 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #195: GFLOPs: 4395.7983. Time: 28.9244 us. Best GFLOPs: 4824.8571
2023-11-11 03:01:25 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #196: GFLOPs: 4688.3466. Time: 27.1196 us. Best GFLOPs: 4824.8571
2023-11-11 03:01:25 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #197: GFLOPs: 4541.3726. Time: 27.9973 us. Best GFLOPs: 4824.8571
2023-11-11 03:01:25 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #198: GFLOPs: 4755.0205. Time: 26.7393 us. Best GFLOPs: 4824.8571
2023-11-11 03:01:25 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #199: GFLOPs: 4507.4848. Time: 28.2077 us. Best GFLOPs: 4824.8571
2023-11-11 03:01:25 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #200: GFLOPs: 4867.3629. Time: 26.1221 us. Best GFLOPs: 4867.3629
2023-11-11 03:01:25 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #201: GFLOPs: 4754.5484. Time: 26.7420 us. Best GFLOPs: 4867.3629
2023-11-11 03:01:25 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #202: GFLOPs: 4557.5475. Time: 27.8979 us. Best GFLOPs: 4867.3629
2023-11-11 03:01:25 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #203: GFLOPs: 4409.2791. Time: 28.8360 us. Best GFLOPs: 4867.3629
2023-11-11 03:01:25 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #204: GFLOPs: 4429.1559. Time: 28.7066 us. Best GFLOPs: 4867.3629
2023-11-11 03:01:25 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #205: GFLOPs: 4684.8130. Time: 27.1400 us. Best GFLOPs: 4867.3629
2023-11-11 03:01:25 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #206: GFLOPs: 4121.8198. Time: 30.8471 us. Best GFLOPs: 4867.3629
2023-11-11 03:01:25 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #207: GFLOPs: 4150.9115. Time: 30.6309 us. Best GFLOPs: 4867.3629
2023-11-11 03:01:25 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #208: GFLOPs: 4545.4018. Time: 27.9724 us. Best GFLOPs: 4867.3629
2023-11-11 03:01:25 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #209: GFLOPs: 4695.7329. Time: 27.0769 us. Best GFLOPs: 4867.3629
2023-11-11 03:01:25 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #210: GFLOPs: 3805.7831. Time: 33.4086 us. Best GFLOPs: 4867.3629
2023-11-11 03:01:25 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #211: GFLOPs: 4394.9027. Time: 28.9303 us. Best GFLOPs: 4867.3629
2023-11-11 03:01:25 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #212: GFLOPs: 4381.7874. Time: 29.0169 us. Best GFLOPs: 4867.3629
2023-11-11 03:01:25 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #213: GFLOPs: 4351.9423. Time: 29.2159 us. Best GFLOPs: 4867.3629
2023-11-11 03:01:25 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #214: GFLOPs: 4543.7316. Time: 27.9827 us. Best GFLOPs: 4867.3629
2023-11-11 03:01:25 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #215: GFLOPs: 4759.5989. Time: 26.7136 us. Best GFLOPs: 4867.3629
2023-11-11 03:01:25 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #216: GFLOPs: 4132.5569. Time: 30.7669 us. Best GFLOPs: 4867.3629
2023-11-11 03:01:25 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #217: GFLOPs: 3875.4861. Time: 32.8078 us. Best GFLOPs: 4867.3629
2023-11-11 03:01:25 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #218: GFLOPs: 4350.7612. Time: 29.2238 us. Best GFLOPs: 4867.3629
2023-11-11 03:01:25 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #219: GFLOPs: 4241.0526. Time: 29.9798 us. Best GFLOPs: 4867.3629
2023-11-11 03:01:25 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #220: GFLOPs: 5062.5501. Time: 25.1150 us. Best GFLOPs: 5062.5501
2023-11-11 03:01:25 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #221: GFLOPs: 4779.2849. Time: 26.6036 us. Best GFLOPs: 5062.5501
2023-11-11 03:01:25 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #222: GFLOPs: 4277.7067. Time: 29.7229 us. Best GFLOPs: 5062.5501
2023-11-11 03:01:25 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #223: GFLOPs: 4427.1037. Time: 28.7199 us. Best GFLOPs: 5062.5501
2023-11-11 03:01:25 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #224: GFLOPs: 4803.0159. Time: 26.4721 us. Best GFLOPs: 5062.5501
2023-11-11 03:01:25 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #225: GFLOPs: 4207.9512. Time: 30.2157 us. Best GFLOPs: 5062.5501
2023-11-11 03:01:25 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #226: GFLOPs: 4207.3979. Time: 30.2196 us. Best GFLOPs: 5062.5501
2023-11-11 03:01:25 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #227: GFLOPs: 4072.5846. Time: 31.2200 us. Best GFLOPs: 5062.5501
2023-11-11 03:01:25 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #228: GFLOPs: 4563.7818. Time: 27.8598 us. Best GFLOPs: 5062.5501
2023-11-11 03:01:25 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #229: GFLOPs: 4266.9788. Time: 29.7977 us. Best GFLOPs: 5062.5501
2023-11-11 03:01:25 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #230: GFLOPs: 4067.6945. Time: 31.2575 us. Best GFLOPs: 5062.5501
2023-11-11 03:01:25 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #231: GFLOPs: 4147.1726. Time: 30.6585 us. Best GFLOPs: 5062.5501
2023-11-11 03:01:25 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #232: GFLOPs: 4416.4926. Time: 28.7889 us. Best GFLOPs: 5062.5501
2023-11-11 03:01:25 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #233: GFLOPs: 5065.9586. Time: 25.0981 us. Best GFLOPs: 5065.9586
2023-11-11 03:01:25 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #234: GFLOPs: 4446.5654. Time: 28.5942 us. Best GFLOPs: 5065.9586
2023-11-11 03:01:25 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #235: GFLOPs: 4067.8292. Time: 31.2565 us. Best GFLOPs: 5065.9586
2023-11-11 03:01:25 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #236: GFLOPs: 4632.5821. Time: 27.4460 us. Best GFLOPs: 5065.9586
2023-11-11 03:01:25 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #237: GFLOPs: 4252.6203. Time: 29.8983 us. Best GFLOPs: 5065.9586
2023-11-11 03:01:25 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #238: GFLOPs: 4282.6069. Time: 29.6889 us. Best GFLOPs: 5065.9586
2023-11-11 03:01:25 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #239: GFLOPs: 4252.7575. Time: 29.8973 us. Best GFLOPs: 5065.9586
2023-11-11 03:01:25 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #240: GFLOPs: 4253.8210. Time: 29.8898 us. Best GFLOPs: 5065.9586
2023-11-11 03:01:25 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #241: GFLOPs: 4307.7933. Time: 29.5153 us. Best GFLOPs: 5065.9586
2023-11-11 03:01:25 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #242: GFLOPs: 4173.4955. Time: 30.4651 us. Best GFLOPs: 5065.9586
2023-11-11 03:01:25 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #243: GFLOPs: 3705.2468. Time: 34.3151 us. Best GFLOPs: 5065.9586
2023-11-11 03:01:25 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #244: GFLOPs: 4216.4088. Time: 30.1550 us. Best GFLOPs: 5065.9586
2023-11-11 03:01:25 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #245: GFLOPs: 3426.1419. Time: 37.1105 us. Best GFLOPs: 5065.9586
2023-11-11 03:01:25 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #246: GFLOPs: 4026.7410. Time: 31.5754 us. Best GFLOPs: 5065.9586
2023-11-11 03:01:25 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #247: GFLOPs: 2619.5325. Time: 48.5377 us. Best GFLOPs: 5065.9586
2023-11-11 03:01:25 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #248: GFLOPs: 4978.3205. Time: 25.5399 us. Best GFLOPs: 5065.9586
2023-11-11 03:01:25 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #249: GFLOPs: 4110.1147. Time: 30.9349 us. Best GFLOPs: 5065.9586
2023-11-11 03:01:25 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #250: GFLOPs: 4007.0039. Time: 31.7309 us. Best GFLOPs: 5065.9586
2023-11-11 03:01:25 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #251: GFLOPs: 4094.3856. Time: 31.0537 us. Best GFLOPs: 5065.9586
2023-11-11 03:01:25 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #252: GFLOPs: 4206.9551. Time: 30.2228 us. Best GFLOPs: 5065.9586
2023-11-11 03:01:25 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #253: GFLOPs: 4844.5495. Time: 26.2452 us. Best GFLOPs: 5065.9586
2023-11-11 03:01:25 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #254: GFLOPs: 447.8677. Time: 283.8918 us. Best GFLOPs: 5065.9586
2023-11-11 03:01:25 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #255: GFLOPs: 1568.9147. Time: 81.0407 us. Best GFLOPs: 5065.9586
2023-11-11 03:01:25 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #256: GFLOPs: 56.9824. Time: 2231.3188 us. Best GFLOPs: 5065.9586
2023-11-11 03:19:56 [INFO] [evolutionary_search.cc:713] Generating candidates......
2023-11-11 03:19:59 [INFO] [evolutionary_search.cc:715] Picked top 102 candidate(s) from database
2023-11-11 03:20:06 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310fb046d8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310f554558)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310dc5ddb8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310bbcd2d8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56310b1dbf38)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310dc5d048)]: 395 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310dc26e38)]: 0 failure(s)
2023-11-11 03:20:13 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310fb046d8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310f554558)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310dc5ddb8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310bbcd2d8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56310b1dbf38)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310dc5d048)]: 792 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310dc26e38)]: 0 failure(s)
2023-11-11 03:20:20 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310fb046d8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310f554558)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310dc5ddb8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310bbcd2d8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56310b1dbf38)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310dc5d048)]: 1183 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310dc26e38)]: 0 failure(s)
2023-11-11 03:20:26 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310fb046d8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310f554558)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310dc5ddb8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310bbcd2d8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56310b1dbf38)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310dc5d048)]: 1577 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310dc26e38)]: 0 failure(s)
2023-11-11 03:20:26 [INFO] [evolutionary_search.cc:723] Sampled 63 candidate(s)
2023-11-11 03:20:46 [INFO] [evolutionary_search.cc:621] Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310fb046d8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310f554558)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310dc5ddb8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310bbcd2d8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56310b1dbf38)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310dc5d048)]: 80 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310dc26e38)]: 0 failure(s)
2023-11-11 03:21:09 [INFO] [evolutionary_search.cc:621] Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310fb046d8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310f554558)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310dc5ddb8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310bbcd2d8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56310b1dbf38)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310dc5d048)]: 68 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310dc26e38)]: 0 failure(s)
2023-11-11 03:21:32 [INFO] [evolutionary_search.cc:621] Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310fb046d8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310f554558)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310dc5ddb8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310bbcd2d8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56310b1dbf38)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310dc5d048)]: 106 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310dc26e38)]: 0 failure(s)
2023-11-11 03:21:55 [INFO] [evolutionary_search.cc:621] Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310fb046d8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310f554558)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310dc5ddb8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310bbcd2d8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56310b1dbf38)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310dc5d048)]: 93 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310dc26e38)]: 0 failure(s)
2023-11-11 03:22:02 [INFO] [evolutionary_search.cc:649] Scores of the best 64 candidates:
[1 : 16]:	1.8120  1.6965  1.6821  1.6762  1.6498  1.5709  1.5587  1.5106  1.4483  1.4471  1.4259  1.4201  1.4159  1.3596  1.3189  1.2949
[17 : 32]:	1.2875  1.2836  1.2816  1.2812  1.2791  1.2639  1.2625  1.2606  1.2588  1.2486  1.2481  1.2407  1.2393  1.2312  1.2308  1.2285
[33 : 48]:	1.2265  1.2219  1.2078  1.2051  1.2043  1.2028  1.2010  1.1840  1.1836  1.1760  1.1756  1.1694  1.1662  1.1605  1.1569  1.1568
[49 : 64]:	1.1489  1.1483  1.1394  1.1332  1.1314  1.0912  1.0836  1.0627  1.0515  1.0457  1.0446  1.0321  1.0319  1.0276  1.0183  1.0140
2023-11-11 03:22:02 [INFO] [evolutionary_search.cc:727] Got 64 candidate(s) with evolutionary search
2023-11-11 03:22:02 [INFO] [evolutionary_search.cc:730] Sending 64 candidates(s) for measurement
2023-11-11 03:22:51 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #257: GFLOPs: 67.3848. Time: 1886.8648 us. Best GFLOPs: 5065.9586
2023-11-11 03:22:51 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #258: GFLOPs: 67.3820. Time: 1886.9428 us. Best GFLOPs: 5065.9586
2023-11-11 03:22:51 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #259: GFLOPs: 65.9176. Time: 1928.8615 us. Best GFLOPs: 5065.9586
2023-11-11 03:22:51 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #260: GFLOPs: 67.5243. Time: 1882.9654 us. Best GFLOPs: 5065.9586
2023-11-11 03:22:51 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #261: GFLOPs: 70.0281. Time: 1815.6434 us. Best GFLOPs: 5065.9586
2023-11-11 03:22:51 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #262: GFLOPs: 80.0186. Time: 1588.9555 us. Best GFLOPs: 5065.9586
2023-11-11 03:22:51 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #263: GFLOPs: 67.9893. Time: 1870.0895 us. Best GFLOPs: 5065.9586
2023-11-11 03:22:51 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #264: GFLOPs: 48.4151. Time: 2626.1661 us. Best GFLOPs: 5065.9586
2023-11-11 03:22:51 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #265: GFLOPs: 68.2023. Time: 1864.2489 us. Best GFLOPs: 5065.9586
2023-11-11 03:22:51 [INFO] [task_scheduler.cc:121] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #266: Error in building:
LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(128), T.int64(28), T.int64(28)), "float32"), p1: T.Buffer((T.int64(4), T.int64(4), T.int64(128), T.int64(128)), "float32"), p2: T.Buffer((T.int64(1), T.int64(128), T.int64(28), T.int64(28)), "float32"), p3: T.Buffer((T.int64(1), T.int64(128), T.int64(1), T.int64(1)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(128), T.int64(28), T.int64(28)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        input_tile_local = T.alloc_buffer((T.int64(128), T.int64(196), T.int64(4), T.int64(4)), scope="local")
        data_pack = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(128), T.int64(196)))
        bgemm = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(128), T.int64(196)))
        inverse_local = T.alloc_buffer((T.int64(128), T.int64(196), T.int64(2), T.int64(2)), scope="local")
        data_pack_local = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(128), T.int64(196)), scope="local")
        bgemm_local = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(128), T.int64(196)), scope="local")
        data_pack_shared = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(128), T.int64(196)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(128), T.int64(128)), scope="shared")
        for ci_p_fused_0 in T.thread_binding(T.int64(98), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 1024, "pragma_unroll_explicit": 1}):
            for ci_p_fused_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(1), T.int64(4), T.int64(4)):
                    with T.block("input_tile"):
                        v_ci = T.axis.spatial(T.int64(128), (ci_p_fused_0 * T.int64(256) + ci_p_fused_1) // T.int64(196) + ax0)
                        v_p = T.axis.spatial(T.int64(196), (ci_p_fused_0 * T.int64(60) + ci_p_fused_1) % T.int64(196) + ax1)
                        v_eps, v_nu = T.axis.remap("SS", [ax2, ax3])
                        T.reads(p0[v_p // T.int64(196), v_ci, v_p % T.int64(196) // T.int64(14) * T.int64(2) + v_eps - T.int64(1), v_p % T.int64(14) * T.int64(2) + v_nu - T.int64(1)])
                        T.writes(input_tile_local[v_ci, v_p, v_eps, v_nu])
                        T.block_attr({"schedule_rule": "None"})
                        input_tile_local[v_ci, v_p, v_eps, v_nu] = T.if_then_else(T.int64(1) <= v_p % T.int64(196) // T.int64(14) * T.int64(2) + v_eps and v_p % T.int64(196) // T.int64(14) * T.int64(2) + v_eps < T.int64(29) and T.int64(1) <= v_p % T.int64(14) * T.int64(2) + v_nu and v_p % T.int64(14) * T.int64(2) + v_nu < T.int64(29), p0[v_p // T.int64(196), v_ci, v_p % T.int64(196) // T.int64(14) * T.int64(2) + v_eps - T.int64(1), v_p % T.int64(14) * T.int64(2) + v_nu - T.int64(1)], T.float32(0))
                for eps in T.unroll(T.int64(4)):
                    for nu in T.unroll(T.int64(4)):
                        with T.block("data_pack_init"):
                            v_eps, v_nu = T.axis.remap("SS", [eps, nu])
                            v_ci = T.axis.spatial(T.int64(128), (ci_p_fused_0 * T.int64(256) + ci_p_fused_1) // T.int64(196))
                            v_p = T.axis.spatial(T.int64(196), (ci_p_fused_0 * T.int64(256) + ci_p_fused_1) % T.int64(196))
                            T.reads()
                            T.writes(data_pack_local[v_eps, v_nu, v_ci, v_p])
                            T.block_attr({"schedule_rule": "conv2d_nchw_winograd_data_pack"})
                            data_pack_local[v_eps, v_nu, v_ci, v_p] = T.float32(0)
                        for r_a in T.unroll(T.int64(4)):
                            for r_b in T.unroll(T.int64(4)):
                                with T.block("data_pack_update"):
                                    v_eps, v_nu = T.axis.remap("SS", [eps, nu])
                                    v_ci = T.axis.spatial(T.int64(128), (ci_p_fused_0 * T.int64(256) + ci_p_fused_1) // T.int64(196))
                                    v_p = T.axis.spatial(T.int64(196), (ci_p_fused_0 * T.int64(256) + ci_p_fused_1) % T.int64(196))
                                    v_r_a, v_r_b = T.axis.remap("RR", [r_a, r_b])
                                    T.reads(data_pack_local[v_eps, v_nu, v_ci, v_p], input_tile_local[v_ci, v_p, v_r_a, v_r_b])
                                    T.writes(data_pack_local[v_eps, v_nu, v_ci, v_p])
                                    T.block_attr({"schedule_rule": "conv2d_nchw_winograd_data_pack"})
                                    data_pack_local[v_eps, v_nu, v_ci, v_p] = data_pack_local[v_eps, v_nu, v_ci, v_p] + input_tile_local[v_ci, v_p, v_r_a, v_r_b] * T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(3), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(1), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(0), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(3), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(1), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(0), T.float32(1), T.float32(0))))))))))))))))) * T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(3), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(1), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(0), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(3), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(1), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(0), T.float32(1), T.float32(0)))))))))))))))))
                for ax0, ax1, ax2, ax3 in T.grid(T.int64(4), T.int64(4), T.int64(1), T.int64(1)):
                    with T.block("data_pack_local"):
                        v0, v1 = T.axis.remap("SS", [ax0, ax1])
                        v2 = T.axis.spatial(T.int64(128), (ci_p_fused_0 * T.int64(256) + ci_p_fused_1) // T.int64(196) + ax2)
                        v3 = T.axis.spatial(T.int64(196), (ci_p_fused_0 * T.int64(60) + ci_p_fused_1) % T.int64(196) + ax3)
                        T.reads(data_pack_local[v0, v1, v2, v3])
                        T.writes(data_pack[v0, v1, v2, v3])
                        data_pack[v0, v1, v2, v3] = data_pack_local[v0, v1, v2, v3]
        for eps_0_nu_0_co_0_p_0_fused in T.thread_binding(T.int64(2), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 1024, "pragma_unroll_explicit": 1}):
            for eps_1_nu_1_co_1_p_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for eps_2_nu_2_co_2_p_2_fused in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                    for eps_3_init, nu_3_init, co_3_init, p_3_init, eps_4_init, nu_4_init, co_4_init, p_4_init in T.grid(T.int64(1), T.int64(2), T.int64(1), T.int64(49), T.int64(1), T.int64(2), T.int64(2), T.int64(2)):
                        with T.block("bgemm_init"):
                            v_eps = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused * T.int64(2) + eps_2_nu_2_co_2_p_2_fused // T.int64(128) + eps_3_init + eps_4_init)
                            v_nu = T.axis.spatial(T.int64(4), nu_3_init * T.int64(2) + nu_4_init)
                            v_co = T.axis.spatial(T.int64(128), eps_2_nu_2_co_2_p_2_fused % T.int64(128) // T.int64(2) * T.int64(2) + co_3_init * T.int64(2) + co_4_init)
                            v_p = T.axis.spatial(T.int64(196), eps_2_nu_2_co_2_p_2_fused % T.int64(2) * T.int64(98) + p_3_init * T.int64(2) + p_4_init)
                            T.reads()
                            T.writes(bgemm_local[v_eps, v_nu, v_co, v_p])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            bgemm_local[v_eps, v_nu, v_co, v_p] = T.float32(0)
                    for ci_0 in range(T.int64(32)):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(13)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("data_pack_shared"):
                                        v0 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused * T.int64(2) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(512) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) // T.int64(3136))
                                        v1 = T.axis.spatial(T.int64(4), (ax0_ax1_ax2_ax3_fused_0 * T.int64(512) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) % T.int64(3136) // T.int64(784))
                                        v2 = T.axis.spatial(T.int64(128), ci_0 * T.int64(4) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(512) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) % T.int64(784) // T.int64(196))
                                        v3 = T.axis.spatial(T.int64(196), (ax0_ax1_ax2_ax3_fused_0 * T.int64(512) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) % T.int64(196))
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1) * T.int64(2) + ax0_ax1_ax2_ax3_fused_2 < T.int64(6272))
                                        T.reads(data_pack[v0, v1, v2, v3])
                                        T.writes(data_pack_shared[v0, v1, v2, v3])
                                        data_pack_shared[v0, v1, v2, v3] = data_pack[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(8)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("p1_shared"):
                                        v0 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused * T.int64(2) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(512) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) // T.int64(2048))
                                        v1 = T.axis.spatial(T.int64(4), (ax0_ax1_ax2_ax3_fused_0 * T.int64(512) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) % T.int64(2048) // T.int64(512))
                                        v2 = T.axis.spatial(T.int64(128), ci_0 * T.int64(4) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(512) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) % T.int64(512) // T.int64(128))
                                        v3 = T.axis.spatial(T.int64(128), (ax0_ax1_ax2_ax3_fused_0 * T.int64(512) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) % T.int64(128))
                                        T.reads(p1[v0, v1, v2, v3])
                                        T.writes(p1_shared[v0, v1, v2, v3])
                                        p1_shared[v0, v1, v2, v3] = p1[v0, v1, v2, v3]
                        for ci_1, eps_3, nu_3, co_3, p_3, ci_2, eps_4, nu_4, co_4, p_4 in T.grid(T.int64(2), T.int64(1), T.int64(2), T.int64(1), T.int64(49), T.int64(2), T.int64(1), T.int64(2), T.int64(2), T.int64(2)):
                            with T.block("bgemm_update"):
                                v_eps = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused * T.int64(2) + eps_2_nu_2_co_2_p_2_fused // T.int64(128) + eps_3 + eps_4)
                                v_nu = T.axis.spatial(T.int64(4), nu_3 * T.int64(2) + nu_4)
                                v_co = T.axis.spatial(T.int64(128), eps_2_nu_2_co_2_p_2_fused % T.int64(128) // T.int64(2) * T.int64(2) + co_3 * T.int64(2) + co_4)
                                v_p = T.axis.spatial(T.int64(196), eps_2_nu_2_co_2_p_2_fused % T.int64(2) * T.int64(98) + p_3 * T.int64(2) + p_4)
                                v_ci = T.axis.reduce(T.int64(128), ci_0 * T.int64(4) + ci_1 * T.int64(2) + ci_2)
                                T.reads(bgemm_local[v_eps, v_nu, v_co, v_p], data_pack_shared[v_eps, v_nu, v_ci, v_p], p1_shared[v_eps, v_nu, v_ci, v_co])
                                T.writes(bgemm_local[v_eps, v_nu, v_co, v_p])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                bgemm_local[v_eps, v_nu, v_co, v_p] = bgemm_local[v_eps, v_nu, v_co, v_p] + data_pack_shared[v_eps, v_nu, v_ci, v_p] * p1_shared[v_eps, v_nu, v_ci, v_co]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(4), T.int64(2), T.int64(98)):
                        with T.block("bgemm_local"):
                            v0 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused * T.int64(2) + eps_2_nu_2_co_2_p_2_fused // T.int64(128) + ax0)
                            v1 = T.axis.spatial(T.int64(4), ax1)
                            v2 = T.axis.spatial(T.int64(128), eps_2_nu_2_co_2_p_2_fused % T.int64(128) // T.int64(2) * T.int64(2) + ax2)
                            v3 = T.axis.spatial(T.int64(196), eps_2_nu_2_co_2_p_2_fused % T.int64(2) * T.int64(98) + ax3)
                            T.reads(bgemm_local[v0, v1, v2, v3])
                            T.writes(bgemm[v0, v1, v2, v3])
                            bgemm[v0, v1, v2, v3] = bgemm_local[v0, v1, v2, v3]
        for n_co_h_0_w_0_fused_0 in T.thread_binding(T.int64(392), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 1024, "pragma_unroll_explicit": 1}):
            for n_co_h_0_w_0_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                    for ax2 in T.unroll(T.int64(2)):
                        for ax3 in T.unroll(T.int64(2)):
                            with T.block("inverse_init"):
                                v_co = T.axis.spatial(T.int64(128), (n_co_h_0_w_0_fused_0 * T.int64(64) + n_co_h_0_w_0_fused_1) // T.int64(196) + ax0)
                                v_p = T.axis.spatial(T.int64(196), (n_co_h_0_w_0_fused_0 * T.int64(64) + n_co_h_0_w_0_fused_1) % T.int64(196) + ax1)
                                v_vh, v_vw = T.axis.remap("SS", [ax2, ax3])
                                T.reads()
                                T.writes(inverse_local[v_co, v_p, v_vh, v_vw])
                                T.block_attr({"schedule_rule": "conv2d_nchw_winograd_inverse"})
                                inverse_local[v_co, v_p, v_vh, v_vw] = T.float32(0)
                            for ax4 in T.unroll(T.int64(4)):
                                for ax5 in T.unroll(T.int64(4)):
                                    with T.block("inverse_update"):
                                        v_co = T.axis.spatial(T.int64(128), (n_co_h_0_w_0_fused_0 * T.int64(64) + n_co_h_0_w_0_fused_1) // T.int64(196) + ax0)
                                        v_p = T.axis.spatial(T.int64(196), (n_co_h_0_w_0_fused_0 * T.int64(64) + n_co_h_0_w_0_fused_1) % T.int64(196) + ax1)
                                        v_vh, v_vw, v_r_a, v_r_b = T.axis.remap("SSRR", [ax2, ax3, ax4, ax5])
                                        T.reads(inverse_local[v_co, v_p, v_vh, v_vw], bgemm[v_r_a, v_r_b, v_co, v_p])
                                        T.writes(inverse_local[v_co, v_p, v_vh, v_vw])
                                        T.block_attr({"schedule_rule": "conv2d_nchw_winograd_inverse"})
                                        inverse_local[v_co, v_p, v_vh, v_vw] = inverse_local[v_co, v_p, v_vh, v_vw] + bgemm[v_r_a, v_r_b, v_co, v_p] * T.Select(v_r_a % T.int64(4) == T.int64(3) and v_vh % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_vh % T.int64(2) == T.int64(0), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_vh % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_vh % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_vh % T.int64(2) == T.int64(1), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_vh % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_vh % T.int64(2) == T.int64(1), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_vh % T.int64(2) == T.int64(0), T.float32(1), T.float32(0))))))))) * T.Select(v_r_b % T.int64(4) == T.int64(3) and v_vw % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_vw % T.int64(2) == T.int64(0), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_vw % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_vw % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_vw % T.int64(2) == T.int64(1), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_vw % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_vw % T.int64(2) == T.int64(1), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_vw % T.int64(2) == T.int64(0), T.float32(1), T.float32(0)))))))))
                for h_1, w_1 in T.grid(T.int64(2), T.int64(2)):
                    with T.block("conv2d_winograd"):
                        v_n = T.axis.spatial(T.int64(1), T.int64(0))
                        v_co = T.axis.spatial(T.int64(128), (n_co_h_0_w_0_fused_0 * T.int64(64) + n_co_h_0_w_0_fused_1) // T.int64(196))
                        v_h = T.axis.spatial(T.int64(28), (n_co_h_0_w_0_fused_0 * T.int64(64) + n_co_h_0_w_0_fused_1) % T.int64(196) // T.int64(14) * T.int64(2) + h_1)
                        v_w = T.axis.spatial(T.int64(28), (n_co_h_0_w_0_fused_0 * T.int64(64) + n_co_h_0_w_0_fused_1) % T.int64(14) * T.int64(2) + w_1)
                        T.reads(inverse_local[v_co, v_n * T.int64(196) + v_h // T.int64(2) * T.int64(14) + v_w // T.int64(2), v_h % T.int64(2), v_w % T.int64(2)], p2[v_n, v_co, v_h, v_w], p3[v_n, v_co, T.int64(0), T.int64(0)])
                        T.writes(T_relu[v_n, v_co, v_h, v_w])
                        T_relu[v_n, v_co, v_h, v_w] = T.max(inverse_local[v_co, v_n * T.int64(196) + v_h // T.int64(2) * T.int64(14) + v_w // T.int64(2), v_h % T.int64(2), v_w % T.int64(2)] + p2[v_n, v_co, v_h, v_w] + p3[v_n, v_co, T.int64(0), T.int64(0)], T.float32(0))
b0 = sch.get_block(name="data_pack", func_name="main")
b1 = sch.get_block(name="bgemm", func_name="main")
b2 = sch.get_block(name="inverse", func_name="main")
b3 = sch.get_block(name="conv2d_winograd", func_name="main")
b4 = sch.get_block(name="T_add", func_name="main")
b5 = sch.get_block(name="T_add_1", func_name="main")
b6 = sch.get_block(name="T_relu", func_name="main")
b7 = sch.get_block(name="root", func_name="main")
b8, b9 = sch.get_producers(block=b2)
sch.compute_inline(block=b9)
b10, = sch.get_consumers(block=b2)
l11, l12, l13, l14 = sch.get_loops(block=b10)
l15, l16 = sch.split(loop=l13, factors=[None, 2], preserve_unit_iters=True)
l17, l18 = sch.split(loop=l14, factors=[None, 2], preserve_unit_iters=True)
sch.reorder(l15, l17, l16, l18)
sch.compute_at(block=b2, loop=l17, preserve_unit_loops=True, index=-1)
sch.set_scope(block=b2, buffer_index=0, storage_scope="local")
l19, l20, l21, l22, l23, l24, l25, l26, l27, l28 = sch.get_loops(block=b2)
sch.unroll(loop=l25)
sch.unroll(loop=l26)
sch.unroll(loop=l27)
sch.unroll(loop=l28)
b29, b30 = sch.get_producers(block=b0)
sch.compute_inline(block=b30)
b31, = sch.get_producers(block=b29)
l32, l33, l34, l35, l36, l37 = sch.get_loops(block=b0)
sch.reorder(l34, l35, l32, l33, l36, l37)
sch.unroll(loop=l32)
sch.unroll(loop=l33)
sch.unroll(loop=l36)
sch.unroll(loop=l37)
l38 = sch.fuse(l34, l35, preserve_unit_iters=True)
v39 = sch.sample_categorical(candidates=[32, 64, 128, 256, 512, 1024], probs=[0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666], decision=3)
l40, l41 = sch.split(loop=l38, factors=[None, v39], preserve_unit_iters=True)
sch.bind(loop=l40, thread_axis="blockIdx.x")
sch.bind(loop=l41, thread_axis="threadIdx.x")
b42 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b42, loop=l41, preserve_unit_loops=True, index=-1)
sch.compute_at(block=b29, loop=l41, preserve_unit_loops=True, index=-1)
sch.set_scope(block=b29, buffer_index=0, storage_scope="local")
sch.compute_inline(block=b31)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l43, l44, l45, l46, l47 = sch.get_loops(block=b1)
v48, v49, v50, v51, v52 = sch.sample_perfect_tile(loop=l43, n=5, max_innermost_factor=64, decision=[2, 1, 2, 1, 1])
l53, l54, l55, l56, l57 = sch.split(loop=l43, factors=[v48, v49, v50, v51, v52], preserve_unit_iters=True)
v58, v59, v60, v61, v62 = sch.sample_perfect_tile(loop=l44, n=5, max_innermost_factor=64, decision=[1, 1, 1, 2, 2])
l63, l64, l65, l66, l67 = sch.split(loop=l44, factors=[v58, v59, v60, v61, v62], preserve_unit_iters=True)
v68, v69, v70, v71, v72 = sch.sample_perfect_tile(loop=l45, n=5, max_innermost_factor=64, decision=[1, 1, 64, 1, 2])
l73, l74, l75, l76, l77 = sch.split(loop=l45, factors=[v68, v69, v70, v71, v72], preserve_unit_iters=True)
v78, v79, v80, v81, v82 = sch.sample_perfect_tile(loop=l46, n=5, max_innermost_factor=64, decision=[1, 1, 2, 49, 2])
l83, l84, l85, l86, l87 = sch.split(loop=l46, factors=[v78, v79, v80, v81, v82], preserve_unit_iters=True)
v88, v89, v90 = sch.sample_perfect_tile(loop=l47, n=3, max_innermost_factor=64, decision=[32, 2, 2])
l91, l92, l93 = sch.split(loop=l47, factors=[v88, v89, v90], preserve_unit_iters=True)
sch.reorder(l53, l63, l73, l83, l54, l64, l74, l84, l55, l65, l75, l85, l91, l92, l56, l66, l76, l86, l93, l57, l67, l77, l87)
l94 = sch.fuse(l53, l63, l73, l83, preserve_unit_iters=True)
sch.bind(loop=l94, thread_axis="blockIdx.x")
l95 = sch.fuse(l54, l64, l74, l84, preserve_unit_iters=True)
sch.bind(loop=l95, thread_axis="vthread.x")
l96 = sch.fuse(l55, l65, l75, l85, preserve_unit_iters=True)
sch.bind(loop=l96, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b97 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b97, loop=l96, preserve_unit_loops=True, index=-1)
b98 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b98, loop=l91, preserve_unit_loops=True, index=-1)
l99, l100, l101, l102, l103, l104, l105, l106 = sch.get_loops(block=b98)
l107 = sch.fuse(l103, l104, l105, l106, preserve_unit_iters=True)
v108 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b98, ann_key="meta_schedule.cooperative_fetch", ann_val=v108)
b109 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b109, loop=l91, preserve_unit_loops=True, index=-1)
l110, l111, l112, l113, l114, l115, l116, l117 = sch.get_loops(block=b109)
l118 = sch.fuse(l114, l115, l116, l117, preserve_unit_iters=True)
v119 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b109, ann_key="meta_schedule.cooperative_fetch", ann_val=v119)
sch.reverse_compute_inline(block=b6)
sch.reverse_compute_inline(block=b5)
sch.reverse_compute_inline(block=b4)
v120 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b7, ann_key="meta_schedule.unroll_explicit", ann_val=v120)
l121, l122, l123, l124, l125, l126 = sch.get_loops(block=b3)
l127 = sch.fuse(l121, l122, l123, l124, preserve_unit_iters=True)
v128 = sch.sample_categorical(candidates=[32, 64, 128, 256, 512, 1024], probs=[0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666], decision=1)
l129, l130 = sch.split(loop=l127, factors=[None, v128], preserve_unit_iters=True)
sch.bind(loop=l129, thread_axis="blockIdx.x")
sch.bind(loop=l130, thread_axis="threadIdx.x")
sch.enter_postproc()
sch.unannotate(block_or_loop=b98, ann_key="meta_schedule.cooperative_fetch")
l131, l132, l133, l134, l135 = sch.get_loops(block=b98)
l136, l137, l138 = sch.split(loop=l135, factors=[None, 256, 2], preserve_unit_iters=True)
sch.vectorize(loop=l138)
sch.bind(loop=l137, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b109, ann_key="meta_schedule.cooperative_fetch")
l139, l140, l141, l142, l143 = sch.get_loops(block=b109)
l144, l145, l146 = sch.split(loop=l143, factors=[None, 256, 2], preserve_unit_iters=True)
sch.vectorize(loop=l146)
sch.bind(loop=l145, thread_axis="threadIdx.x")
b147 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b147, ann_key="meta_schedule.unroll_explicit")
b148, b149, b150, b151, b152, b153, b154, b155, b156 = sch.get_child_blocks(b147)
l157, l158, l159, l160, l161, l162 = sch.get_loops(block=b148)
l163, l164, l165, l166, l167, l168 = sch.get_loops(block=b149)
sch.annotate(block_or_loop=l163, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l163, ann_key="pragma_unroll_explicit", ann_val=1)
l169, l170, l171, l172, l173, l174 = sch.get_loops(block=b150)
l175, l176, l177, l178, l179, l180, l181 = sch.get_loops(block=b151)
l182, l183, l184, l185, l186, l187, l188 = sch.get_loops(block=b152)
l189, l190, l191, l192, l193, l194, l195, l196, l197, l198, l199, l200, l201, l202 = sch.get_loops(block=b153)
sch.annotate(block_or_loop=l189, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l189, ann_key="pragma_unroll_explicit", ann_val=1)
l203, l204, l205, l206, l207, l208, l209 = sch.get_loops(block=b154)
l210, l211, l212, l213, l214, l215, l216, l217 = sch.get_loops(block=b155)
sch.annotate(block_or_loop=l210, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l210, ann_key="pragma_unroll_explicit", ann_val=1)
l218, l219, l220, l221 = sch.get_loops(block=b156)
b222 = sch.get_block(name="data_pack", func_name="main")
l223, l224, l225, l226, l227, l228 = sch.get_loops(block=b222)
b229 = sch.decompose_reduction(block=b222, loop=l227)
b230 = sch.get_block(name="bgemm", func_name="main")
l231, l232, l233, l234, l235, l236, l237, l238, l239, l240, l241, l242, l243, l244 = sch.get_loops(block=b230)
b245 = sch.decompose_reduction(block=b230, loop=l234)
b246 = sch.get_block(name="inverse", func_name="main")
l247, l248, l249, l250, l251, l252, l253, l254 = sch.get_loops(block=b246)
b255 = sch.decompose_reduction(block=b246, loop=l253)
2023-11-11 03:22:51 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #267: GFLOPs: 67.5767. Time: 1881.5052 us. Best GFLOPs: 5065.9586
2023-11-11 03:22:51 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #268: GFLOPs: 67.9651. Time: 1870.7532 us. Best GFLOPs: 5065.9586
2023-11-11 03:22:51 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #269: GFLOPs: 61.3184. Time: 2073.5372 us. Best GFLOPs: 5065.9586
2023-11-11 03:22:51 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #270: GFLOPs: 61.3122. Time: 2073.7463 us. Best GFLOPs: 5065.9586
2023-11-11 03:22:51 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #271: GFLOPs: 4248.9734. Time: 29.9239 us. Best GFLOPs: 5065.9586
2023-11-11 03:22:51 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #272: GFLOPs: 4949.3014. Time: 25.6897 us. Best GFLOPs: 5065.9586
2023-11-11 03:22:51 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #273: GFLOPs: 4953.4330. Time: 25.6683 us. Best GFLOPs: 5065.9586
2023-11-11 03:22:51 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #274: GFLOPs: 4956.0031. Time: 25.6549 us. Best GFLOPs: 5065.9586
2023-11-11 03:22:51 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #275: GFLOPs: 4955.7722. Time: 25.6561 us. Best GFLOPs: 5065.9586
2023-11-11 03:22:51 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #276: GFLOPs: 56.5116. Time: 2249.9100 us. Best GFLOPs: 5065.9586
2023-11-11 03:22:51 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #277: GFLOPs: 134.5340. Time: 945.0844 us. Best GFLOPs: 5065.9586
2023-11-11 03:22:51 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #278: GFLOPs: 4507.8020. Time: 28.2058 us. Best GFLOPs: 5065.9586
2023-11-11 03:22:51 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #279: GFLOPs: 4261.8507. Time: 29.8335 us. Best GFLOPs: 5065.9586
2023-11-11 03:22:51 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #280: GFLOPs: 4629.5996. Time: 27.4637 us. Best GFLOPs: 5065.9586
2023-11-11 03:22:51 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #281: GFLOPs: 61.3413. Time: 2072.7640 us. Best GFLOPs: 5065.9586
2023-11-11 03:22:51 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #282: GFLOPs: 4544.1191. Time: 27.9803 us. Best GFLOPs: 5065.9586
2023-11-11 03:22:51 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #283: GFLOPs: 4862.7070. Time: 26.1472 us. Best GFLOPs: 5065.9586
2023-11-11 03:22:51 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #284: GFLOPs: 4883.7859. Time: 26.0343 us. Best GFLOPs: 5065.9586
2023-11-11 03:22:51 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #285: GFLOPs: 37.8751. Time: 3356.9791 us. Best GFLOPs: 5065.9586
2023-11-11 03:22:51 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #286: GFLOPs: 4933.4725. Time: 25.7721 us. Best GFLOPs: 5065.9586
2023-11-11 03:22:51 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #287: GFLOPs: 2708.2796. Time: 46.9471 us. Best GFLOPs: 5065.9586
2023-11-11 03:22:51 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #288: GFLOPs: 2927.7488. Time: 43.4279 us. Best GFLOPs: 5065.9586
2023-11-11 03:22:51 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #289: GFLOPs: 2856.5784. Time: 44.5099 us. Best GFLOPs: 5065.9586
2023-11-11 03:22:51 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #290: GFLOPs: 60.7726. Time: 2092.1601 us. Best GFLOPs: 5065.9586
2023-11-11 03:22:51 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #291: GFLOPs: 49.1597. Time: 2586.3876 us. Best GFLOPs: 5065.9586
2023-11-11 03:22:51 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #292: GFLOPs: 4908.9532. Time: 25.9008 us. Best GFLOPs: 5065.9586
2023-11-11 03:22:51 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #293: GFLOPs: 61.2351. Time: 2076.3586 us. Best GFLOPs: 5065.9586
2023-11-11 03:22:51 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #294: GFLOPs: 2935.6672. Time: 43.3108 us. Best GFLOPs: 5065.9586
2023-11-11 03:22:51 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #295: GFLOPs: 4814.7596. Time: 26.4075 us. Best GFLOPs: 5065.9586
2023-11-11 03:22:51 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #296: GFLOPs: 61.2468. Time: 2075.9615 us. Best GFLOPs: 5065.9586
2023-11-11 03:22:51 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #297: GFLOPs: 3821.9582. Time: 33.2672 us. Best GFLOPs: 5065.9586
2023-11-11 03:22:51 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #298: GFLOPs: 61.5087. Time: 2067.1217 us. Best GFLOPs: 5065.9586
2023-11-11 03:22:51 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #299: GFLOPs: 3822.1588. Time: 33.2655 us. Best GFLOPs: 5065.9586
2023-11-11 03:22:51 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #300: GFLOPs: 3374.3140. Time: 37.6805 us. Best GFLOPs: 5065.9586
2023-11-11 03:22:51 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #301: GFLOPs: 1742.4820. Time: 72.9683 us. Best GFLOPs: 5065.9586
2023-11-11 03:22:51 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #302: GFLOPs: 4707.7039. Time: 27.0081 us. Best GFLOPs: 5065.9586
2023-11-11 03:22:51 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #303: GFLOPs: 68.3580. Time: 1860.0012 us. Best GFLOPs: 5065.9586
2023-11-11 03:22:51 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #304: GFLOPs: 69.9190. Time: 1818.4751 us. Best GFLOPs: 5065.9586
2023-11-11 03:22:51 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #305: GFLOPs: 68.5832. Time: 1853.8951 us. Best GFLOPs: 5065.9586
2023-11-11 03:22:51 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #306: GFLOPs: 3362.7833. Time: 37.8097 us. Best GFLOPs: 5065.9586
2023-11-11 03:22:51 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #307: GFLOPs: 68.1420. Time: 1865.8987 us. Best GFLOPs: 5065.9586
2023-11-11 03:22:51 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #308: GFLOPs: 2607.4193. Time: 48.7632 us. Best GFLOPs: 5065.9586
2023-11-11 03:22:51 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #309: GFLOPs: 68.1032. Time: 1866.9606 us. Best GFLOPs: 5065.9586
2023-11-11 03:22:51 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #310: GFLOPs: 69.6637. Time: 1825.1404 us. Best GFLOPs: 5065.9586
2023-11-11 03:22:51 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #311: GFLOPs: 1638.2991. Time: 77.6085 us. Best GFLOPs: 5065.9586
2023-11-11 03:22:51 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #312: GFLOPs: 28.8263. Time: 4410.7686 us. Best GFLOPs: 5065.9586
2023-11-11 03:22:51 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #313: GFLOPs: 5114.6288. Time: 24.8593 us. Best GFLOPs: 5114.6288
2023-11-11 03:22:51 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #314: GFLOPs: 67.1140. Time: 1894.4773 us. Best GFLOPs: 5114.6288
2023-11-11 03:22:51 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #315: GFLOPs: 28.8863. Time: 4401.5970 us. Best GFLOPs: 5114.6288
2023-11-11 03:22:51 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #316: GFLOPs: 66.5050. Time: 1911.8274 us. Best GFLOPs: 5114.6288
2023-11-11 03:22:51 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #317: GFLOPs: 38.9464. Time: 3264.6440 us. Best GFLOPs: 5114.6288
2023-11-11 03:22:51 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #318: GFLOPs: 2092.3468. Time: 60.7672 us. Best GFLOPs: 5114.6288
2023-11-11 03:22:51 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #319: GFLOPs: 17.7663. Time: 7156.5994 us. Best GFLOPs: 5114.6288
2023-11-11 03:22:51 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #320: GFLOPs: 65.4240. Time: 1943.4141 us. Best GFLOPs: 5114.6288
2023-11-11 04:27:25 [INFO] [evolutionary_search.cc:713] Generating candidates......
2023-11-11 04:27:29 [INFO] [evolutionary_search.cc:715] Picked top 102 candidate(s) from database
2023-11-11 04:27:35 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310fb046d8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310f554558)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310dc5ddb8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310bbcd2d8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56310b1dbf38)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310dc5d048)]: 399 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310dc26e38)]: 0 failure(s)
2023-11-11 04:27:41 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310fb046d8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310f554558)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310dc5ddb8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310bbcd2d8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56310b1dbf38)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310dc5d048)]: 796 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310dc26e38)]: 0 failure(s)
2023-11-11 04:27:48 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310fb046d8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310f554558)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310dc5ddb8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310bbcd2d8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56310b1dbf38)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310dc5d048)]: 1187 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310dc26e38)]: 0 failure(s)
2023-11-11 04:27:55 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310fb046d8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310f554558)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310dc5ddb8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310bbcd2d8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56310b1dbf38)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310dc5d048)]: 1584 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310dc26e38)]: 0 failure(s)
2023-11-11 04:27:55 [INFO] [evolutionary_search.cc:723] Sampled 56 candidate(s)
2023-11-11 04:28:15 [INFO] [evolutionary_search.cc:621] Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310fb046d8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310f554558)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310dc5ddb8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310bbcd2d8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56310b1dbf38)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310dc5d048)]: 126 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310dc26e38)]: 0 failure(s)
2023-11-11 04:28:38 [INFO] [evolutionary_search.cc:621] Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310fb046d8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310f554558)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310dc5ddb8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310bbcd2d8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56310b1dbf38)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310dc5d048)]: 81 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310dc26e38)]: 0 failure(s)
2023-11-11 04:29:02 [INFO] [evolutionary_search.cc:621] Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310fb046d8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310f554558)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310dc5ddb8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310bbcd2d8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56310b1dbf38)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310dc5d048)]: 126 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310dc26e38)]: 0 failure(s)
2023-11-11 04:29:26 [INFO] [evolutionary_search.cc:621] Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310fb046d8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310f554558)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310dc5ddb8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310bbcd2d8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56310b1dbf38)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310dc5d048)]: 113 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310dc26e38)]: 0 failure(s)
2023-11-11 04:29:33 [INFO] [evolutionary_search.cc:649] Scores of the best 64 candidates:
[1 : 16]:	1.0057  0.9954  0.9895  0.9845  0.9821  0.9809  0.9808  0.9792  0.9785  0.9785  0.9785  0.9783  0.9766  0.9761  0.9757  0.9756
[17 : 32]:	0.9748  0.9717  0.9696  0.9673  0.9647  0.9588  0.9564  0.9547  0.9547  0.9546  0.9540  0.9540  0.9535  0.9535  0.9530  0.9529
[33 : 48]:	0.9529  0.9491  0.9485  0.9479  0.9474  0.9468  0.9468  0.9461  0.9448  0.9447  0.9440  0.9436  0.9425  0.9424  0.9410  0.9409
[49 : 64]:	0.9394  0.9386  0.9361  0.9358  0.9356  0.9352  0.9348  0.9346  0.9329  0.9326  0.9309  0.9308  0.9307  0.9304  0.9301  0.9299
2023-11-11 04:29:33 [INFO] [evolutionary_search.cc:727] Got 64 candidate(s) with evolutionary search
2023-11-11 04:29:33 [INFO] [evolutionary_search.cc:730] Sending 64 candidates(s) for measurement
2023-11-11 04:30:03 [INFO] [task_scheduler.cc:121] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #321: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/home2/xiachunwei/Software/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/home2/xiachunwei/Software/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/home2/xiachunwei/Software/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/home2/xiachunwei/Software/tvm/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/home2/xiachunwei/Software/tvm/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/home2/xiachunwei/Software/tvm/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/home2/xiachunwei/Software/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/home2/xiachunwei/Software/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  4: _ZN3tvm7runtime13PackedFun
  3: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  2: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  0: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  File "/home2/xiachunwei/Software/tvm/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(128), T.int64(28), T.int64(28)), "float32"), p1: T.Buffer((T.int64(4), T.int64(4), T.int64(128), T.int64(128)), "float32"), p2: T.Buffer((T.int64(1), T.int64(128), T.int64(28), T.int64(28)), "float32"), p3: T.Buffer((T.int64(1), T.int64(128), T.int64(1), T.int64(1)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(128), T.int64(28), T.int64(28)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        input_tile_local = T.alloc_buffer((T.int64(128), T.int64(196), T.int64(4), T.int64(4)), scope="local")
        data_pack = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(128), T.int64(196)))
        bgemm = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(128), T.int64(196)))
        inverse_local = T.alloc_buffer((T.int64(128), T.int64(196), T.int64(2), T.int64(2)), scope="local")
        data_pack_local = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(128), T.int64(196)), scope="local")
        bgemm_local = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(128), T.int64(196)), scope="local")
        data_pack_shared = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(128), T.int64(196)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(128), T.int64(128)), scope="shared")
        for ci_p_fused_0 in T.thread_binding(T.int64(98), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 1024, "pragma_unroll_explicit": 1}):
            for ci_p_fused_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(1), T.int64(4), T.int64(4)):
                    with T.block("input_tile"):
                        v_ci = T.axis.spatial(T.int64(128), (ci_p_fused_0 * T.int64(256) + ci_p_fused_1) // T.int64(196) + ax0)
                        v_p = T.axis.spatial(T.int64(196), (ci_p_fused_0 * T.int64(60) + ci_p_fused_1) % T.int64(196) + ax1)
                        v_eps, v_nu = T.axis.remap("SS", [ax2, ax3])
                        T.reads(p0[v_p // T.int64(196), v_ci, v_p % T.int64(196) // T.int64(14) * T.int64(2) + v_eps - T.int64(1), v_p % T.int64(14) * T.int64(2) + v_nu - T.int64(1)])
                        T.writes(input_tile_local[v_ci, v_p, v_eps, v_nu])
                        T.block_attr({"schedule_rule": "None"})
                        input_tile_local[v_ci, v_p, v_eps, v_nu] = T.if_then_else(T.int64(1) <= v_p % T.int64(196) // T.int64(14) * T.int64(2) + v_eps and v_p % T.int64(196) // T.int64(14) * T.int64(2) + v_eps < T.int64(29) and T.int64(1) <= v_p % T.int64(14) * T.int64(2) + v_nu and v_p % T.int64(14) * T.int64(2) + v_nu < T.int64(29), p0[v_p // T.int64(196), v_ci, v_p % T.int64(196) // T.int64(14) * T.int64(2) + v_eps - T.int64(1), v_p % T.int64(14) * T.int64(2) + v_nu - T.int64(1)], T.float32(0))
                for eps in T.unroll(T.int64(4)):
                    for nu in T.unroll(T.int64(4)):
                        with T.block("data_pack_init"):
                            v_eps, v_nu = T.axis.remap("SS", [eps, nu])
                            v_ci = T.axis.spatial(T.int64(128), (ci_p_fused_0 * T.int64(256) + ci_p_fused_1) // T.int64(196))
                            v_p = T.axis.spatial(T.int64(196), (ci_p_fused_0 * T.int64(256) + ci_p_fused_1) % T.int64(196))
                            T.reads()
                            T.writes(data_pack_local[v_eps, v_nu, v_ci, v_p])
                            T.block_attr({"schedule_rule": "conv2d_nchw_winograd_data_pack"})
                            data_pack_local[v_eps, v_nu, v_ci, v_p] = T.float32(0)
                        for r_a in T.unroll(T.int64(4)):
                            for r_b in T.unroll(T.int64(4)):
                                with T.block("data_pack_update"):
                                    v_eps, v_nu = T.axis.remap("SS", [eps, nu])
                                    v_ci = T.axis.spatial(T.int64(128), (ci_p_fused_0 * T.int64(256) + ci_p_fused_1) // T.int64(196))
                                    v_p = T.axis.spatial(T.int64(196), (ci_p_fused_0 * T.int64(256) + ci_p_fused_1) % T.int64(196))
                                    v_r_a, v_r_b = T.axis.remap("RR", [r_a, r_b])
                                    T.reads(data_pack_local[v_eps, v_nu, v_ci, v_p], input_tile_local[v_ci, v_p, v_r_a, v_r_b])
                                    T.writes(data_pack_local[v_eps, v_nu, v_ci, v_p])
                                    T.block_attr({"schedule_rule": "conv2d_nchw_winograd_data_pack"})
                                    data_pack_local[v_eps, v_nu, v_ci, v_p] = data_pack_local[v_eps, v_nu, v_ci, v_p] + input_tile_local[v_ci, v_p, v_r_a, v_r_b] * T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(3), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(1), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(0), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(3), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(1), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(0), T.float32(1), T.float32(0))))))))))))))))) * T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(3), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(1), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(0), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(3), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(1), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(0), T.float32(1), T.float32(0)))))))))))))))))
                for ax0, ax1, ax2, ax3 in T.grid(T.int64(4), T.int64(4), T.int64(1), T.int64(1)):
                    with T.block("data_pack_local"):
                        v0, v1 = T.axis.remap("SS", [ax0, ax1])
                        v2 = T.axis.spatial(T.int64(128), (ci_p_fused_0 * T.int64(256) + ci_p_fused_1) // T.int64(196) + ax2)
                        v3 = T.axis.spatial(T.int64(196), (ci_p_fused_0 * T.int64(60) + ci_p_fused_1) % T.int64(196) + ax3)
                        T.reads(data_pack_local[v0, v1, v2, v3])
                        T.writes(data_pack[v0, v1, v2, v3])
                        data_pack[v0, v1, v2, v3] = data_pack_local[v0, v1, v2, v3]
        for eps_0_nu_0_co_0_p_0_fused in T.thread_binding(T.int64(224), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 1024, "pragma_unroll_explicit": 1}):
            for eps_1_nu_1_co_1_p_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for eps_2_nu_2_co_2_p_2_fused in T.thread_binding(T.int64(224), thread="threadIdx.x"):
                    for eps_3_init, nu_3_init, co_3_init, p_3_init, eps_4_init, nu_4_init, co_4_init, p_4_init in T.grid(T.int64(1), T.int64(1), T.int64(4), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(2)):
                        with T.block("bgemm_init"):
                            v_eps = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(56) + eps_3_init + eps_4_init)
                            v_nu = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused % T.int64(56) // T.int64(14) + nu_3_init + nu_4_init)
                            v_co = T.axis.spatial(T.int64(128), eps_0_nu_0_co_0_p_0_fused % T.int64(14) // T.int64(7) * T.int64(64) + eps_2_nu_2_co_2_p_2_fused // T.int64(14) * T.int64(4) + co_3_init + co_4_init)
                            v_p = T.axis.spatial(T.int64(196), eps_0_nu_0_co_0_p_0_fused % T.int64(7) * T.int64(28) + eps_2_nu_2_co_2_p_2_fused % T.int64(14) * T.int64(2) + p_3_init * T.int64(2) + p_4_init)
                            T.reads()
                            T.writes(bgemm_local[v_eps, v_nu, v_co, v_p])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            bgemm_local[v_eps, v_nu, v_co, v_p] = T.float32(0)
                    for ci_0 in range(T.int64(1)):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(16)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(224), thread="threadIdx.x"):
                                with T.block("data_pack_shared"):
                                    v0 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(56))
                                    v1 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused % T.int64(56) // T.int64(14))
                                    v2 = T.axis.spatial(T.int64(128), (ax0_ax1_ax2_ax3_fused_0 * T.int64(224) + ax0_ax1_ax2_ax3_fused_1) // T.int64(28))
                                    v3 = T.axis.spatial(T.int64(196), eps_0_nu_0_co_0_p_0_fused % T.int64(7) * T.int64(28) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(224) + ax0_ax1_ax2_ax3_fused_1) % T.int64(28))
                                    T.reads(data_pack[v0, v1, v2, v3])
                                    T.writes(data_pack_shared[v0, v1, v2, v3])
                                    data_pack_shared[v0, v1, v2, v3] = data_pack[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(37)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(224), thread="threadIdx.x"):
                                with T.block("p1_shared"):
                                    v0 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(56))
                                    v1 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused % T.int64(56) // T.int64(14))
                                    v2 = T.axis.spatial(T.int64(128), (ax0_ax1_ax2_ax3_fused_0 * T.int64(224) + ax0_ax1_ax2_ax3_fused_1) // T.int64(64))
                                    v3 = T.axis.spatial(T.int64(128), eps_0_nu_0_co_0_p_0_fused % T.int64(14) // T.int64(7) * T.int64(64) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(224) + ax0_ax1_ax2_ax3_fused_1) % T.int64(64))
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * T.int64(224) + ax0_ax1_ax2_ax3_fused_1 < T.int64(8192))
                                    T.reads(p1[v0, v1, v2, v3])
                                    T.writes(p1_shared[v0, v1, v2, v3])
                                    p1_shared[v0, v1, v2, v3] = p1[v0, v1, v2, v3]
                        for ci_1, eps_3, nu_3, co_3, p_3, ci_2, eps_4, nu_4, co_4, p_4 in T.grid(T.int64(128), T.int64(1), T.int64(1), T.int64(4), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(2)):
                            with T.block("bgemm_update"):
                                v_eps = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(56) + eps_3 + eps_4)
                                v_nu = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused % T.int64(56) // T.int64(14) + nu_3 + nu_4)
                                v_co = T.axis.spatial(T.int64(128), eps_0_nu_0_co_0_p_0_fused % T.int64(14) // T.int64(7) * T.int64(64) + eps_2_nu_2_co_2_p_2_fused // T.int64(14) * T.int64(4) + co_3 + co_4)
                                v_p = T.axis.spatial(T.int64(196), eps_0_nu_0_co_0_p_0_fused % T.int64(7) * T.int64(28) + eps_2_nu_2_co_2_p_2_fused % T.int64(14) * T.int64(2) + p_3 * T.int64(2) + p_4)
                                v_ci = T.axis.reduce(T.int64(128), ci_0 * T.int64(128) + ci_1 + ci_2)
                                T.reads(bgemm_local[v_eps, v_nu, v_co, v_p], data_pack_shared[v_eps, v_nu, v_ci, v_p], p1_shared[v_eps, v_nu, v_ci, v_co])
                                T.writes(bgemm_local[v_eps, v_nu, v_co, v_p])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                bgemm_local[v_eps, v_nu, v_co, v_p] = bgemm_local[v_eps, v_nu, v_co, v_p] + data_pack_shared[v_eps, v_nu, v_ci, v_p] * p1_shared[v_eps, v_nu, v_ci, v_co]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(1), T.int64(4), T.int64(2)):
                        with T.block("bgemm_local"):
                            v0 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(56) + ax0)
                            v1 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused % T.int64(56) // T.int64(14) + ax1)
                            v2 = T.axis.spatial(T.int64(128), eps_0_nu_0_co_0_p_0_fused % T.int64(14) // T.int64(7) * T.int64(64) + eps_2_nu_2_co_2_p_2_fused // T.int64(14) * T.int64(4) + ax2)
                            v3 = T.axis.spatial(T.int64(196), eps_0_nu_0_co_0_p_0_fused % T.int64(7) * T.int64(28) + eps_2_nu_2_co_2_p_2_fused % T.int64(14) * T.int64(2) + ax3)
                            T.reads(bgemm_local[v0, v1, v2, v3])
                            T.writes(bgemm[v0, v1, v2, v3])
                            bgemm[v0, v1, v2, v3] = bgemm_local[v0, v1, v2, v3]
        for n_co_h_0_w_0_fused_0 in T.thread_binding(T.int64(25), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 1024, "pragma_unroll_explicit": 1}):
            for n_co_h_0_w_0_fused_1 in T.thread_binding(T.int64(1024), thread="threadIdx.x"):
                for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                    for ax2 in T.unroll(T.int64(2)):
                        for ax3 in T.unroll(T.int64(2)):
                            with T.block("inverse_init"):
                                v_co = T.axis.spatial(T.int64(128), (n_co_h_0_w_0_fused_0 * T.int64(1024) + n_co_h_0_w_0_fused_1) // T.int64(196) + ax0)
                                v_p = T.axis.spatial(T.int64(196), (n_co_h_0_w_0_fused_0 * T.int64(1024) + n_co_h_0_w_0_fused_1) % T.int64(196) + ax1)
                                v_vh, v_vw = T.axis.remap("SS", [ax2, ax3])
                                T.where(n_co_h_0_w_0_fused_0 * T.int64(1024) + n_co_h_0_w_0_fused_1 < T.int64(25088))
                                T.reads()
                                T.writes(inverse_local[v_co, v_p, v_vh, v_vw])
                                T.block_attr({"schedule_rule": "conv2d_nchw_winograd_inverse"})
                                inverse_local[v_co, v_p, v_vh, v_vw] = T.float32(0)
                            for ax4 in T.unroll(T.int64(4)):
                                for ax5 in T.unroll(T.int64(4)):
                                    with T.block("inverse_update"):
                                        v_co = T.axis.spatial(T.int64(128), (n_co_h_0_w_0_fused_0 * T.int64(1024) + n_co_h_0_w_0_fused_1) // T.int64(196) + ax0)
                                        v_p = T.axis.spatial(T.int64(196), (n_co_h_0_w_0_fused_0 * T.int64(1024) + n_co_h_0_w_0_fused_1) % T.int64(196) + ax1)
                                        v_vh, v_vw, v_r_a, v_r_b = T.axis.remap("SSRR", [ax2, ax3, ax4, ax5])
                                        T.where(n_co_h_0_w_0_fused_0 * T.int64(1024) + n_co_h_0_w_0_fused_1 < T.int64(25088))
                                        T.reads(inverse_local[v_co, v_p, v_vh, v_vw], bgemm[v_r_a, v_r_b, v_co, v_p])
                                        T.writes(inverse_local[v_co, v_p, v_vh, v_vw])
                                        T.block_attr({"schedule_rule": "conv2d_nchw_winograd_inverse"})
                                        inverse_local[v_co, v_p, v_vh, v_vw] = inverse_local[v_co, v_p, v_vh, v_vw] + bgemm[v_r_a, v_r_b, v_co, v_p] * T.Select(v_r_a % T.int64(4) == T.int64(3) and v_vh % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_vh % T.int64(2) == T.int64(0), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_vh % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_vh % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_vh % T.int64(2) == T.int64(1), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_vh % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_vh % T.int64(2) == T.int64(1), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_vh % T.int64(2) == T.int64(0), T.float32(1), T.float32(0))))))))) * T.Select(v_r_b % T.int64(4) == T.int64(3) and v_vw % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_vw % T.int64(2) == T.int64(0), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_vw % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_vw % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_vw % T.int64(2) == T.int64(1), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_vw % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_vw % T.int64(2) == T.int64(1), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_vw % T.int64(2) == T.int64(0), T.float32(1), T.float32(0)))))))))
                for h_1, w_1 in T.grid(T.int64(2), T.int64(2)):
                    with T.block("conv2d_winograd"):
                        v_n = T.axis.spatial(T.int64(1), T.int64(0))
                        v_co = T.axis.spatial(T.int64(128), (n_co_h_0_w_0_fused_0 * T.int64(1024) + n_co_h_0_w_0_fused_1) // T.int64(196))
                        v_h = T.axis.spatial(T.int64(28), (n_co_h_0_w_0_fused_0 * T.int64(1024) + n_co_h_0_w_0_fused_1) % T.int64(196) // T.int64(14) * T.int64(2) + h_1)
                        v_w = T.axis.spatial(T.int64(28), (n_co_h_0_w_0_fused_0 * T.int64(1024) + n_co_h_0_w_0_fused_1) % T.int64(14) * T.int64(2) + w_1)
                        T.where(n_co_h_0_w_0_fused_0 * T.int64(1024) + n_co_h_0_w_0_fused_1 < T.int64(25088))
                        T.reads(inverse_local[v_co, v_n * T.int64(196) + v_h // T.int64(2) * T.int64(14) + v_w // T.int64(2), v_h % T.int64(2), v_w % T.int64(2)], p2[v_n, v_co, v_h, v_w], p3[v_n, v_co, T.int64(0), T.int64(0)])
                        T.writes(T_relu[v_n, v_co, v_h, v_w])
                        T_relu[v_n, v_co, v_h, v_w] = T.max(inverse_local[v_co, v_n * T.int64(196) + v_h // T.int64(2) * T.int64(14) + v_w // T.int64(2), v_h % T.int64(2), v_w % T.int64(2)] + p2[v_n, v_co, v_h, v_w] + p3[v_n, v_co, T.int64(0), T.int64(0)], T.float32(0))
b0 = sch.get_block(name="data_pack", func_name="main")
b1 = sch.get_block(name="bgemm", func_name="main")
b2 = sch.get_block(name="inverse", func_name="main")
b3 = sch.get_block(name="conv2d_winograd", func_name="main")
b4 = sch.get_block(name="T_add", func_name="main")
b5 = sch.get_block(name="T_add_1", func_name="main")
b6 = sch.get_block(name="T_relu", func_name="main")
b7 = sch.get_block(name="root", func_name="main")
b8, b9 = sch.get_producers(block=b2)
sch.compute_inline(block=b9)
b10, = sch.get_consumers(block=b2)
l11, l12, l13, l14 = sch.get_loops(block=b10)
l15, l16 = sch.split(loop=l13, factors=[None, 2], preserve_unit_iters=True)
l17, l18 = sch.split(loop=l14, factors=[None, 2], preserve_unit_iters=True)
sch.reorder(l15, l17, l16, l18)
sch.compute_at(block=b2, loop=l17, preserve_unit_loops=True, index=-1)
sch.set_scope(block=b2, buffer_index=0, storage_scope="local")
l19, l20, l21, l22, l23, l24, l25, l26, l27, l28 = sch.get_loops(block=b2)
sch.unroll(loop=l25)
sch.unroll(loop=l26)
sch.unroll(loop=l27)
sch.unroll(loop=l28)
b29, b30 = sch.get_producers(block=b0)
sch.compute_inline(block=b30)
b31, = sch.get_producers(block=b29)
l32, l33, l34, l35, l36, l37 = sch.get_loops(block=b0)
sch.reorder(l34, l35, l32, l33, l36, l37)
sch.unroll(loop=l32)
sch.unroll(loop=l33)
sch.unroll(loop=l36)
sch.unroll(loop=l37)
l38 = sch.fuse(l34, l35, preserve_unit_iters=True)
v39 = sch.sample_categorical(candidates=[32, 64, 128, 256, 512, 1024], probs=[0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666], decision=3)
l40, l41 = sch.split(loop=l38, factors=[None, v39], preserve_unit_iters=True)
sch.bind(loop=l40, thread_axis="blockIdx.x")
sch.bind(loop=l41, thread_axis="threadIdx.x")
b42 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b42, loop=l41, preserve_unit_loops=True, index=-1)
sch.compute_at(block=b29, loop=l41, preserve_unit_loops=True, index=-1)
sch.set_scope(block=b29, buffer_index=0, storage_scope="local")
sch.compute_inline(block=b31)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l43, l44, l45, l46, l47 = sch.get_loops(block=b1)
v48, v49, v50, v51, v52 = sch.sample_perfect_tile(loop=l43, n=5, max_innermost_factor=64, decision=[4, 1, 1, 1, 1])
l53, l54, l55, l56, l57 = sch.split(loop=l43, factors=[v48, v49, v50, v51, v52], preserve_unit_iters=True)
v58, v59, v60, v61, v62 = sch.sample_perfect_tile(loop=l44, n=5, max_innermost_factor=64, decision=[4, 1, 1, 1, 1])
l63, l64, l65, l66, l67 = sch.split(loop=l44, factors=[v58, v59, v60, v61, v62], preserve_unit_iters=True)
v68, v69, v70, v71, v72 = sch.sample_perfect_tile(loop=l45, n=5, max_innermost_factor=64, decision=[2, 1, 16, 4, 1])
l73, l74, l75, l76, l77 = sch.split(loop=l45, factors=[v68, v69, v70, v71, v72], preserve_unit_iters=True)
v78, v79, v80, v81, v82 = sch.sample_perfect_tile(loop=l46, n=5, max_innermost_factor=64, decision=[7, 1, 14, 1, 2])
l83, l84, l85, l86, l87 = sch.split(loop=l46, factors=[v78, v79, v80, v81, v82], preserve_unit_iters=True)
v88, v89, v90 = sch.sample_perfect_tile(loop=l47, n=3, max_innermost_factor=64, decision=[1, 128, 1])
l91, l92, l93 = sch.split(loop=l47, factors=[v88, v89, v90], preserve_unit_iters=True)
sch.reorder(l53, l63, l73, l83, l54, l64, l74, l84, l55, l65, l75, l85, l91, l92, l56, l66, l76, l86, l93, l57, l67, l77, l87)
l94 = sch.fuse(l53, l63, l73, l83, preserve_unit_iters=True)
sch.bind(loop=l94, thread_axis="blockIdx.x")
l95 = sch.fuse(l54, l64, l74, l84, preserve_unit_iters=True)
sch.bind(loop=l95, thread_axis="vthread.x")
l96 = sch.fuse(l55, l65, l75, l85, preserve_unit_iters=True)
sch.bind(loop=l96, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b97 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b97, loop=l96, preserve_unit_loops=True, index=-1)
b98 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b98, loop=l91, preserve_unit_loops=True, index=-1)
l99, l100, l101, l102, l103, l104, l105, l106 = sch.get_loops(block=b98)
l107 = sch.fuse(l103, l104, l105, l106, preserve_unit_iters=True)
v108 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b98, ann_key="meta_schedule.cooperative_fetch", ann_val=v108)
b109 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b109, loop=l91, preserve_unit_loops=True, index=-1)
l110, l111, l112, l113, l114, l115, l116, l117 = sch.get_loops(block=b109)
l118 = sch.fuse(l114, l115, l116, l117, preserve_unit_iters=True)
v119 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b109, ann_key="meta_schedule.cooperative_fetch", ann_val=v119)
sch.reverse_compute_inline(block=b6)
sch.reverse_compute_inline(block=b5)
sch.reverse_compute_inline(block=b4)
v120 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b7, ann_key="meta_schedule.unroll_explicit", ann_val=v120)
l121, l122, l123, l124, l125, l126 = sch.get_loops(block=b3)
l127 = sch.fuse(l121, l122, l123, l124, preserve_unit_iters=True)
v128 = sch.sample_categorical(candidates=[32, 64, 128, 256, 512, 1024], probs=[0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666], decision=5)
l129, l130 = sch.split(loop=l127, factors=[None, v128], preserve_unit_iters=True)
sch.bind(loop=l129, thread_axis="blockIdx.x")
sch.bind(loop=l130, thread_axis="threadIdx.x")
sch.enter_postproc()
sch.unannotate(block_or_loop=b98, ann_key="meta_schedule.cooperative_fetch")
l131, l132, l133, l134, l135 = sch.get_loops(block=b98)
l136, l137 = sch.split(loop=l135, factors=[None, 224], preserve_unit_iters=True)
sch.bind(loop=l137, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b109, ann_key="meta_schedule.cooperative_fetch")
l138, l139, l140, l141, l142 = sch.get_loops(block=b109)
l143, l144 = sch.split(loop=l142, factors=[None, 224], preserve_unit_iters=True)
sch.bind(loop=l144, thread_axis="threadIdx.x")
b145 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b145, ann_key="meta_schedule.unroll_explicit")
b146, b147, b148, b149, b150, b151, b152, b153, b154 = sch.get_child_blocks(b145)
l155, l156, l157, l158, l159, l160 = sch.get_loops(block=b146)
l161, l162, l163, l164, l165, l166 = sch.get_loops(block=b147)
sch.annotate(block_or_loop=l161, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l161, ann_key="pragma_unroll_explicit", ann_val=1)
l167, l168, l169, l170, l171, l172 = sch.get_loops(block=b148)
l173, l174, l175, l176, l177, l178 = sch.get_loops(block=b149)
l179, l180, l181, l182, l183, l184 = sch.get_loops(block=b150)
l185, l186, l187, l188, l189, l190, l191, l192, l193, l194, l195, l196, l197, l198 = sch.get_loops(block=b151)
sch.annotate(block_or_loop=l185, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l185, ann_key="pragma_unroll_explicit", ann_val=1)
l199, l200, l201, l202, l203, l204, l205 = sch.get_loops(block=b152)
l206, l207, l208, l209, l210, l211, l212, l213 = sch.get_loops(block=b153)
sch.annotate(block_or_loop=l206, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l206, ann_key="pragma_unroll_explicit", ann_val=1)
l214, l215, l216, l217 = sch.get_loops(block=b154)
b218 = sch.get_block(name="data_pack", func_name="main")
l219, l220, l221, l222, l223, l224 = sch.get_loops(block=b218)
b225 = sch.decompose_reduction(block=b218, loop=l223)
b226 = sch.get_block(name="bgemm", func_name="main")
l227, l228, l229, l230, l231, l232, l233, l234, l235, l236, l237, l238, l239, l240 = sch.get_loops(block=b226)
b241 = sch.decompose_reduction(block=b226, loop=l230)
b242 = sch.get_block(name="inverse", func_name="main")
l243, l244, l245, l246, l247, l248, l249, l250 = sch.get_loops(block=b242)
b251 = sch.decompose_reduction(block=b242, loop=l249)
2023-11-11 04:30:03 [INFO] [task_scheduler.cc:121] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #322: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/home2/xiachunwei/Software/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/home2/xiachunwei/Software/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/home2/xiachunwei/Software/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/home2/xiachunwei/Software/tvm/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/home2/xiachunwei/Software/tvm/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/home2/xiachunwei/Software/tvm/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/home2/xiachunwei/Software/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/home2/xiachunwei/Software/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  4: _ZN3tvm7runtime13PackedFun
  3: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  2: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  0: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  File "/home2/xiachunwei/Software/tvm/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(128), T.int64(28), T.int64(28)), "float32"), p1: T.Buffer((T.int64(4), T.int64(4), T.int64(128), T.int64(128)), "float32"), p2: T.Buffer((T.int64(1), T.int64(128), T.int64(28), T.int64(28)), "float32"), p3: T.Buffer((T.int64(1), T.int64(128), T.int64(1), T.int64(1)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(128), T.int64(28), T.int64(28)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        input_tile_local = T.alloc_buffer((T.int64(128), T.int64(196), T.int64(4), T.int64(4)), scope="local")
        data_pack = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(128), T.int64(196)))
        bgemm = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(128), T.int64(196)))
        inverse_local = T.alloc_buffer((T.int64(128), T.int64(196), T.int64(2), T.int64(2)), scope="local")
        data_pack_local = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(128), T.int64(196)), scope="local")
        bgemm_local = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(128), T.int64(196)), scope="local")
        data_pack_shared = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(128), T.int64(196)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(128), T.int64(128)), scope="shared")
        for ci_p_fused_0 in T.thread_binding(T.int64(98), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 1024, "pragma_unroll_explicit": 1}):
            for ci_p_fused_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(1), T.int64(4), T.int64(4)):
                    with T.block("input_tile"):
                        v_ci = T.axis.spatial(T.int64(128), (ci_p_fused_0 * T.int64(256) + ci_p_fused_1) // T.int64(196) + ax0)
                        v_p = T.axis.spatial(T.int64(196), (ci_p_fused_0 * T.int64(60) + ci_p_fused_1) % T.int64(196) + ax1)
                        v_eps, v_nu = T.axis.remap("SS", [ax2, ax3])
                        T.reads(p0[v_p // T.int64(196), v_ci, v_p % T.int64(196) // T.int64(14) * T.int64(2) + v_eps - T.int64(1), v_p % T.int64(14) * T.int64(2) + v_nu - T.int64(1)])
                        T.writes(input_tile_local[v_ci, v_p, v_eps, v_nu])
                        T.block_attr({"schedule_rule": "None"})
                        input_tile_local[v_ci, v_p, v_eps, v_nu] = T.if_then_else(T.int64(1) <= v_p % T.int64(196) // T.int64(14) * T.int64(2) + v_eps and v_p % T.int64(196) // T.int64(14) * T.int64(2) + v_eps < T.int64(29) and T.int64(1) <= v_p % T.int64(14) * T.int64(2) + v_nu and v_p % T.int64(14) * T.int64(2) + v_nu < T.int64(29), p0[v_p // T.int64(196), v_ci, v_p % T.int64(196) // T.int64(14) * T.int64(2) + v_eps - T.int64(1), v_p % T.int64(14) * T.int64(2) + v_nu - T.int64(1)], T.float32(0))
                for eps in T.unroll(T.int64(4)):
                    for nu in T.unroll(T.int64(4)):
                        with T.block("data_pack_init"):
                            v_eps, v_nu = T.axis.remap("SS", [eps, nu])
                            v_ci = T.axis.spatial(T.int64(128), (ci_p_fused_0 * T.int64(256) + ci_p_fused_1) // T.int64(196))
                            v_p = T.axis.spatial(T.int64(196), (ci_p_fused_0 * T.int64(256) + ci_p_fused_1) % T.int64(196))
                            T.reads()
                            T.writes(data_pack_local[v_eps, v_nu, v_ci, v_p])
                            T.block_attr({"schedule_rule": "conv2d_nchw_winograd_data_pack"})
                            data_pack_local[v_eps, v_nu, v_ci, v_p] = T.float32(0)
                        for r_a in T.unroll(T.int64(4)):
                            for r_b in T.unroll(T.int64(4)):
                                with T.block("data_pack_update"):
                                    v_eps, v_nu = T.axis.remap("SS", [eps, nu])
                                    v_ci = T.axis.spatial(T.int64(128), (ci_p_fused_0 * T.int64(256) + ci_p_fused_1) // T.int64(196))
                                    v_p = T.axis.spatial(T.int64(196), (ci_p_fused_0 * T.int64(256) + ci_p_fused_1) % T.int64(196))
                                    v_r_a, v_r_b = T.axis.remap("RR", [r_a, r_b])
                                    T.reads(data_pack_local[v_eps, v_nu, v_ci, v_p], input_tile_local[v_ci, v_p, v_r_a, v_r_b])
                                    T.writes(data_pack_local[v_eps, v_nu, v_ci, v_p])
                                    T.block_attr({"schedule_rule": "conv2d_nchw_winograd_data_pack"})
                                    data_pack_local[v_eps, v_nu, v_ci, v_p] = data_pack_local[v_eps, v_nu, v_ci, v_p] + input_tile_local[v_ci, v_p, v_r_a, v_r_b] * T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(3), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(1), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(0), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(3), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(1), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(0), T.float32(1), T.float32(0))))))))))))))))) * T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(3), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(1), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(0), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(3), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(1), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(0), T.float32(1), T.float32(0)))))))))))))))))
                for ax0, ax1, ax2, ax3 in T.grid(T.int64(4), T.int64(4), T.int64(1), T.int64(1)):
                    with T.block("data_pack_local"):
                        v0, v1 = T.axis.remap("SS", [ax0, ax1])
                        v2 = T.axis.spatial(T.int64(128), (ci_p_fused_0 * T.int64(256) + ci_p_fused_1) // T.int64(196) + ax2)
                        v3 = T.axis.spatial(T.int64(196), (ci_p_fused_0 * T.int64(60) + ci_p_fused_1) % T.int64(196) + ax3)
                        T.reads(data_pack_local[v0, v1, v2, v3])
                        T.writes(data_pack[v0, v1, v2, v3])
                        data_pack[v0, v1, v2, v3] = data_pack_local[v0, v1, v2, v3]
        for eps_0_nu_0_co_0_p_0_fused in T.thread_binding(T.int64(224), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 1024, "pragma_unroll_explicit": 1}):
            for eps_1_nu_1_co_1_p_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for eps_2_nu_2_co_2_p_2_fused in T.thread_binding(T.int64(224), thread="threadIdx.x"):
                    for eps_3_init, nu_3_init, co_3_init, p_3_init, eps_4_init, nu_4_init, co_4_init, p_4_init in T.grid(T.int64(1), T.int64(1), T.int64(4), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(2)):
                        with T.block("bgemm_init"):
                            v_eps = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(56) + eps_3_init + eps_4_init)
                            v_nu = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused % T.int64(56) // T.int64(14) + nu_3_init + nu_4_init)
                            v_co = T.axis.spatial(T.int64(128), eps_0_nu_0_co_0_p_0_fused % T.int64(14) // T.int64(7) * T.int64(64) + eps_2_nu_2_co_2_p_2_fused // T.int64(14) * T.int64(4) + co_3_init + co_4_init)
                            v_p = T.axis.spatial(T.int64(196), eps_0_nu_0_co_0_p_0_fused % T.int64(7) * T.int64(28) + eps_2_nu_2_co_2_p_2_fused % T.int64(14) * T.int64(2) + p_3_init * T.int64(2) + p_4_init)
                            T.reads()
                            T.writes(bgemm_local[v_eps, v_nu, v_co, v_p])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            bgemm_local[v_eps, v_nu, v_co, v_p] = T.float32(0)
                    for ci_0 in range(T.int64(4)):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(2)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(224), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("data_pack_shared"):
                                        v0 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(56))
                                        v1 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused % T.int64(56) // T.int64(14))
                                        v2 = T.axis.spatial(T.int64(128), ci_0 * T.int64(32) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(448) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) // T.int64(28))
                                        v3 = T.axis.spatial(T.int64(196), eps_0_nu_0_co_0_p_0_fused % T.int64(7) * T.int64(28) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(448) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) % T.int64(28))
                                        T.reads(data_pack[v0, v1, v2, v3])
                                        T.writes(data_pack_shared[v0, v1, v2, v3])
                                        data_pack_shared[v0, v1, v2, v3] = data_pack[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(10)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(224), thread="threadIdx.x"):
                                with T.block("p1_shared"):
                                    v0 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(56))
                                    v1 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused % T.int64(56) // T.int64(14))
                                    v2 = T.axis.spatial(T.int64(128), ci_0 * T.int64(32) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(224) + ax0_ax1_ax2_ax3_fused_1) // T.int64(64))
                                    v3 = T.axis.spatial(T.int64(128), eps_0_nu_0_co_0_p_0_fused % T.int64(14) // T.int64(7) * T.int64(64) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(224) + ax0_ax1_ax2_ax3_fused_1) % T.int64(64))
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * T.int64(224) + ax0_ax1_ax2_ax3_fused_1 < T.int64(2048))
                                    T.reads(p1[v0, v1, v2, v3])
                                    T.writes(p1_shared[v0, v1, v2, v3])
                                    p1_shared[v0, v1, v2, v3] = p1[v0, v1, v2, v3]
                        for ci_1, eps_3, nu_3, co_3, p_3, ci_2, eps_4, nu_4, co_4, p_4 in T.grid(T.int64(32), T.int64(1), T.int64(1), T.int64(4), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(2)):
                            with T.block("bgemm_update"):
                                v_eps = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(56) + eps_3 + eps_4)
                                v_nu = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused % T.int64(56) // T.int64(14) + nu_3 + nu_4)
                                v_co = T.axis.spatial(T.int64(128), eps_0_nu_0_co_0_p_0_fused % T.int64(14) // T.int64(7) * T.int64(64) + eps_2_nu_2_co_2_p_2_fused // T.int64(14) * T.int64(4) + co_3 + co_4)
                                v_p = T.axis.spatial(T.int64(196), eps_0_nu_0_co_0_p_0_fused % T.int64(7) * T.int64(28) + eps_2_nu_2_co_2_p_2_fused % T.int64(14) * T.int64(2) + p_3 * T.int64(2) + p_4)
                                v_ci = T.axis.reduce(T.int64(128), ci_0 * T.int64(32) + ci_1 + ci_2)
                                T.reads(bgemm_local[v_eps, v_nu, v_co, v_p], data_pack_shared[v_eps, v_nu, v_ci, v_p], p1_shared[v_eps, v_nu, v_ci, v_co])
                                T.writes(bgemm_local[v_eps, v_nu, v_co, v_p])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                bgemm_local[v_eps, v_nu, v_co, v_p] = bgemm_local[v_eps, v_nu, v_co, v_p] + data_pack_shared[v_eps, v_nu, v_ci, v_p] * p1_shared[v_eps, v_nu, v_ci, v_co]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(1), T.int64(4), T.int64(2)):
                        with T.block("bgemm_local"):
                            v0 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(56) + ax0)
                            v1 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused % T.int64(56) // T.int64(14) + ax1)
                            v2 = T.axis.spatial(T.int64(128), eps_0_nu_0_co_0_p_0_fused % T.int64(14) // T.int64(7) * T.int64(64) + eps_2_nu_2_co_2_p_2_fused // T.int64(14) * T.int64(4) + ax2)
                            v3 = T.axis.spatial(T.int64(196), eps_0_nu_0_co_0_p_0_fused % T.int64(7) * T.int64(28) + eps_2_nu_2_co_2_p_2_fused % T.int64(14) * T.int64(2) + ax3)
                            T.reads(bgemm_local[v0, v1, v2, v3])
                            T.writes(bgemm[v0, v1, v2, v3])
                            bgemm[v0, v1, v2, v3] = bgemm_local[v0, v1, v2, v3]
        for n_co_h_0_w_0_fused_0 in T.thread_binding(T.int64(25), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 1024, "pragma_unroll_explicit": 1}):
            for n_co_h_0_w_0_fused_1 in T.thread_binding(T.int64(1024), thread="threadIdx.x"):
                for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                    for ax2 in T.unroll(T.int64(2)):
                        for ax3 in T.unroll(T.int64(2)):
                            with T.block("inverse_init"):
                                v_co = T.axis.spatial(T.int64(128), (n_co_h_0_w_0_fused_0 * T.int64(1024) + n_co_h_0_w_0_fused_1) // T.int64(196) + ax0)
                                v_p = T.axis.spatial(T.int64(196), (n_co_h_0_w_0_fused_0 * T.int64(1024) + n_co_h_0_w_0_fused_1) % T.int64(196) + ax1)
                                v_vh, v_vw = T.axis.remap("SS", [ax2, ax3])
                                T.where(n_co_h_0_w_0_fused_0 * T.int64(1024) + n_co_h_0_w_0_fused_1 < T.int64(25088))
                                T.reads()
                                T.writes(inverse_local[v_co, v_p, v_vh, v_vw])
                                T.block_attr({"schedule_rule": "conv2d_nchw_winograd_inverse"})
                                inverse_local[v_co, v_p, v_vh, v_vw] = T.float32(0)
                            for ax4 in T.unroll(T.int64(4)):
                                for ax5 in T.unroll(T.int64(4)):
                                    with T.block("inverse_update"):
                                        v_co = T.axis.spatial(T.int64(128), (n_co_h_0_w_0_fused_0 * T.int64(1024) + n_co_h_0_w_0_fused_1) // T.int64(196) + ax0)
                                        v_p = T.axis.spatial(T.int64(196), (n_co_h_0_w_0_fused_0 * T.int64(1024) + n_co_h_0_w_0_fused_1) % T.int64(196) + ax1)
                                        v_vh, v_vw, v_r_a, v_r_b = T.axis.remap("SSRR", [ax2, ax3, ax4, ax5])
                                        T.where(n_co_h_0_w_0_fused_0 * T.int64(1024) + n_co_h_0_w_0_fused_1 < T.int64(25088))
                                        T.reads(inverse_local[v_co, v_p, v_vh, v_vw], bgemm[v_r_a, v_r_b, v_co, v_p])
                                        T.writes(inverse_local[v_co, v_p, v_vh, v_vw])
                                        T.block_attr({"schedule_rule": "conv2d_nchw_winograd_inverse"})
                                        inverse_local[v_co, v_p, v_vh, v_vw] = inverse_local[v_co, v_p, v_vh, v_vw] + bgemm[v_r_a, v_r_b, v_co, v_p] * T.Select(v_r_a % T.int64(4) == T.int64(3) and v_vh % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_vh % T.int64(2) == T.int64(0), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_vh % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_vh % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_vh % T.int64(2) == T.int64(1), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_vh % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_vh % T.int64(2) == T.int64(1), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_vh % T.int64(2) == T.int64(0), T.float32(1), T.float32(0))))))))) * T.Select(v_r_b % T.int64(4) == T.int64(3) and v_vw % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_vw % T.int64(2) == T.int64(0), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_vw % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_vw % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_vw % T.int64(2) == T.int64(1), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_vw % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_vw % T.int64(2) == T.int64(1), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_vw % T.int64(2) == T.int64(0), T.float32(1), T.float32(0)))))))))
                for h_1, w_1 in T.grid(T.int64(2), T.int64(2)):
                    with T.block("conv2d_winograd"):
                        v_n = T.axis.spatial(T.int64(1), T.int64(0))
                        v_co = T.axis.spatial(T.int64(128), (n_co_h_0_w_0_fused_0 * T.int64(1024) + n_co_h_0_w_0_fused_1) // T.int64(196))
                        v_h = T.axis.spatial(T.int64(28), (n_co_h_0_w_0_fused_0 * T.int64(1024) + n_co_h_0_w_0_fused_1) % T.int64(196) // T.int64(14) * T.int64(2) + h_1)
                        v_w = T.axis.spatial(T.int64(28), (n_co_h_0_w_0_fused_0 * T.int64(1024) + n_co_h_0_w_0_fused_1) % T.int64(14) * T.int64(2) + w_1)
                        T.where(n_co_h_0_w_0_fused_0 * T.int64(1024) + n_co_h_0_w_0_fused_1 < T.int64(25088))
                        T.reads(inverse_local[v_co, v_n * T.int64(196) + v_h // T.int64(2) * T.int64(14) + v_w // T.int64(2), v_h % T.int64(2), v_w % T.int64(2)], p2[v_n, v_co, v_h, v_w], p3[v_n, v_co, T.int64(0), T.int64(0)])
                        T.writes(T_relu[v_n, v_co, v_h, v_w])
                        T_relu[v_n, v_co, v_h, v_w] = T.max(inverse_local[v_co, v_n * T.int64(196) + v_h // T.int64(2) * T.int64(14) + v_w // T.int64(2), v_h % T.int64(2), v_w % T.int64(2)] + p2[v_n, v_co, v_h, v_w] + p3[v_n, v_co, T.int64(0), T.int64(0)], T.float32(0))
b0 = sch.get_block(name="data_pack", func_name="main")
b1 = sch.get_block(name="bgemm", func_name="main")
b2 = sch.get_block(name="inverse", func_name="main")
b3 = sch.get_block(name="conv2d_winograd", func_name="main")
b4 = sch.get_block(name="T_add", func_name="main")
b5 = sch.get_block(name="T_add_1", func_name="main")
b6 = sch.get_block(name="T_relu", func_name="main")
b7 = sch.get_block(name="root", func_name="main")
b8, b9 = sch.get_producers(block=b2)
sch.compute_inline(block=b9)
b10, = sch.get_consumers(block=b2)
l11, l12, l13, l14 = sch.get_loops(block=b10)
l15, l16 = sch.split(loop=l13, factors=[None, 2], preserve_unit_iters=True)
l17, l18 = sch.split(loop=l14, factors=[None, 2], preserve_unit_iters=True)
sch.reorder(l15, l17, l16, l18)
sch.compute_at(block=b2, loop=l17, preserve_unit_loops=True, index=-1)
sch.set_scope(block=b2, buffer_index=0, storage_scope="local")
l19, l20, l21, l22, l23, l24, l25, l26, l27, l28 = sch.get_loops(block=b2)
sch.unroll(loop=l25)
sch.unroll(loop=l26)
sch.unroll(loop=l27)
sch.unroll(loop=l28)
b29, b30 = sch.get_producers(block=b0)
sch.compute_inline(block=b30)
b31, = sch.get_producers(block=b29)
l32, l33, l34, l35, l36, l37 = sch.get_loops(block=b0)
sch.reorder(l34, l35, l32, l33, l36, l37)
sch.unroll(loop=l32)
sch.unroll(loop=l33)
sch.unroll(loop=l36)
sch.unroll(loop=l37)
l38 = sch.fuse(l34, l35, preserve_unit_iters=True)
v39 = sch.sample_categorical(candidates=[32, 64, 128, 256, 512, 1024], probs=[0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666], decision=3)
l40, l41 = sch.split(loop=l38, factors=[None, v39], preserve_unit_iters=True)
sch.bind(loop=l40, thread_axis="blockIdx.x")
sch.bind(loop=l41, thread_axis="threadIdx.x")
b42 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b42, loop=l41, preserve_unit_loops=True, index=-1)
sch.compute_at(block=b29, loop=l41, preserve_unit_loops=True, index=-1)
sch.set_scope(block=b29, buffer_index=0, storage_scope="local")
sch.compute_inline(block=b31)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l43, l44, l45, l46, l47 = sch.get_loops(block=b1)
v48, v49, v50, v51, v52 = sch.sample_perfect_tile(loop=l43, n=5, max_innermost_factor=64, decision=[4, 1, 1, 1, 1])
l53, l54, l55, l56, l57 = sch.split(loop=l43, factors=[v48, v49, v50, v51, v52], preserve_unit_iters=True)
v58, v59, v60, v61, v62 = sch.sample_perfect_tile(loop=l44, n=5, max_innermost_factor=64, decision=[4, 1, 1, 1, 1])
l63, l64, l65, l66, l67 = sch.split(loop=l44, factors=[v58, v59, v60, v61, v62], preserve_unit_iters=True)
v68, v69, v70, v71, v72 = sch.sample_perfect_tile(loop=l45, n=5, max_innermost_factor=64, decision=[2, 1, 16, 4, 1])
l73, l74, l75, l76, l77 = sch.split(loop=l45, factors=[v68, v69, v70, v71, v72], preserve_unit_iters=True)
v78, v79, v80, v81, v82 = sch.sample_perfect_tile(loop=l46, n=5, max_innermost_factor=64, decision=[7, 1, 14, 1, 2])
l83, l84, l85, l86, l87 = sch.split(loop=l46, factors=[v78, v79, v80, v81, v82], preserve_unit_iters=True)
v88, v89, v90 = sch.sample_perfect_tile(loop=l47, n=3, max_innermost_factor=64, decision=[4, 32, 1])
l91, l92, l93 = sch.split(loop=l47, factors=[v88, v89, v90], preserve_unit_iters=True)
sch.reorder(l53, l63, l73, l83, l54, l64, l74, l84, l55, l65, l75, l85, l91, l92, l56, l66, l76, l86, l93, l57, l67, l77, l87)
l94 = sch.fuse(l53, l63, l73, l83, preserve_unit_iters=True)
sch.bind(loop=l94, thread_axis="blockIdx.x")
l95 = sch.fuse(l54, l64, l74, l84, preserve_unit_iters=True)
sch.bind(loop=l95, thread_axis="vthread.x")
l96 = sch.fuse(l55, l65, l75, l85, preserve_unit_iters=True)
sch.bind(loop=l96, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b97 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b97, loop=l96, preserve_unit_loops=True, index=-1)
b98 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b98, loop=l91, preserve_unit_loops=True, index=-1)
l99, l100, l101, l102, l103, l104, l105, l106 = sch.get_loops(block=b98)
l107 = sch.fuse(l103, l104, l105, l106, preserve_unit_iters=True)
v108 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b98, ann_key="meta_schedule.cooperative_fetch", ann_val=v108)
b109 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b109, loop=l91, preserve_unit_loops=True, index=-1)
l110, l111, l112, l113, l114, l115, l116, l117 = sch.get_loops(block=b109)
l118 = sch.fuse(l114, l115, l116, l117, preserve_unit_iters=True)
v119 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b109, ann_key="meta_schedule.cooperative_fetch", ann_val=v119)
sch.reverse_compute_inline(block=b6)
sch.reverse_compute_inline(block=b5)
sch.reverse_compute_inline(block=b4)
v120 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b7, ann_key="meta_schedule.unroll_explicit", ann_val=v120)
l121, l122, l123, l124, l125, l126 = sch.get_loops(block=b3)
l127 = sch.fuse(l121, l122, l123, l124, preserve_unit_iters=True)
v128 = sch.sample_categorical(candidates=[32, 64, 128, 256, 512, 1024], probs=[0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666], decision=5)
l129, l130 = sch.split(loop=l127, factors=[None, v128], preserve_unit_iters=True)
sch.bind(loop=l129, thread_axis="blockIdx.x")
sch.bind(loop=l130, thread_axis="threadIdx.x")
sch.enter_postproc()
sch.unannotate(block_or_loop=b98, ann_key="meta_schedule.cooperative_fetch")
l131, l132, l133, l134, l135 = sch.get_loops(block=b98)
l136, l137, l138 = sch.split(loop=l135, factors=[None, 224, 2], preserve_unit_iters=True)
sch.vectorize(loop=l138)
sch.bind(loop=l137, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b109, ann_key="meta_schedule.cooperative_fetch")
l139, l140, l141, l142, l143 = sch.get_loops(block=b109)
l144, l145 = sch.split(loop=l143, factors=[None, 224], preserve_unit_iters=True)
sch.bind(loop=l145, thread_axis="threadIdx.x")
b146 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b146, ann_key="meta_schedule.unroll_explicit")
b147, b148, b149, b150, b151, b152, b153, b154, b155 = sch.get_child_blocks(b146)
l156, l157, l158, l159, l160, l161 = sch.get_loops(block=b147)
l162, l163, l164, l165, l166, l167 = sch.get_loops(block=b148)
sch.annotate(block_or_loop=l162, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l162, ann_key="pragma_unroll_explicit", ann_val=1)
l168, l169, l170, l171, l172, l173 = sch.get_loops(block=b149)
l174, l175, l176, l177, l178, l179, l180 = sch.get_loops(block=b150)
l181, l182, l183, l184, l185, l186 = sch.get_loops(block=b151)
l187, l188, l189, l190, l191, l192, l193, l194, l195, l196, l197, l198, l199, l200 = sch.get_loops(block=b152)
sch.annotate(block_or_loop=l187, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l187, ann_key="pragma_unroll_explicit", ann_val=1)
l201, l202, l203, l204, l205, l206, l207 = sch.get_loops(block=b153)
l208, l209, l210, l211, l212, l213, l214, l215 = sch.get_loops(block=b154)
sch.annotate(block_or_loop=l208, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l208, ann_key="pragma_unroll_explicit", ann_val=1)
l216, l217, l218, l219 = sch.get_loops(block=b155)
b220 = sch.get_block(name="data_pack", func_name="main")
l221, l222, l223, l224, l225, l226 = sch.get_loops(block=b220)
b227 = sch.decompose_reduction(block=b220, loop=l225)
b228 = sch.get_block(name="bgemm", func_name="main")
l229, l230, l231, l232, l233, l234, l235, l236, l237, l238, l239, l240, l241, l242 = sch.get_loops(block=b228)
b243 = sch.decompose_reduction(block=b228, loop=l232)
b244 = sch.get_block(name="inverse", func_name="main")
l245, l246, l247, l248, l249, l250, l251, l252 = sch.get_loops(block=b244)
b253 = sch.decompose_reduction(block=b244, loop=l251)
2023-11-11 04:30:03 [INFO] [task_scheduler.cc:121] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #323: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/home2/xiachunwei/Software/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/home2/xiachunwei/Software/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/home2/xiachunwei/Software/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/home2/xiachunwei/Software/tvm/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/home2/xiachunwei/Software/tvm/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/home2/xiachunwei/Software/tvm/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/home2/xiachunwei/Software/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/home2/xiachunwei/Software/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  4: _ZN3tvm7runtime13PackedFun
  3: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  2: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  0: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  File "/home2/xiachunwei/Software/tvm/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(128), T.int64(28), T.int64(28)), "float32"), p1: T.Buffer((T.int64(4), T.int64(4), T.int64(128), T.int64(128)), "float32"), p2: T.Buffer((T.int64(1), T.int64(128), T.int64(28), T.int64(28)), "float32"), p3: T.Buffer((T.int64(1), T.int64(128), T.int64(1), T.int64(1)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(128), T.int64(28), T.int64(28)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        input_tile_local = T.alloc_buffer((T.int64(128), T.int64(196), T.int64(4), T.int64(4)), scope="local")
        data_pack = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(128), T.int64(196)))
        bgemm = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(128), T.int64(196)))
        inverse_local = T.alloc_buffer((T.int64(128), T.int64(196), T.int64(2), T.int64(2)), scope="local")
        data_pack_local = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(128), T.int64(196)), scope="local")
        bgemm_local = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(128), T.int64(196)), scope="local")
        data_pack_shared = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(128), T.int64(196)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(128), T.int64(128)), scope="shared")
        for ci_p_fused_0 in T.thread_binding(T.int64(98), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 1024, "pragma_unroll_explicit": 1}):
            for ci_p_fused_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(1), T.int64(4), T.int64(4)):
                    with T.block("input_tile"):
                        v_ci = T.axis.spatial(T.int64(128), (ci_p_fused_0 * T.int64(256) + ci_p_fused_1) // T.int64(196) + ax0)
                        v_p = T.axis.spatial(T.int64(196), (ci_p_fused_0 * T.int64(60) + ci_p_fused_1) % T.int64(196) + ax1)
                        v_eps, v_nu = T.axis.remap("SS", [ax2, ax3])
                        T.reads(p0[v_p // T.int64(196), v_ci, v_p % T.int64(196) // T.int64(14) * T.int64(2) + v_eps - T.int64(1), v_p % T.int64(14) * T.int64(2) + v_nu - T.int64(1)])
                        T.writes(input_tile_local[v_ci, v_p, v_eps, v_nu])
                        T.block_attr({"schedule_rule": "None"})
                        input_tile_local[v_ci, v_p, v_eps, v_nu] = T.if_then_else(T.int64(1) <= v_p % T.int64(196) // T.int64(14) * T.int64(2) + v_eps and v_p % T.int64(196) // T.int64(14) * T.int64(2) + v_eps < T.int64(29) and T.int64(1) <= v_p % T.int64(14) * T.int64(2) + v_nu and v_p % T.int64(14) * T.int64(2) + v_nu < T.int64(29), p0[v_p // T.int64(196), v_ci, v_p % T.int64(196) // T.int64(14) * T.int64(2) + v_eps - T.int64(1), v_p % T.int64(14) * T.int64(2) + v_nu - T.int64(1)], T.float32(0))
                for eps in T.unroll(T.int64(4)):
                    for nu in T.unroll(T.int64(4)):
                        with T.block("data_pack_init"):
                            v_eps, v_nu = T.axis.remap("SS", [eps, nu])
                            v_ci = T.axis.spatial(T.int64(128), (ci_p_fused_0 * T.int64(256) + ci_p_fused_1) // T.int64(196))
                            v_p = T.axis.spatial(T.int64(196), (ci_p_fused_0 * T.int64(256) + ci_p_fused_1) % T.int64(196))
                            T.reads()
                            T.writes(data_pack_local[v_eps, v_nu, v_ci, v_p])
                            T.block_attr({"schedule_rule": "conv2d_nchw_winograd_data_pack"})
                            data_pack_local[v_eps, v_nu, v_ci, v_p] = T.float32(0)
                        for r_a in T.unroll(T.int64(4)):
                            for r_b in T.unroll(T.int64(4)):
                                with T.block("data_pack_update"):
                                    v_eps, v_nu = T.axis.remap("SS", [eps, nu])
                                    v_ci = T.axis.spatial(T.int64(128), (ci_p_fused_0 * T.int64(256) + ci_p_fused_1) // T.int64(196))
                                    v_p = T.axis.spatial(T.int64(196), (ci_p_fused_0 * T.int64(256) + ci_p_fused_1) % T.int64(196))
                                    v_r_a, v_r_b = T.axis.remap("RR", [r_a, r_b])
                                    T.reads(data_pack_local[v_eps, v_nu, v_ci, v_p], input_tile_local[v_ci, v_p, v_r_a, v_r_b])
                                    T.writes(data_pack_local[v_eps, v_nu, v_ci, v_p])
                                    T.block_attr({"schedule_rule": "conv2d_nchw_winograd_data_pack"})
                                    data_pack_local[v_eps, v_nu, v_ci, v_p] = data_pack_local[v_eps, v_nu, v_ci, v_p] + input_tile_local[v_ci, v_p, v_r_a, v_r_b] * T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(3), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(1), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(0), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(3), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(1), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(0), T.float32(1), T.float32(0))))))))))))))))) * T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(3), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(1), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(0), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(3), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(1), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(0), T.float32(1), T.float32(0)))))))))))))))))
                for ax0, ax1, ax2, ax3 in T.grid(T.int64(4), T.int64(4), T.int64(1), T.int64(1)):
                    with T.block("data_pack_local"):
                        v0, v1 = T.axis.remap("SS", [ax0, ax1])
                        v2 = T.axis.spatial(T.int64(128), (ci_p_fused_0 * T.int64(256) + ci_p_fused_1) // T.int64(196) + ax2)
                        v3 = T.axis.spatial(T.int64(196), (ci_p_fused_0 * T.int64(60) + ci_p_fused_1) % T.int64(196) + ax3)
                        T.reads(data_pack_local[v0, v1, v2, v3])
                        T.writes(data_pack[v0, v1, v2, v3])
                        data_pack[v0, v1, v2, v3] = data_pack_local[v0, v1, v2, v3]
        for eps_0_nu_0_co_0_p_0_fused in T.thread_binding(T.int64(224), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 1024, "pragma_unroll_explicit": 1}):
            for eps_1_nu_1_co_1_p_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for eps_2_nu_2_co_2_p_2_fused in T.thread_binding(T.int64(224), thread="threadIdx.x"):
                    for eps_3_init, nu_3_init, co_3_init, p_3_init, eps_4_init, nu_4_init, co_4_init, p_4_init in T.grid(T.int64(1), T.int64(1), T.int64(2), T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(2)):
                        with T.block("bgemm_init"):
                            v_eps = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(56) + eps_3_init + eps_4_init)
                            v_nu = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused % T.int64(56) // T.int64(14) + nu_3_init + nu_4_init)
                            v_co = T.axis.spatial(T.int64(128), eps_0_nu_0_co_0_p_0_fused % T.int64(14) // T.int64(7) * T.int64(64) + eps_2_nu_2_co_2_p_2_fused // T.int64(14) * T.int64(4) + co_3_init * T.int64(2) + co_4_init)
                            v_p = T.axis.spatial(T.int64(196), eps_0_nu_0_co_0_p_0_fused % T.int64(7) * T.int64(28) + eps_2_nu_2_co_2_p_2_fused % T.int64(14) * T.int64(2) + p_3_init * T.int64(2) + p_4_init)
                            T.reads()
                            T.writes(bgemm_local[v_eps, v_nu, v_co, v_p])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            bgemm_local[v_eps, v_nu, v_co, v_p] = T.float32(0)
                    for ci_0 in range(T.int64(4)):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(4)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(224), thread="threadIdx.x"):
                                with T.block("data_pack_shared"):
                                    v0 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(56))
                                    v1 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused % T.int64(56) // T.int64(14))
                                    v2 = T.axis.spatial(T.int64(128), ci_0 * T.int64(32) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(224) + ax0_ax1_ax2_ax3_fused_1) // T.int64(28))
                                    v3 = T.axis.spatial(T.int64(196), eps_0_nu_0_co_0_p_0_fused % T.int64(7) * T.int64(28) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(224) + ax0_ax1_ax2_ax3_fused_1) % T.int64(28))
                                    T.reads(data_pack[v0, v1, v2, v3])
                                    T.writes(data_pack_shared[v0, v1, v2, v3])
                                    data_pack_shared[v0, v1, v2, v3] = data_pack[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(10)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(224), thread="threadIdx.x"):
                                with T.block("p1_shared"):
                                    v0 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(56))
                                    v1 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused % T.int64(56) // T.int64(14))
                                    v2 = T.axis.spatial(T.int64(128), ci_0 * T.int64(32) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(224) + ax0_ax1_ax2_ax3_fused_1) // T.int64(64))
                                    v3 = T.axis.spatial(T.int64(128), eps_0_nu_0_co_0_p_0_fused % T.int64(14) // T.int64(7) * T.int64(64) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(224) + ax0_ax1_ax2_ax3_fused_1) % T.int64(64))
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * T.int64(224) + ax0_ax1_ax2_ax3_fused_1 < T.int64(2048))
                                    T.reads(p1[v0, v1, v2, v3])
                                    T.writes(p1_shared[v0, v1, v2, v3])
                                    p1_shared[v0, v1, v2, v3] = p1[v0, v1, v2, v3]
                        for ci_1, eps_3, nu_3, co_3, p_3, ci_2, eps_4, nu_4, co_4, p_4 in T.grid(T.int64(32), T.int64(1), T.int64(1), T.int64(2), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(2)):
                            with T.block("bgemm_update"):
                                v_eps = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(56) + eps_3 + eps_4)
                                v_nu = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused % T.int64(56) // T.int64(14) + nu_3 + nu_4)
                                v_co = T.axis.spatial(T.int64(128), eps_0_nu_0_co_0_p_0_fused % T.int64(14) // T.int64(7) * T.int64(64) + eps_2_nu_2_co_2_p_2_fused // T.int64(14) * T.int64(4) + co_3 * T.int64(2) + co_4)
                                v_p = T.axis.spatial(T.int64(196), eps_0_nu_0_co_0_p_0_fused % T.int64(7) * T.int64(28) + eps_2_nu_2_co_2_p_2_fused % T.int64(14) * T.int64(2) + p_3 * T.int64(2) + p_4)
                                v_ci = T.axis.reduce(T.int64(128), ci_0 * T.int64(32) + ci_1 + ci_2)
                                T.reads(bgemm_local[v_eps, v_nu, v_co, v_p], data_pack_shared[v_eps, v_nu, v_ci, v_p], p1_shared[v_eps, v_nu, v_ci, v_co])
                                T.writes(bgemm_local[v_eps, v_nu, v_co, v_p])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                bgemm_local[v_eps, v_nu, v_co, v_p] = bgemm_local[v_eps, v_nu, v_co, v_p] + data_pack_shared[v_eps, v_nu, v_ci, v_p] * p1_shared[v_eps, v_nu, v_ci, v_co]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(1), T.int64(4), T.int64(2)):
                        with T.block("bgemm_local"):
                            v0 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(56) + ax0)
                            v1 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused % T.int64(56) // T.int64(14) + ax1)
                            v2 = T.axis.spatial(T.int64(128), eps_0_nu_0_co_0_p_0_fused % T.int64(14) // T.int64(7) * T.int64(64) + eps_2_nu_2_co_2_p_2_fused // T.int64(14) * T.int64(4) + ax2)
                            v3 = T.axis.spatial(T.int64(196), eps_0_nu_0_co_0_p_0_fused % T.int64(7) * T.int64(28) + eps_2_nu_2_co_2_p_2_fused % T.int64(14) * T.int64(2) + ax3)
                            T.reads(bgemm_local[v0, v1, v2, v3])
                            T.writes(bgemm[v0, v1, v2, v3])
                            bgemm[v0, v1, v2, v3] = bgemm_local[v0, v1, v2, v3]
        for n_co_h_0_w_0_fused_0 in T.thread_binding(T.int64(25), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 1024, "pragma_unroll_explicit": 1}):
            for n_co_h_0_w_0_fused_1 in T.thread_binding(T.int64(1024), thread="threadIdx.x"):
                for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                    for ax2 in T.unroll(T.int64(2)):
                        for ax3 in T.unroll(T.int64(2)):
                            with T.block("inverse_init"):
                                v_co = T.axis.spatial(T.int64(128), (n_co_h_0_w_0_fused_0 * T.int64(1024) + n_co_h_0_w_0_fused_1) // T.int64(196) + ax0)
                                v_p = T.axis.spatial(T.int64(196), (n_co_h_0_w_0_fused_0 * T.int64(1024) + n_co_h_0_w_0_fused_1) % T.int64(196) + ax1)
                                v_vh, v_vw = T.axis.remap("SS", [ax2, ax3])
                                T.where(n_co_h_0_w_0_fused_0 * T.int64(1024) + n_co_h_0_w_0_fused_1 < T.int64(25088))
                                T.reads()
                                T.writes(inverse_local[v_co, v_p, v_vh, v_vw])
                                T.block_attr({"schedule_rule": "conv2d_nchw_winograd_inverse"})
                                inverse_local[v_co, v_p, v_vh, v_vw] = T.float32(0)
                            for ax4 in T.unroll(T.int64(4)):
                                for ax5 in T.unroll(T.int64(4)):
                                    with T.block("inverse_update"):
                                        v_co = T.axis.spatial(T.int64(128), (n_co_h_0_w_0_fused_0 * T.int64(1024) + n_co_h_0_w_0_fused_1) // T.int64(196) + ax0)
                                        v_p = T.axis.spatial(T.int64(196), (n_co_h_0_w_0_fused_0 * T.int64(1024) + n_co_h_0_w_0_fused_1) % T.int64(196) + ax1)
                                        v_vh, v_vw, v_r_a, v_r_b = T.axis.remap("SSRR", [ax2, ax3, ax4, ax5])
                                        T.where(n_co_h_0_w_0_fused_0 * T.int64(1024) + n_co_h_0_w_0_fused_1 < T.int64(25088))
                                        T.reads(inverse_local[v_co, v_p, v_vh, v_vw], bgemm[v_r_a, v_r_b, v_co, v_p])
                                        T.writes(inverse_local[v_co, v_p, v_vh, v_vw])
                                        T.block_attr({"schedule_rule": "conv2d_nchw_winograd_inverse"})
                                        inverse_local[v_co, v_p, v_vh, v_vw] = inverse_local[v_co, v_p, v_vh, v_vw] + bgemm[v_r_a, v_r_b, v_co, v_p] * T.Select(v_r_a % T.int64(4) == T.int64(3) and v_vh % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_vh % T.int64(2) == T.int64(0), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_vh % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_vh % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_vh % T.int64(2) == T.int64(1), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_vh % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_vh % T.int64(2) == T.int64(1), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_vh % T.int64(2) == T.int64(0), T.float32(1), T.float32(0))))))))) * T.Select(v_r_b % T.int64(4) == T.int64(3) and v_vw % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_vw % T.int64(2) == T.int64(0), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_vw % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_vw % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_vw % T.int64(2) == T.int64(1), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_vw % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_vw % T.int64(2) == T.int64(1), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_vw % T.int64(2) == T.int64(0), T.float32(1), T.float32(0)))))))))
                for h_1, w_1 in T.grid(T.int64(2), T.int64(2)):
                    with T.block("conv2d_winograd"):
                        v_n = T.axis.spatial(T.int64(1), T.int64(0))
                        v_co = T.axis.spatial(T.int64(128), (n_co_h_0_w_0_fused_0 * T.int64(1024) + n_co_h_0_w_0_fused_1) // T.int64(196))
                        v_h = T.axis.spatial(T.int64(28), (n_co_h_0_w_0_fused_0 * T.int64(1024) + n_co_h_0_w_0_fused_1) % T.int64(196) // T.int64(14) * T.int64(2) + h_1)
                        v_w = T.axis.spatial(T.int64(28), (n_co_h_0_w_0_fused_0 * T.int64(1024) + n_co_h_0_w_0_fused_1) % T.int64(14) * T.int64(2) + w_1)
                        T.where(n_co_h_0_w_0_fused_0 * T.int64(1024) + n_co_h_0_w_0_fused_1 < T.int64(25088))
                        T.reads(inverse_local[v_co, v_n * T.int64(196) + v_h // T.int64(2) * T.int64(14) + v_w // T.int64(2), v_h % T.int64(2), v_w % T.int64(2)], p2[v_n, v_co, v_h, v_w], p3[v_n, v_co, T.int64(0), T.int64(0)])
                        T.writes(T_relu[v_n, v_co, v_h, v_w])
                        T_relu[v_n, v_co, v_h, v_w] = T.max(inverse_local[v_co, v_n * T.int64(196) + v_h // T.int64(2) * T.int64(14) + v_w // T.int64(2), v_h % T.int64(2), v_w % T.int64(2)] + p2[v_n, v_co, v_h, v_w] + p3[v_n, v_co, T.int64(0), T.int64(0)], T.float32(0))
b0 = sch.get_block(name="data_pack", func_name="main")
b1 = sch.get_block(name="bgemm", func_name="main")
b2 = sch.get_block(name="inverse", func_name="main")
b3 = sch.get_block(name="conv2d_winograd", func_name="main")
b4 = sch.get_block(name="T_add", func_name="main")
b5 = sch.get_block(name="T_add_1", func_name="main")
b6 = sch.get_block(name="T_relu", func_name="main")
b7 = sch.get_block(name="root", func_name="main")
b8, b9 = sch.get_producers(block=b2)
sch.compute_inline(block=b9)
b10, = sch.get_consumers(block=b2)
l11, l12, l13, l14 = sch.get_loops(block=b10)
l15, l16 = sch.split(loop=l13, factors=[None, 2], preserve_unit_iters=True)
l17, l18 = sch.split(loop=l14, factors=[None, 2], preserve_unit_iters=True)
sch.reorder(l15, l17, l16, l18)
sch.compute_at(block=b2, loop=l17, preserve_unit_loops=True, index=-1)
sch.set_scope(block=b2, buffer_index=0, storage_scope="local")
l19, l20, l21, l22, l23, l24, l25, l26, l27, l28 = sch.get_loops(block=b2)
sch.unroll(loop=l25)
sch.unroll(loop=l26)
sch.unroll(loop=l27)
sch.unroll(loop=l28)
b29, b30 = sch.get_producers(block=b0)
sch.compute_inline(block=b30)
b31, = sch.get_producers(block=b29)
l32, l33, l34, l35, l36, l37 = sch.get_loops(block=b0)
sch.reorder(l34, l35, l32, l33, l36, l37)
sch.unroll(loop=l32)
sch.unroll(loop=l33)
sch.unroll(loop=l36)
sch.unroll(loop=l37)
l38 = sch.fuse(l34, l35, preserve_unit_iters=True)
v39 = sch.sample_categorical(candidates=[32, 64, 128, 256, 512, 1024], probs=[0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666], decision=3)
l40, l41 = sch.split(loop=l38, factors=[None, v39], preserve_unit_iters=True)
sch.bind(loop=l40, thread_axis="blockIdx.x")
sch.bind(loop=l41, thread_axis="threadIdx.x")
b42 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b42, loop=l41, preserve_unit_loops=True, index=-1)
sch.compute_at(block=b29, loop=l41, preserve_unit_loops=True, index=-1)
sch.set_scope(block=b29, buffer_index=0, storage_scope="local")
sch.compute_inline(block=b31)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l43, l44, l45, l46, l47 = sch.get_loops(block=b1)
v48, v49, v50, v51, v52 = sch.sample_perfect_tile(loop=l43, n=5, max_innermost_factor=64, decision=[4, 1, 1, 1, 1])
l53, l54, l55, l56, l57 = sch.split(loop=l43, factors=[v48, v49, v50, v51, v52], preserve_unit_iters=True)
v58, v59, v60, v61, v62 = sch.sample_perfect_tile(loop=l44, n=5, max_innermost_factor=64, decision=[4, 1, 1, 1, 1])
l63, l64, l65, l66, l67 = sch.split(loop=l44, factors=[v58, v59, v60, v61, v62], preserve_unit_iters=True)
v68, v69, v70, v71, v72 = sch.sample_perfect_tile(loop=l45, n=5, max_innermost_factor=64, decision=[2, 1, 16, 2, 2])
l73, l74, l75, l76, l77 = sch.split(loop=l45, factors=[v68, v69, v70, v71, v72], preserve_unit_iters=True)
v78, v79, v80, v81, v82 = sch.sample_perfect_tile(loop=l46, n=5, max_innermost_factor=64, decision=[7, 1, 14, 1, 2])
l83, l84, l85, l86, l87 = sch.split(loop=l46, factors=[v78, v79, v80, v81, v82], preserve_unit_iters=True)
v88, v89, v90 = sch.sample_perfect_tile(loop=l47, n=3, max_innermost_factor=64, decision=[4, 32, 1])
l91, l92, l93 = sch.split(loop=l47, factors=[v88, v89, v90], preserve_unit_iters=True)
sch.reorder(l53, l63, l73, l83, l54, l64, l74, l84, l55, l65, l75, l85, l91, l92, l56, l66, l76, l86, l93, l57, l67, l77, l87)
l94 = sch.fuse(l53, l63, l73, l83, preserve_unit_iters=True)
sch.bind(loop=l94, thread_axis="blockIdx.x")
l95 = sch.fuse(l54, l64, l74, l84, preserve_unit_iters=True)
sch.bind(loop=l95, thread_axis="vthread.x")
l96 = sch.fuse(l55, l65, l75, l85, preserve_unit_iters=True)
sch.bind(loop=l96, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b97 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b97, loop=l96, preserve_unit_loops=True, index=-1)
b98 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b98, loop=l91, preserve_unit_loops=True, index=-1)
l99, l100, l101, l102, l103, l104, l105, l106 = sch.get_loops(block=b98)
l107 = sch.fuse(l103, l104, l105, l106, preserve_unit_iters=True)
v108 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b98, ann_key="meta_schedule.cooperative_fetch", ann_val=v108)
b109 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b109, loop=l91, preserve_unit_loops=True, index=-1)
l110, l111, l112, l113, l114, l115, l116, l117 = sch.get_loops(block=b109)
l118 = sch.fuse(l114, l115, l116, l117, preserve_unit_iters=True)
v119 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b109, ann_key="meta_schedule.cooperative_fetch", ann_val=v119)
sch.reverse_compute_inline(block=b6)
sch.reverse_compute_inline(block=b5)
sch.reverse_compute_inline(block=b4)
v120 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b7, ann_key="meta_schedule.unroll_explicit", ann_val=v120)
l121, l122, l123, l124, l125, l126 = sch.get_loops(block=b3)
l127 = sch.fuse(l121, l122, l123, l124, preserve_unit_iters=True)
v128 = sch.sample_categorical(candidates=[32, 64, 128, 256, 512, 1024], probs=[0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666], decision=5)
l129, l130 = sch.split(loop=l127, factors=[None, v128], preserve_unit_iters=True)
sch.bind(loop=l129, thread_axis="blockIdx.x")
sch.bind(loop=l130, thread_axis="threadIdx.x")
sch.enter_postproc()
sch.unannotate(block_or_loop=b98, ann_key="meta_schedule.cooperative_fetch")
l131, l132, l133, l134, l135 = sch.get_loops(block=b98)
l136, l137 = sch.split(loop=l135, factors=[None, 224], preserve_unit_iters=True)
sch.bind(loop=l137, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b109, ann_key="meta_schedule.cooperative_fetch")
l138, l139, l140, l141, l142 = sch.get_loops(block=b109)
l143, l144 = sch.split(loop=l142, factors=[None, 224], preserve_unit_iters=True)
sch.bind(loop=l144, thread_axis="threadIdx.x")
b145 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b145, ann_key="meta_schedule.unroll_explicit")
b146, b147, b148, b149, b150, b151, b152, b153, b154 = sch.get_child_blocks(b145)
l155, l156, l157, l158, l159, l160 = sch.get_loops(block=b146)
l161, l162, l163, l164, l165, l166 = sch.get_loops(block=b147)
sch.annotate(block_or_loop=l161, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l161, ann_key="pragma_unroll_explicit", ann_val=1)
l167, l168, l169, l170, l171, l172 = sch.get_loops(block=b148)
l173, l174, l175, l176, l177, l178 = sch.get_loops(block=b149)
l179, l180, l181, l182, l183, l184 = sch.get_loops(block=b150)
l185, l186, l187, l188, l189, l190, l191, l192, l193, l194, l195, l196, l197, l198 = sch.get_loops(block=b151)
sch.annotate(block_or_loop=l185, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l185, ann_key="pragma_unroll_explicit", ann_val=1)
l199, l200, l201, l202, l203, l204, l205 = sch.get_loops(block=b152)
l206, l207, l208, l209, l210, l211, l212, l213 = sch.get_loops(block=b153)
sch.annotate(block_or_loop=l206, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l206, ann_key="pragma_unroll_explicit", ann_val=1)
l214, l215, l216, l217 = sch.get_loops(block=b154)
b218 = sch.get_block(name="data_pack", func_name="main")
l219, l220, l221, l222, l223, l224 = sch.get_loops(block=b218)
b225 = sch.decompose_reduction(block=b218, loop=l223)
b226 = sch.get_block(name="bgemm", func_name="main")
l227, l228, l229, l230, l231, l232, l233, l234, l235, l236, l237, l238, l239, l240 = sch.get_loops(block=b226)
b241 = sch.decompose_reduction(block=b226, loop=l230)
b242 = sch.get_block(name="inverse", func_name="main")
l243, l244, l245, l246, l247, l248, l249, l250 = sch.get_loops(block=b242)
b251 = sch.decompose_reduction(block=b242, loop=l249)
2023-11-11 04:30:03 [INFO] [task_scheduler.cc:121] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #324: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/home2/xiachunwei/Software/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/home2/xiachunwei/Software/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/home2/xiachunwei/Software/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/home2/xiachunwei/Software/tvm/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/home2/xiachunwei/Software/tvm/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/home2/xiachunwei/Software/tvm/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/home2/xiachunwei/Software/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/home2/xiachunwei/Software/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  4: _ZN3tvm7runtime13PackedFun
  3: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  2: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  0: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  File "/home2/xiachunwei/Software/tvm/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(128), T.int64(28), T.int64(28)), "float32"), p1: T.Buffer((T.int64(4), T.int64(4), T.int64(128), T.int64(128)), "float32"), p2: T.Buffer((T.int64(1), T.int64(128), T.int64(28), T.int64(28)), "float32"), p3: T.Buffer((T.int64(1), T.int64(128), T.int64(1), T.int64(1)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(128), T.int64(28), T.int64(28)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        input_tile_local = T.alloc_buffer((T.int64(128), T.int64(196), T.int64(4), T.int64(4)), scope="local")
        data_pack = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(128), T.int64(196)))
        bgemm = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(128), T.int64(196)))
        inverse_local = T.alloc_buffer((T.int64(128), T.int64(196), T.int64(2), T.int64(2)), scope="local")
        data_pack_local = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(128), T.int64(196)), scope="local")
        bgemm_local = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(128), T.int64(196)), scope="local")
        data_pack_shared = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(128), T.int64(196)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(128), T.int64(128)), scope="shared")
        for ci_p_fused_0 in T.thread_binding(T.int64(98), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 1024, "pragma_unroll_explicit": 1}):
            for ci_p_fused_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(1), T.int64(4), T.int64(4)):
                    with T.block("input_tile"):
                        v_ci = T.axis.spatial(T.int64(128), (ci_p_fused_0 * T.int64(256) + ci_p_fused_1) // T.int64(196) + ax0)
                        v_p = T.axis.spatial(T.int64(196), (ci_p_fused_0 * T.int64(60) + ci_p_fused_1) % T.int64(196) + ax1)
                        v_eps, v_nu = T.axis.remap("SS", [ax2, ax3])
                        T.reads(p0[v_p // T.int64(196), v_ci, v_p % T.int64(196) // T.int64(14) * T.int64(2) + v_eps - T.int64(1), v_p % T.int64(14) * T.int64(2) + v_nu - T.int64(1)])
                        T.writes(input_tile_local[v_ci, v_p, v_eps, v_nu])
                        T.block_attr({"schedule_rule": "None"})
                        input_tile_local[v_ci, v_p, v_eps, v_nu] = T.if_then_else(T.int64(1) <= v_p % T.int64(196) // T.int64(14) * T.int64(2) + v_eps and v_p % T.int64(196) // T.int64(14) * T.int64(2) + v_eps < T.int64(29) and T.int64(1) <= v_p % T.int64(14) * T.int64(2) + v_nu and v_p % T.int64(14) * T.int64(2) + v_nu < T.int64(29), p0[v_p // T.int64(196), v_ci, v_p % T.int64(196) // T.int64(14) * T.int64(2) + v_eps - T.int64(1), v_p % T.int64(14) * T.int64(2) + v_nu - T.int64(1)], T.float32(0))
                for eps in T.unroll(T.int64(4)):
                    for nu in T.unroll(T.int64(4)):
                        with T.block("data_pack_init"):
                            v_eps, v_nu = T.axis.remap("SS", [eps, nu])
                            v_ci = T.axis.spatial(T.int64(128), (ci_p_fused_0 * T.int64(256) + ci_p_fused_1) // T.int64(196))
                            v_p = T.axis.spatial(T.int64(196), (ci_p_fused_0 * T.int64(256) + ci_p_fused_1) % T.int64(196))
                            T.reads()
                            T.writes(data_pack_local[v_eps, v_nu, v_ci, v_p])
                            T.block_attr({"schedule_rule": "conv2d_nchw_winograd_data_pack"})
                            data_pack_local[v_eps, v_nu, v_ci, v_p] = T.float32(0)
                        for r_a in T.unroll(T.int64(4)):
                            for r_b in T.unroll(T.int64(4)):
                                with T.block("data_pack_update"):
                                    v_eps, v_nu = T.axis.remap("SS", [eps, nu])
                                    v_ci = T.axis.spatial(T.int64(128), (ci_p_fused_0 * T.int64(256) + ci_p_fused_1) // T.int64(196))
                                    v_p = T.axis.spatial(T.int64(196), (ci_p_fused_0 * T.int64(256) + ci_p_fused_1) % T.int64(196))
                                    v_r_a, v_r_b = T.axis.remap("RR", [r_a, r_b])
                                    T.reads(data_pack_local[v_eps, v_nu, v_ci, v_p], input_tile_local[v_ci, v_p, v_r_a, v_r_b])
                                    T.writes(data_pack_local[v_eps, v_nu, v_ci, v_p])
                                    T.block_attr({"schedule_rule": "conv2d_nchw_winograd_data_pack"})
                                    data_pack_local[v_eps, v_nu, v_ci, v_p] = data_pack_local[v_eps, v_nu, v_ci, v_p] + input_tile_local[v_ci, v_p, v_r_a, v_r_b] * T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(3), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(1), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(0), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(3), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(1), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(0), T.float32(1), T.float32(0))))))))))))))))) * T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(3), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(1), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(0), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(3), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(1), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(0), T.float32(1), T.float32(0)))))))))))))))))
                for ax0, ax1, ax2, ax3 in T.grid(T.int64(4), T.int64(4), T.int64(1), T.int64(1)):
                    with T.block("data_pack_local"):
                        v0, v1 = T.axis.remap("SS", [ax0, ax1])
                        v2 = T.axis.spatial(T.int64(128), (ci_p_fused_0 * T.int64(256) + ci_p_fused_1) // T.int64(196) + ax2)
                        v3 = T.axis.spatial(T.int64(196), (ci_p_fused_0 * T.int64(60) + ci_p_fused_1) % T.int64(196) + ax3)
                        T.reads(data_pack_local[v0, v1, v2, v3])
                        T.writes(data_pack[v0, v1, v2, v3])
                        data_pack[v0, v1, v2, v3] = data_pack_local[v0, v1, v2, v3]
        for eps_0_nu_0_co_0_p_0_fused in T.thread_binding(T.int64(224), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 1024, "pragma_unroll_explicit": 1}):
            for eps_1_nu_1_co_1_p_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for eps_2_nu_2_co_2_p_2_fused in T.thread_binding(T.int64(224), thread="threadIdx.x"):
                    for eps_3_init, nu_3_init, co_3_init, p_3_init, eps_4_init, nu_4_init, co_4_init, p_4_init in T.grid(T.int64(1), T.int64(1), T.int64(4), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(2)):
                        with T.block("bgemm_init"):
                            v_eps = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(56) + eps_3_init + eps_4_init)
                            v_nu = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused % T.int64(56) // T.int64(14) + nu_3_init + nu_4_init)
                            v_co = T.axis.spatial(T.int64(128), eps_0_nu_0_co_0_p_0_fused % T.int64(14) // T.int64(7) * T.int64(64) + eps_2_nu_2_co_2_p_2_fused // T.int64(14) * T.int64(4) + co_3_init + co_4_init)
                            v_p = T.axis.spatial(T.int64(196), eps_0_nu_0_co_0_p_0_fused % T.int64(7) * T.int64(28) + eps_2_nu_2_co_2_p_2_fused % T.int64(14) * T.int64(2) + p_3_init * T.int64(2) + p_4_init)
                            T.reads()
                            T.writes(bgemm_local[v_eps, v_nu, v_co, v_p])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            bgemm_local[v_eps, v_nu, v_co, v_p] = T.float32(0)
                    for ci_0 in range(T.int64(4)):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(1)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(224), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("data_pack_shared"):
                                        v0 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(56))
                                        v1 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused % T.int64(56) // T.int64(14))
                                        v2 = T.axis.spatial(T.int64(128), ci_0 * T.int64(32) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(896) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) // T.int64(28))
                                        v3 = T.axis.spatial(T.int64(196), eps_0_nu_0_co_0_p_0_fused % T.int64(7) * T.int64(28) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(896) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) % T.int64(28))
                                        T.reads(data_pack[v0, v1, v2, v3])
                                        T.writes(data_pack_shared[v0, v1, v2, v3])
                                        data_pack_shared[v0, v1, v2, v3] = data_pack[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(3)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(224), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("p1_shared"):
                                        v0 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(56))
                                        v1 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused % T.int64(56) // T.int64(14))
                                        v2 = T.axis.spatial(T.int64(128), ci_0 * T.int64(32) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(896) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) // T.int64(64))
                                        v3 = T.axis.spatial(T.int64(128), eps_0_nu_0_co_0_p_0_fused % T.int64(14) // T.int64(7) * T.int64(64) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(896) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) % T.int64(64))
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * T.int64(224) + ax0_ax1_ax2_ax3_fused_1) * T.int64(4) + ax0_ax1_ax2_ax3_fused_2 < T.int64(2048))
                                        T.reads(p1[v0, v1, v2, v3])
                                        T.writes(p1_shared[v0, v1, v2, v3])
                                        p1_shared[v0, v1, v2, v3] = p1[v0, v1, v2, v3]
                        for ci_1, eps_3, nu_3, co_3, p_3, ci_2, eps_4, nu_4, co_4, p_4 in T.grid(T.int64(32), T.int64(1), T.int64(1), T.int64(4), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(2)):
                            with T.block("bgemm_update"):
                                v_eps = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(56) + eps_3 + eps_4)
                                v_nu = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused % T.int64(56) // T.int64(14) + nu_3 + nu_4)
                                v_co = T.axis.spatial(T.int64(128), eps_0_nu_0_co_0_p_0_fused % T.int64(14) // T.int64(7) * T.int64(64) + eps_2_nu_2_co_2_p_2_fused // T.int64(14) * T.int64(4) + co_3 + co_4)
                                v_p = T.axis.spatial(T.int64(196), eps_0_nu_0_co_0_p_0_fused % T.int64(7) * T.int64(28) + eps_2_nu_2_co_2_p_2_fused % T.int64(14) * T.int64(2) + p_3 * T.int64(2) + p_4)
                                v_ci = T.axis.reduce(T.int64(128), ci_0 * T.int64(32) + ci_1 + ci_2)
                                T.reads(bgemm_local[v_eps, v_nu, v_co, v_p], data_pack_shared[v_eps, v_nu, v_ci, v_p], p1_shared[v_eps, v_nu, v_ci, v_co])
                                T.writes(bgemm_local[v_eps, v_nu, v_co, v_p])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                bgemm_local[v_eps, v_nu, v_co, v_p] = bgemm_local[v_eps, v_nu, v_co, v_p] + data_pack_shared[v_eps, v_nu, v_ci, v_p] * p1_shared[v_eps, v_nu, v_ci, v_co]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(1), T.int64(4), T.int64(2)):
                        with T.block("bgemm_local"):
                            v0 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(56) + ax0)
                            v1 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused % T.int64(56) // T.int64(14) + ax1)
                            v2 = T.axis.spatial(T.int64(128), eps_0_nu_0_co_0_p_0_fused % T.int64(14) // T.int64(7) * T.int64(64) + eps_2_nu_2_co_2_p_2_fused // T.int64(14) * T.int64(4) + ax2)
                            v3 = T.axis.spatial(T.int64(196), eps_0_nu_0_co_0_p_0_fused % T.int64(7) * T.int64(28) + eps_2_nu_2_co_2_p_2_fused % T.int64(14) * T.int64(2) + ax3)
                            T.reads(bgemm_local[v0, v1, v2, v3])
                            T.writes(bgemm[v0, v1, v2, v3])
                            bgemm[v0, v1, v2, v3] = bgemm_local[v0, v1, v2, v3]
        for n_co_h_0_w_0_fused_0 in T.thread_binding(T.int64(25), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 1024, "pragma_unroll_explicit": 1}):
            for n_co_h_0_w_0_fused_1 in T.thread_binding(T.int64(1024), thread="threadIdx.x"):
                for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                    for ax2 in T.unroll(T.int64(2)):
                        for ax3 in T.unroll(T.int64(2)):
                            with T.block("inverse_init"):
                                v_co = T.axis.spatial(T.int64(128), (n_co_h_0_w_0_fused_0 * T.int64(1024) + n_co_h_0_w_0_fused_1) // T.int64(196) + ax0)
                                v_p = T.axis.spatial(T.int64(196), (n_co_h_0_w_0_fused_0 * T.int64(1024) + n_co_h_0_w_0_fused_1) % T.int64(196) + ax1)
                                v_vh, v_vw = T.axis.remap("SS", [ax2, ax3])
                                T.where(n_co_h_0_w_0_fused_0 * T.int64(1024) + n_co_h_0_w_0_fused_1 < T.int64(25088))
                                T.reads()
                                T.writes(inverse_local[v_co, v_p, v_vh, v_vw])
                                T.block_attr({"schedule_rule": "conv2d_nchw_winograd_inverse"})
                                inverse_local[v_co, v_p, v_vh, v_vw] = T.float32(0)
                            for ax4 in T.unroll(T.int64(4)):
                                for ax5 in T.unroll(T.int64(4)):
                                    with T.block("inverse_update"):
                                        v_co = T.axis.spatial(T.int64(128), (n_co_h_0_w_0_fused_0 * T.int64(1024) + n_co_h_0_w_0_fused_1) // T.int64(196) + ax0)
                                        v_p = T.axis.spatial(T.int64(196), (n_co_h_0_w_0_fused_0 * T.int64(1024) + n_co_h_0_w_0_fused_1) % T.int64(196) + ax1)
                                        v_vh, v_vw, v_r_a, v_r_b = T.axis.remap("SSRR", [ax2, ax3, ax4, ax5])
                                        T.where(n_co_h_0_w_0_fused_0 * T.int64(1024) + n_co_h_0_w_0_fused_1 < T.int64(25088))
                                        T.reads(inverse_local[v_co, v_p, v_vh, v_vw], bgemm[v_r_a, v_r_b, v_co, v_p])
                                        T.writes(inverse_local[v_co, v_p, v_vh, v_vw])
                                        T.block_attr({"schedule_rule": "conv2d_nchw_winograd_inverse"})
                                        inverse_local[v_co, v_p, v_vh, v_vw] = inverse_local[v_co, v_p, v_vh, v_vw] + bgemm[v_r_a, v_r_b, v_co, v_p] * T.Select(v_r_a % T.int64(4) == T.int64(3) and v_vh % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_vh % T.int64(2) == T.int64(0), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_vh % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_vh % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_vh % T.int64(2) == T.int64(1), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_vh % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_vh % T.int64(2) == T.int64(1), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_vh % T.int64(2) == T.int64(0), T.float32(1), T.float32(0))))))))) * T.Select(v_r_b % T.int64(4) == T.int64(3) and v_vw % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_vw % T.int64(2) == T.int64(0), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_vw % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_vw % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_vw % T.int64(2) == T.int64(1), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_vw % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_vw % T.int64(2) == T.int64(1), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_vw % T.int64(2) == T.int64(0), T.float32(1), T.float32(0)))))))))
                for h_1, w_1 in T.grid(T.int64(2), T.int64(2)):
                    with T.block("conv2d_winograd"):
                        v_n = T.axis.spatial(T.int64(1), T.int64(0))
                        v_co = T.axis.spatial(T.int64(128), (n_co_h_0_w_0_fused_0 * T.int64(1024) + n_co_h_0_w_0_fused_1) // T.int64(196))
                        v_h = T.axis.spatial(T.int64(28), (n_co_h_0_w_0_fused_0 * T.int64(1024) + n_co_h_0_w_0_fused_1) % T.int64(196) // T.int64(14) * T.int64(2) + h_1)
                        v_w = T.axis.spatial(T.int64(28), (n_co_h_0_w_0_fused_0 * T.int64(1024) + n_co_h_0_w_0_fused_1) % T.int64(14) * T.int64(2) + w_1)
                        T.where(n_co_h_0_w_0_fused_0 * T.int64(1024) + n_co_h_0_w_0_fused_1 < T.int64(25088))
                        T.reads(inverse_local[v_co, v_n * T.int64(196) + v_h // T.int64(2) * T.int64(14) + v_w // T.int64(2), v_h % T.int64(2), v_w % T.int64(2)], p2[v_n, v_co, v_h, v_w], p3[v_n, v_co, T.int64(0), T.int64(0)])
                        T.writes(T_relu[v_n, v_co, v_h, v_w])
                        T_relu[v_n, v_co, v_h, v_w] = T.max(inverse_local[v_co, v_n * T.int64(196) + v_h // T.int64(2) * T.int64(14) + v_w // T.int64(2), v_h % T.int64(2), v_w % T.int64(2)] + p2[v_n, v_co, v_h, v_w] + p3[v_n, v_co, T.int64(0), T.int64(0)], T.float32(0))
b0 = sch.get_block(name="data_pack", func_name="main")
b1 = sch.get_block(name="bgemm", func_name="main")
b2 = sch.get_block(name="inverse", func_name="main")
b3 = sch.get_block(name="conv2d_winograd", func_name="main")
b4 = sch.get_block(name="T_add", func_name="main")
b5 = sch.get_block(name="T_add_1", func_name="main")
b6 = sch.get_block(name="T_relu", func_name="main")
b7 = sch.get_block(name="root", func_name="main")
b8, b9 = sch.get_producers(block=b2)
sch.compute_inline(block=b9)
b10, = sch.get_consumers(block=b2)
l11, l12, l13, l14 = sch.get_loops(block=b10)
l15, l16 = sch.split(loop=l13, factors=[None, 2], preserve_unit_iters=True)
l17, l18 = sch.split(loop=l14, factors=[None, 2], preserve_unit_iters=True)
sch.reorder(l15, l17, l16, l18)
sch.compute_at(block=b2, loop=l17, preserve_unit_loops=True, index=-1)
sch.set_scope(block=b2, buffer_index=0, storage_scope="local")
l19, l20, l21, l22, l23, l24, l25, l26, l27, l28 = sch.get_loops(block=b2)
sch.unroll(loop=l25)
sch.unroll(loop=l26)
sch.unroll(loop=l27)
sch.unroll(loop=l28)
b29, b30 = sch.get_producers(block=b0)
sch.compute_inline(block=b30)
b31, = sch.get_producers(block=b29)
l32, l33, l34, l35, l36, l37 = sch.get_loops(block=b0)
sch.reorder(l34, l35, l32, l33, l36, l37)
sch.unroll(loop=l32)
sch.unroll(loop=l33)
sch.unroll(loop=l36)
sch.unroll(loop=l37)
l38 = sch.fuse(l34, l35, preserve_unit_iters=True)
v39 = sch.sample_categorical(candidates=[32, 64, 128, 256, 512, 1024], probs=[0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666], decision=3)
l40, l41 = sch.split(loop=l38, factors=[None, v39], preserve_unit_iters=True)
sch.bind(loop=l40, thread_axis="blockIdx.x")
sch.bind(loop=l41, thread_axis="threadIdx.x")
b42 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b42, loop=l41, preserve_unit_loops=True, index=-1)
sch.compute_at(block=b29, loop=l41, preserve_unit_loops=True, index=-1)
sch.set_scope(block=b29, buffer_index=0, storage_scope="local")
sch.compute_inline(block=b31)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l43, l44, l45, l46, l47 = sch.get_loops(block=b1)
v48, v49, v50, v51, v52 = sch.sample_perfect_tile(loop=l43, n=5, max_innermost_factor=64, decision=[4, 1, 1, 1, 1])
l53, l54, l55, l56, l57 = sch.split(loop=l43, factors=[v48, v49, v50, v51, v52], preserve_unit_iters=True)
v58, v59, v60, v61, v62 = sch.sample_perfect_tile(loop=l44, n=5, max_innermost_factor=64, decision=[4, 1, 1, 1, 1])
l63, l64, l65, l66, l67 = sch.split(loop=l44, factors=[v58, v59, v60, v61, v62], preserve_unit_iters=True)
v68, v69, v70, v71, v72 = sch.sample_perfect_tile(loop=l45, n=5, max_innermost_factor=64, decision=[2, 1, 16, 4, 1])
l73, l74, l75, l76, l77 = sch.split(loop=l45, factors=[v68, v69, v70, v71, v72], preserve_unit_iters=True)
v78, v79, v80, v81, v82 = sch.sample_perfect_tile(loop=l46, n=5, max_innermost_factor=64, decision=[7, 1, 14, 1, 2])
l83, l84, l85, l86, l87 = sch.split(loop=l46, factors=[v78, v79, v80, v81, v82], preserve_unit_iters=True)
v88, v89, v90 = sch.sample_perfect_tile(loop=l47, n=3, max_innermost_factor=64, decision=[4, 32, 1])
l91, l92, l93 = sch.split(loop=l47, factors=[v88, v89, v90], preserve_unit_iters=True)
sch.reorder(l53, l63, l73, l83, l54, l64, l74, l84, l55, l65, l75, l85, l91, l92, l56, l66, l76, l86, l93, l57, l67, l77, l87)
l94 = sch.fuse(l53, l63, l73, l83, preserve_unit_iters=True)
sch.bind(loop=l94, thread_axis="blockIdx.x")
l95 = sch.fuse(l54, l64, l74, l84, preserve_unit_iters=True)
sch.bind(loop=l95, thread_axis="vthread.x")
l96 = sch.fuse(l55, l65, l75, l85, preserve_unit_iters=True)
sch.bind(loop=l96, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b97 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b97, loop=l96, preserve_unit_loops=True, index=-1)
b98 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b98, loop=l91, preserve_unit_loops=True, index=-1)
l99, l100, l101, l102, l103, l104, l105, l106 = sch.get_loops(block=b98)
l107 = sch.fuse(l103, l104, l105, l106, preserve_unit_iters=True)
v108 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b98, ann_key="meta_schedule.cooperative_fetch", ann_val=v108)
b109 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b109, loop=l91, preserve_unit_loops=True, index=-1)
l110, l111, l112, l113, l114, l115, l116, l117 = sch.get_loops(block=b109)
l118 = sch.fuse(l114, l115, l116, l117, preserve_unit_iters=True)
v119 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b109, ann_key="meta_schedule.cooperative_fetch", ann_val=v119)
sch.reverse_compute_inline(block=b6)
sch.reverse_compute_inline(block=b5)
sch.reverse_compute_inline(block=b4)
v120 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b7, ann_key="meta_schedule.unroll_explicit", ann_val=v120)
l121, l122, l123, l124, l125, l126 = sch.get_loops(block=b3)
l127 = sch.fuse(l121, l122, l123, l124, preserve_unit_iters=True)
v128 = sch.sample_categorical(candidates=[32, 64, 128, 256, 512, 1024], probs=[0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666], decision=5)
l129, l130 = sch.split(loop=l127, factors=[None, v128], preserve_unit_iters=True)
sch.bind(loop=l129, thread_axis="blockIdx.x")
sch.bind(loop=l130, thread_axis="threadIdx.x")
sch.enter_postproc()
sch.unannotate(block_or_loop=b98, ann_key="meta_schedule.cooperative_fetch")
l131, l132, l133, l134, l135 = sch.get_loops(block=b98)
l136, l137, l138 = sch.split(loop=l135, factors=[None, 224, 4], preserve_unit_iters=True)
sch.vectorize(loop=l138)
sch.bind(loop=l137, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b109, ann_key="meta_schedule.cooperative_fetch")
l139, l140, l141, l142, l143 = sch.get_loops(block=b109)
l144, l145, l146 = sch.split(loop=l143, factors=[None, 224, 4], preserve_unit_iters=True)
sch.vectorize(loop=l146)
sch.bind(loop=l145, thread_axis="threadIdx.x")
b147 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b147, ann_key="meta_schedule.unroll_explicit")
b148, b149, b150, b151, b152, b153, b154, b155, b156 = sch.get_child_blocks(b147)
l157, l158, l159, l160, l161, l162 = sch.get_loops(block=b148)
l163, l164, l165, l166, l167, l168 = sch.get_loops(block=b149)
sch.annotate(block_or_loop=l163, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l163, ann_key="pragma_unroll_explicit", ann_val=1)
l169, l170, l171, l172, l173, l174 = sch.get_loops(block=b150)
l175, l176, l177, l178, l179, l180, l181 = sch.get_loops(block=b151)
l182, l183, l184, l185, l186, l187, l188 = sch.get_loops(block=b152)
l189, l190, l191, l192, l193, l194, l195, l196, l197, l198, l199, l200, l201, l202 = sch.get_loops(block=b153)
sch.annotate(block_or_loop=l189, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l189, ann_key="pragma_unroll_explicit", ann_val=1)
l203, l204, l205, l206, l207, l208, l209 = sch.get_loops(block=b154)
l210, l211, l212, l213, l214, l215, l216, l217 = sch.get_loops(block=b155)
sch.annotate(block_or_loop=l210, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l210, ann_key="pragma_unroll_explicit", ann_val=1)
l218, l219, l220, l221 = sch.get_loops(block=b156)
b222 = sch.get_block(name="data_pack", func_name="main")
l223, l224, l225, l226, l227, l228 = sch.get_loops(block=b222)
b229 = sch.decompose_reduction(block=b222, loop=l227)
b230 = sch.get_block(name="bgemm", func_name="main")
l231, l232, l233, l234, l235, l236, l237, l238, l239, l240, l241, l242, l243, l244 = sch.get_loops(block=b230)
b245 = sch.decompose_reduction(block=b230, loop=l234)
b246 = sch.get_block(name="inverse", func_name="main")
l247, l248, l249, l250, l251, l252, l253, l254 = sch.get_loops(block=b246)
b255 = sch.decompose_reduction(block=b246, loop=l253)
2023-11-11 04:30:03 [INFO] [task_scheduler.cc:121] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #325: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/home2/xiachunwei/Software/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/home2/xiachunwei/Software/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/home2/xiachunwei/Software/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/home2/xiachunwei/Software/tvm/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/home2/xiachunwei/Software/tvm/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/home2/xiachunwei/Software/tvm/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/home2/xiachunwei/Software/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/home2/xiachunwei/Software/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  4: _ZN3tvm7runtime13PackedFun
  3: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  2: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  0: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  File "/home2/xiachunwei/Software/tvm/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(128), T.int64(28), T.int64(28)), "float32"), p1: T.Buffer((T.int64(4), T.int64(4), T.int64(128), T.int64(128)), "float32"), p2: T.Buffer((T.int64(1), T.int64(128), T.int64(28), T.int64(28)), "float32"), p3: T.Buffer((T.int64(1), T.int64(128), T.int64(1), T.int64(1)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(128), T.int64(28), T.int64(28)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        input_tile_local = T.alloc_buffer((T.int64(128), T.int64(196), T.int64(4), T.int64(4)), scope="local")
        data_pack = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(128), T.int64(196)))
        bgemm = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(128), T.int64(196)))
        inverse_local = T.alloc_buffer((T.int64(128), T.int64(196), T.int64(2), T.int64(2)), scope="local")
        data_pack_local = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(128), T.int64(196)), scope="local")
        bgemm_local = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(128), T.int64(196)), scope="local")
        data_pack_shared = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(128), T.int64(196)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(128), T.int64(128)), scope="shared")
        for ci_p_fused_0 in T.thread_binding(T.int64(98), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 1024, "pragma_unroll_explicit": 1}):
            for ci_p_fused_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(1), T.int64(4), T.int64(4)):
                    with T.block("input_tile"):
                        v_ci = T.axis.spatial(T.int64(128), (ci_p_fused_0 * T.int64(256) + ci_p_fused_1) // T.int64(196) + ax0)
                        v_p = T.axis.spatial(T.int64(196), (ci_p_fused_0 * T.int64(60) + ci_p_fused_1) % T.int64(196) + ax1)
                        v_eps, v_nu = T.axis.remap("SS", [ax2, ax3])
                        T.reads(p0[v_p // T.int64(196), v_ci, v_p % T.int64(196) // T.int64(14) * T.int64(2) + v_eps - T.int64(1), v_p % T.int64(14) * T.int64(2) + v_nu - T.int64(1)])
                        T.writes(input_tile_local[v_ci, v_p, v_eps, v_nu])
                        T.block_attr({"schedule_rule": "None"})
                        input_tile_local[v_ci, v_p, v_eps, v_nu] = T.if_then_else(T.int64(1) <= v_p % T.int64(196) // T.int64(14) * T.int64(2) + v_eps and v_p % T.int64(196) // T.int64(14) * T.int64(2) + v_eps < T.int64(29) and T.int64(1) <= v_p % T.int64(14) * T.int64(2) + v_nu and v_p % T.int64(14) * T.int64(2) + v_nu < T.int64(29), p0[v_p // T.int64(196), v_ci, v_p % T.int64(196) // T.int64(14) * T.int64(2) + v_eps - T.int64(1), v_p % T.int64(14) * T.int64(2) + v_nu - T.int64(1)], T.float32(0))
                for eps in T.unroll(T.int64(4)):
                    for nu in T.unroll(T.int64(4)):
                        with T.block("data_pack_init"):
                            v_eps, v_nu = T.axis.remap("SS", [eps, nu])
                            v_ci = T.axis.spatial(T.int64(128), (ci_p_fused_0 * T.int64(256) + ci_p_fused_1) // T.int64(196))
                            v_p = T.axis.spatial(T.int64(196), (ci_p_fused_0 * T.int64(256) + ci_p_fused_1) % T.int64(196))
                            T.reads()
                            T.writes(data_pack_local[v_eps, v_nu, v_ci, v_p])
                            T.block_attr({"schedule_rule": "conv2d_nchw_winograd_data_pack"})
                            data_pack_local[v_eps, v_nu, v_ci, v_p] = T.float32(0)
                        for r_a in T.unroll(T.int64(4)):
                            for r_b in T.unroll(T.int64(4)):
                                with T.block("data_pack_update"):
                                    v_eps, v_nu = T.axis.remap("SS", [eps, nu])
                                    v_ci = T.axis.spatial(T.int64(128), (ci_p_fused_0 * T.int64(256) + ci_p_fused_1) // T.int64(196))
                                    v_p = T.axis.spatial(T.int64(196), (ci_p_fused_0 * T.int64(256) + ci_p_fused_1) % T.int64(196))
                                    v_r_a, v_r_b = T.axis.remap("RR", [r_a, r_b])
                                    T.reads(data_pack_local[v_eps, v_nu, v_ci, v_p], input_tile_local[v_ci, v_p, v_r_a, v_r_b])
                                    T.writes(data_pack_local[v_eps, v_nu, v_ci, v_p])
                                    T.block_attr({"schedule_rule": "conv2d_nchw_winograd_data_pack"})
                                    data_pack_local[v_eps, v_nu, v_ci, v_p] = data_pack_local[v_eps, v_nu, v_ci, v_p] + input_tile_local[v_ci, v_p, v_r_a, v_r_b] * T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(3), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(1), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(0), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(3), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(1), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(0), T.float32(1), T.float32(0))))))))))))))))) * T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(3), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(1), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(0), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(3), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(1), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(0), T.float32(1), T.float32(0)))))))))))))))))
                for ax0, ax1, ax2, ax3 in T.grid(T.int64(4), T.int64(4), T.int64(1), T.int64(1)):
                    with T.block("data_pack_local"):
                        v0, v1 = T.axis.remap("SS", [ax0, ax1])
                        v2 = T.axis.spatial(T.int64(128), (ci_p_fused_0 * T.int64(256) + ci_p_fused_1) // T.int64(196) + ax2)
                        v3 = T.axis.spatial(T.int64(196), (ci_p_fused_0 * T.int64(60) + ci_p_fused_1) % T.int64(196) + ax3)
                        T.reads(data_pack_local[v0, v1, v2, v3])
                        T.writes(data_pack[v0, v1, v2, v3])
                        data_pack[v0, v1, v2, v3] = data_pack_local[v0, v1, v2, v3]
        for eps_0_nu_0_co_0_p_0_fused in T.thread_binding(T.int64(224), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 1024, "pragma_unroll_explicit": 1}):
            for eps_1_nu_1_co_1_p_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for eps_2_nu_2_co_2_p_2_fused in T.thread_binding(T.int64(224), thread="threadIdx.x"):
                    for eps_3_init, nu_3_init, co_3_init, p_3_init, eps_4_init, nu_4_init, co_4_init, p_4_init in T.grid(T.int64(1), T.int64(1), T.int64(4), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(2)):
                        with T.block("bgemm_init"):
                            v_eps = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(56) + eps_3_init + eps_4_init)
                            v_nu = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused % T.int64(56) // T.int64(14) + nu_3_init + nu_4_init)
                            v_co = T.axis.spatial(T.int64(128), eps_0_nu_0_co_0_p_0_fused % T.int64(14) // T.int64(7) * T.int64(64) + eps_2_nu_2_co_2_p_2_fused // T.int64(14) * T.int64(4) + co_3_init + co_4_init)
                            v_p = T.axis.spatial(T.int64(196), eps_0_nu_0_co_0_p_0_fused % T.int64(7) * T.int64(28) + eps_2_nu_2_co_2_p_2_fused % T.int64(14) * T.int64(2) + p_3_init * T.int64(2) + p_4_init)
                            T.reads()
                            T.writes(bgemm_local[v_eps, v_nu, v_co, v_p])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            bgemm_local[v_eps, v_nu, v_co, v_p] = T.float32(0)
                    for ci_0 in range(T.int64(1)):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(16)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(224), thread="threadIdx.x"):
                                with T.block("data_pack_shared"):
                                    v0 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(56))
                                    v1 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused % T.int64(56) // T.int64(14))
                                    v2 = T.axis.spatial(T.int64(128), (ax0_ax1_ax2_ax3_fused_0 * T.int64(224) + ax0_ax1_ax2_ax3_fused_1) // T.int64(28))
                                    v3 = T.axis.spatial(T.int64(196), eps_0_nu_0_co_0_p_0_fused % T.int64(7) * T.int64(28) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(224) + ax0_ax1_ax2_ax3_fused_1) % T.int64(28))
                                    T.reads(data_pack[v0, v1, v2, v3])
                                    T.writes(data_pack_shared[v0, v1, v2, v3])
                                    data_pack_shared[v0, v1, v2, v3] = data_pack[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(10)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(224), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("p1_shared"):
                                        v0 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(56))
                                        v1 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused % T.int64(56) // T.int64(14))
                                        v2 = T.axis.spatial(T.int64(128), (ax0_ax1_ax2_ax3_fused_0 * T.int64(896) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) // T.int64(64))
                                        v3 = T.axis.spatial(T.int64(128), eps_0_nu_0_co_0_p_0_fused % T.int64(14) // T.int64(7) * T.int64(64) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(896) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) % T.int64(64))
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * T.int64(224) + ax0_ax1_ax2_ax3_fused_1) * T.int64(4) + ax0_ax1_ax2_ax3_fused_2 < T.int64(8192))
                                        T.reads(p1[v0, v1, v2, v3])
                                        T.writes(p1_shared[v0, v1, v2, v3])
                                        p1_shared[v0, v1, v2, v3] = p1[v0, v1, v2, v3]
                        for ci_1, eps_3, nu_3, co_3, p_3, ci_2, eps_4, nu_4, co_4, p_4 in T.grid(T.int64(128), T.int64(1), T.int64(1), T.int64(4), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(2)):
                            with T.block("bgemm_update"):
                                v_eps = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(56) + eps_3 + eps_4)
                                v_nu = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused % T.int64(56) // T.int64(14) + nu_3 + nu_4)
                                v_co = T.axis.spatial(T.int64(128), eps_0_nu_0_co_0_p_0_fused % T.int64(14) // T.int64(7) * T.int64(64) + eps_2_nu_2_co_2_p_2_fused // T.int64(14) * T.int64(4) + co_3 + co_4)
                                v_p = T.axis.spatial(T.int64(196), eps_0_nu_0_co_0_p_0_fused % T.int64(7) * T.int64(28) + eps_2_nu_2_co_2_p_2_fused % T.int64(14) * T.int64(2) + p_3 * T.int64(2) + p_4)
                                v_ci = T.axis.reduce(T.int64(128), ci_0 * T.int64(128) + ci_1 + ci_2)
                                T.reads(bgemm_local[v_eps, v_nu, v_co, v_p], data_pack_shared[v_eps, v_nu, v_ci, v_p], p1_shared[v_eps, v_nu, v_ci, v_co])
                                T.writes(bgemm_local[v_eps, v_nu, v_co, v_p])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                bgemm_local[v_eps, v_nu, v_co, v_p] = bgemm_local[v_eps, v_nu, v_co, v_p] + data_pack_shared[v_eps, v_nu, v_ci, v_p] * p1_shared[v_eps, v_nu, v_ci, v_co]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(1), T.int64(4), T.int64(2)):
                        with T.block("bgemm_local"):
                            v0 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(56) + ax0)
                            v1 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused % T.int64(56) // T.int64(14) + ax1)
                            v2 = T.axis.spatial(T.int64(128), eps_0_nu_0_co_0_p_0_fused % T.int64(14) // T.int64(7) * T.int64(64) + eps_2_nu_2_co_2_p_2_fused // T.int64(14) * T.int64(4) + ax2)
                            v3 = T.axis.spatial(T.int64(196), eps_0_nu_0_co_0_p_0_fused % T.int64(7) * T.int64(28) + eps_2_nu_2_co_2_p_2_fused % T.int64(14) * T.int64(2) + ax3)
                            T.reads(bgemm_local[v0, v1, v2, v3])
                            T.writes(bgemm[v0, v1, v2, v3])
                            bgemm[v0, v1, v2, v3] = bgemm_local[v0, v1, v2, v3]
        for n_co_h_0_w_0_fused_0 in T.thread_binding(T.int64(25), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 1024, "pragma_unroll_explicit": 1}):
            for n_co_h_0_w_0_fused_1 in T.thread_binding(T.int64(1024), thread="threadIdx.x"):
                for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                    for ax2 in T.unroll(T.int64(2)):
                        for ax3 in T.unroll(T.int64(2)):
                            with T.block("inverse_init"):
                                v_co = T.axis.spatial(T.int64(128), (n_co_h_0_w_0_fused_0 * T.int64(1024) + n_co_h_0_w_0_fused_1) // T.int64(196) + ax0)
                                v_p = T.axis.spatial(T.int64(196), (n_co_h_0_w_0_fused_0 * T.int64(1024) + n_co_h_0_w_0_fused_1) % T.int64(196) + ax1)
                                v_vh, v_vw = T.axis.remap("SS", [ax2, ax3])
                                T.where(n_co_h_0_w_0_fused_0 * T.int64(1024) + n_co_h_0_w_0_fused_1 < T.int64(25088))
                                T.reads()
                                T.writes(inverse_local[v_co, v_p, v_vh, v_vw])
                                T.block_attr({"schedule_rule": "conv2d_nchw_winograd_inverse"})
                                inverse_local[v_co, v_p, v_vh, v_vw] = T.float32(0)
                            for ax4 in T.unroll(T.int64(4)):
                                for ax5 in T.unroll(T.int64(4)):
                                    with T.block("inverse_update"):
                                        v_co = T.axis.spatial(T.int64(128), (n_co_h_0_w_0_fused_0 * T.int64(1024) + n_co_h_0_w_0_fused_1) // T.int64(196) + ax0)
                                        v_p = T.axis.spatial(T.int64(196), (n_co_h_0_w_0_fused_0 * T.int64(1024) + n_co_h_0_w_0_fused_1) % T.int64(196) + ax1)
                                        v_vh, v_vw, v_r_a, v_r_b = T.axis.remap("SSRR", [ax2, ax3, ax4, ax5])
                                        T.where(n_co_h_0_w_0_fused_0 * T.int64(1024) + n_co_h_0_w_0_fused_1 < T.int64(25088))
                                        T.reads(inverse_local[v_co, v_p, v_vh, v_vw], bgemm[v_r_a, v_r_b, v_co, v_p])
                                        T.writes(inverse_local[v_co, v_p, v_vh, v_vw])
                                        T.block_attr({"schedule_rule": "conv2d_nchw_winograd_inverse"})
                                        inverse_local[v_co, v_p, v_vh, v_vw] = inverse_local[v_co, v_p, v_vh, v_vw] + bgemm[v_r_a, v_r_b, v_co, v_p] * T.Select(v_r_a % T.int64(4) == T.int64(3) and v_vh % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_vh % T.int64(2) == T.int64(0), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_vh % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_vh % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_vh % T.int64(2) == T.int64(1), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_vh % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_vh % T.int64(2) == T.int64(1), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_vh % T.int64(2) == T.int64(0), T.float32(1), T.float32(0))))))))) * T.Select(v_r_b % T.int64(4) == T.int64(3) and v_vw % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_vw % T.int64(2) == T.int64(0), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_vw % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_vw % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_vw % T.int64(2) == T.int64(1), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_vw % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_vw % T.int64(2) == T.int64(1), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_vw % T.int64(2) == T.int64(0), T.float32(1), T.float32(0)))))))))
                for h_1, w_1 in T.grid(T.int64(2), T.int64(2)):
                    with T.block("conv2d_winograd"):
                        v_n = T.axis.spatial(T.int64(1), T.int64(0))
                        v_co = T.axis.spatial(T.int64(128), (n_co_h_0_w_0_fused_0 * T.int64(1024) + n_co_h_0_w_0_fused_1) // T.int64(196))
                        v_h = T.axis.spatial(T.int64(28), (n_co_h_0_w_0_fused_0 * T.int64(1024) + n_co_h_0_w_0_fused_1) % T.int64(196) // T.int64(14) * T.int64(2) + h_1)
                        v_w = T.axis.spatial(T.int64(28), (n_co_h_0_w_0_fused_0 * T.int64(1024) + n_co_h_0_w_0_fused_1) % T.int64(14) * T.int64(2) + w_1)
                        T.where(n_co_h_0_w_0_fused_0 * T.int64(1024) + n_co_h_0_w_0_fused_1 < T.int64(25088))
                        T.reads(inverse_local[v_co, v_n * T.int64(196) + v_h // T.int64(2) * T.int64(14) + v_w // T.int64(2), v_h % T.int64(2), v_w % T.int64(2)], p2[v_n, v_co, v_h, v_w], p3[v_n, v_co, T.int64(0), T.int64(0)])
                        T.writes(T_relu[v_n, v_co, v_h, v_w])
                        T_relu[v_n, v_co, v_h, v_w] = T.max(inverse_local[v_co, v_n * T.int64(196) + v_h // T.int64(2) * T.int64(14) + v_w // T.int64(2), v_h % T.int64(2), v_w % T.int64(2)] + p2[v_n, v_co, v_h, v_w] + p3[v_n, v_co, T.int64(0), T.int64(0)], T.float32(0))
b0 = sch.get_block(name="data_pack", func_name="main")
b1 = sch.get_block(name="bgemm", func_name="main")
b2 = sch.get_block(name="inverse", func_name="main")
b3 = sch.get_block(name="conv2d_winograd", func_name="main")
b4 = sch.get_block(name="T_add", func_name="main")
b5 = sch.get_block(name="T_add_1", func_name="main")
b6 = sch.get_block(name="T_relu", func_name="main")
b7 = sch.get_block(name="root", func_name="main")
b8, b9 = sch.get_producers(block=b2)
sch.compute_inline(block=b9)
b10, = sch.get_consumers(block=b2)
l11, l12, l13, l14 = sch.get_loops(block=b10)
l15, l16 = sch.split(loop=l13, factors=[None, 2], preserve_unit_iters=True)
l17, l18 = sch.split(loop=l14, factors=[None, 2], preserve_unit_iters=True)
sch.reorder(l15, l17, l16, l18)
sch.compute_at(block=b2, loop=l17, preserve_unit_loops=True, index=-1)
sch.set_scope(block=b2, buffer_index=0, storage_scope="local")
l19, l20, l21, l22, l23, l24, l25, l26, l27, l28 = sch.get_loops(block=b2)
sch.unroll(loop=l25)
sch.unroll(loop=l26)
sch.unroll(loop=l27)
sch.unroll(loop=l28)
b29, b30 = sch.get_producers(block=b0)
sch.compute_inline(block=b30)
b31, = sch.get_producers(block=b29)
l32, l33, l34, l35, l36, l37 = sch.get_loops(block=b0)
sch.reorder(l34, l35, l32, l33, l36, l37)
sch.unroll(loop=l32)
sch.unroll(loop=l33)
sch.unroll(loop=l36)
sch.unroll(loop=l37)
l38 = sch.fuse(l34, l35, preserve_unit_iters=True)
v39 = sch.sample_categorical(candidates=[32, 64, 128, 256, 512, 1024], probs=[0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666], decision=3)
l40, l41 = sch.split(loop=l38, factors=[None, v39], preserve_unit_iters=True)
sch.bind(loop=l40, thread_axis="blockIdx.x")
sch.bind(loop=l41, thread_axis="threadIdx.x")
b42 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b42, loop=l41, preserve_unit_loops=True, index=-1)
sch.compute_at(block=b29, loop=l41, preserve_unit_loops=True, index=-1)
sch.set_scope(block=b29, buffer_index=0, storage_scope="local")
sch.compute_inline(block=b31)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l43, l44, l45, l46, l47 = sch.get_loops(block=b1)
v48, v49, v50, v51, v52 = sch.sample_perfect_tile(loop=l43, n=5, max_innermost_factor=64, decision=[4, 1, 1, 1, 1])
l53, l54, l55, l56, l57 = sch.split(loop=l43, factors=[v48, v49, v50, v51, v52], preserve_unit_iters=True)
v58, v59, v60, v61, v62 = sch.sample_perfect_tile(loop=l44, n=5, max_innermost_factor=64, decision=[4, 1, 1, 1, 1])
l63, l64, l65, l66, l67 = sch.split(loop=l44, factors=[v58, v59, v60, v61, v62], preserve_unit_iters=True)
v68, v69, v70, v71, v72 = sch.sample_perfect_tile(loop=l45, n=5, max_innermost_factor=64, decision=[2, 1, 16, 4, 1])
l73, l74, l75, l76, l77 = sch.split(loop=l45, factors=[v68, v69, v70, v71, v72], preserve_unit_iters=True)
v78, v79, v80, v81, v82 = sch.sample_perfect_tile(loop=l46, n=5, max_innermost_factor=64, decision=[7, 1, 14, 1, 2])
l83, l84, l85, l86, l87 = sch.split(loop=l46, factors=[v78, v79, v80, v81, v82], preserve_unit_iters=True)
v88, v89, v90 = sch.sample_perfect_tile(loop=l47, n=3, max_innermost_factor=64, decision=[1, 128, 1])
l91, l92, l93 = sch.split(loop=l47, factors=[v88, v89, v90], preserve_unit_iters=True)
sch.reorder(l53, l63, l73, l83, l54, l64, l74, l84, l55, l65, l75, l85, l91, l92, l56, l66, l76, l86, l93, l57, l67, l77, l87)
l94 = sch.fuse(l53, l63, l73, l83, preserve_unit_iters=True)
sch.bind(loop=l94, thread_axis="blockIdx.x")
l95 = sch.fuse(l54, l64, l74, l84, preserve_unit_iters=True)
sch.bind(loop=l95, thread_axis="vthread.x")
l96 = sch.fuse(l55, l65, l75, l85, preserve_unit_iters=True)
sch.bind(loop=l96, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b97 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b97, loop=l96, preserve_unit_loops=True, index=-1)
b98 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b98, loop=l91, preserve_unit_loops=True, index=-1)
l99, l100, l101, l102, l103, l104, l105, l106 = sch.get_loops(block=b98)
l107 = sch.fuse(l103, l104, l105, l106, preserve_unit_iters=True)
v108 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b98, ann_key="meta_schedule.cooperative_fetch", ann_val=v108)
b109 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b109, loop=l91, preserve_unit_loops=True, index=-1)
l110, l111, l112, l113, l114, l115, l116, l117 = sch.get_loops(block=b109)
l118 = sch.fuse(l114, l115, l116, l117, preserve_unit_iters=True)
v119 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b109, ann_key="meta_schedule.cooperative_fetch", ann_val=v119)
sch.reverse_compute_inline(block=b6)
sch.reverse_compute_inline(block=b5)
sch.reverse_compute_inline(block=b4)
v120 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b7, ann_key="meta_schedule.unroll_explicit", ann_val=v120)
l121, l122, l123, l124, l125, l126 = sch.get_loops(block=b3)
l127 = sch.fuse(l121, l122, l123, l124, preserve_unit_iters=True)
v128 = sch.sample_categorical(candidates=[32, 64, 128, 256, 512, 1024], probs=[0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666], decision=5)
l129, l130 = sch.split(loop=l127, factors=[None, v128], preserve_unit_iters=True)
sch.bind(loop=l129, thread_axis="blockIdx.x")
sch.bind(loop=l130, thread_axis="threadIdx.x")
sch.enter_postproc()
sch.unannotate(block_or_loop=b98, ann_key="meta_schedule.cooperative_fetch")
l131, l132, l133, l134, l135 = sch.get_loops(block=b98)
l136, l137 = sch.split(loop=l135, factors=[None, 224], preserve_unit_iters=True)
sch.bind(loop=l137, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b109, ann_key="meta_schedule.cooperative_fetch")
l138, l139, l140, l141, l142 = sch.get_loops(block=b109)
l143, l144, l145 = sch.split(loop=l142, factors=[None, 224, 4], preserve_unit_iters=True)
sch.vectorize(loop=l145)
sch.bind(loop=l144, thread_axis="threadIdx.x")
b146 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b146, ann_key="meta_schedule.unroll_explicit")
b147, b148, b149, b150, b151, b152, b153, b154, b155 = sch.get_child_blocks(b146)
l156, l157, l158, l159, l160, l161 = sch.get_loops(block=b147)
l162, l163, l164, l165, l166, l167 = sch.get_loops(block=b148)
sch.annotate(block_or_loop=l162, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l162, ann_key="pragma_unroll_explicit", ann_val=1)
l168, l169, l170, l171, l172, l173 = sch.get_loops(block=b149)
l174, l175, l176, l177, l178, l179 = sch.get_loops(block=b150)
l180, l181, l182, l183, l184, l185, l186 = sch.get_loops(block=b151)
l187, l188, l189, l190, l191, l192, l193, l194, l195, l196, l197, l198, l199, l200 = sch.get_loops(block=b152)
sch.annotate(block_or_loop=l187, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l187, ann_key="pragma_unroll_explicit", ann_val=1)
l201, l202, l203, l204, l205, l206, l207 = sch.get_loops(block=b153)
l208, l209, l210, l211, l212, l213, l214, l215 = sch.get_loops(block=b154)
sch.annotate(block_or_loop=l208, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l208, ann_key="pragma_unroll_explicit", ann_val=1)
l216, l217, l218, l219 = sch.get_loops(block=b155)
b220 = sch.get_block(name="data_pack", func_name="main")
l221, l222, l223, l224, l225, l226 = sch.get_loops(block=b220)
b227 = sch.decompose_reduction(block=b220, loop=l225)
b228 = sch.get_block(name="bgemm", func_name="main")
l229, l230, l231, l232, l233, l234, l235, l236, l237, l238, l239, l240, l241, l242 = sch.get_loops(block=b228)
b243 = sch.decompose_reduction(block=b228, loop=l232)
b244 = sch.get_block(name="inverse", func_name="main")
l245, l246, l247, l248, l249, l250, l251, l252 = sch.get_loops(block=b244)
b253 = sch.decompose_reduction(block=b244, loop=l251)
2023-11-11 04:30:03 [INFO] [task_scheduler.cc:121] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #326: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/home2/xiachunwei/Software/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/home2/xiachunwei/Software/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/home2/xiachunwei/Software/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/home2/xiachunwei/Software/tvm/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/home2/xiachunwei/Software/tvm/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/home2/xiachunwei/Software/tvm/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/home2/xiachunwei/Software/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/home2/xiachunwei/Software/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  4: _ZN3tvm7runtime13PackedFun
  3: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  2: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  0: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  File "/home2/xiachunwei/Software/tvm/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(128), T.int64(28), T.int64(28)), "float32"), p1: T.Buffer((T.int64(4), T.int64(4), T.int64(128), T.int64(128)), "float32"), p2: T.Buffer((T.int64(1), T.int64(128), T.int64(28), T.int64(28)), "float32"), p3: T.Buffer((T.int64(1), T.int64(128), T.int64(1), T.int64(1)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(128), T.int64(28), T.int64(28)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        input_tile_local = T.alloc_buffer((T.int64(128), T.int64(196), T.int64(4), T.int64(4)), scope="local")
        data_pack = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(128), T.int64(196)))
        bgemm = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(128), T.int64(196)))
        inverse_local = T.alloc_buffer((T.int64(128), T.int64(196), T.int64(2), T.int64(2)), scope="local")
        data_pack_local = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(128), T.int64(196)), scope="local")
        bgemm_local = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(128), T.int64(196)), scope="local")
        data_pack_shared = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(128), T.int64(196)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(128), T.int64(128)), scope="shared")
        for ci_p_fused_0 in T.thread_binding(T.int64(98), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 1024, "pragma_unroll_explicit": 1}):
            for ci_p_fused_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(1), T.int64(4), T.int64(4)):
                    with T.block("input_tile"):
                        v_ci = T.axis.spatial(T.int64(128), (ci_p_fused_0 * T.int64(256) + ci_p_fused_1) // T.int64(196) + ax0)
                        v_p = T.axis.spatial(T.int64(196), (ci_p_fused_0 * T.int64(60) + ci_p_fused_1) % T.int64(196) + ax1)
                        v_eps, v_nu = T.axis.remap("SS", [ax2, ax3])
                        T.reads(p0[v_p // T.int64(196), v_ci, v_p % T.int64(196) // T.int64(14) * T.int64(2) + v_eps - T.int64(1), v_p % T.int64(14) * T.int64(2) + v_nu - T.int64(1)])
                        T.writes(input_tile_local[v_ci, v_p, v_eps, v_nu])
                        T.block_attr({"schedule_rule": "None"})
                        input_tile_local[v_ci, v_p, v_eps, v_nu] = T.if_then_else(T.int64(1) <= v_p % T.int64(196) // T.int64(14) * T.int64(2) + v_eps and v_p % T.int64(196) // T.int64(14) * T.int64(2) + v_eps < T.int64(29) and T.int64(1) <= v_p % T.int64(14) * T.int64(2) + v_nu and v_p % T.int64(14) * T.int64(2) + v_nu < T.int64(29), p0[v_p // T.int64(196), v_ci, v_p % T.int64(196) // T.int64(14) * T.int64(2) + v_eps - T.int64(1), v_p % T.int64(14) * T.int64(2) + v_nu - T.int64(1)], T.float32(0))
                for eps in T.unroll(T.int64(4)):
                    for nu in T.unroll(T.int64(4)):
                        with T.block("data_pack_init"):
                            v_eps, v_nu = T.axis.remap("SS", [eps, nu])
                            v_ci = T.axis.spatial(T.int64(128), (ci_p_fused_0 * T.int64(256) + ci_p_fused_1) // T.int64(196))
                            v_p = T.axis.spatial(T.int64(196), (ci_p_fused_0 * T.int64(256) + ci_p_fused_1) % T.int64(196))
                            T.reads()
                            T.writes(data_pack_local[v_eps, v_nu, v_ci, v_p])
                            T.block_attr({"schedule_rule": "conv2d_nchw_winograd_data_pack"})
                            data_pack_local[v_eps, v_nu, v_ci, v_p] = T.float32(0)
                        for r_a in T.unroll(T.int64(4)):
                            for r_b in T.unroll(T.int64(4)):
                                with T.block("data_pack_update"):
                                    v_eps, v_nu = T.axis.remap("SS", [eps, nu])
                                    v_ci = T.axis.spatial(T.int64(128), (ci_p_fused_0 * T.int64(256) + ci_p_fused_1) // T.int64(196))
                                    v_p = T.axis.spatial(T.int64(196), (ci_p_fused_0 * T.int64(256) + ci_p_fused_1) % T.int64(196))
                                    v_r_a, v_r_b = T.axis.remap("RR", [r_a, r_b])
                                    T.reads(data_pack_local[v_eps, v_nu, v_ci, v_p], input_tile_local[v_ci, v_p, v_r_a, v_r_b])
                                    T.writes(data_pack_local[v_eps, v_nu, v_ci, v_p])
                                    T.block_attr({"schedule_rule": "conv2d_nchw_winograd_data_pack"})
                                    data_pack_local[v_eps, v_nu, v_ci, v_p] = data_pack_local[v_eps, v_nu, v_ci, v_p] + input_tile_local[v_ci, v_p, v_r_a, v_r_b] * T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(3), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(1), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(0), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(3), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(1), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(0), T.float32(1), T.float32(0))))))))))))))))) * T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(3), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(1), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(0), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(3), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(1), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(0), T.float32(1), T.float32(0)))))))))))))))))
                for ax0, ax1, ax2, ax3 in T.grid(T.int64(4), T.int64(4), T.int64(1), T.int64(1)):
                    with T.block("data_pack_local"):
                        v0, v1 = T.axis.remap("SS", [ax0, ax1])
                        v2 = T.axis.spatial(T.int64(128), (ci_p_fused_0 * T.int64(256) + ci_p_fused_1) // T.int64(196) + ax2)
                        v3 = T.axis.spatial(T.int64(196), (ci_p_fused_0 * T.int64(60) + ci_p_fused_1) % T.int64(196) + ax3)
                        T.reads(data_pack_local[v0, v1, v2, v3])
                        T.writes(data_pack[v0, v1, v2, v3])
                        data_pack[v0, v1, v2, v3] = data_pack_local[v0, v1, v2, v3]
        for eps_0_nu_0_co_0_p_0_fused in T.thread_binding(T.int64(224), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 1024, "pragma_unroll_explicit": 1}):
            for eps_1_nu_1_co_1_p_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for eps_2_nu_2_co_2_p_2_fused in T.thread_binding(T.int64(224), thread="threadIdx.x"):
                    for eps_3_init, nu_3_init, co_3_init, p_3_init, eps_4_init, nu_4_init, co_4_init, p_4_init in T.grid(T.int64(1), T.int64(1), T.int64(4), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(2)):
                        with T.block("bgemm_init"):
                            v_eps = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(56) + eps_3_init + eps_4_init)
                            v_nu = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused % T.int64(56) // T.int64(14) + nu_3_init + nu_4_init)
                            v_co = T.axis.spatial(T.int64(128), eps_0_nu_0_co_0_p_0_fused % T.int64(14) // T.int64(7) * T.int64(64) + eps_2_nu_2_co_2_p_2_fused // T.int64(14) * T.int64(4) + co_3_init + co_4_init)
                            v_p = T.axis.spatial(T.int64(196), eps_0_nu_0_co_0_p_0_fused % T.int64(7) * T.int64(28) + eps_2_nu_2_co_2_p_2_fused % T.int64(14) * T.int64(2) + p_3_init * T.int64(2) + p_4_init)
                            T.reads()
                            T.writes(bgemm_local[v_eps, v_nu, v_co, v_p])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            bgemm_local[v_eps, v_nu, v_co, v_p] = T.float32(0)
                    for ci_0 in range(T.int64(1)):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(8)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(224), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("data_pack_shared"):
                                        v0 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(56))
                                        v1 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused % T.int64(56) // T.int64(14))
                                        v2 = T.axis.spatial(T.int64(128), (ax0_ax1_ax2_ax3_fused_0 * T.int64(448) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) // T.int64(28))
                                        v3 = T.axis.spatial(T.int64(196), eps_0_nu_0_co_0_p_0_fused % T.int64(7) * T.int64(28) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(448) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) % T.int64(28))
                                        T.reads(data_pack[v0, v1, v2, v3])
                                        T.writes(data_pack_shared[v0, v1, v2, v3])
                                        data_pack_shared[v0, v1, v2, v3] = data_pack[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(37)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(224), thread="threadIdx.x"):
                                with T.block("p1_shared"):
                                    v0 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(56))
                                    v1 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused % T.int64(56) // T.int64(14))
                                    v2 = T.axis.spatial(T.int64(128), (ax0_ax1_ax2_ax3_fused_0 * T.int64(224) + ax0_ax1_ax2_ax3_fused_1) // T.int64(64))
                                    v3 = T.axis.spatial(T.int64(128), eps_0_nu_0_co_0_p_0_fused % T.int64(14) // T.int64(7) * T.int64(64) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(224) + ax0_ax1_ax2_ax3_fused_1) % T.int64(64))
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * T.int64(224) + ax0_ax1_ax2_ax3_fused_1 < T.int64(8192))
                                    T.reads(p1[v0, v1, v2, v3])
                                    T.writes(p1_shared[v0, v1, v2, v3])
                                    p1_shared[v0, v1, v2, v3] = p1[v0, v1, v2, v3]
                        for ci_1, eps_3, nu_3, co_3, p_3, ci_2, eps_4, nu_4, co_4, p_4 in T.grid(T.int64(32), T.int64(1), T.int64(1), T.int64(4), T.int64(1), T.int64(4), T.int64(1), T.int64(1), T.int64(1), T.int64(2)):
                            with T.block("bgemm_update"):
                                v_eps = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(56) + eps_3 + eps_4)
                                v_nu = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused % T.int64(56) // T.int64(14) + nu_3 + nu_4)
                                v_co = T.axis.spatial(T.int64(128), eps_0_nu_0_co_0_p_0_fused % T.int64(14) // T.int64(7) * T.int64(64) + eps_2_nu_2_co_2_p_2_fused // T.int64(14) * T.int64(4) + co_3 + co_4)
                                v_p = T.axis.spatial(T.int64(196), eps_0_nu_0_co_0_p_0_fused % T.int64(7) * T.int64(28) + eps_2_nu_2_co_2_p_2_fused % T.int64(14) * T.int64(2) + p_3 * T.int64(2) + p_4)
                                v_ci = T.axis.reduce(T.int64(128), ci_0 * T.int64(128) + ci_1 * T.int64(4) + ci_2)
                                T.reads(bgemm_local[v_eps, v_nu, v_co, v_p], data_pack_shared[v_eps, v_nu, v_ci, v_p], p1_shared[v_eps, v_nu, v_ci, v_co])
                                T.writes(bgemm_local[v_eps, v_nu, v_co, v_p])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                bgemm_local[v_eps, v_nu, v_co, v_p] = bgemm_local[v_eps, v_nu, v_co, v_p] + data_pack_shared[v_eps, v_nu, v_ci, v_p] * p1_shared[v_eps, v_nu, v_ci, v_co]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(1), T.int64(4), T.int64(2)):
                        with T.block("bgemm_local"):
                            v0 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(56) + ax0)
                            v1 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused % T.int64(56) // T.int64(14) + ax1)
                            v2 = T.axis.spatial(T.int64(128), eps_0_nu_0_co_0_p_0_fused % T.int64(14) // T.int64(7) * T.int64(64) + eps_2_nu_2_co_2_p_2_fused // T.int64(14) * T.int64(4) + ax2)
                            v3 = T.axis.spatial(T.int64(196), eps_0_nu_0_co_0_p_0_fused % T.int64(7) * T.int64(28) + eps_2_nu_2_co_2_p_2_fused % T.int64(14) * T.int64(2) + ax3)
                            T.reads(bgemm_local[v0, v1, v2, v3])
                            T.writes(bgemm[v0, v1, v2, v3])
                            bgemm[v0, v1, v2, v3] = bgemm_local[v0, v1, v2, v3]
        for n_co_h_0_w_0_fused_0 in T.thread_binding(T.int64(25), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 1024, "pragma_unroll_explicit": 1}):
            for n_co_h_0_w_0_fused_1 in T.thread_binding(T.int64(1024), thread="threadIdx.x"):
                for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                    for ax2 in T.unroll(T.int64(2)):
                        for ax3 in T.unroll(T.int64(2)):
                            with T.block("inverse_init"):
                                v_co = T.axis.spatial(T.int64(128), (n_co_h_0_w_0_fused_0 * T.int64(1024) + n_co_h_0_w_0_fused_1) // T.int64(196) + ax0)
                                v_p = T.axis.spatial(T.int64(196), (n_co_h_0_w_0_fused_0 * T.int64(1024) + n_co_h_0_w_0_fused_1) % T.int64(196) + ax1)
                                v_vh, v_vw = T.axis.remap("SS", [ax2, ax3])
                                T.where(n_co_h_0_w_0_fused_0 * T.int64(1024) + n_co_h_0_w_0_fused_1 < T.int64(25088))
                                T.reads()
                                T.writes(inverse_local[v_co, v_p, v_vh, v_vw])
                                T.block_attr({"schedule_rule": "conv2d_nchw_winograd_inverse"})
                                inverse_local[v_co, v_p, v_vh, v_vw] = T.float32(0)
                            for ax4 in T.unroll(T.int64(4)):
                                for ax5 in T.unroll(T.int64(4)):
                                    with T.block("inverse_update"):
                                        v_co = T.axis.spatial(T.int64(128), (n_co_h_0_w_0_fused_0 * T.int64(1024) + n_co_h_0_w_0_fused_1) // T.int64(196) + ax0)
                                        v_p = T.axis.spatial(T.int64(196), (n_co_h_0_w_0_fused_0 * T.int64(1024) + n_co_h_0_w_0_fused_1) % T.int64(196) + ax1)
                                        v_vh, v_vw, v_r_a, v_r_b = T.axis.remap("SSRR", [ax2, ax3, ax4, ax5])
                                        T.where(n_co_h_0_w_0_fused_0 * T.int64(1024) + n_co_h_0_w_0_fused_1 < T.int64(25088))
                                        T.reads(inverse_local[v_co, v_p, v_vh, v_vw], bgemm[v_r_a, v_r_b, v_co, v_p])
                                        T.writes(inverse_local[v_co, v_p, v_vh, v_vw])
                                        T.block_attr({"schedule_rule": "conv2d_nchw_winograd_inverse"})
                                        inverse_local[v_co, v_p, v_vh, v_vw] = inverse_local[v_co, v_p, v_vh, v_vw] + bgemm[v_r_a, v_r_b, v_co, v_p] * T.Select(v_r_a % T.int64(4) == T.int64(3) and v_vh % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_vh % T.int64(2) == T.int64(0), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_vh % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_vh % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_vh % T.int64(2) == T.int64(1), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_vh % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_vh % T.int64(2) == T.int64(1), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_vh % T.int64(2) == T.int64(0), T.float32(1), T.float32(0))))))))) * T.Select(v_r_b % T.int64(4) == T.int64(3) and v_vw % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_vw % T.int64(2) == T.int64(0), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_vw % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_vw % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_vw % T.int64(2) == T.int64(1), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_vw % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_vw % T.int64(2) == T.int64(1), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_vw % T.int64(2) == T.int64(0), T.float32(1), T.float32(0)))))))))
                for h_1, w_1 in T.grid(T.int64(2), T.int64(2)):
                    with T.block("conv2d_winograd"):
                        v_n = T.axis.spatial(T.int64(1), T.int64(0))
                        v_co = T.axis.spatial(T.int64(128), (n_co_h_0_w_0_fused_0 * T.int64(1024) + n_co_h_0_w_0_fused_1) // T.int64(196))
                        v_h = T.axis.spatial(T.int64(28), (n_co_h_0_w_0_fused_0 * T.int64(1024) + n_co_h_0_w_0_fused_1) % T.int64(196) // T.int64(14) * T.int64(2) + h_1)
                        v_w = T.axis.spatial(T.int64(28), (n_co_h_0_w_0_fused_0 * T.int64(1024) + n_co_h_0_w_0_fused_1) % T.int64(14) * T.int64(2) + w_1)
                        T.where(n_co_h_0_w_0_fused_0 * T.int64(1024) + n_co_h_0_w_0_fused_1 < T.int64(25088))
                        T.reads(inverse_local[v_co, v_n * T.int64(196) + v_h // T.int64(2) * T.int64(14) + v_w // T.int64(2), v_h % T.int64(2), v_w % T.int64(2)], p2[v_n, v_co, v_h, v_w], p3[v_n, v_co, T.int64(0), T.int64(0)])
                        T.writes(T_relu[v_n, v_co, v_h, v_w])
                        T_relu[v_n, v_co, v_h, v_w] = T.max(inverse_local[v_co, v_n * T.int64(196) + v_h // T.int64(2) * T.int64(14) + v_w // T.int64(2), v_h % T.int64(2), v_w % T.int64(2)] + p2[v_n, v_co, v_h, v_w] + p3[v_n, v_co, T.int64(0), T.int64(0)], T.float32(0))
b0 = sch.get_block(name="data_pack", func_name="main")
b1 = sch.get_block(name="bgemm", func_name="main")
b2 = sch.get_block(name="inverse", func_name="main")
b3 = sch.get_block(name="conv2d_winograd", func_name="main")
b4 = sch.get_block(name="T_add", func_name="main")
b5 = sch.get_block(name="T_add_1", func_name="main")
b6 = sch.get_block(name="T_relu", func_name="main")
b7 = sch.get_block(name="root", func_name="main")
b8, b9 = sch.get_producers(block=b2)
sch.compute_inline(block=b9)
b10, = sch.get_consumers(block=b2)
l11, l12, l13, l14 = sch.get_loops(block=b10)
l15, l16 = sch.split(loop=l13, factors=[None, 2], preserve_unit_iters=True)
l17, l18 = sch.split(loop=l14, factors=[None, 2], preserve_unit_iters=True)
sch.reorder(l15, l17, l16, l18)
sch.compute_at(block=b2, loop=l17, preserve_unit_loops=True, index=-1)
sch.set_scope(block=b2, buffer_index=0, storage_scope="local")
l19, l20, l21, l22, l23, l24, l25, l26, l27, l28 = sch.get_loops(block=b2)
sch.unroll(loop=l25)
sch.unroll(loop=l26)
sch.unroll(loop=l27)
sch.unroll(loop=l28)
b29, b30 = sch.get_producers(block=b0)
sch.compute_inline(block=b30)
b31, = sch.get_producers(block=b29)
l32, l33, l34, l35, l36, l37 = sch.get_loops(block=b0)
sch.reorder(l34, l35, l32, l33, l36, l37)
sch.unroll(loop=l32)
sch.unroll(loop=l33)
sch.unroll(loop=l36)
sch.unroll(loop=l37)
l38 = sch.fuse(l34, l35, preserve_unit_iters=True)
v39 = sch.sample_categorical(candidates=[32, 64, 128, 256, 512, 1024], probs=[0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666], decision=3)
l40, l41 = sch.split(loop=l38, factors=[None, v39], preserve_unit_iters=True)
sch.bind(loop=l40, thread_axis="blockIdx.x")
sch.bind(loop=l41, thread_axis="threadIdx.x")
b42 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b42, loop=l41, preserve_unit_loops=True, index=-1)
sch.compute_at(block=b29, loop=l41, preserve_unit_loops=True, index=-1)
sch.set_scope(block=b29, buffer_index=0, storage_scope="local")
sch.compute_inline(block=b31)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l43, l44, l45, l46, l47 = sch.get_loops(block=b1)
v48, v49, v50, v51, v52 = sch.sample_perfect_tile(loop=l43, n=5, max_innermost_factor=64, decision=[4, 1, 1, 1, 1])
l53, l54, l55, l56, l57 = sch.split(loop=l43, factors=[v48, v49, v50, v51, v52], preserve_unit_iters=True)
v58, v59, v60, v61, v62 = sch.sample_perfect_tile(loop=l44, n=5, max_innermost_factor=64, decision=[4, 1, 1, 1, 1])
l63, l64, l65, l66, l67 = sch.split(loop=l44, factors=[v58, v59, v60, v61, v62], preserve_unit_iters=True)
v68, v69, v70, v71, v72 = sch.sample_perfect_tile(loop=l45, n=5, max_innermost_factor=64, decision=[2, 1, 16, 4, 1])
l73, l74, l75, l76, l77 = sch.split(loop=l45, factors=[v68, v69, v70, v71, v72], preserve_unit_iters=True)
v78, v79, v80, v81, v82 = sch.sample_perfect_tile(loop=l46, n=5, max_innermost_factor=64, decision=[7, 1, 14, 1, 2])
l83, l84, l85, l86, l87 = sch.split(loop=l46, factors=[v78, v79, v80, v81, v82], preserve_unit_iters=True)
v88, v89, v90 = sch.sample_perfect_tile(loop=l47, n=3, max_innermost_factor=64, decision=[1, 32, 4])
l91, l92, l93 = sch.split(loop=l47, factors=[v88, v89, v90], preserve_unit_iters=True)
sch.reorder(l53, l63, l73, l83, l54, l64, l74, l84, l55, l65, l75, l85, l91, l92, l56, l66, l76, l86, l93, l57, l67, l77, l87)
l94 = sch.fuse(l53, l63, l73, l83, preserve_unit_iters=True)
sch.bind(loop=l94, thread_axis="blockIdx.x")
l95 = sch.fuse(l54, l64, l74, l84, preserve_unit_iters=True)
sch.bind(loop=l95, thread_axis="vthread.x")
l96 = sch.fuse(l55, l65, l75, l85, preserve_unit_iters=True)
sch.bind(loop=l96, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b97 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b97, loop=l96, preserve_unit_loops=True, index=-1)
b98 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b98, loop=l91, preserve_unit_loops=True, index=-1)
l99, l100, l101, l102, l103, l104, l105, l106 = sch.get_loops(block=b98)
l107 = sch.fuse(l103, l104, l105, l106, preserve_unit_iters=True)
v108 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b98, ann_key="meta_schedule.cooperative_fetch", ann_val=v108)
b109 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b109, loop=l91, preserve_unit_loops=True, index=-1)
l110, l111, l112, l113, l114, l115, l116, l117 = sch.get_loops(block=b109)
l118 = sch.fuse(l114, l115, l116, l117, preserve_unit_iters=True)
v119 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b109, ann_key="meta_schedule.cooperative_fetch", ann_val=v119)
sch.reverse_compute_inline(block=b6)
sch.reverse_compute_inline(block=b5)
sch.reverse_compute_inline(block=b4)
v120 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b7, ann_key="meta_schedule.unroll_explicit", ann_val=v120)
l121, l122, l123, l124, l125, l126 = sch.get_loops(block=b3)
l127 = sch.fuse(l121, l122, l123, l124, preserve_unit_iters=True)
v128 = sch.sample_categorical(candidates=[32, 64, 128, 256, 512, 1024], probs=[0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666], decision=5)
l129, l130 = sch.split(loop=l127, factors=[None, v128], preserve_unit_iters=True)
sch.bind(loop=l129, thread_axis="blockIdx.x")
sch.bind(loop=l130, thread_axis="threadIdx.x")
sch.enter_postproc()
sch.unannotate(block_or_loop=b98, ann_key="meta_schedule.cooperative_fetch")
l131, l132, l133, l134, l135 = sch.get_loops(block=b98)
l136, l137, l138 = sch.split(loop=l135, factors=[None, 224, 2], preserve_unit_iters=True)
sch.vectorize(loop=l138)
sch.bind(loop=l137, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b109, ann_key="meta_schedule.cooperative_fetch")
l139, l140, l141, l142, l143 = sch.get_loops(block=b109)
l144, l145 = sch.split(loop=l143, factors=[None, 224], preserve_unit_iters=True)
sch.bind(loop=l145, thread_axis="threadIdx.x")
b146 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b146, ann_key="meta_schedule.unroll_explicit")
b147, b148, b149, b150, b151, b152, b153, b154, b155 = sch.get_child_blocks(b146)
l156, l157, l158, l159, l160, l161 = sch.get_loops(block=b147)
l162, l163, l164, l165, l166, l167 = sch.get_loops(block=b148)
sch.annotate(block_or_loop=l162, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l162, ann_key="pragma_unroll_explicit", ann_val=1)
l168, l169, l170, l171, l172, l173 = sch.get_loops(block=b149)
l174, l175, l176, l177, l178, l179, l180 = sch.get_loops(block=b150)
l181, l182, l183, l184, l185, l186 = sch.get_loops(block=b151)
l187, l188, l189, l190, l191, l192, l193, l194, l195, l196, l197, l198, l199, l200 = sch.get_loops(block=b152)
sch.annotate(block_or_loop=l187, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l187, ann_key="pragma_unroll_explicit", ann_val=1)
l201, l202, l203, l204, l205, l206, l207 = sch.get_loops(block=b153)
l208, l209, l210, l211, l212, l213, l214, l215 = sch.get_loops(block=b154)
sch.annotate(block_or_loop=l208, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l208, ann_key="pragma_unroll_explicit", ann_val=1)
l216, l217, l218, l219 = sch.get_loops(block=b155)
b220 = sch.get_block(name="data_pack", func_name="main")
l221, l222, l223, l224, l225, l226 = sch.get_loops(block=b220)
b227 = sch.decompose_reduction(block=b220, loop=l225)
b228 = sch.get_block(name="bgemm", func_name="main")
l229, l230, l231, l232, l233, l234, l235, l236, l237, l238, l239, l240, l241, l242 = sch.get_loops(block=b228)
b243 = sch.decompose_reduction(block=b228, loop=l232)
b244 = sch.get_block(name="inverse", func_name="main")
l245, l246, l247, l248, l249, l250, l251, l252 = sch.get_loops(block=b244)
b253 = sch.decompose_reduction(block=b244, loop=l251)
2023-11-11 04:30:03 [INFO] [task_scheduler.cc:121] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #327: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/home2/xiachunwei/Software/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/home2/xiachunwei/Software/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/home2/xiachunwei/Software/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/home2/xiachunwei/Software/tvm/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/home2/xiachunwei/Software/tvm/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/home2/xiachunwei/Software/tvm/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/home2/xiachunwei/Software/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/home2/xiachunwei/Software/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  4: _ZN3tvm7runtime13PackedFun
  3: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  2: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  0: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  File "/home2/xiachunwei/Software/tvm/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(128), T.int64(28), T.int64(28)), "float32"), p1: T.Buffer((T.int64(4), T.int64(4), T.int64(128), T.int64(128)), "float32"), p2: T.Buffer((T.int64(1), T.int64(128), T.int64(28), T.int64(28)), "float32"), p3: T.Buffer((T.int64(1), T.int64(128), T.int64(1), T.int64(1)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(128), T.int64(28), T.int64(28)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        input_tile_local = T.alloc_buffer((T.int64(128), T.int64(196), T.int64(4), T.int64(4)), scope="local")
        data_pack = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(128), T.int64(196)))
        bgemm = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(128), T.int64(196)))
        inverse_local = T.alloc_buffer((T.int64(128), T.int64(196), T.int64(2), T.int64(2)), scope="local")
        data_pack_local = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(128), T.int64(196)), scope="local")
        bgemm_local = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(128), T.int64(196)), scope="local")
        data_pack_shared = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(128), T.int64(196)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(128), T.int64(128)), scope="shared")
        for ci_p_fused_0 in T.thread_binding(T.int64(98), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 1024, "pragma_unroll_explicit": 1}):
            for ci_p_fused_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(1), T.int64(4), T.int64(4)):
                    with T.block("input_tile"):
                        v_ci = T.axis.spatial(T.int64(128), (ci_p_fused_0 * T.int64(256) + ci_p_fused_1) // T.int64(196) + ax0)
                        v_p = T.axis.spatial(T.int64(196), (ci_p_fused_0 * T.int64(60) + ci_p_fused_1) % T.int64(196) + ax1)
                        v_eps, v_nu = T.axis.remap("SS", [ax2, ax3])
                        T.reads(p0[v_p // T.int64(196), v_ci, v_p % T.int64(196) // T.int64(14) * T.int64(2) + v_eps - T.int64(1), v_p % T.int64(14) * T.int64(2) + v_nu - T.int64(1)])
                        T.writes(input_tile_local[v_ci, v_p, v_eps, v_nu])
                        T.block_attr({"schedule_rule": "None"})
                        input_tile_local[v_ci, v_p, v_eps, v_nu] = T.if_then_else(T.int64(1) <= v_p % T.int64(196) // T.int64(14) * T.int64(2) + v_eps and v_p % T.int64(196) // T.int64(14) * T.int64(2) + v_eps < T.int64(29) and T.int64(1) <= v_p % T.int64(14) * T.int64(2) + v_nu and v_p % T.int64(14) * T.int64(2) + v_nu < T.int64(29), p0[v_p // T.int64(196), v_ci, v_p % T.int64(196) // T.int64(14) * T.int64(2) + v_eps - T.int64(1), v_p % T.int64(14) * T.int64(2) + v_nu - T.int64(1)], T.float32(0))
                for eps in T.unroll(T.int64(4)):
                    for nu in T.unroll(T.int64(4)):
                        with T.block("data_pack_init"):
                            v_eps, v_nu = T.axis.remap("SS", [eps, nu])
                            v_ci = T.axis.spatial(T.int64(128), (ci_p_fused_0 * T.int64(256) + ci_p_fused_1) // T.int64(196))
                            v_p = T.axis.spatial(T.int64(196), (ci_p_fused_0 * T.int64(256) + ci_p_fused_1) % T.int64(196))
                            T.reads()
                            T.writes(data_pack_local[v_eps, v_nu, v_ci, v_p])
                            T.block_attr({"schedule_rule": "conv2d_nchw_winograd_data_pack"})
                            data_pack_local[v_eps, v_nu, v_ci, v_p] = T.float32(0)
                        for r_a in T.unroll(T.int64(4)):
                            for r_b in T.unroll(T.int64(4)):
                                with T.block("data_pack_update"):
                                    v_eps, v_nu = T.axis.remap("SS", [eps, nu])
                                    v_ci = T.axis.spatial(T.int64(128), (ci_p_fused_0 * T.int64(256) + ci_p_fused_1) // T.int64(196))
                                    v_p = T.axis.spatial(T.int64(196), (ci_p_fused_0 * T.int64(256) + ci_p_fused_1) % T.int64(196))
                                    v_r_a, v_r_b = T.axis.remap("RR", [r_a, r_b])
                                    T.reads(data_pack_local[v_eps, v_nu, v_ci, v_p], input_tile_local[v_ci, v_p, v_r_a, v_r_b])
                                    T.writes(data_pack_local[v_eps, v_nu, v_ci, v_p])
                                    T.block_attr({"schedule_rule": "conv2d_nchw_winograd_data_pack"})
                                    data_pack_local[v_eps, v_nu, v_ci, v_p] = data_pack_local[v_eps, v_nu, v_ci, v_p] + input_tile_local[v_ci, v_p, v_r_a, v_r_b] * T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(3), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(1), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(0), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(3), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(1), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(0), T.float32(1), T.float32(0))))))))))))))))) * T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(3), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(1), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(0), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(3), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(1), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(0), T.float32(1), T.float32(0)))))))))))))))))
                for ax0, ax1, ax2, ax3 in T.grid(T.int64(4), T.int64(4), T.int64(1), T.int64(1)):
                    with T.block("data_pack_local"):
                        v0, v1 = T.axis.remap("SS", [ax0, ax1])
                        v2 = T.axis.spatial(T.int64(128), (ci_p_fused_0 * T.int64(256) + ci_p_fused_1) // T.int64(196) + ax2)
                        v3 = T.axis.spatial(T.int64(196), (ci_p_fused_0 * T.int64(60) + ci_p_fused_1) % T.int64(196) + ax3)
                        T.reads(data_pack_local[v0, v1, v2, v3])
                        T.writes(data_pack[v0, v1, v2, v3])
                        data_pack[v0, v1, v2, v3] = data_pack_local[v0, v1, v2, v3]
        for eps_0_nu_0_co_0_p_0_fused in T.thread_binding(T.int64(224), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 1024, "pragma_unroll_explicit": 1}):
            for eps_1_nu_1_co_1_p_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for eps_2_nu_2_co_2_p_2_fused in T.thread_binding(T.int64(224), thread="threadIdx.x"):
                    for eps_3_init, nu_3_init, co_3_init, p_3_init, eps_4_init, nu_4_init, co_4_init, p_4_init in T.grid(T.int64(1), T.int64(1), T.int64(4), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(2)):
                        with T.block("bgemm_init"):
                            v_eps = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(56) + eps_3_init + eps_4_init)
                            v_nu = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused % T.int64(56) // T.int64(14) + nu_3_init + nu_4_init)
                            v_co = T.axis.spatial(T.int64(128), eps_0_nu_0_co_0_p_0_fused % T.int64(14) // T.int64(7) * T.int64(64) + eps_2_nu_2_co_2_p_2_fused // T.int64(14) * T.int64(4) + co_3_init + co_4_init)
                            v_p = T.axis.spatial(T.int64(196), eps_0_nu_0_co_0_p_0_fused % T.int64(7) * T.int64(28) + eps_2_nu_2_co_2_p_2_fused % T.int64(14) * T.int64(2) + p_3_init * T.int64(2) + p_4_init)
                            T.reads()
                            T.writes(bgemm_local[v_eps, v_nu, v_co, v_p])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            bgemm_local[v_eps, v_nu, v_co, v_p] = T.float32(0)
                    for ci_0 in range(T.int64(4)):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(4)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(224), thread="threadIdx.x"):
                                with T.block("data_pack_shared"):
                                    v0 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(56))
                                    v1 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused % T.int64(56) // T.int64(14))
                                    v2 = T.axis.spatial(T.int64(128), ci_0 * T.int64(32) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(224) + ax0_ax1_ax2_ax3_fused_1) // T.int64(28))
                                    v3 = T.axis.spatial(T.int64(196), eps_0_nu_0_co_0_p_0_fused % T.int64(7) * T.int64(28) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(224) + ax0_ax1_ax2_ax3_fused_1) % T.int64(28))
                                    T.reads(data_pack[v0, v1, v2, v3])
                                    T.writes(data_pack_shared[v0, v1, v2, v3])
                                    data_pack_shared[v0, v1, v2, v3] = data_pack[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(3)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(224), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("p1_shared"):
                                        v0 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(56))
                                        v1 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused % T.int64(56) // T.int64(14))
                                        v2 = T.axis.spatial(T.int64(128), ci_0 * T.int64(32) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(896) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) // T.int64(64))
                                        v3 = T.axis.spatial(T.int64(128), eps_0_nu_0_co_0_p_0_fused % T.int64(14) // T.int64(7) * T.int64(64) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(896) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) % T.int64(64))
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * T.int64(224) + ax0_ax1_ax2_ax3_fused_1) * T.int64(4) + ax0_ax1_ax2_ax3_fused_2 < T.int64(2048))
                                        T.reads(p1[v0, v1, v2, v3])
                                        T.writes(p1_shared[v0, v1, v2, v3])
                                        p1_shared[v0, v1, v2, v3] = p1[v0, v1, v2, v3]
                        for ci_1, eps_3, nu_3, co_3, p_3, ci_2, eps_4, nu_4, co_4, p_4 in T.grid(T.int64(32), T.int64(1), T.int64(1), T.int64(4), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(2)):
                            with T.block("bgemm_update"):
                                v_eps = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(56) + eps_3 + eps_4)
                                v_nu = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused % T.int64(56) // T.int64(14) + nu_3 + nu_4)
                                v_co = T.axis.spatial(T.int64(128), eps_0_nu_0_co_0_p_0_fused % T.int64(14) // T.int64(7) * T.int64(64) + eps_2_nu_2_co_2_p_2_fused // T.int64(14) * T.int64(4) + co_3 + co_4)
                                v_p = T.axis.spatial(T.int64(196), eps_0_nu_0_co_0_p_0_fused % T.int64(7) * T.int64(28) + eps_2_nu_2_co_2_p_2_fused % T.int64(14) * T.int64(2) + p_3 * T.int64(2) + p_4)
                                v_ci = T.axis.reduce(T.int64(128), ci_0 * T.int64(32) + ci_1 + ci_2)
                                T.reads(bgemm_local[v_eps, v_nu, v_co, v_p], data_pack_shared[v_eps, v_nu, v_ci, v_p], p1_shared[v_eps, v_nu, v_ci, v_co])
                                T.writes(bgemm_local[v_eps, v_nu, v_co, v_p])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                bgemm_local[v_eps, v_nu, v_co, v_p] = bgemm_local[v_eps, v_nu, v_co, v_p] + data_pack_shared[v_eps, v_nu, v_ci, v_p] * p1_shared[v_eps, v_nu, v_ci, v_co]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(1), T.int64(4), T.int64(2)):
                        with T.block("bgemm_local"):
                            v0 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(56) + ax0)
                            v1 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused % T.int64(56) // T.int64(14) + ax1)
                            v2 = T.axis.spatial(T.int64(128), eps_0_nu_0_co_0_p_0_fused % T.int64(14) // T.int64(7) * T.int64(64) + eps_2_nu_2_co_2_p_2_fused // T.int64(14) * T.int64(4) + ax2)
                            v3 = T.axis.spatial(T.int64(196), eps_0_nu_0_co_0_p_0_fused % T.int64(7) * T.int64(28) + eps_2_nu_2_co_2_p_2_fused % T.int64(14) * T.int64(2) + ax3)
                            T.reads(bgemm_local[v0, v1, v2, v3])
                            T.writes(bgemm[v0, v1, v2, v3])
                            bgemm[v0, v1, v2, v3] = bgemm_local[v0, v1, v2, v3]
        for n_co_h_0_w_0_fused_0 in T.thread_binding(T.int64(25), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 1024, "pragma_unroll_explicit": 1}):
            for n_co_h_0_w_0_fused_1 in T.thread_binding(T.int64(1024), thread="threadIdx.x"):
                for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                    for ax2 in T.unroll(T.int64(2)):
                        for ax3 in T.unroll(T.int64(2)):
                            with T.block("inverse_init"):
                                v_co = T.axis.spatial(T.int64(128), (n_co_h_0_w_0_fused_0 * T.int64(1024) + n_co_h_0_w_0_fused_1) // T.int64(196) + ax0)
                                v_p = T.axis.spatial(T.int64(196), (n_co_h_0_w_0_fused_0 * T.int64(1024) + n_co_h_0_w_0_fused_1) % T.int64(196) + ax1)
                                v_vh, v_vw = T.axis.remap("SS", [ax2, ax3])
                                T.where(n_co_h_0_w_0_fused_0 * T.int64(1024) + n_co_h_0_w_0_fused_1 < T.int64(25088))
                                T.reads()
                                T.writes(inverse_local[v_co, v_p, v_vh, v_vw])
                                T.block_attr({"schedule_rule": "conv2d_nchw_winograd_inverse"})
                                inverse_local[v_co, v_p, v_vh, v_vw] = T.float32(0)
                            for ax4 in T.unroll(T.int64(4)):
                                for ax5 in T.unroll(T.int64(4)):
                                    with T.block("inverse_update"):
                                        v_co = T.axis.spatial(T.int64(128), (n_co_h_0_w_0_fused_0 * T.int64(1024) + n_co_h_0_w_0_fused_1) // T.int64(196) + ax0)
                                        v_p = T.axis.spatial(T.int64(196), (n_co_h_0_w_0_fused_0 * T.int64(1024) + n_co_h_0_w_0_fused_1) % T.int64(196) + ax1)
                                        v_vh, v_vw, v_r_a, v_r_b = T.axis.remap("SSRR", [ax2, ax3, ax4, ax5])
                                        T.where(n_co_h_0_w_0_fused_0 * T.int64(1024) + n_co_h_0_w_0_fused_1 < T.int64(25088))
                                        T.reads(inverse_local[v_co, v_p, v_vh, v_vw], bgemm[v_r_a, v_r_b, v_co, v_p])
                                        T.writes(inverse_local[v_co, v_p, v_vh, v_vw])
                                        T.block_attr({"schedule_rule": "conv2d_nchw_winograd_inverse"})
                                        inverse_local[v_co, v_p, v_vh, v_vw] = inverse_local[v_co, v_p, v_vh, v_vw] + bgemm[v_r_a, v_r_b, v_co, v_p] * T.Select(v_r_a % T.int64(4) == T.int64(3) and v_vh % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_vh % T.int64(2) == T.int64(0), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_vh % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_vh % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_vh % T.int64(2) == T.int64(1), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_vh % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_vh % T.int64(2) == T.int64(1), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_vh % T.int64(2) == T.int64(0), T.float32(1), T.float32(0))))))))) * T.Select(v_r_b % T.int64(4) == T.int64(3) and v_vw % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_vw % T.int64(2) == T.int64(0), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_vw % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_vw % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_vw % T.int64(2) == T.int64(1), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_vw % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_vw % T.int64(2) == T.int64(1), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_vw % T.int64(2) == T.int64(0), T.float32(1), T.float32(0)))))))))
                for h_1, w_1 in T.grid(T.int64(2), T.int64(2)):
                    with T.block("conv2d_winograd"):
                        v_n = T.axis.spatial(T.int64(1), T.int64(0))
                        v_co = T.axis.spatial(T.int64(128), (n_co_h_0_w_0_fused_0 * T.int64(1024) + n_co_h_0_w_0_fused_1) // T.int64(196))
                        v_h = T.axis.spatial(T.int64(28), (n_co_h_0_w_0_fused_0 * T.int64(1024) + n_co_h_0_w_0_fused_1) % T.int64(196) // T.int64(14) * T.int64(2) + h_1)
                        v_w = T.axis.spatial(T.int64(28), (n_co_h_0_w_0_fused_0 * T.int64(1024) + n_co_h_0_w_0_fused_1) % T.int64(14) * T.int64(2) + w_1)
                        T.where(n_co_h_0_w_0_fused_0 * T.int64(1024) + n_co_h_0_w_0_fused_1 < T.int64(25088))
                        T.reads(inverse_local[v_co, v_n * T.int64(196) + v_h // T.int64(2) * T.int64(14) + v_w // T.int64(2), v_h % T.int64(2), v_w % T.int64(2)], p2[v_n, v_co, v_h, v_w], p3[v_n, v_co, T.int64(0), T.int64(0)])
                        T.writes(T_relu[v_n, v_co, v_h, v_w])
                        T_relu[v_n, v_co, v_h, v_w] = T.max(inverse_local[v_co, v_n * T.int64(196) + v_h // T.int64(2) * T.int64(14) + v_w // T.int64(2), v_h % T.int64(2), v_w % T.int64(2)] + p2[v_n, v_co, v_h, v_w] + p3[v_n, v_co, T.int64(0), T.int64(0)], T.float32(0))
b0 = sch.get_block(name="data_pack", func_name="main")
b1 = sch.get_block(name="bgemm", func_name="main")
b2 = sch.get_block(name="inverse", func_name="main")
b3 = sch.get_block(name="conv2d_winograd", func_name="main")
b4 = sch.get_block(name="T_add", func_name="main")
b5 = sch.get_block(name="T_add_1", func_name="main")
b6 = sch.get_block(name="T_relu", func_name="main")
b7 = sch.get_block(name="root", func_name="main")
b8, b9 = sch.get_producers(block=b2)
sch.compute_inline(block=b9)
b10, = sch.get_consumers(block=b2)
l11, l12, l13, l14 = sch.get_loops(block=b10)
l15, l16 = sch.split(loop=l13, factors=[None, 2], preserve_unit_iters=True)
l17, l18 = sch.split(loop=l14, factors=[None, 2], preserve_unit_iters=True)
sch.reorder(l15, l17, l16, l18)
sch.compute_at(block=b2, loop=l17, preserve_unit_loops=True, index=-1)
sch.set_scope(block=b2, buffer_index=0, storage_scope="local")
l19, l20, l21, l22, l23, l24, l25, l26, l27, l28 = sch.get_loops(block=b2)
sch.unroll(loop=l25)
sch.unroll(loop=l26)
sch.unroll(loop=l27)
sch.unroll(loop=l28)
b29, b30 = sch.get_producers(block=b0)
sch.compute_inline(block=b30)
b31, = sch.get_producers(block=b29)
l32, l33, l34, l35, l36, l37 = sch.get_loops(block=b0)
sch.reorder(l34, l35, l32, l33, l36, l37)
sch.unroll(loop=l32)
sch.unroll(loop=l33)
sch.unroll(loop=l36)
sch.unroll(loop=l37)
l38 = sch.fuse(l34, l35, preserve_unit_iters=True)
v39 = sch.sample_categorical(candidates=[32, 64, 128, 256, 512, 1024], probs=[0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666], decision=3)
l40, l41 = sch.split(loop=l38, factors=[None, v39], preserve_unit_iters=True)
sch.bind(loop=l40, thread_axis="blockIdx.x")
sch.bind(loop=l41, thread_axis="threadIdx.x")
b42 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b42, loop=l41, preserve_unit_loops=True, index=-1)
sch.compute_at(block=b29, loop=l41, preserve_unit_loops=True, index=-1)
sch.set_scope(block=b29, buffer_index=0, storage_scope="local")
sch.compute_inline(block=b31)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l43, l44, l45, l46, l47 = sch.get_loops(block=b1)
v48, v49, v50, v51, v52 = sch.sample_perfect_tile(loop=l43, n=5, max_innermost_factor=64, decision=[4, 1, 1, 1, 1])
l53, l54, l55, l56, l57 = sch.split(loop=l43, factors=[v48, v49, v50, v51, v52], preserve_unit_iters=True)
v58, v59, v60, v61, v62 = sch.sample_perfect_tile(loop=l44, n=5, max_innermost_factor=64, decision=[4, 1, 1, 1, 1])
l63, l64, l65, l66, l67 = sch.split(loop=l44, factors=[v58, v59, v60, v61, v62], preserve_unit_iters=True)
v68, v69, v70, v71, v72 = sch.sample_perfect_tile(loop=l45, n=5, max_innermost_factor=64, decision=[2, 1, 16, 4, 1])
l73, l74, l75, l76, l77 = sch.split(loop=l45, factors=[v68, v69, v70, v71, v72], preserve_unit_iters=True)
v78, v79, v80, v81, v82 = sch.sample_perfect_tile(loop=l46, n=5, max_innermost_factor=64, decision=[7, 1, 14, 1, 2])
l83, l84, l85, l86, l87 = sch.split(loop=l46, factors=[v78, v79, v80, v81, v82], preserve_unit_iters=True)
v88, v89, v90 = sch.sample_perfect_tile(loop=l47, n=3, max_innermost_factor=64, decision=[4, 32, 1])
l91, l92, l93 = sch.split(loop=l47, factors=[v88, v89, v90], preserve_unit_iters=True)
sch.reorder(l53, l63, l73, l83, l54, l64, l74, l84, l55, l65, l75, l85, l91, l92, l56, l66, l76, l86, l93, l57, l67, l77, l87)
l94 = sch.fuse(l53, l63, l73, l83, preserve_unit_iters=True)
sch.bind(loop=l94, thread_axis="blockIdx.x")
l95 = sch.fuse(l54, l64, l74, l84, preserve_unit_iters=True)
sch.bind(loop=l95, thread_axis="vthread.x")
l96 = sch.fuse(l55, l65, l75, l85, preserve_unit_iters=True)
sch.bind(loop=l96, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b97 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b97, loop=l96, preserve_unit_loops=True, index=-1)
b98 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b98, loop=l91, preserve_unit_loops=True, index=-1)
l99, l100, l101, l102, l103, l104, l105, l106 = sch.get_loops(block=b98)
l107 = sch.fuse(l103, l104, l105, l106, preserve_unit_iters=True)
v108 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b98, ann_key="meta_schedule.cooperative_fetch", ann_val=v108)
b109 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b109, loop=l91, preserve_unit_loops=True, index=-1)
l110, l111, l112, l113, l114, l115, l116, l117 = sch.get_loops(block=b109)
l118 = sch.fuse(l114, l115, l116, l117, preserve_unit_iters=True)
v119 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b109, ann_key="meta_schedule.cooperative_fetch", ann_val=v119)
sch.reverse_compute_inline(block=b6)
sch.reverse_compute_inline(block=b5)
sch.reverse_compute_inline(block=b4)
v120 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b7, ann_key="meta_schedule.unroll_explicit", ann_val=v120)
l121, l122, l123, l124, l125, l126 = sch.get_loops(block=b3)
l127 = sch.fuse(l121, l122, l123, l124, preserve_unit_iters=True)
v128 = sch.sample_categorical(candidates=[32, 64, 128, 256, 512, 1024], probs=[0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666], decision=5)
l129, l130 = sch.split(loop=l127, factors=[None, v128], preserve_unit_iters=True)
sch.bind(loop=l129, thread_axis="blockIdx.x")
sch.bind(loop=l130, thread_axis="threadIdx.x")
sch.enter_postproc()
sch.unannotate(block_or_loop=b98, ann_key="meta_schedule.cooperative_fetch")
l131, l132, l133, l134, l135 = sch.get_loops(block=b98)
l136, l137 = sch.split(loop=l135, factors=[None, 224], preserve_unit_iters=True)
sch.bind(loop=l137, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b109, ann_key="meta_schedule.cooperative_fetch")
l138, l139, l140, l141, l142 = sch.get_loops(block=b109)
l143, l144, l145 = sch.split(loop=l142, factors=[None, 224, 4], preserve_unit_iters=True)
sch.vectorize(loop=l145)
sch.bind(loop=l144, thread_axis="threadIdx.x")
b146 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b146, ann_key="meta_schedule.unroll_explicit")
b147, b148, b149, b150, b151, b152, b153, b154, b155 = sch.get_child_blocks(b146)
l156, l157, l158, l159, l160, l161 = sch.get_loops(block=b147)
l162, l163, l164, l165, l166, l167 = sch.get_loops(block=b148)
sch.annotate(block_or_loop=l162, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l162, ann_key="pragma_unroll_explicit", ann_val=1)
l168, l169, l170, l171, l172, l173 = sch.get_loops(block=b149)
l174, l175, l176, l177, l178, l179 = sch.get_loops(block=b150)
l180, l181, l182, l183, l184, l185, l186 = sch.get_loops(block=b151)
l187, l188, l189, l190, l191, l192, l193, l194, l195, l196, l197, l198, l199, l200 = sch.get_loops(block=b152)
sch.annotate(block_or_loop=l187, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l187, ann_key="pragma_unroll_explicit", ann_val=1)
l201, l202, l203, l204, l205, l206, l207 = sch.get_loops(block=b153)
l208, l209, l210, l211, l212, l213, l214, l215 = sch.get_loops(block=b154)
sch.annotate(block_or_loop=l208, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l208, ann_key="pragma_unroll_explicit", ann_val=1)
l216, l217, l218, l219 = sch.get_loops(block=b155)
b220 = sch.get_block(name="data_pack", func_name="main")
l221, l222, l223, l224, l225, l226 = sch.get_loops(block=b220)
b227 = sch.decompose_reduction(block=b220, loop=l225)
b228 = sch.get_block(name="bgemm", func_name="main")
l229, l230, l231, l232, l233, l234, l235, l236, l237, l238, l239, l240, l241, l242 = sch.get_loops(block=b228)
b243 = sch.decompose_reduction(block=b228, loop=l232)
b244 = sch.get_block(name="inverse", func_name="main")
l245, l246, l247, l248, l249, l250, l251, l252 = sch.get_loops(block=b244)
b253 = sch.decompose_reduction(block=b244, loop=l251)
2023-11-11 04:30:03 [INFO] [task_scheduler.cc:121] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #328: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/home2/xiachunwei/Software/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/home2/xiachunwei/Software/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/home2/xiachunwei/Software/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/home2/xiachunwei/Software/tvm/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/home2/xiachunwei/Software/tvm/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/home2/xiachunwei/Software/tvm/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/home2/xiachunwei/Software/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/home2/xiachunwei/Software/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  4: _ZN3tvm7runtime13PackedFun
  3: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  2: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  0: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  File "/home2/xiachunwei/Software/tvm/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(128), T.int64(28), T.int64(28)), "float32"), p1: T.Buffer((T.int64(4), T.int64(4), T.int64(128), T.int64(128)), "float32"), p2: T.Buffer((T.int64(1), T.int64(128), T.int64(28), T.int64(28)), "float32"), p3: T.Buffer((T.int64(1), T.int64(128), T.int64(1), T.int64(1)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(128), T.int64(28), T.int64(28)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        input_tile_local = T.alloc_buffer((T.int64(128), T.int64(196), T.int64(4), T.int64(4)), scope="local")
        data_pack = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(128), T.int64(196)))
        bgemm = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(128), T.int64(196)))
        inverse_local = T.alloc_buffer((T.int64(128), T.int64(196), T.int64(2), T.int64(2)), scope="local")
        data_pack_local = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(128), T.int64(196)), scope="local")
        bgemm_local = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(128), T.int64(196)), scope="local")
        data_pack_shared = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(128), T.int64(196)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(128), T.int64(128)), scope="shared")
        for ci_p_fused_0 in T.thread_binding(T.int64(49), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 1024, "pragma_unroll_explicit": 1}):
            for ci_p_fused_1 in T.thread_binding(T.int64(512), thread="threadIdx.x"):
                for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(1), T.int64(4), T.int64(4)):
                    with T.block("input_tile"):
                        v_ci = T.axis.spatial(T.int64(128), (ci_p_fused_0 * T.int64(512) + ci_p_fused_1) // T.int64(196) + ax0)
                        v_p = T.axis.spatial(T.int64(196), (ci_p_fused_0 * T.int64(120) + ci_p_fused_1) % T.int64(196) + ax1)
                        v_eps, v_nu = T.axis.remap("SS", [ax2, ax3])
                        T.reads(p0[v_p // T.int64(196), v_ci, v_p % T.int64(196) // T.int64(14) * T.int64(2) + v_eps - T.int64(1), v_p % T.int64(14) * T.int64(2) + v_nu - T.int64(1)])
                        T.writes(input_tile_local[v_ci, v_p, v_eps, v_nu])
                        T.block_attr({"schedule_rule": "None"})
                        input_tile_local[v_ci, v_p, v_eps, v_nu] = T.if_then_else(T.int64(1) <= v_p % T.int64(196) // T.int64(14) * T.int64(2) + v_eps and v_p % T.int64(196) // T.int64(14) * T.int64(2) + v_eps < T.int64(29) and T.int64(1) <= v_p % T.int64(14) * T.int64(2) + v_nu and v_p % T.int64(14) * T.int64(2) + v_nu < T.int64(29), p0[v_p // T.int64(196), v_ci, v_p % T.int64(196) // T.int64(14) * T.int64(2) + v_eps - T.int64(1), v_p % T.int64(14) * T.int64(2) + v_nu - T.int64(1)], T.float32(0))
                for eps in T.unroll(T.int64(4)):
                    for nu in T.unroll(T.int64(4)):
                        with T.block("data_pack_init"):
                            v_eps, v_nu = T.axis.remap("SS", [eps, nu])
                            v_ci = T.axis.spatial(T.int64(128), (ci_p_fused_0 * T.int64(512) + ci_p_fused_1) // T.int64(196))
                            v_p = T.axis.spatial(T.int64(196), (ci_p_fused_0 * T.int64(512) + ci_p_fused_1) % T.int64(196))
                            T.reads()
                            T.writes(data_pack_local[v_eps, v_nu, v_ci, v_p])
                            T.block_attr({"schedule_rule": "conv2d_nchw_winograd_data_pack"})
                            data_pack_local[v_eps, v_nu, v_ci, v_p] = T.float32(0)
                        for r_a in T.unroll(T.int64(4)):
                            for r_b in T.unroll(T.int64(4)):
                                with T.block("data_pack_update"):
                                    v_eps, v_nu = T.axis.remap("SS", [eps, nu])
                                    v_ci = T.axis.spatial(T.int64(128), (ci_p_fused_0 * T.int64(512) + ci_p_fused_1) // T.int64(196))
                                    v_p = T.axis.spatial(T.int64(196), (ci_p_fused_0 * T.int64(512) + ci_p_fused_1) % T.int64(196))
                                    v_r_a, v_r_b = T.axis.remap("RR", [r_a, r_b])
                                    T.reads(data_pack_local[v_eps, v_nu, v_ci, v_p], input_tile_local[v_ci, v_p, v_r_a, v_r_b])
                                    T.writes(data_pack_local[v_eps, v_nu, v_ci, v_p])
                                    T.block_attr({"schedule_rule": "conv2d_nchw_winograd_data_pack"})
                                    data_pack_local[v_eps, v_nu, v_ci, v_p] = data_pack_local[v_eps, v_nu, v_ci, v_p] + input_tile_local[v_ci, v_p, v_r_a, v_r_b] * T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(3), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(1), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(0), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(3), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(1), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(0), T.float32(1), T.float32(0))))))))))))))))) * T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(3), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(1), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(0), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(3), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(1), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(0), T.float32(1), T.float32(0)))))))))))))))))
                for ax0, ax1, ax2, ax3 in T.grid(T.int64(4), T.int64(4), T.int64(1), T.int64(1)):
                    with T.block("data_pack_local"):
                        v0, v1 = T.axis.remap("SS", [ax0, ax1])
                        v2 = T.axis.spatial(T.int64(128), (ci_p_fused_0 * T.int64(512) + ci_p_fused_1) // T.int64(196) + ax2)
                        v3 = T.axis.spatial(T.int64(196), (ci_p_fused_0 * T.int64(120) + ci_p_fused_1) % T.int64(196) + ax3)
                        T.reads(data_pack_local[v0, v1, v2, v3])
                        T.writes(data_pack[v0, v1, v2, v3])
                        data_pack[v0, v1, v2, v3] = data_pack_local[v0, v1, v2, v3]
        for eps_0_nu_0_co_0_p_0_fused in T.thread_binding(T.int64(448), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 1024, "pragma_unroll_explicit": 1}):
            for eps_1_nu_1_co_1_p_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for eps_2_nu_2_co_2_p_2_fused in T.thread_binding(T.int64(224), thread="threadIdx.x"):
                    for eps_3_init, nu_3_init, co_3_init, p_3_init, eps_4_init, nu_4_init, co_4_init, p_4_init in T.grid(T.int64(1), T.int64(1), T.int64(2), T.int64(2), T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("bgemm_init"):
                            v_eps = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(112) + eps_3_init + eps_4_init)
                            v_nu = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused % T.int64(112) // T.int64(56) * T.int64(2) + eps_2_nu_2_co_2_p_2_fused // T.int64(112) + nu_3_init + nu_4_init)
                            v_co = T.axis.spatial(T.int64(128), eps_0_nu_0_co_0_p_0_fused % T.int64(56) // T.int64(7) * T.int64(16) + eps_2_nu_2_co_2_p_2_fused % T.int64(112) // T.int64(14) * T.int64(2) + co_3_init + co_4_init)
                            v_p = T.axis.spatial(T.int64(196), eps_0_nu_0_co_0_p_0_fused % T.int64(7) * T.int64(28) + eps_2_nu_2_co_2_p_2_fused % T.int64(14) * T.int64(2) + p_3_init + p_4_init)
                            T.reads()
                            T.writes(bgemm_local[v_eps, v_nu, v_co, v_p])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            bgemm_local[v_eps, v_nu, v_co, v_p] = T.float32(0)
                    for ci_0 in range(T.int64(1)):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(8)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(224), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("data_pack_shared"):
                                        v0 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(112))
                                        v1 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused % T.int64(112) // T.int64(56) * T.int64(2) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(896) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) // T.int64(3584))
                                        v2 = T.axis.spatial(T.int64(128), (ax0_ax1_ax2_ax3_fused_0 * T.int64(896) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) % T.int64(3584) // T.int64(28))
                                        v3 = T.axis.spatial(T.int64(196), eps_0_nu_0_co_0_p_0_fused % T.int64(7) * T.int64(28) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(896) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) % T.int64(28))
                                        T.reads(data_pack[v0, v1, v2, v3])
                                        T.writes(data_pack_shared[v0, v1, v2, v3])
                                        data_pack_shared[v0, v1, v2, v3] = data_pack[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(5)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(224), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("p1_shared"):
                                        v0 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(112))
                                        v1 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused % T.int64(112) // T.int64(56) * T.int64(2) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(896) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) // T.int64(2048))
                                        v2 = T.axis.spatial(T.int64(128), (ax0_ax1_ax2_ax3_fused_0 * T.int64(896) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) % T.int64(2048) // T.int64(16))
                                        v3 = T.axis.spatial(T.int64(128), eps_0_nu_0_co_0_p_0_fused % T.int64(56) // T.int64(7) * T.int64(16) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(896) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) % T.int64(16))
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * T.int64(224) + ax0_ax1_ax2_ax3_fused_1) * T.int64(4) + ax0_ax1_ax2_ax3_fused_2 < T.int64(4096))
                                        T.reads(p1[v0, v1, v2, v3])
                                        T.writes(p1_shared[v0, v1, v2, v3])
                                        p1_shared[v0, v1, v2, v3] = p1[v0, v1, v2, v3]
                        for ci_1, eps_3, nu_3, co_3, p_3, ci_2, eps_4, nu_4, co_4, p_4 in T.grid(T.int64(16), T.int64(1), T.int64(1), T.int64(2), T.int64(2), T.int64(8), T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                            with T.block("bgemm_update"):
                                v_eps = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(112) + eps_3 + eps_4)
                                v_nu = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused % T.int64(112) // T.int64(56) * T.int64(2) + eps_2_nu_2_co_2_p_2_fused // T.int64(112) + nu_3 + nu_4)
                                v_co = T.axis.spatial(T.int64(128), eps_0_nu_0_co_0_p_0_fused % T.int64(56) // T.int64(7) * T.int64(16) + eps_2_nu_2_co_2_p_2_fused % T.int64(112) // T.int64(14) * T.int64(2) + co_3 + co_4)
                                v_p = T.axis.spatial(T.int64(196), eps_0_nu_0_co_0_p_0_fused % T.int64(7) * T.int64(28) + eps_2_nu_2_co_2_p_2_fused % T.int64(14) * T.int64(2) + p_3 + p_4)
                                v_ci = T.axis.reduce(T.int64(128), ci_0 * T.int64(128) + ci_1 * T.int64(8) + ci_2)
                                T.reads(bgemm_local[v_eps, v_nu, v_co, v_p], data_pack_shared[v_eps, v_nu, v_ci, v_p], p1_shared[v_eps, v_nu, v_ci, v_co])
                                T.writes(bgemm_local[v_eps, v_nu, v_co, v_p])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                bgemm_local[v_eps, v_nu, v_co, v_p] = bgemm_local[v_eps, v_nu, v_co, v_p] + data_pack_shared[v_eps, v_nu, v_ci, v_p] * p1_shared[v_eps, v_nu, v_ci, v_co]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(1), T.int64(2), T.int64(2)):
                        with T.block("bgemm_local"):
                            v0 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(112) + ax0)
                            v1 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused % T.int64(112) // T.int64(56) * T.int64(2) + eps_2_nu_2_co_2_p_2_fused // T.int64(112) + ax1)
                            v2 = T.axis.spatial(T.int64(128), eps_0_nu_0_co_0_p_0_fused % T.int64(56) // T.int64(7) * T.int64(16) + eps_2_nu_2_co_2_p_2_fused % T.int64(112) // T.int64(14) * T.int64(2) + ax2)
                            v3 = T.axis.spatial(T.int64(196), eps_0_nu_0_co_0_p_0_fused % T.int64(7) * T.int64(28) + eps_2_nu_2_co_2_p_2_fused % T.int64(14) * T.int64(2) + ax3)
                            T.reads(bgemm_local[v0, v1, v2, v3])
                            T.writes(bgemm[v0, v1, v2, v3])
                            bgemm[v0, v1, v2, v3] = bgemm_local[v0, v1, v2, v3]
        for n_co_h_0_w_0_fused_0 in T.thread_binding(T.int64(392), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 1024, "pragma_unroll_explicit": 1}):
            for n_co_h_0_w_0_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                    for ax2 in T.unroll(T.int64(2)):
                        for ax3 in T.unroll(T.int64(2)):
                            with T.block("inverse_init"):
                                v_co = T.axis.spatial(T.int64(128), (n_co_h_0_w_0_fused_0 * T.int64(64) + n_co_h_0_w_0_fused_1) // T.int64(196) + ax0)
                                v_p = T.axis.spatial(T.int64(196), (n_co_h_0_w_0_fused_0 * T.int64(64) + n_co_h_0_w_0_fused_1) % T.int64(196) + ax1)
                                v_vh, v_vw = T.axis.remap("SS", [ax2, ax3])
                                T.reads()
                                T.writes(inverse_local[v_co, v_p, v_vh, v_vw])
                                T.block_attr({"schedule_rule": "conv2d_nchw_winograd_inverse"})
                                inverse_local[v_co, v_p, v_vh, v_vw] = T.float32(0)
                            for ax4 in T.unroll(T.int64(4)):
                                for ax5 in T.unroll(T.int64(4)):
                                    with T.block("inverse_update"):
                                        v_co = T.axis.spatial(T.int64(128), (n_co_h_0_w_0_fused_0 * T.int64(64) + n_co_h_0_w_0_fused_1) // T.int64(196) + ax0)
                                        v_p = T.axis.spatial(T.int64(196), (n_co_h_0_w_0_fused_0 * T.int64(64) + n_co_h_0_w_0_fused_1) % T.int64(196) + ax1)
                                        v_vh, v_vw, v_r_a, v_r_b = T.axis.remap("SSRR", [ax2, ax3, ax4, ax5])
                                        T.reads(inverse_local[v_co, v_p, v_vh, v_vw], bgemm[v_r_a, v_r_b, v_co, v_p])
                                        T.writes(inverse_local[v_co, v_p, v_vh, v_vw])
                                        T.block_attr({"schedule_rule": "conv2d_nchw_winograd_inverse"})
                                        inverse_local[v_co, v_p, v_vh, v_vw] = inverse_local[v_co, v_p, v_vh, v_vw] + bgemm[v_r_a, v_r_b, v_co, v_p] * T.Select(v_r_a % T.int64(4) == T.int64(3) and v_vh % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_vh % T.int64(2) == T.int64(0), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_vh % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_vh % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_vh % T.int64(2) == T.int64(1), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_vh % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_vh % T.int64(2) == T.int64(1), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_vh % T.int64(2) == T.int64(0), T.float32(1), T.float32(0))))))))) * T.Select(v_r_b % T.int64(4) == T.int64(3) and v_vw % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_vw % T.int64(2) == T.int64(0), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_vw % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_vw % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_vw % T.int64(2) == T.int64(1), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_vw % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_vw % T.int64(2) == T.int64(1), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_vw % T.int64(2) == T.int64(0), T.float32(1), T.float32(0)))))))))
                for h_1, w_1 in T.grid(T.int64(2), T.int64(2)):
                    with T.block("conv2d_winograd"):
                        v_n = T.axis.spatial(T.int64(1), T.int64(0))
                        v_co = T.axis.spatial(T.int64(128), (n_co_h_0_w_0_fused_0 * T.int64(64) + n_co_h_0_w_0_fused_1) // T.int64(196))
                        v_h = T.axis.spatial(T.int64(28), (n_co_h_0_w_0_fused_0 * T.int64(64) + n_co_h_0_w_0_fused_1) % T.int64(196) // T.int64(14) * T.int64(2) + h_1)
                        v_w = T.axis.spatial(T.int64(28), (n_co_h_0_w_0_fused_0 * T.int64(64) + n_co_h_0_w_0_fused_1) % T.int64(14) * T.int64(2) + w_1)
                        T.reads(inverse_local[v_co, v_n * T.int64(196) + v_h // T.int64(2) * T.int64(14) + v_w // T.int64(2), v_h % T.int64(2), v_w % T.int64(2)], p2[v_n, v_co, v_h, v_w], p3[v_n, v_co, T.int64(0), T.int64(0)])
                        T.writes(T_relu[v_n, v_co, v_h, v_w])
                        T_relu[v_n, v_co, v_h, v_w] = T.max(inverse_local[v_co, v_n * T.int64(196) + v_h // T.int64(2) * T.int64(14) + v_w // T.int64(2), v_h % T.int64(2), v_w % T.int64(2)] + p2[v_n, v_co, v_h, v_w] + p3[v_n, v_co, T.int64(0), T.int64(0)], T.float32(0))
b0 = sch.get_block(name="data_pack", func_name="main")
b1 = sch.get_block(name="bgemm", func_name="main")
b2 = sch.get_block(name="inverse", func_name="main")
b3 = sch.get_block(name="conv2d_winograd", func_name="main")
b4 = sch.get_block(name="T_add", func_name="main")
b5 = sch.get_block(name="T_add_1", func_name="main")
b6 = sch.get_block(name="T_relu", func_name="main")
b7 = sch.get_block(name="root", func_name="main")
b8, b9 = sch.get_producers(block=b2)
sch.compute_inline(block=b9)
b10, = sch.get_consumers(block=b2)
l11, l12, l13, l14 = sch.get_loops(block=b10)
l15, l16 = sch.split(loop=l13, factors=[None, 2], preserve_unit_iters=True)
l17, l18 = sch.split(loop=l14, factors=[None, 2], preserve_unit_iters=True)
sch.reorder(l15, l17, l16, l18)
sch.compute_at(block=b2, loop=l17, preserve_unit_loops=True, index=-1)
sch.set_scope(block=b2, buffer_index=0, storage_scope="local")
l19, l20, l21, l22, l23, l24, l25, l26, l27, l28 = sch.get_loops(block=b2)
sch.unroll(loop=l25)
sch.unroll(loop=l26)
sch.unroll(loop=l27)
sch.unroll(loop=l28)
b29, b30 = sch.get_producers(block=b0)
sch.compute_inline(block=b30)
b31, = sch.get_producers(block=b29)
l32, l33, l34, l35, l36, l37 = sch.get_loops(block=b0)
sch.reorder(l34, l35, l32, l33, l36, l37)
sch.unroll(loop=l32)
sch.unroll(loop=l33)
sch.unroll(loop=l36)
sch.unroll(loop=l37)
l38 = sch.fuse(l34, l35, preserve_unit_iters=True)
v39 = sch.sample_categorical(candidates=[32, 64, 128, 256, 512, 1024], probs=[0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666], decision=4)
l40, l41 = sch.split(loop=l38, factors=[None, v39], preserve_unit_iters=True)
sch.bind(loop=l40, thread_axis="blockIdx.x")
sch.bind(loop=l41, thread_axis="threadIdx.x")
b42 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b42, loop=l41, preserve_unit_loops=True, index=-1)
sch.compute_at(block=b29, loop=l41, preserve_unit_loops=True, index=-1)
sch.set_scope(block=b29, buffer_index=0, storage_scope="local")
sch.compute_inline(block=b31)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l43, l44, l45, l46, l47 = sch.get_loops(block=b1)
v48, v49, v50, v51, v52 = sch.sample_perfect_tile(loop=l43, n=5, max_innermost_factor=64, decision=[4, 1, 1, 1, 1])
l53, l54, l55, l56, l57 = sch.split(loop=l43, factors=[v48, v49, v50, v51, v52], preserve_unit_iters=True)
v58, v59, v60, v61, v62 = sch.sample_perfect_tile(loop=l44, n=5, max_innermost_factor=64, decision=[2, 1, 2, 1, 1])
l63, l64, l65, l66, l67 = sch.split(loop=l44, factors=[v58, v59, v60, v61, v62], preserve_unit_iters=True)
v68, v69, v70, v71, v72 = sch.sample_perfect_tile(loop=l45, n=5, max_innermost_factor=64, decision=[8, 1, 8, 2, 1])
l73, l74, l75, l76, l77 = sch.split(loop=l45, factors=[v68, v69, v70, v71, v72], preserve_unit_iters=True)
v78, v79, v80, v81, v82 = sch.sample_perfect_tile(loop=l46, n=5, max_innermost_factor=64, decision=[7, 1, 14, 2, 1])
l83, l84, l85, l86, l87 = sch.split(loop=l46, factors=[v78, v79, v80, v81, v82], preserve_unit_iters=True)
v88, v89, v90 = sch.sample_perfect_tile(loop=l47, n=3, max_innermost_factor=64, decision=[1, 16, 8])
l91, l92, l93 = sch.split(loop=l47, factors=[v88, v89, v90], preserve_unit_iters=True)
sch.reorder(l53, l63, l73, l83, l54, l64, l74, l84, l55, l65, l75, l85, l91, l92, l56, l66, l76, l86, l93, l57, l67, l77, l87)
l94 = sch.fuse(l53, l63, l73, l83, preserve_unit_iters=True)
sch.bind(loop=l94, thread_axis="blockIdx.x")
l95 = sch.fuse(l54, l64, l74, l84, preserve_unit_iters=True)
sch.bind(loop=l95, thread_axis="vthread.x")
l96 = sch.fuse(l55, l65, l75, l85, preserve_unit_iters=True)
sch.bind(loop=l96, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b97 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b97, loop=l96, preserve_unit_loops=True, index=-1)
b98 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b98, loop=l91, preserve_unit_loops=True, index=-1)
l99, l100, l101, l102, l103, l104, l105, l106 = sch.get_loops(block=b98)
l107 = sch.fuse(l103, l104, l105, l106, preserve_unit_iters=True)
v108 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b98, ann_key="meta_schedule.cooperative_fetch", ann_val=v108)
b109 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b109, loop=l91, preserve_unit_loops=True, index=-1)
l110, l111, l112, l113, l114, l115, l116, l117 = sch.get_loops(block=b109)
l118 = sch.fuse(l114, l115, l116, l117, preserve_unit_iters=True)
v119 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b109, ann_key="meta_schedule.cooperative_fetch", ann_val=v119)
sch.reverse_compute_inline(block=b6)
sch.reverse_compute_inline(block=b5)
sch.reverse_compute_inline(block=b4)
v120 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b7, ann_key="meta_schedule.unroll_explicit", ann_val=v120)
l121, l122, l123, l124, l125, l126 = sch.get_loops(block=b3)
l127 = sch.fuse(l121, l122, l123, l124, preserve_unit_iters=True)
v128 = sch.sample_categorical(candidates=[32, 64, 128, 256, 512, 1024], probs=[0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666], decision=1)
l129, l130 = sch.split(loop=l127, factors=[None, v128], preserve_unit_iters=True)
sch.bind(loop=l129, thread_axis="blockIdx.x")
sch.bind(loop=l130, thread_axis="threadIdx.x")
sch.enter_postproc()
sch.unannotate(block_or_loop=b98, ann_key="meta_schedule.cooperative_fetch")
l131, l132, l133, l134, l135 = sch.get_loops(block=b98)
l136, l137, l138 = sch.split(loop=l135, factors=[None, 224, 4], preserve_unit_iters=True)
sch.vectorize(loop=l138)
sch.bind(loop=l137, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b109, ann_key="meta_schedule.cooperative_fetch")
l139, l140, l141, l142, l143 = sch.get_loops(block=b109)
l144, l145, l146 = sch.split(loop=l143, factors=[None, 224, 4], preserve_unit_iters=True)
sch.vectorize(loop=l146)
sch.bind(loop=l145, thread_axis="threadIdx.x")
b147 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b147, ann_key="meta_schedule.unroll_explicit")
b148, b149, b150, b151, b152, b153, b154, b155, b156 = sch.get_child_blocks(b147)
l157, l158, l159, l160, l161, l162 = sch.get_loops(block=b148)
l163, l164, l165, l166, l167, l168 = sch.get_loops(block=b149)
sch.annotate(block_or_loop=l163, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l163, ann_key="pragma_unroll_explicit", ann_val=1)
l169, l170, l171, l172, l173, l174 = sch.get_loops(block=b150)
l175, l176, l177, l178, l179, l180, l181 = sch.get_loops(block=b151)
l182, l183, l184, l185, l186, l187, l188 = sch.get_loops(block=b152)
l189, l190, l191, l192, l193, l194, l195, l196, l197, l198, l199, l200, l201, l202 = sch.get_loops(block=b153)
sch.annotate(block_or_loop=l189, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l189, ann_key="pragma_unroll_explicit", ann_val=1)
l203, l204, l205, l206, l207, l208, l209 = sch.get_loops(block=b154)
l210, l211, l212, l213, l214, l215, l216, l217 = sch.get_loops(block=b155)
sch.annotate(block_or_loop=l210, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l210, ann_key="pragma_unroll_explicit", ann_val=1)
l218, l219, l220, l221 = sch.get_loops(block=b156)
b222 = sch.get_block(name="data_pack", func_name="main")
l223, l224, l225, l226, l227, l228 = sch.get_loops(block=b222)
b229 = sch.decompose_reduction(block=b222, loop=l227)
b230 = sch.get_block(name="bgemm", func_name="main")
l231, l232, l233, l234, l235, l236, l237, l238, l239, l240, l241, l242, l243, l244 = sch.get_loops(block=b230)
b245 = sch.decompose_reduction(block=b230, loop=l234)
b246 = sch.get_block(name="inverse", func_name="main")
l247, l248, l249, l250, l251, l252, l253, l254 = sch.get_loops(block=b246)
b255 = sch.decompose_reduction(block=b246, loop=l253)
2023-11-11 04:30:03 [INFO] [task_scheduler.cc:121] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #329: Error in running:
LocalRunner: An exception occurred
Subprocess terminated
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(128), T.int64(28), T.int64(28)), "float32"), p1: T.Buffer((T.int64(4), T.int64(4), T.int64(128), T.int64(128)), "float32"), p2: T.Buffer((T.int64(1), T.int64(128), T.int64(28), T.int64(28)), "float32"), p3: T.Buffer((T.int64(1), T.int64(128), T.int64(1), T.int64(1)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(128), T.int64(28), T.int64(28)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        input_tile_local = T.alloc_buffer((T.int64(128), T.int64(196), T.int64(4), T.int64(4)), scope="local")
        data_pack = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(128), T.int64(196)))
        bgemm = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(128), T.int64(196)))
        inverse_local = T.alloc_buffer((T.int64(128), T.int64(196), T.int64(2), T.int64(2)), scope="local")
        data_pack_local = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(128), T.int64(196)), scope="local")
        bgemm_local = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(128), T.int64(196)), scope="local")
        data_pack_shared = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(128), T.int64(196)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(128), T.int64(128)), scope="shared")
        for ci_p_fused_0 in T.thread_binding(T.int64(98), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 1024, "pragma_unroll_explicit": 1}):
            for ci_p_fused_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(1), T.int64(4), T.int64(4)):
                    with T.block("input_tile"):
                        v_ci = T.axis.spatial(T.int64(128), (ci_p_fused_0 * T.int64(256) + ci_p_fused_1) // T.int64(196) + ax0)
                        v_p = T.axis.spatial(T.int64(196), (ci_p_fused_0 * T.int64(60) + ci_p_fused_1) % T.int64(196) + ax1)
                        v_eps, v_nu = T.axis.remap("SS", [ax2, ax3])
                        T.reads(p0[v_p // T.int64(196), v_ci, v_p % T.int64(196) // T.int64(14) * T.int64(2) + v_eps - T.int64(1), v_p % T.int64(14) * T.int64(2) + v_nu - T.int64(1)])
                        T.writes(input_tile_local[v_ci, v_p, v_eps, v_nu])
                        T.block_attr({"schedule_rule": "None"})
                        input_tile_local[v_ci, v_p, v_eps, v_nu] = T.if_then_else(T.int64(1) <= v_p % T.int64(196) // T.int64(14) * T.int64(2) + v_eps and v_p % T.int64(196) // T.int64(14) * T.int64(2) + v_eps < T.int64(29) and T.int64(1) <= v_p % T.int64(14) * T.int64(2) + v_nu and v_p % T.int64(14) * T.int64(2) + v_nu < T.int64(29), p0[v_p // T.int64(196), v_ci, v_p % T.int64(196) // T.int64(14) * T.int64(2) + v_eps - T.int64(1), v_p % T.int64(14) * T.int64(2) + v_nu - T.int64(1)], T.float32(0))
                for eps in T.unroll(T.int64(4)):
                    for nu in T.unroll(T.int64(4)):
                        with T.block("data_pack_init"):
                            v_eps, v_nu = T.axis.remap("SS", [eps, nu])
                            v_ci = T.axis.spatial(T.int64(128), (ci_p_fused_0 * T.int64(256) + ci_p_fused_1) // T.int64(196))
                            v_p = T.axis.spatial(T.int64(196), (ci_p_fused_0 * T.int64(256) + ci_p_fused_1) % T.int64(196))
                            T.reads()
                            T.writes(data_pack_local[v_eps, v_nu, v_ci, v_p])
                            T.block_attr({"schedule_rule": "conv2d_nchw_winograd_data_pack"})
                            data_pack_local[v_eps, v_nu, v_ci, v_p] = T.float32(0)
                        for r_a in T.unroll(T.int64(4)):
                            for r_b in T.unroll(T.int64(4)):
                                with T.block("data_pack_update"):
                                    v_eps, v_nu = T.axis.remap("SS", [eps, nu])
                                    v_ci = T.axis.spatial(T.int64(128), (ci_p_fused_0 * T.int64(256) + ci_p_fused_1) // T.int64(196))
                                    v_p = T.axis.spatial(T.int64(196), (ci_p_fused_0 * T.int64(256) + ci_p_fused_1) % T.int64(196))
                                    v_r_a, v_r_b = T.axis.remap("RR", [r_a, r_b])
                                    T.reads(data_pack_local[v_eps, v_nu, v_ci, v_p], input_tile_local[v_ci, v_p, v_r_a, v_r_b])
                                    T.writes(data_pack_local[v_eps, v_nu, v_ci, v_p])
                                    T.block_attr({"schedule_rule": "conv2d_nchw_winograd_data_pack"})
                                    data_pack_local[v_eps, v_nu, v_ci, v_p] = data_pack_local[v_eps, v_nu, v_ci, v_p] + input_tile_local[v_ci, v_p, v_r_a, v_r_b] * T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(3), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(1), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(0), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(3), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(1), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(0), T.float32(1), T.float32(0))))))))))))))))) * T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(3), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(1), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(0), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(3), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(1), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(0), T.float32(1), T.float32(0)))))))))))))))))
                for ax0, ax1, ax2, ax3 in T.grid(T.int64(4), T.int64(4), T.int64(1), T.int64(1)):
                    with T.block("data_pack_local"):
                        v0, v1 = T.axis.remap("SS", [ax0, ax1])
                        v2 = T.axis.spatial(T.int64(128), (ci_p_fused_0 * T.int64(256) + ci_p_fused_1) // T.int64(196) + ax2)
                        v3 = T.axis.spatial(T.int64(196), (ci_p_fused_0 * T.int64(60) + ci_p_fused_1) % T.int64(196) + ax3)
                        T.reads(data_pack_local[v0, v1, v2, v3])
                        T.writes(data_pack[v0, v1, v2, v3])
                        data_pack[v0, v1, v2, v3] = data_pack_local[v0, v1, v2, v3]
        for eps_0_nu_0_co_0_p_0_fused in T.thread_binding(T.int64(224), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 1024, "pragma_unroll_explicit": 1}):
            for eps_1_nu_1_co_1_p_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for eps_2_nu_2_co_2_p_2_fused in T.thread_binding(T.int64(224), thread="threadIdx.x"):
                    for eps_3_init, nu_3_init, co_3_init, p_3_init, eps_4_init, nu_4_init, co_4_init, p_4_init in T.grid(T.int64(1), T.int64(1), T.int64(4), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(2)):
                        with T.block("bgemm_init"):
                            v_eps = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(56) + eps_3_init + eps_4_init)
                            v_nu = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused % T.int64(56) // T.int64(14) + nu_3_init + nu_4_init)
                            v_co = T.axis.spatial(T.int64(128), eps_0_nu_0_co_0_p_0_fused % T.int64(14) // T.int64(7) * T.int64(64) + eps_2_nu_2_co_2_p_2_fused // T.int64(14) * T.int64(4) + co_3_init + co_4_init)
                            v_p = T.axis.spatial(T.int64(196), eps_0_nu_0_co_0_p_0_fused % T.int64(7) * T.int64(28) + eps_2_nu_2_co_2_p_2_fused % T.int64(14) * T.int64(2) + p_3_init * T.int64(2) + p_4_init)
                            T.reads()
                            T.writes(bgemm_local[v_eps, v_nu, v_co, v_p])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            bgemm_local[v_eps, v_nu, v_co, v_p] = T.float32(0)
                    for ci_0 in range(T.int64(4)):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(4)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(224), thread="threadIdx.x"):
                                with T.block("data_pack_shared"):
                                    v0 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(56))
                                    v1 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused % T.int64(56) // T.int64(14))
                                    v2 = T.axis.spatial(T.int64(128), ci_0 * T.int64(32) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(224) + ax0_ax1_ax2_ax3_fused_1) // T.int64(28))
                                    v3 = T.axis.spatial(T.int64(196), eps_0_nu_0_co_0_p_0_fused % T.int64(7) * T.int64(28) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(224) + ax0_ax1_ax2_ax3_fused_1) % T.int64(28))
                                    T.reads(data_pack[v0, v1, v2, v3])
                                    T.writes(data_pack_shared[v0, v1, v2, v3])
                                    data_pack_shared[v0, v1, v2, v3] = data_pack[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(5)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(224), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("p1_shared"):
                                        v0 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(56))
                                        v1 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused % T.int64(56) // T.int64(14))
                                        v2 = T.axis.spatial(T.int64(128), ci_0 * T.int64(32) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(448) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) // T.int64(64))
                                        v3 = T.axis.spatial(T.int64(128), eps_0_nu_0_co_0_p_0_fused % T.int64(14) // T.int64(7) * T.int64(64) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(448) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) % T.int64(64))
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * T.int64(224) + ax0_ax1_ax2_ax3_fused_1) * T.int64(2) + ax0_ax1_ax2_ax3_fused_2 < T.int64(2048))
                                        T.reads(p1[v0, v1, v2, v3])
                                        T.writes(p1_shared[v0, v1, v2, v3])
                                        p1_shared[v0, v1, v2, v3] = p1[v0, v1, v2, v3]
                        for ci_1, eps_3, nu_3, co_3, p_3, ci_2, eps_4, nu_4, co_4, p_4 in T.grid(T.int64(32), T.int64(1), T.int64(1), T.int64(4), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(2)):
                            with T.block("bgemm_update"):
                                v_eps = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(56) + eps_3 + eps_4)
                                v_nu = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused % T.int64(56) // T.int64(14) + nu_3 + nu_4)
                                v_co = T.axis.spatial(T.int64(128), eps_0_nu_0_co_0_p_0_fused % T.int64(14) // T.int64(7) * T.int64(64) + eps_2_nu_2_co_2_p_2_fused // T.int64(14) * T.int64(4) + co_3 + co_4)
                                v_p = T.axis.spatial(T.int64(196), eps_0_nu_0_co_0_p_0_fused % T.int64(7) * T.int64(28) + eps_2_nu_2_co_2_p_2_fused % T.int64(14) * T.int64(2) + p_3 * T.int64(2) + p_4)
                                v_ci = T.axis.reduce(T.int64(128), ci_0 * T.int64(32) + ci_1 + ci_2)
                                T.reads(bgemm_local[v_eps, v_nu, v_co, v_p], data_pack_shared[v_eps, v_nu, v_ci, v_p], p1_shared[v_eps, v_nu, v_ci, v_co])
                                T.writes(bgemm_local[v_eps, v_nu, v_co, v_p])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                bgemm_local[v_eps, v_nu, v_co, v_p] = bgemm_local[v_eps, v_nu, v_co, v_p] + data_pack_shared[v_eps, v_nu, v_ci, v_p] * p1_shared[v_eps, v_nu, v_ci, v_co]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(1), T.int64(4), T.int64(2)):
                        with T.block("bgemm_local"):
                            v0 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(56) + ax0)
                            v1 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused % T.int64(56) // T.int64(14) + ax1)
                            v2 = T.axis.spatial(T.int64(128), eps_0_nu_0_co_0_p_0_fused % T.int64(14) // T.int64(7) * T.int64(64) + eps_2_nu_2_co_2_p_2_fused // T.int64(14) * T.int64(4) + ax2)
                            v3 = T.axis.spatial(T.int64(196), eps_0_nu_0_co_0_p_0_fused % T.int64(7) * T.int64(28) + eps_2_nu_2_co_2_p_2_fused % T.int64(14) * T.int64(2) + ax3)
                            T.reads(bgemm_local[v0, v1, v2, v3])
                            T.writes(bgemm[v0, v1, v2, v3])
                            bgemm[v0, v1, v2, v3] = bgemm_local[v0, v1, v2, v3]
        for n_co_h_0_w_0_fused_0 in T.thread_binding(T.int64(25), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 1024, "pragma_unroll_explicit": 1}):
            for n_co_h_0_w_0_fused_1 in T.thread_binding(T.int64(1024), thread="threadIdx.x"):
                for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                    for ax2 in T.unroll(T.int64(2)):
                        for ax3 in T.unroll(T.int64(2)):
                            with T.block("inverse_init"):
                                v_co = T.axis.spatial(T.int64(128), (n_co_h_0_w_0_fused_0 * T.int64(1024) + n_co_h_0_w_0_fused_1) // T.int64(196) + ax0)
                                v_p = T.axis.spatial(T.int64(196), (n_co_h_0_w_0_fused_0 * T.int64(1024) + n_co_h_0_w_0_fused_1) % T.int64(196) + ax1)
                                v_vh, v_vw = T.axis.remap("SS", [ax2, ax3])
                                T.where(n_co_h_0_w_0_fused_0 * T.int64(1024) + n_co_h_0_w_0_fused_1 < T.int64(25088))
                                T.reads()
                                T.writes(inverse_local[v_co, v_p, v_vh, v_vw])
                                T.block_attr({"schedule_rule": "conv2d_nchw_winograd_inverse"})
                                inverse_local[v_co, v_p, v_vh, v_vw] = T.float32(0)
                            for ax4 in T.unroll(T.int64(4)):
                                for ax5 in T.unroll(T.int64(4)):
                                    with T.block("inverse_update"):
                                        v_co = T.axis.spatial(T.int64(128), (n_co_h_0_w_0_fused_0 * T.int64(1024) + n_co_h_0_w_0_fused_1) // T.int64(196) + ax0)
                                        v_p = T.axis.spatial(T.int64(196), (n_co_h_0_w_0_fused_0 * T.int64(1024) + n_co_h_0_w_0_fused_1) % T.int64(196) + ax1)
                                        v_vh, v_vw, v_r_a, v_r_b = T.axis.remap("SSRR", [ax2, ax3, ax4, ax5])
                                        T.where(n_co_h_0_w_0_fused_0 * T.int64(1024) + n_co_h_0_w_0_fused_1 < T.int64(25088))
                                        T.reads(inverse_local[v_co, v_p, v_vh, v_vw], bgemm[v_r_a, v_r_b, v_co, v_p])
                                        T.writes(inverse_local[v_co, v_p, v_vh, v_vw])
                                        T.block_attr({"schedule_rule": "conv2d_nchw_winograd_inverse"})
                                        inverse_local[v_co, v_p, v_vh, v_vw] = inverse_local[v_co, v_p, v_vh, v_vw] + bgemm[v_r_a, v_r_b, v_co, v_p] * T.Select(v_r_a % T.int64(4) == T.int64(3) and v_vh % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_vh % T.int64(2) == T.int64(0), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_vh % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_vh % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_vh % T.int64(2) == T.int64(1), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_vh % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_vh % T.int64(2) == T.int64(1), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_vh % T.int64(2) == T.int64(0), T.float32(1), T.float32(0))))))))) * T.Select(v_r_b % T.int64(4) == T.int64(3) and v_vw % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_vw % T.int64(2) == T.int64(0), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_vw % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_vw % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_vw % T.int64(2) == T.int64(1), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_vw % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_vw % T.int64(2) == T.int64(1), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_vw % T.int64(2) == T.int64(0), T.float32(1), T.float32(0)))))))))
                for h_1, w_1 in T.grid(T.int64(2), T.int64(2)):
                    with T.block("conv2d_winograd"):
                        v_n = T.axis.spatial(T.int64(1), T.int64(0))
                        v_co = T.axis.spatial(T.int64(128), (n_co_h_0_w_0_fused_0 * T.int64(1024) + n_co_h_0_w_0_fused_1) // T.int64(196))
                        v_h = T.axis.spatial(T.int64(28), (n_co_h_0_w_0_fused_0 * T.int64(1024) + n_co_h_0_w_0_fused_1) % T.int64(196) // T.int64(14) * T.int64(2) + h_1)
                        v_w = T.axis.spatial(T.int64(28), (n_co_h_0_w_0_fused_0 * T.int64(1024) + n_co_h_0_w_0_fused_1) % T.int64(14) * T.int64(2) + w_1)
                        T.where(n_co_h_0_w_0_fused_0 * T.int64(1024) + n_co_h_0_w_0_fused_1 < T.int64(25088))
                        T.reads(inverse_local[v_co, v_n * T.int64(196) + v_h // T.int64(2) * T.int64(14) + v_w // T.int64(2), v_h % T.int64(2), v_w % T.int64(2)], p2[v_n, v_co, v_h, v_w], p3[v_n, v_co, T.int64(0), T.int64(0)])
                        T.writes(T_relu[v_n, v_co, v_h, v_w])
                        T_relu[v_n, v_co, v_h, v_w] = T.max(inverse_local[v_co, v_n * T.int64(196) + v_h // T.int64(2) * T.int64(14) + v_w // T.int64(2), v_h % T.int64(2), v_w % T.int64(2)] + p2[v_n, v_co, v_h, v_w] + p3[v_n, v_co, T.int64(0), T.int64(0)], T.float32(0))
b0 = sch.get_block(name="data_pack", func_name="main")
b1 = sch.get_block(name="bgemm", func_name="main")
b2 = sch.get_block(name="inverse", func_name="main")
b3 = sch.get_block(name="conv2d_winograd", func_name="main")
b4 = sch.get_block(name="T_add", func_name="main")
b5 = sch.get_block(name="T_add_1", func_name="main")
b6 = sch.get_block(name="T_relu", func_name="main")
b7 = sch.get_block(name="root", func_name="main")
b8, b9 = sch.get_producers(block=b2)
sch.compute_inline(block=b9)
b10, = sch.get_consumers(block=b2)
l11, l12, l13, l14 = sch.get_loops(block=b10)
l15, l16 = sch.split(loop=l13, factors=[None, 2], preserve_unit_iters=True)
l17, l18 = sch.split(loop=l14, factors=[None, 2], preserve_unit_iters=True)
sch.reorder(l15, l17, l16, l18)
sch.compute_at(block=b2, loop=l17, preserve_unit_loops=True, index=-1)
sch.set_scope(block=b2, buffer_index=0, storage_scope="local")
l19, l20, l21, l22, l23, l24, l25, l26, l27, l28 = sch.get_loops(block=b2)
sch.unroll(loop=l25)
sch.unroll(loop=l26)
sch.unroll(loop=l27)
sch.unroll(loop=l28)
b29, b30 = sch.get_producers(block=b0)
sch.compute_inline(block=b30)
b31, = sch.get_producers(block=b29)
l32, l33, l34, l35, l36, l37 = sch.get_loops(block=b0)
sch.reorder(l34, l35, l32, l33, l36, l37)
sch.unroll(loop=l32)
sch.unroll(loop=l33)
sch.unroll(loop=l36)
sch.unroll(loop=l37)
l38 = sch.fuse(l34, l35, preserve_unit_iters=True)
v39 = sch.sample_categorical(candidates=[32, 64, 128, 256, 512, 1024], probs=[0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666], decision=3)
l40, l41 = sch.split(loop=l38, factors=[None, v39], preserve_unit_iters=True)
sch.bind(loop=l40, thread_axis="blockIdx.x")
sch.bind(loop=l41, thread_axis="threadIdx.x")
b42 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b42, loop=l41, preserve_unit_loops=True, index=-1)
sch.compute_at(block=b29, loop=l41, preserve_unit_loops=True, index=-1)
sch.set_scope(block=b29, buffer_index=0, storage_scope="local")
sch.compute_inline(block=b31)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l43, l44, l45, l46, l47 = sch.get_loops(block=b1)
v48, v49, v50, v51, v52 = sch.sample_perfect_tile(loop=l43, n=5, max_innermost_factor=64, decision=[4, 1, 1, 1, 1])
l53, l54, l55, l56, l57 = sch.split(loop=l43, factors=[v48, v49, v50, v51, v52], preserve_unit_iters=True)
v58, v59, v60, v61, v62 = sch.sample_perfect_tile(loop=l44, n=5, max_innermost_factor=64, decision=[4, 1, 1, 1, 1])
l63, l64, l65, l66, l67 = sch.split(loop=l44, factors=[v58, v59, v60, v61, v62], preserve_unit_iters=True)
v68, v69, v70, v71, v72 = sch.sample_perfect_tile(loop=l45, n=5, max_innermost_factor=64, decision=[2, 1, 16, 4, 1])
l73, l74, l75, l76, l77 = sch.split(loop=l45, factors=[v68, v69, v70, v71, v72], preserve_unit_iters=True)
v78, v79, v80, v81, v82 = sch.sample_perfect_tile(loop=l46, n=5, max_innermost_factor=64, decision=[7, 1, 14, 1, 2])
l83, l84, l85, l86, l87 = sch.split(loop=l46, factors=[v78, v79, v80, v81, v82], preserve_unit_iters=True)
v88, v89, v90 = sch.sample_perfect_tile(loop=l47, n=3, max_innermost_factor=64, decision=[4, 32, 1])
l91, l92, l93 = sch.split(loop=l47, factors=[v88, v89, v90], preserve_unit_iters=True)
sch.reorder(l53, l63, l73, l83, l54, l64, l74, l84, l55, l65, l75, l85, l91, l92, l56, l66, l76, l86, l93, l57, l67, l77, l87)
l94 = sch.fuse(l53, l63, l73, l83, preserve_unit_iters=True)
sch.bind(loop=l94, thread_axis="blockIdx.x")
l95 = sch.fuse(l54, l64, l74, l84, preserve_unit_iters=True)
sch.bind(loop=l95, thread_axis="vthread.x")
l96 = sch.fuse(l55, l65, l75, l85, preserve_unit_iters=True)
sch.bind(loop=l96, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b97 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b97, loop=l96, preserve_unit_loops=True, index=-1)
b98 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b98, loop=l91, preserve_unit_loops=True, index=-1)
l99, l100, l101, l102, l103, l104, l105, l106 = sch.get_loops(block=b98)
l107 = sch.fuse(l103, l104, l105, l106, preserve_unit_iters=True)
v108 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b98, ann_key="meta_schedule.cooperative_fetch", ann_val=v108)
b109 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b109, loop=l91, preserve_unit_loops=True, index=-1)
l110, l111, l112, l113, l114, l115, l116, l117 = sch.get_loops(block=b109)
l118 = sch.fuse(l114, l115, l116, l117, preserve_unit_iters=True)
v119 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b109, ann_key="meta_schedule.cooperative_fetch", ann_val=v119)
sch.reverse_compute_inline(block=b6)
sch.reverse_compute_inline(block=b5)
sch.reverse_compute_inline(block=b4)
v120 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b7, ann_key="meta_schedule.unroll_explicit", ann_val=v120)
l121, l122, l123, l124, l125, l126 = sch.get_loops(block=b3)
l127 = sch.fuse(l121, l122, l123, l124, preserve_unit_iters=True)
v128 = sch.sample_categorical(candidates=[32, 64, 128, 256, 512, 1024], probs=[0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666], decision=5)
l129, l130 = sch.split(loop=l127, factors=[None, v128], preserve_unit_iters=True)
sch.bind(loop=l129, thread_axis="blockIdx.x")
sch.bind(loop=l130, thread_axis="threadIdx.x")
sch.enter_postproc()
sch.unannotate(block_or_loop=b98, ann_key="meta_schedule.cooperative_fetch")
l131, l132, l133, l134, l135 = sch.get_loops(block=b98)
l136, l137 = sch.split(loop=l135, factors=[None, 224], preserve_unit_iters=True)
sch.bind(loop=l137, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b109, ann_key="meta_schedule.cooperative_fetch")
l138, l139, l140, l141, l142 = sch.get_loops(block=b109)
l143, l144, l145 = sch.split(loop=l142, factors=[None, 224, 2], preserve_unit_iters=True)
sch.vectorize(loop=l145)
sch.bind(loop=l144, thread_axis="threadIdx.x")
b146 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b146, ann_key="meta_schedule.unroll_explicit")
b147, b148, b149, b150, b151, b152, b153, b154, b155 = sch.get_child_blocks(b146)
l156, l157, l158, l159, l160, l161 = sch.get_loops(block=b147)
l162, l163, l164, l165, l166, l167 = sch.get_loops(block=b148)
sch.annotate(block_or_loop=l162, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l162, ann_key="pragma_unroll_explicit", ann_val=1)
l168, l169, l170, l171, l172, l173 = sch.get_loops(block=b149)
l174, l175, l176, l177, l178, l179 = sch.get_loops(block=b150)
l180, l181, l182, l183, l184, l185, l186 = sch.get_loops(block=b151)
l187, l188, l189, l190, l191, l192, l193, l194, l195, l196, l197, l198, l199, l200 = sch.get_loops(block=b152)
sch.annotate(block_or_loop=l187, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l187, ann_key="pragma_unroll_explicit", ann_val=1)
l201, l202, l203, l204, l205, l206, l207 = sch.get_loops(block=b153)
l208, l209, l210, l211, l212, l213, l214, l215 = sch.get_loops(block=b154)
sch.annotate(block_or_loop=l208, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l208, ann_key="pragma_unroll_explicit", ann_val=1)
l216, l217, l218, l219 = sch.get_loops(block=b155)
b220 = sch.get_block(name="data_pack", func_name="main")
l221, l222, l223, l224, l225, l226 = sch.get_loops(block=b220)
b227 = sch.decompose_reduction(block=b220, loop=l225)
b228 = sch.get_block(name="bgemm", func_name="main")
l229, l230, l231, l232, l233, l234, l235, l236, l237, l238, l239, l240, l241, l242 = sch.get_loops(block=b228)
b243 = sch.decompose_reduction(block=b228, loop=l232)
b244 = sch.get_block(name="inverse", func_name="main")
l245, l246, l247, l248, l249, l250, l251, l252 = sch.get_loops(block=b244)
b253 = sch.decompose_reduction(block=b244, loop=l251)
2023-11-11 04:30:03 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #330: GFLOPs: 4975.4950. Time: 25.5544 us. Best GFLOPs: 5114.6288
2023-11-11 04:30:03 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #331: GFLOPs: 4881.1093. Time: 26.0486 us. Best GFLOPs: 5114.6288
2023-11-11 04:30:03 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #332: GFLOPs: 4867.7409. Time: 26.1201 us. Best GFLOPs: 5114.6288
2023-11-11 04:30:03 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #333: GFLOPs: 4888.0159. Time: 26.0118 us. Best GFLOPs: 5114.6288
2023-11-11 04:30:03 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #334: GFLOPs: 5070.4207. Time: 25.0760 us. Best GFLOPs: 5114.6288
2023-11-11 04:30:03 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #335: GFLOPs: 5151.1462. Time: 24.6830 us. Best GFLOPs: 5151.1462
2023-11-11 04:30:03 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #336: GFLOPs: 4862.6727. Time: 26.1473 us. Best GFLOPs: 5151.1462
2023-11-11 04:30:03 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #337: GFLOPs: 5085.4322. Time: 25.0020 us. Best GFLOPs: 5151.1462
2023-11-11 04:30:03 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #338: GFLOPs: 5137.3069. Time: 24.7495 us. Best GFLOPs: 5151.1462
2023-11-11 04:30:03 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #339: GFLOPs: 4864.3326. Time: 26.1384 us. Best GFLOPs: 5151.1462
2023-11-11 04:30:03 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #340: GFLOPs: 4928.6478. Time: 25.7973 us. Best GFLOPs: 5151.1462
2023-11-11 04:30:03 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #341: GFLOPs: 4266.5042. Time: 29.8010 us. Best GFLOPs: 5151.1462
2023-11-11 04:30:03 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #342: GFLOPs: 3678.0814. Time: 34.5686 us. Best GFLOPs: 5151.1462
2023-11-11 04:30:03 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #343: GFLOPs: 5150.3585. Time: 24.6868 us. Best GFLOPs: 5151.1462
2023-11-11 04:30:03 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #344: GFLOPs: 4627.6558. Time: 27.4752 us. Best GFLOPs: 5151.1462
2023-11-11 04:30:03 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #345: GFLOPs: 4627.4126. Time: 27.4767 us. Best GFLOPs: 5151.1462
2023-11-11 04:30:03 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #346: GFLOPs: 4627.7779. Time: 27.4745 us. Best GFLOPs: 5151.1462
2023-11-11 04:30:03 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #347: GFLOPs: 4193.6959. Time: 30.3184 us. Best GFLOPs: 5151.1462
2023-11-11 04:30:03 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #348: GFLOPs: 4629.3985. Time: 27.4649 us. Best GFLOPs: 5151.1462
2023-11-11 04:30:03 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #349: GFLOPs: 4843.8995. Time: 26.2487 us. Best GFLOPs: 5151.1462
2023-11-11 04:30:03 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #350: GFLOPs: 5052.0884. Time: 25.1670 us. Best GFLOPs: 5151.1462
2023-11-11 04:30:03 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #351: GFLOPs: 4821.7887. Time: 26.3690 us. Best GFLOPs: 5151.1462
2023-11-11 04:30:03 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #352: GFLOPs: 4191.3477. Time: 30.3353 us. Best GFLOPs: 5151.1462
2023-11-11 04:30:03 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #353: GFLOPs: 5052.8572. Time: 25.1632 us. Best GFLOPs: 5151.1462
2023-11-11 04:30:03 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #354: GFLOPs: 4758.0151. Time: 26.7225 us. Best GFLOPs: 5151.1462
2023-11-11 04:30:03 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #355: GFLOPs: 3681.7246. Time: 34.5344 us. Best GFLOPs: 5151.1462
2023-11-11 04:30:03 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #356: GFLOPs: 4862.1761. Time: 26.1500 us. Best GFLOPs: 5151.1462
2023-11-11 04:30:03 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #357: GFLOPs: 4736.3809. Time: 26.8445 us. Best GFLOPs: 5151.1462
2023-11-11 04:30:03 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #358: GFLOPs: 4776.1301. Time: 26.6211 us. Best GFLOPs: 5151.1462
2023-11-11 04:30:03 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #359: GFLOPs: 4395.8261. Time: 28.9243 us. Best GFLOPs: 5151.1462
2023-11-11 04:30:03 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #360: GFLOPs: 4840.1806. Time: 26.2689 us. Best GFLOPs: 5151.1462
2023-11-11 04:30:03 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #361: GFLOPs: 4735.9443. Time: 26.8470 us. Best GFLOPs: 5151.1462
2023-11-11 04:30:03 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #362: GFLOPs: 5139.5570. Time: 24.7387 us. Best GFLOPs: 5151.1462
2023-11-11 04:30:03 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #363: GFLOPs: 4395.5636. Time: 28.9260 us. Best GFLOPs: 5151.1462
2023-11-11 04:30:03 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #364: GFLOPs: 4854.3562. Time: 26.1921 us. Best GFLOPs: 5151.1462
2023-11-11 04:30:03 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #365: GFLOPs: 4937.1031. Time: 25.7532 us. Best GFLOPs: 5151.1462
2023-11-11 04:30:03 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #366: GFLOPs: 4353.5119. Time: 29.2054 us. Best GFLOPs: 5151.1462
2023-11-11 04:30:03 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #367: GFLOPs: 4396.6153. Time: 28.9191 us. Best GFLOPs: 5151.1462
2023-11-11 04:30:03 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #368: GFLOPs: 4366.1076. Time: 29.1211 us. Best GFLOPs: 5151.1462
2023-11-11 04:30:03 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #369: GFLOPs: 4332.8639. Time: 29.3446 us. Best GFLOPs: 5151.1462
2023-11-11 04:30:03 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #370: GFLOPs: 4957.6845. Time: 25.6462 us. Best GFLOPs: 5151.1462
2023-11-11 04:30:03 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #371: GFLOPs: 4344.8657. Time: 29.2635 us. Best GFLOPs: 5151.1462
2023-11-11 04:30:03 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #372: GFLOPs: 4617.5005. Time: 27.5357 us. Best GFLOPs: 5151.1462
2023-11-11 04:30:03 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #373: GFLOPs: 4965.0848. Time: 25.6080 us. Best GFLOPs: 5151.1462
2023-11-11 04:30:03 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #374: GFLOPs: 4781.5005. Time: 26.5912 us. Best GFLOPs: 5151.1462
2023-11-11 04:30:03 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #375: GFLOPs: 4319.7299. Time: 29.4338 us. Best GFLOPs: 5151.1462
2023-11-11 04:30:03 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #376: GFLOPs: 4802.5188. Time: 26.4749 us. Best GFLOPs: 5151.1462
2023-11-11 04:30:03 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #377: GFLOPs: 4821.3406. Time: 26.3715 us. Best GFLOPs: 5151.1462
2023-11-11 04:30:03 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #378: GFLOPs: 4770.2837. Time: 26.6538 us. Best GFLOPs: 5151.1462
2023-11-11 04:30:03 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #379: GFLOPs: 4320.4148. Time: 29.4291 us. Best GFLOPs: 5151.1462
2023-11-11 04:30:03 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #380: GFLOPs: 4294.0654. Time: 29.6097 us. Best GFLOPs: 5151.1462
2023-11-11 04:30:03 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #381: GFLOPs: 4215.9132. Time: 30.1586 us. Best GFLOPs: 5151.1462
2023-11-11 04:30:03 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #382: GFLOPs: 1234.5722. Time: 102.9879 us. Best GFLOPs: 5151.1462
2023-11-11 04:30:03 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #383: GFLOPs: 1609.9124. Time: 78.9770 us. Best GFLOPs: 5151.1462
2023-11-11 04:30:03 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #384: GFLOPs: 882.1839. Time: 144.1264 us. Best GFLOPs: 5151.1462
2023-11-11 05:10:02 [INFO] [evolutionary_search.cc:713] Generating candidates......
2023-11-11 05:10:06 [INFO] [evolutionary_search.cc:715] Picked top 102 candidate(s) from database
2023-11-11 05:10:12 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310fb046d8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310f554558)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310dc5ddb8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310bbcd2d8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56310b1dbf38)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310dc5d048)]: 400 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310dc26e38)]: 0 failure(s)
2023-11-11 05:10:19 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310fb046d8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310f554558)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310dc5ddb8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310bbcd2d8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56310b1dbf38)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310dc5d048)]: 797 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310dc26e38)]: 0 failure(s)
2023-11-11 05:10:25 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310fb046d8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310f554558)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310dc5ddb8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310bbcd2d8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56310b1dbf38)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310dc5d048)]: 1197 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310dc26e38)]: 0 failure(s)
2023-11-11 05:10:31 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310fb046d8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310f554558)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310dc5ddb8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310bbcd2d8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56310b1dbf38)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310dc5d048)]: 1599 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310dc26e38)]: 0 failure(s)
2023-11-11 05:10:37 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310fb046d8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310f554558)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310dc5ddb8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310bbcd2d8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56310b1dbf38)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310dc5d048)]: 1995 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310dc26e38)]: 0 failure(s)
2023-11-11 05:10:37 [INFO] [evolutionary_search.cc:723] Sampled 55 candidate(s)
2023-11-11 05:10:57 [INFO] [evolutionary_search.cc:621] Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310fb046d8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310f554558)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310dc5ddb8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310bbcd2d8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56310b1dbf38)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310dc5d048)]: 127 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310dc26e38)]: 0 failure(s)
2023-11-11 05:11:21 [INFO] [evolutionary_search.cc:621] Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310fb046d8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310f554558)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310dc5ddb8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310bbcd2d8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56310b1dbf38)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310dc5d048)]: 104 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310dc26e38)]: 0 failure(s)
2023-11-11 05:11:46 [INFO] [evolutionary_search.cc:621] Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310fb046d8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310f554558)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310dc5ddb8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310bbcd2d8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56310b1dbf38)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310dc5d048)]: 113 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310dc26e38)]: 0 failure(s)
2023-11-11 05:12:10 [INFO] [evolutionary_search.cc:621] Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310fb046d8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310f554558)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310dc5ddb8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310bbcd2d8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56310b1dbf38)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310dc5d048)]: 130 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310dc26e38)]: 0 failure(s)
2023-11-11 05:12:17 [INFO] [evolutionary_search.cc:649] Scores of the best 64 candidates:
[1 : 16]:	1.0733  1.0629  1.0588  1.0206  1.0105  1.0083  1.0063  1.0015  0.9993  0.9969  0.9948  0.9872  0.9842  0.9830  0.9820  0.9799
[17 : 32]:	0.9783  0.9769  0.9705  0.9694  0.9687  0.9617  0.9611  0.9581  0.9580  0.9572  0.9554  0.9549  0.9544  0.9544  0.9543  0.9538
[33 : 48]:	0.9537  0.9530  0.9525  0.9524  0.9524  0.9523  0.9512  0.9506  0.9479  0.9474  0.9470  0.9460  0.9452  0.9451  0.9448  0.9445
[49 : 64]:	0.9438  0.9438  0.9434  0.9431  0.9413  0.9412  0.9404  0.9402  0.9399  0.9397  0.9396  0.9395  0.9393  0.9391  0.9388  0.9381
2023-11-11 05:12:18 [INFO] [evolutionary_search.cc:727] Got 64 candidate(s) with evolutionary search
2023-11-11 05:12:18 [INFO] [evolutionary_search.cc:730] Sending 64 candidates(s) for measurement
2023-11-11 05:12:54 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #385: GFLOPs: 5018.8949. Time: 25.3335 us. Best GFLOPs: 5151.1462
2023-11-11 05:12:54 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #386: GFLOPs: 4825.8625. Time: 26.3468 us. Best GFLOPs: 5151.1462
2023-11-11 05:12:54 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #387: GFLOPs: 4803.2898. Time: 26.4706 us. Best GFLOPs: 5151.1462
2023-11-11 05:12:54 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #388: GFLOPs: 4889.2409. Time: 26.0053 us. Best GFLOPs: 5151.1462
2023-11-11 05:12:54 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #389: GFLOPs: 5167.1627. Time: 24.6065 us. Best GFLOPs: 5167.1627
2023-11-11 05:12:54 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #390: GFLOPs: 5170.2330. Time: 24.5919 us. Best GFLOPs: 5170.2330
2023-11-11 05:12:54 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #391: GFLOPs: 5219.0136. Time: 24.3621 us. Best GFLOPs: 5219.0136
2023-11-11 05:12:54 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #392: GFLOPs: 5218.6970. Time: 24.3635 us. Best GFLOPs: 5219.0136
2023-11-11 05:12:54 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #393: GFLOPs: 4785.9939. Time: 26.5663 us. Best GFLOPs: 5219.0136
2023-11-11 05:12:54 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #394: GFLOPs: 5184.8407. Time: 24.5226 us. Best GFLOPs: 5219.0136
2023-11-11 05:12:54 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #395: GFLOPs: 5101.2820. Time: 24.9243 us. Best GFLOPs: 5219.0136
2023-11-11 05:12:54 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #396: GFLOPs: 5101.5398. Time: 24.9231 us. Best GFLOPs: 5219.0136
2023-11-11 05:12:54 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #397: GFLOPs: 5144.0238. Time: 24.7172 us. Best GFLOPs: 5219.0136
2023-11-11 05:12:54 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #398: GFLOPs: 5166.3007. Time: 24.6106 us. Best GFLOPs: 5219.0136
2023-11-11 05:12:54 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #399: GFLOPs: 5224.6574. Time: 24.3358 us. Best GFLOPs: 5224.6574
2023-11-11 05:12:54 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #400: GFLOPs: 5154.2564. Time: 24.6682 us. Best GFLOPs: 5224.6574
2023-11-11 05:12:54 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #401: GFLOPs: 5122.6152. Time: 24.8205 us. Best GFLOPs: 5224.6574
2023-11-11 05:12:54 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #402: GFLOPs: 4015.2142. Time: 31.6661 us. Best GFLOPs: 5224.6574
2023-11-11 05:12:54 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #403: GFLOPs: 5071.8486. Time: 25.0690 us. Best GFLOPs: 5224.6574
2023-11-11 05:12:54 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #404: GFLOPs: 5325.8415. Time: 23.8734 us. Best GFLOPs: 5325.8415
2023-11-11 05:12:54 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #405: GFLOPs: 4984.3483. Time: 25.5090 us. Best GFLOPs: 5325.8415
2023-11-11 05:12:54 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #406: GFLOPs: 5176.3843. Time: 24.5627 us. Best GFLOPs: 5325.8415
2023-11-11 05:12:54 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #407: GFLOPs: 4831.9208. Time: 26.3138 us. Best GFLOPs: 5325.8415
2023-11-11 05:12:54 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #408: GFLOPs: 4851.3688. Time: 26.2083 us. Best GFLOPs: 5325.8415
2023-11-11 05:12:54 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #409: GFLOPs: 4450.1071. Time: 28.5714 us. Best GFLOPs: 5325.8415
2023-11-11 05:12:54 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #410: GFLOPs: 4857.6987. Time: 26.1741 us. Best GFLOPs: 5325.8415
2023-11-11 05:12:54 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #411: GFLOPs: 4915.2677. Time: 25.8676 us. Best GFLOPs: 5325.8415
2023-11-11 05:12:54 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #412: GFLOPs: 4858.1537. Time: 26.1717 us. Best GFLOPs: 5325.8415
2023-11-11 05:12:54 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #413: GFLOPs: 5023.7377. Time: 25.3090 us. Best GFLOPs: 5325.8415
2023-11-11 05:12:54 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #414: GFLOPs: 5023.7335. Time: 25.3091 us. Best GFLOPs: 5325.8415
2023-11-11 05:12:54 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #415: GFLOPs: 4995.6735. Time: 25.4512 us. Best GFLOPs: 5325.8415
2023-11-11 05:12:54 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #416: GFLOPs: 5028.3104. Time: 25.2860 us. Best GFLOPs: 5325.8415
2023-11-11 05:12:54 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #417: GFLOPs: 4995.8107. Time: 25.4505 us. Best GFLOPs: 5325.8415
2023-11-11 05:12:54 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #418: GFLOPs: 5002.3843. Time: 25.4171 us. Best GFLOPs: 5325.8415
2023-11-11 05:12:54 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #419: GFLOPs: 5028.7927. Time: 25.2836 us. Best GFLOPs: 5325.8415
2023-11-11 05:12:54 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #420: GFLOPs: 5023.3272. Time: 25.3111 us. Best GFLOPs: 5325.8415
2023-11-11 05:12:54 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #421: GFLOPs: 5030.5104. Time: 25.2750 us. Best GFLOPs: 5325.8415
2023-11-11 05:12:54 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #422: GFLOPs: 4920.7702. Time: 25.8386 us. Best GFLOPs: 5325.8415
2023-11-11 05:12:54 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #423: GFLOPs: 5004.4160. Time: 25.4068 us. Best GFLOPs: 5325.8415
2023-11-11 05:12:54 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #424: GFLOPs: 5119.6989. Time: 24.8347 us. Best GFLOPs: 5325.8415
2023-11-11 05:12:54 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #425: GFLOPs: 4991.0591. Time: 25.4748 us. Best GFLOPs: 5325.8415
2023-11-11 05:12:54 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #426: GFLOPs: 4030.3088. Time: 31.5475 us. Best GFLOPs: 5325.8415
2023-11-11 05:12:54 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #427: GFLOPs: 4930.0349. Time: 25.7901 us. Best GFLOPs: 5325.8415
2023-11-11 05:12:54 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #428: GFLOPs: 4564.5429. Time: 27.8551 us. Best GFLOPs: 5325.8415
2023-11-11 05:12:54 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #429: GFLOPs: 4856.4326. Time: 26.1809 us. Best GFLOPs: 5325.8415
2023-11-11 05:12:54 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #430: GFLOPs: 4620.5889. Time: 27.5173 us. Best GFLOPs: 5325.8415
2023-11-11 05:12:54 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #431: GFLOPs: 4901.9860. Time: 25.9376 us. Best GFLOPs: 5325.8415
2023-11-11 05:12:54 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #432: GFLOPs: 4964.7301. Time: 25.6098 us. Best GFLOPs: 5325.8415
2023-11-11 05:12:54 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #433: GFLOPs: 4805.8429. Time: 26.4565 us. Best GFLOPs: 5325.8415
2023-11-11 05:12:54 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #434: GFLOPs: 4603.7521. Time: 27.6179 us. Best GFLOPs: 5325.8415
2023-11-11 05:12:54 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #435: GFLOPs: 4986.9834. Time: 25.4956 us. Best GFLOPs: 5325.8415
2023-11-11 05:12:54 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #436: GFLOPs: 4948.1040. Time: 25.6959 us. Best GFLOPs: 5325.8415
2023-11-11 05:12:54 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #437: GFLOPs: 4872.2589. Time: 26.0959 us. Best GFLOPs: 5325.8415
2023-11-11 05:12:54 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #438: GFLOPs: 4979.0859. Time: 25.5360 us. Best GFLOPs: 5325.8415
2023-11-11 05:12:54 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #439: GFLOPs: 4963.1663. Time: 25.6179 us. Best GFLOPs: 5325.8415
2023-11-11 05:12:54 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #440: GFLOPs: 4986.6830. Time: 25.4971 us. Best GFLOPs: 5325.8415
2023-11-11 05:12:54 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #441: GFLOPs: 4964.8861. Time: 25.6090 us. Best GFLOPs: 5325.8415
2023-11-11 05:12:54 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #442: GFLOPs: 4957.7719. Time: 25.6458 us. Best GFLOPs: 5325.8415
2023-11-11 05:12:54 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #443: GFLOPs: 4944.7448. Time: 25.7134 us. Best GFLOPs: 5325.8415
2023-11-11 05:12:54 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #444: GFLOPs: 4893.9173. Time: 25.9804 us. Best GFLOPs: 5325.8415
2023-11-11 05:12:54 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #445: GFLOPs: 4979.3517. Time: 25.5346 us. Best GFLOPs: 5325.8415
2023-11-11 05:12:54 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #446: GFLOPs: 445.1752. Time: 285.6088 us. Best GFLOPs: 5325.8415
2023-11-11 05:12:54 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #447: GFLOPs: 2603.8872. Time: 48.8293 us. Best GFLOPs: 5325.8415
2023-11-11 05:12:54 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #448: GFLOPs: 8.6863. Time: 14637.4947 us. Best GFLOPs: 5325.8415
2023-11-11 06:05:07 [INFO] [evolutionary_search.cc:713] Generating candidates......
2023-11-11 06:05:10 [INFO] [evolutionary_search.cc:715] Picked top 102 candidate(s) from database
2023-11-11 06:05:17 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310fb046d8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310f554558)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310dc5ddb8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310bbcd2d8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56310b1dbf38)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310dc5d048)]: 391 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310dc26e38)]: 0 failure(s)
2023-11-11 06:05:24 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310fb046d8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310f554558)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310dc5ddb8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310bbcd2d8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56310b1dbf38)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310dc5d048)]: 786 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310dc26e38)]: 0 failure(s)
2023-11-11 06:05:30 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310fb046d8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310f554558)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310dc5ddb8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310bbcd2d8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56310b1dbf38)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310dc5d048)]: 1186 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310dc26e38)]: 0 failure(s)
2023-11-11 06:05:37 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310fb046d8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310f554558)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310dc5ddb8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310bbcd2d8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56310b1dbf38)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310dc5d048)]: 1583 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310dc26e38)]: 0 failure(s)
2023-11-11 06:05:37 [INFO] [evolutionary_search.cc:723] Sampled 57 candidate(s)
2023-11-11 06:05:57 [INFO] [evolutionary_search.cc:621] Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310fb046d8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310f554558)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310dc5ddb8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310bbcd2d8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56310b1dbf38)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310dc5d048)]: 146 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310dc26e38)]: 0 failure(s)
2023-11-11 06:06:22 [INFO] [evolutionary_search.cc:621] Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310fb046d8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310f554558)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310dc5ddb8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310bbcd2d8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56310b1dbf38)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310dc5d048)]: 144 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310dc26e38)]: 0 failure(s)
2023-11-11 06:06:45 [INFO] [evolutionary_search.cc:621] Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310fb046d8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310f554558)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310dc5ddb8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310bbcd2d8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56310b1dbf38)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310dc5d048)]: 109 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310dc26e38)]: 0 failure(s)
2023-11-11 06:07:09 [INFO] [evolutionary_search.cc:621] Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310fb046d8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310f554558)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310dc5ddb8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310bbcd2d8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56310b1dbf38)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310dc5d048)]: 144 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310dc26e38)]: 0 failure(s)
2023-11-11 06:07:16 [INFO] [evolutionary_search.cc:649] Scores of the best 64 candidates:
[1 : 16]:	1.1226  1.1204  1.0990  1.0559  1.0555  1.0542  1.0539  1.0504  1.0371  1.0353  1.0324  1.0151  1.0095  1.0092  1.0055  0.9959
[17 : 32]:	0.9959  0.9942  0.9904  0.9903  0.9866  0.9857  0.9848  0.9837  0.9819  0.9815  0.9760  0.9756  0.9748  0.9747  0.9742  0.9732
[33 : 48]:	0.9729  0.9725  0.9725  0.9697  0.9697  0.9687  0.9679  0.9679  0.9679  0.9661  0.9659  0.9646  0.9639  0.9638  0.9633  0.9631
[49 : 64]:	0.9626  0.9626  0.9626  0.9625  0.9625  0.9620  0.9614  0.9609  0.9608  0.9606  0.9599  0.9594  0.9590  0.9589  0.9578  0.9571
2023-11-11 06:07:17 [INFO] [evolutionary_search.cc:727] Got 64 candidate(s) with evolutionary search
2023-11-11 06:07:17 [INFO] [evolutionary_search.cc:730] Sending 64 candidates(s) for measurement
2023-11-11 06:07:53 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #449: GFLOPs: 4224.3148. Time: 30.0986 us. Best GFLOPs: 5325.8415
2023-11-11 06:07:53 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #450: GFLOPs: 231.4078. Time: 549.4456 us. Best GFLOPs: 5325.8415
2023-11-11 06:07:53 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #451: GFLOPs: 4019.4026. Time: 31.6331 us. Best GFLOPs: 5325.8415
2023-11-11 06:07:53 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #452: GFLOPs: 5258.4369. Time: 24.1794 us. Best GFLOPs: 5325.8415
2023-11-11 06:07:53 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #453: GFLOPs: 5128.4927. Time: 24.7921 us. Best GFLOPs: 5325.8415
2023-11-11 06:07:53 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #454: GFLOPs: 5143.5510. Time: 24.7195 us. Best GFLOPs: 5325.8415
2023-11-11 06:07:53 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #455: GFLOPs: 5212.8533. Time: 24.3909 us. Best GFLOPs: 5325.8415
2023-11-11 06:07:53 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #456: GFLOPs: 5512.4432. Time: 23.0653 us. Best GFLOPs: 5512.4432
2023-11-11 06:07:53 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #457: GFLOPs: 3964.9976. Time: 32.0671 us. Best GFLOPs: 5512.4432
2023-11-11 06:07:53 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #458: GFLOPs: 5467.1796. Time: 23.2562 us. Best GFLOPs: 5512.4432
2023-11-11 06:07:53 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #459: GFLOPs: 5121.3795. Time: 24.8265 us. Best GFLOPs: 5512.4432
2023-11-11 06:07:53 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #460: GFLOPs: 5053.9154. Time: 25.1579 us. Best GFLOPs: 5512.4432
2023-11-11 06:07:53 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #461: GFLOPs: 5037.1168. Time: 25.2418 us. Best GFLOPs: 5512.4432
2023-11-11 06:07:53 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #462: GFLOPs: 5121.0022. Time: 24.8283 us. Best GFLOPs: 5512.4432
2023-11-11 06:07:53 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #463: GFLOPs: 3853.6542. Time: 32.9936 us. Best GFLOPs: 5512.4432
2023-11-11 06:07:53 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #464: GFLOPs: 5496.4716. Time: 23.1323 us. Best GFLOPs: 5512.4432
2023-11-11 06:07:53 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #465: GFLOPs: 5496.5551. Time: 23.1319 us. Best GFLOPs: 5512.4432
2023-11-11 06:07:53 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #466: GFLOPs: 5406.0790. Time: 23.5191 us. Best GFLOPs: 5512.4432
2023-11-11 06:07:53 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #467: GFLOPs: 5185.1878. Time: 24.5210 us. Best GFLOPs: 5512.4432
2023-11-11 06:07:53 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #468: GFLOPs: 5522.7350. Time: 23.0223 us. Best GFLOPs: 5522.7350
2023-11-11 06:07:53 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #469: GFLOPs: 5496.5915. Time: 23.1318 us. Best GFLOPs: 5522.7350
2023-11-11 06:07:53 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #470: GFLOPs: 3847.7250. Time: 33.0445 us. Best GFLOPs: 5522.7350
2023-11-11 06:07:53 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #471: GFLOPs: 5404.7092. Time: 23.5250 us. Best GFLOPs: 5522.7350
2023-11-11 06:07:53 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #472: GFLOPs: 5184.9447. Time: 24.5221 us. Best GFLOPs: 5522.7350
2023-11-11 06:07:53 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #473: GFLOPs: 5152.7917. Time: 24.6752 us. Best GFLOPs: 5522.7350
2023-11-11 06:07:53 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #474: GFLOPs: 5225.6698. Time: 24.3310 us. Best GFLOPs: 5522.7350
2023-11-11 06:07:53 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #475: GFLOPs: 5184.3139. Time: 24.5251 us. Best GFLOPs: 5522.7350
2023-11-11 06:07:53 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #476: GFLOPs: 5378.5929. Time: 23.6393 us. Best GFLOPs: 5522.7350
2023-11-11 06:07:53 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #477: GFLOPs: 5228.0335. Time: 24.3200 us. Best GFLOPs: 5522.7350
2023-11-11 06:07:53 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #478: GFLOPs: 5126.5266. Time: 24.8016 us. Best GFLOPs: 5522.7350
2023-11-11 06:07:53 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #479: GFLOPs: 5098.8998. Time: 24.9360 us. Best GFLOPs: 5522.7350
2023-11-11 06:07:53 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #480: GFLOPs: 5183.4874. Time: 24.5290 us. Best GFLOPs: 5522.7350
2023-11-11 06:07:53 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #481: GFLOPs: 5553.0639. Time: 22.8965 us. Best GFLOPs: 5553.0639
2023-11-11 06:07:53 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #482: GFLOPs: 3711.2012. Time: 34.2601 us. Best GFLOPs: 5553.0639
2023-11-11 06:07:53 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #483: GFLOPs: 3681.2365. Time: 34.5389 us. Best GFLOPs: 5553.0639
2023-11-11 06:07:53 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #484: GFLOPs: 5183.7992. Time: 24.5276 us. Best GFLOPs: 5553.0639
2023-11-11 06:07:53 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #485: GFLOPs: 5184.8754. Time: 24.5225 us. Best GFLOPs: 5553.0639
2023-11-11 06:07:53 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #486: GFLOPs: 5146.7478. Time: 24.7041 us. Best GFLOPs: 5553.0639
2023-11-11 06:07:53 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #487: GFLOPs: 5104.0753. Time: 24.9107 us. Best GFLOPs: 5553.0639
2023-11-11 06:07:53 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #488: GFLOPs: 5382.8511. Time: 23.6206 us. Best GFLOPs: 5553.0639
2023-11-11 06:07:53 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #489: GFLOPs: 5194.9860. Time: 24.4748 us. Best GFLOPs: 5553.0639
2023-11-11 06:07:53 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #490: GFLOPs: 5108.2025. Time: 24.8906 us. Best GFLOPs: 5553.0639
2023-11-11 06:07:53 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #491: GFLOPs: 5188.7661. Time: 24.5041 us. Best GFLOPs: 5553.0639
2023-11-11 06:07:53 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #492: GFLOPs: 5378.8588. Time: 23.6381 us. Best GFLOPs: 5553.0639
2023-11-11 06:07:53 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #493: GFLOPs: 5149.1256. Time: 24.6927 us. Best GFLOPs: 5553.0639
2023-11-11 06:07:53 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #494: GFLOPs: 5185.3958. Time: 24.5200 us. Best GFLOPs: 5553.0639
2023-11-11 06:07:53 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #495: GFLOPs: 5177.1805. Time: 24.5589 us. Best GFLOPs: 5553.0639
2023-11-11 06:07:53 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #496: GFLOPs: 5259.1420. Time: 24.1762 us. Best GFLOPs: 5553.0639
2023-11-11 06:07:53 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #497: GFLOPs: 5238.0053. Time: 24.2737 us. Best GFLOPs: 5553.0639
2023-11-11 06:07:53 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #498: GFLOPs: 5225.6661. Time: 24.3311 us. Best GFLOPs: 5553.0639
2023-11-11 06:07:53 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #499: GFLOPs: 5179.4311. Time: 24.5483 us. Best GFLOPs: 5553.0639
2023-11-11 06:07:53 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #500: GFLOPs: 5198.5539. Time: 24.4580 us. Best GFLOPs: 5553.0639
2023-11-11 06:07:53 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #501: GFLOPs: 5197.6815. Time: 24.4621 us. Best GFLOPs: 5553.0639
2023-11-11 06:07:53 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #502: GFLOPs: 5195.5890. Time: 24.4719 us. Best GFLOPs: 5553.0639
2023-11-11 06:07:53 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #503: GFLOPs: 5351.8336. Time: 23.7575 us. Best GFLOPs: 5553.0639
2023-11-11 06:07:53 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #504: GFLOPs: 5223.6491. Time: 24.3405 us. Best GFLOPs: 5553.0639
2023-11-11 06:07:53 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #505: GFLOPs: 5238.1519. Time: 24.2731 us. Best GFLOPs: 5553.0639
2023-11-11 06:07:53 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #506: GFLOPs: 5197.9104. Time: 24.4610 us. Best GFLOPs: 5553.0639
2023-11-11 06:07:53 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #507: GFLOPs: 3889.9586. Time: 32.6857 us. Best GFLOPs: 5553.0639
2023-11-11 06:07:53 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #508: GFLOPs: 5309.8095. Time: 23.9455 us. Best GFLOPs: 5553.0639
2023-11-11 06:07:53 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #509: GFLOPs: 5237.7918. Time: 24.2747 us. Best GFLOPs: 5553.0639
2023-11-11 06:07:53 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #510: GFLOPs: 2401.8615. Time: 52.9364 us. Best GFLOPs: 5553.0639
2023-11-11 06:07:53 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #511: GFLOPs: 1067.8653. Time: 119.0656 us. Best GFLOPs: 5553.0639
2023-11-11 06:07:53 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #512: GFLOPs: 130.8508. Time: 971.6865 us. Best GFLOPs: 5553.0639
2023-11-11 07:02:48 [INFO] [evolutionary_search.cc:713] Generating candidates......
2023-11-11 07:02:51 [INFO] [evolutionary_search.cc:715] Picked top 102 candidate(s) from database
2023-11-11 07:02:57 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310fb046d8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310f554558)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310dc5ddb8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310bbcd2d8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56310b1dbf38)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310dc5d048)]: 402 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310dc26e38)]: 0 failure(s)
2023-11-11 07:03:04 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310fb046d8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310f554558)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310dc5ddb8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310bbcd2d8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56310b1dbf38)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310dc5d048)]: 796 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310dc26e38)]: 0 failure(s)
2023-11-11 07:03:10 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310fb046d8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310f554558)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310dc5ddb8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310bbcd2d8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56310b1dbf38)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310dc5d048)]: 1190 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310dc26e38)]: 0 failure(s)
2023-11-11 07:03:17 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310fb046d8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310f554558)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310dc5ddb8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310bbcd2d8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56310b1dbf38)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310dc5d048)]: 1588 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310dc26e38)]: 0 failure(s)
2023-11-11 07:03:17 [INFO] [evolutionary_search.cc:723] Sampled 52 candidate(s)
2023-11-11 07:03:36 [INFO] [evolutionary_search.cc:621] Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310fb046d8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310f554558)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310dc5ddb8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310bbcd2d8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56310b1dbf38)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310dc5d048)]: 140 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310dc26e38)]: 0 failure(s)
2023-11-11 07:04:00 [INFO] [evolutionary_search.cc:621] Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310fb046d8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310f554558)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310dc5ddb8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310bbcd2d8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56310b1dbf38)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310dc5d048)]: 121 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310dc26e38)]: 0 failure(s)
2023-11-11 07:04:23 [INFO] [evolutionary_search.cc:621] Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310fb046d8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310f554558)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310dc5ddb8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310bbcd2d8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56310b1dbf38)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310dc5d048)]: 146 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310dc26e38)]: 0 failure(s)
2023-11-11 07:04:47 [INFO] [evolutionary_search.cc:621] Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310fb046d8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310f554558)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310dc5ddb8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310bbcd2d8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56310b1dbf38)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310dc5d048)]: 127 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310dc26e38)]: 0 failure(s)
2023-11-11 07:04:54 [INFO] [evolutionary_search.cc:649] Scores of the best 64 candidates:
[1 : 16]:	1.0500  1.0363  1.0223  1.0171  1.0099  1.0065  1.0050  1.0047  1.0044  1.0039  0.9942  0.9931  0.9921  0.9918  0.9918  0.9916
[17 : 32]:	0.9901  0.9901  0.9899  0.9898  0.9894  0.9893  0.9892  0.9888  0.9867  0.9866  0.9864  0.9864  0.9864  0.9853  0.9847  0.9832
[33 : 48]:	0.9830  0.9825  0.9821  0.9804  0.9788  0.9786  0.9768  0.9755  0.9734  0.9720  0.9715  0.9683  0.9681  0.9681  0.9678  0.9677
[49 : 64]:	0.9672  0.9667  0.9664  0.9660  0.9660  0.9656  0.9654  0.9649  0.9645  0.9639  0.9633  0.9630  0.9628  0.9622  0.9605  0.9605
2023-11-11 07:04:54 [INFO] [evolutionary_search.cc:727] Got 64 candidate(s) with evolutionary search
2023-11-11 07:04:54 [INFO] [evolutionary_search.cc:730] Sending 64 candidates(s) for measurement
2023-11-11 07:05:55 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #513: GFLOPs: 3105.3076. Time: 40.9447 us. Best GFLOPs: 5553.0639
2023-11-11 07:05:55 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #514: GFLOPs: 4130.2567. Time: 30.7840 us. Best GFLOPs: 5553.0639
2023-11-11 07:05:55 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #515: GFLOPs: 4822.3328. Time: 26.3661 us. Best GFLOPs: 5553.0639
2023-11-11 07:05:55 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #516: GFLOPs: 4592.3364. Time: 27.6866 us. Best GFLOPs: 5553.0639
2023-11-11 07:05:55 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #517: GFLOPs: 5341.6456. Time: 23.8028 us. Best GFLOPs: 5553.0639
2023-11-11 07:05:55 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #518: GFLOPs: 5341.4813. Time: 23.8035 us. Best GFLOPs: 5553.0639
2023-11-11 07:05:55 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #519: GFLOPs: 4576.4463. Time: 27.7827 us. Best GFLOPs: 5553.0639
2023-11-11 07:05:55 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #520: GFLOPs: 4577.1588. Time: 27.7784 us. Best GFLOPs: 5553.0639
2023-11-11 07:05:55 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #521: GFLOPs: 5340.8946. Time: 23.8061 us. Best GFLOPs: 5553.0639
2023-11-11 07:05:55 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #522: GFLOPs: 5341.1275. Time: 23.8051 us. Best GFLOPs: 5553.0639
2023-11-11 07:05:55 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #523: GFLOPs: 5395.3068. Time: 23.5660 us. Best GFLOPs: 5553.0639
2023-11-11 07:05:55 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #524: GFLOPs: 5441.9887. Time: 23.3639 us. Best GFLOPs: 5553.0639
2023-11-11 07:05:55 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #525: GFLOPs: 5457.0722. Time: 23.2993 us. Best GFLOPs: 5553.0639
2023-11-11 07:05:55 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #526: GFLOPs: 5317.4400. Time: 23.9111 us. Best GFLOPs: 5553.0639
2023-11-11 07:05:55 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #527: GFLOPs: 5419.8498. Time: 23.4593 us. Best GFLOPs: 5553.0639
2023-11-11 07:05:55 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #528: GFLOPs: 5398.0820. Time: 23.5539 us. Best GFLOPs: 5553.0639
2023-11-11 07:05:55 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #529: GFLOPs: 4782.2016. Time: 26.5873 us. Best GFLOPs: 5553.0639
2023-11-11 07:05:55 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #530: GFLOPs: 5369.6258. Time: 23.6787 us. Best GFLOPs: 5553.0639
2023-11-11 07:05:55 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #531: GFLOPs: 5421.4619. Time: 23.4523 us. Best GFLOPs: 5553.0639
2023-11-11 07:05:55 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #532: GFLOPs: 4757.5734. Time: 26.7250 us. Best GFLOPs: 5553.0639
2023-11-11 07:05:55 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #533: GFLOPs: 4781.8383. Time: 26.5894 us. Best GFLOPs: 5553.0639
2023-11-11 07:05:55 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #534: GFLOPs: 5459.1208. Time: 23.2906 us. Best GFLOPs: 5553.0639
2023-11-11 07:05:55 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #535: GFLOPs: 4789.7002. Time: 26.5457 us. Best GFLOPs: 5553.0639
2023-11-11 07:05:55 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #536: GFLOPs: 5388.7084. Time: 23.5949 us. Best GFLOPs: 5553.0639
2023-11-11 07:05:55 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #537: GFLOPs: 5369.0463. Time: 23.6813 us. Best GFLOPs: 5553.0639
2023-11-11 07:05:55 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #538: GFLOPs: 5368.3305. Time: 23.6845 us. Best GFLOPs: 5553.0639
2023-11-11 07:05:55 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #539: GFLOPs: 5369.7711. Time: 23.6781 us. Best GFLOPs: 5553.0639
2023-11-11 07:05:55 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #540: GFLOPs: 5435.1616. Time: 23.3932 us. Best GFLOPs: 5553.0639
2023-11-11 07:05:55 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #541: GFLOPs: 5455.9435. Time: 23.3041 us. Best GFLOPs: 5553.0639
2023-11-11 07:05:55 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #542: GFLOPs: 5325.9126. Time: 23.8731 us. Best GFLOPs: 5553.0639
2023-11-11 07:05:55 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #543: GFLOPs: 4762.8811. Time: 26.6952 us. Best GFLOPs: 5553.0639
2023-11-11 07:05:55 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #544: GFLOPs: 5326.7092. Time: 23.8695 us. Best GFLOPs: 5553.0639
2023-11-11 07:05:55 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #545: GFLOPs: 5298.7307. Time: 23.9956 us. Best GFLOPs: 5553.0639
2023-11-11 07:05:55 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #546: GFLOPs: 5458.1177. Time: 23.2948 us. Best GFLOPs: 5553.0639
2023-11-11 07:05:55 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #547: GFLOPs: 5391.2048. Time: 23.5840 us. Best GFLOPs: 5553.0639
2023-11-11 07:05:55 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #548: GFLOPs: 5298.6335. Time: 23.9960 us. Best GFLOPs: 5553.0639
2023-11-11 07:05:55 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #549: GFLOPs: 5362.3506. Time: 23.7109 us. Best GFLOPs: 5553.0639
2023-11-11 07:05:55 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #550: GFLOPs: 5400.8722. Time: 23.5418 us. Best GFLOPs: 5553.0639
2023-11-11 07:05:55 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #551: GFLOPs: 5429.5662. Time: 23.4173 us. Best GFLOPs: 5553.0639
2023-11-11 07:05:55 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #552: GFLOPs: 5560.2558. Time: 22.8669 us. Best GFLOPs: 5560.2558
2023-11-11 07:05:55 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #553: GFLOPs: 5428.3054. Time: 23.4228 us. Best GFLOPs: 5560.2558
2023-11-11 07:05:55 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #554: GFLOPs: 5425.7667. Time: 23.4337 us. Best GFLOPs: 5560.2558
2023-11-11 07:05:55 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #555: GFLOPs: 5501.8716. Time: 23.1096 us. Best GFLOPs: 5560.2558
2023-11-11 07:05:55 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #556: GFLOPs: 5354.1443. Time: 23.7472 us. Best GFLOPs: 5560.2558
2023-11-11 07:05:55 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #557: GFLOPs: 5358.4502. Time: 23.7281 us. Best GFLOPs: 5560.2558
2023-11-11 07:05:55 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #558: GFLOPs: 5359.8220. Time: 23.7221 us. Best GFLOPs: 5560.2558
2023-11-11 07:05:55 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #559: GFLOPs: 5363.0105. Time: 23.7079 us. Best GFLOPs: 5560.2558
2023-11-11 07:05:55 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #560: GFLOPs: 5335.9510. Time: 23.8282 us. Best GFLOPs: 5560.2558
2023-11-11 07:05:55 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #561: GFLOPs: 5359.9301. Time: 23.7216 us. Best GFLOPs: 5560.2558
2023-11-11 07:05:55 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #562: GFLOPs: 5237.5581. Time: 24.2758 us. Best GFLOPs: 5560.2558
2023-11-11 07:05:55 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #563: GFLOPs: 5361.6555. Time: 23.7139 us. Best GFLOPs: 5560.2558
2023-11-11 07:05:55 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #564: GFLOPs: 5336.7740. Time: 23.8245 us. Best GFLOPs: 5560.2558
2023-11-11 07:05:55 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #565: GFLOPs: 5038.6973. Time: 25.2339 us. Best GFLOPs: 5560.2558
2023-11-11 07:05:55 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #566: GFLOPs: 5481.8417. Time: 23.1940 us. Best GFLOPs: 5560.2558
2023-11-11 07:05:55 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #567: GFLOPs: 5046.8853. Time: 25.1930 us. Best GFLOPs: 5560.2558
2023-11-11 07:05:55 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #568: GFLOPs: 5337.7409. Time: 23.8202 us. Best GFLOPs: 5560.2558
2023-11-11 07:05:55 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #569: GFLOPs: 5449.8598. Time: 23.3301 us. Best GFLOPs: 5560.2558
2023-11-11 07:05:55 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #570: GFLOPs: 5481.8785. Time: 23.1939 us. Best GFLOPs: 5560.2558
2023-11-11 07:05:55 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #571: GFLOPs: 5184.1813. Time: 24.5258 us. Best GFLOPs: 5560.2558
2023-11-11 07:05:55 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #572: GFLOPs: 4360.1198. Time: 29.1611 us. Best GFLOPs: 5560.2558
2023-11-11 07:05:55 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #573: GFLOPs: 5303.3671. Time: 23.9746 us. Best GFLOPs: 5560.2558
2023-11-11 07:05:55 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #574: GFLOPs: 332.0539. Time: 382.9077 us. Best GFLOPs: 5560.2558
2023-11-11 07:05:55 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #575: GFLOPs: 96.7935. Time: 1313.5792 us. Best GFLOPs: 5560.2558
2023-11-11 07:05:55 [INFO] [task_scheduler.cc:121] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #576: Error in building:
LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(128), T.int64(28), T.int64(28)), "float32"), p1: T.Buffer((T.int64(4), T.int64(4), T.int64(128), T.int64(128)), "float32"), p2: T.Buffer((T.int64(1), T.int64(128), T.int64(28), T.int64(28)), "float32"), p3: T.Buffer((T.int64(1), T.int64(128), T.int64(1), T.int64(1)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(128), T.int64(28), T.int64(28)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        input_tile_local = T.alloc_buffer((T.int64(128), T.int64(196), T.int64(4), T.int64(4)), scope="local")
        data_pack = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(128), T.int64(196)))
        bgemm = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(128), T.int64(196)))
        inverse_local = T.alloc_buffer((T.int64(128), T.int64(196), T.int64(2), T.int64(2)), scope="local")
        data_pack_local = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(128), T.int64(196)), scope="local")
        bgemm_local = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(128), T.int64(196)), scope="local")
        data_pack_shared = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(128), T.int64(196)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(128), T.int64(128)), scope="shared")
        for ci_p_fused_0 in T.thread_binding(T.int64(784), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 1024, "pragma_unroll_explicit": 1}):
            for ci_p_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(1), T.int64(4), T.int64(4)):
                    with T.block("input_tile"):
                        v_ci = T.axis.spatial(T.int64(128), (ci_p_fused_0 * T.int64(32) + ci_p_fused_1) // T.int64(196) + ax0)
                        v_p = T.axis.spatial(T.int64(196), (ci_p_fused_0 * T.int64(32) + ci_p_fused_1) % T.int64(196) + ax1)
                        v_eps, v_nu = T.axis.remap("SS", [ax2, ax3])
                        T.reads(p0[v_p // T.int64(196), v_ci, v_p % T.int64(196) // T.int64(14) * T.int64(2) + v_eps - T.int64(1), v_p % T.int64(14) * T.int64(2) + v_nu - T.int64(1)])
                        T.writes(input_tile_local[v_ci, v_p, v_eps, v_nu])
                        T.block_attr({"schedule_rule": "None"})
                        input_tile_local[v_ci, v_p, v_eps, v_nu] = T.if_then_else(T.int64(1) <= v_p % T.int64(196) // T.int64(14) * T.int64(2) + v_eps and v_p % T.int64(196) // T.int64(14) * T.int64(2) + v_eps < T.int64(29) and T.int64(1) <= v_p % T.int64(14) * T.int64(2) + v_nu and v_p % T.int64(14) * T.int64(2) + v_nu < T.int64(29), p0[v_p // T.int64(196), v_ci, v_p % T.int64(196) // T.int64(14) * T.int64(2) + v_eps - T.int64(1), v_p % T.int64(14) * T.int64(2) + v_nu - T.int64(1)], T.float32(0))
                for eps in T.unroll(T.int64(4)):
                    for nu in T.unroll(T.int64(4)):
                        with T.block("data_pack_init"):
                            v_eps, v_nu = T.axis.remap("SS", [eps, nu])
                            v_ci = T.axis.spatial(T.int64(128), (ci_p_fused_0 * T.int64(32) + ci_p_fused_1) // T.int64(196))
                            v_p = T.axis.spatial(T.int64(196), (ci_p_fused_0 * T.int64(32) + ci_p_fused_1) % T.int64(196))
                            T.reads()
                            T.writes(data_pack_local[v_eps, v_nu, v_ci, v_p])
                            T.block_attr({"schedule_rule": "conv2d_nchw_winograd_data_pack"})
                            data_pack_local[v_eps, v_nu, v_ci, v_p] = T.float32(0)
                        for r_a in T.unroll(T.int64(4)):
                            for r_b in T.unroll(T.int64(4)):
                                with T.block("data_pack_update"):
                                    v_eps, v_nu = T.axis.remap("SS", [eps, nu])
                                    v_ci = T.axis.spatial(T.int64(128), (ci_p_fused_0 * T.int64(32) + ci_p_fused_1) // T.int64(196))
                                    v_p = T.axis.spatial(T.int64(196), (ci_p_fused_0 * T.int64(32) + ci_p_fused_1) % T.int64(196))
                                    v_r_a, v_r_b = T.axis.remap("RR", [r_a, r_b])
                                    T.reads(data_pack_local[v_eps, v_nu, v_ci, v_p], input_tile_local[v_ci, v_p, v_r_a, v_r_b])
                                    T.writes(data_pack_local[v_eps, v_nu, v_ci, v_p])
                                    T.block_attr({"schedule_rule": "conv2d_nchw_winograd_data_pack"})
                                    data_pack_local[v_eps, v_nu, v_ci, v_p] = data_pack_local[v_eps, v_nu, v_ci, v_p] + input_tile_local[v_ci, v_p, v_r_a, v_r_b] * T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(3), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(1), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(0), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(3), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(1), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(0), T.float32(1), T.float32(0))))))))))))))))) * T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(3), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(1), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(0), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(3), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(1), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(0), T.float32(1), T.float32(0)))))))))))))))))
                for ax0, ax1, ax2, ax3 in T.grid(T.int64(4), T.int64(4), T.int64(1), T.int64(1)):
                    with T.block("data_pack_local"):
                        v0, v1 = T.axis.remap("SS", [ax0, ax1])
                        v2 = T.axis.spatial(T.int64(128), (ci_p_fused_0 * T.int64(32) + ci_p_fused_1) // T.int64(196) + ax2)
                        v3 = T.axis.spatial(T.int64(196), (ci_p_fused_0 * T.int64(32) + ci_p_fused_1) % T.int64(196) + ax3)
                        T.reads(data_pack_local[v0, v1, v2, v3])
                        T.writes(data_pack[v0, v1, v2, v3])
                        data_pack[v0, v1, v2, v3] = data_pack_local[v0, v1, v2, v3]
        for eps_0_nu_0_co_0_p_0_fused in T.thread_binding(T.int64(14), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 1024, "pragma_unroll_explicit": 1}):
            for eps_1_nu_1_co_1_p_1_fused in T.thread_binding(T.int64(8), thread="vthread.x"):
                for eps_2_nu_2_co_2_p_2_fused in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                    for eps_3_init, nu_3_init, co_3_init, p_3_init, eps_4_init, nu_4_init, co_4_init, p_4_init in T.grid(T.int64(1), T.int64(1), T.int64(2), T.int64(1), T.int64(1), T.int64(1), T.int64(8), T.int64(7)):
                        with T.block("bgemm_init"):
                            v_eps = T.axis.spatial(T.int64(4), eps_1_nu_1_co_1_p_1_fused // T.int64(2) + eps_3_init + eps_4_init)
                            v_nu = T.axis.spatial(T.int64(4), eps_1_nu_1_co_1_p_1_fused % T.int64(2) * T.int64(2) + eps_2_nu_2_co_2_p_2_fused // T.int64(16) + nu_3_init + nu_4_init)
                            v_co = T.axis.spatial(T.int64(128), eps_2_nu_2_co_2_p_2_fused % T.int64(16) // T.int64(2) * T.int64(16) + co_3_init * T.int64(8) + co_4_init)
                            v_p = T.axis.spatial(T.int64(196), eps_0_nu_0_co_0_p_0_fused * T.int64(14) + eps_2_nu_2_co_2_p_2_fused % T.int64(2) * T.int64(7) + p_3_init * T.int64(7) + p_4_init)
                            T.reads()
                            T.writes(bgemm_local[v_eps, v_nu, v_co, v_p])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            bgemm_local[v_eps, v_nu, v_co, v_p] = T.float32(0)
                    for ci_0 in range(T.int64(32)):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(14)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("data_pack_shared"):
                                        v0 = T.axis.spatial(T.int64(4), (ax0_ax1_ax2_ax3_fused_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) // T.int64(224))
                                        v1 = T.axis.spatial(T.int64(4), (ax0_ax1_ax2_ax3_fused_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) % T.int64(224) // T.int64(56))
                                        v2 = T.axis.spatial(T.int64(128), ci_0 * T.int64(4) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) % T.int64(56) // T.int64(14))
                                        v3 = T.axis.spatial(T.int64(196), eps_0_nu_0_co_0_p_0_fused * T.int64(14) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) % T.int64(14))
                                        T.reads(data_pack[v0, v1, v2, v3])
                                        T.writes(data_pack_shared[v0, v1, v2, v3])
                                        data_pack_shared[v0, v1, v2, v3] = data_pack[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(256)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                                with T.block("p1_shared"):
                                    v0 = T.axis.spatial(T.int64(4), (ax0_ax1_ax2_ax3_fused_0 * T.int64(32) + ax0_ax1_ax2_ax3_fused_1) // T.int64(2048))
                                    v1 = T.axis.spatial(T.int64(4), (ax0_ax1_ax2_ax3_fused_0 * T.int64(32) + ax0_ax1_ax2_ax3_fused_1) % T.int64(2048) // T.int64(512))
                                    v2 = T.axis.spatial(T.int64(128), ci_0 * T.int64(4) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(32) + ax0_ax1_ax2_ax3_fused_1) % T.int64(512) // T.int64(128))
                                    v3 = T.axis.spatial(T.int64(128), (ax0_ax1_ax2_ax3_fused_0 * T.int64(32) + ax0_ax1_ax2_ax3_fused_1) % T.int64(128))
                                    T.reads(p1[v0, v1, v2, v3])
                                    T.writes(p1_shared[v0, v1, v2, v3])
                                    p1_shared[v0, v1, v2, v3] = p1[v0, v1, v2, v3]
                        for ci_1, eps_3, nu_3, co_3, p_3, ci_2, eps_4, nu_4, co_4, p_4 in T.grid(T.int64(2), T.int64(1), T.int64(1), T.int64(2), T.int64(1), T.int64(2), T.int64(1), T.int64(1), T.int64(8), T.int64(7)):
                            with T.block("bgemm_update"):
                                v_eps = T.axis.spatial(T.int64(4), eps_1_nu_1_co_1_p_1_fused // T.int64(2) + eps_3 + eps_4)
                                v_nu = T.axis.spatial(T.int64(4), eps_1_nu_1_co_1_p_1_fused % T.int64(2) * T.int64(2) + eps_2_nu_2_co_2_p_2_fused // T.int64(16) + nu_3 + nu_4)
                                v_co = T.axis.spatial(T.int64(128), eps_2_nu_2_co_2_p_2_fused % T.int64(16) // T.int64(2) * T.int64(16) + co_3 * T.int64(8) + co_4)
                                v_p = T.axis.spatial(T.int64(196), eps_0_nu_0_co_0_p_0_fused * T.int64(14) + eps_2_nu_2_co_2_p_2_fused % T.int64(2) * T.int64(7) + p_3 * T.int64(7) + p_4)
                                v_ci = T.axis.reduce(T.int64(128), ci_0 * T.int64(4) + ci_1 * T.int64(2) + ci_2)
                                T.reads(bgemm_local[v_eps, v_nu, v_co, v_p], data_pack_shared[v_eps, v_nu, v_ci, v_p], p1_shared[v_eps, v_nu, v_ci, v_co])
                                T.writes(bgemm_local[v_eps, v_nu, v_co, v_p])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                bgemm_local[v_eps, v_nu, v_co, v_p] = bgemm_local[v_eps, v_nu, v_co, v_p] + data_pack_shared[v_eps, v_nu, v_ci, v_p] * p1_shared[v_eps, v_nu, v_ci, v_co]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(1), T.int64(16), T.int64(7)):
                        with T.block("bgemm_local"):
                            v0 = T.axis.spatial(T.int64(4), eps_1_nu_1_co_1_p_1_fused // T.int64(2) + ax0)
                            v1 = T.axis.spatial(T.int64(4), eps_1_nu_1_co_1_p_1_fused % T.int64(2) * T.int64(2) + eps_2_nu_2_co_2_p_2_fused // T.int64(16) + ax1)
                            v2 = T.axis.spatial(T.int64(128), eps_2_nu_2_co_2_p_2_fused % T.int64(16) // T.int64(2) * T.int64(16) + ax2)
                            v3 = T.axis.spatial(T.int64(196), eps_0_nu_0_co_0_p_0_fused * T.int64(14) + eps_2_nu_2_co_2_p_2_fused % T.int64(2) * T.int64(7) + ax3)
                            T.reads(bgemm_local[v0, v1, v2, v3])
                            T.writes(bgemm[v0, v1, v2, v3])
                            bgemm[v0, v1, v2, v3] = bgemm_local[v0, v1, v2, v3]
        for n_co_h_0_w_0_fused_0 in T.thread_binding(T.int64(98), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 1024, "pragma_unroll_explicit": 1}):
            for n_co_h_0_w_0_fused_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                    for ax2 in T.unroll(T.int64(2)):
                        for ax3 in T.unroll(T.int64(2)):
                            with T.block("inverse_init"):
                                v_co = T.axis.spatial(T.int64(128), (n_co_h_0_w_0_fused_0 * T.int64(256) + n_co_h_0_w_0_fused_1) // T.int64(196) + ax0)
                                v_p = T.axis.spatial(T.int64(196), (n_co_h_0_w_0_fused_0 * T.int64(256) + n_co_h_0_w_0_fused_1) % T.int64(196) + ax1)
                                v_vh, v_vw = T.axis.remap("SS", [ax2, ax3])
                                T.reads()
                                T.writes(inverse_local[v_co, v_p, v_vh, v_vw])
                                T.block_attr({"schedule_rule": "conv2d_nchw_winograd_inverse"})
                                inverse_local[v_co, v_p, v_vh, v_vw] = T.float32(0)
                            for ax4 in T.unroll(T.int64(4)):
                                for ax5 in T.unroll(T.int64(4)):
                                    with T.block("inverse_update"):
                                        v_co = T.axis.spatial(T.int64(128), (n_co_h_0_w_0_fused_0 * T.int64(256) + n_co_h_0_w_0_fused_1) // T.int64(196) + ax0)
                                        v_p = T.axis.spatial(T.int64(196), (n_co_h_0_w_0_fused_0 * T.int64(256) + n_co_h_0_w_0_fused_1) % T.int64(196) + ax1)
                                        v_vh, v_vw, v_r_a, v_r_b = T.axis.remap("SSRR", [ax2, ax3, ax4, ax5])
                                        T.reads(inverse_local[v_co, v_p, v_vh, v_vw], bgemm[v_r_a, v_r_b, v_co, v_p])
                                        T.writes(inverse_local[v_co, v_p, v_vh, v_vw])
                                        T.block_attr({"schedule_rule": "conv2d_nchw_winograd_inverse"})
                                        inverse_local[v_co, v_p, v_vh, v_vw] = inverse_local[v_co, v_p, v_vh, v_vw] + bgemm[v_r_a, v_r_b, v_co, v_p] * T.Select(v_r_a % T.int64(4) == T.int64(3) and v_vh % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_vh % T.int64(2) == T.int64(0), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_vh % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_vh % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_vh % T.int64(2) == T.int64(1), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_vh % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_vh % T.int64(2) == T.int64(1), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_vh % T.int64(2) == T.int64(0), T.float32(1), T.float32(0))))))))) * T.Select(v_r_b % T.int64(4) == T.int64(3) and v_vw % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_vw % T.int64(2) == T.int64(0), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_vw % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_vw % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_vw % T.int64(2) == T.int64(1), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_vw % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_vw % T.int64(2) == T.int64(1), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_vw % T.int64(2) == T.int64(0), T.float32(1), T.float32(0)))))))))
                for h_1, w_1 in T.grid(T.int64(2), T.int64(2)):
                    with T.block("conv2d_winograd"):
                        v_n = T.axis.spatial(T.int64(1), T.int64(0))
                        v_co = T.axis.spatial(T.int64(128), (n_co_h_0_w_0_fused_0 * T.int64(256) + n_co_h_0_w_0_fused_1) // T.int64(196))
                        v_h = T.axis.spatial(T.int64(28), (n_co_h_0_w_0_fused_0 * T.int64(256) + n_co_h_0_w_0_fused_1) % T.int64(196) // T.int64(14) * T.int64(2) + h_1)
                        v_w = T.axis.spatial(T.int64(28), (n_co_h_0_w_0_fused_0 * T.int64(256) + n_co_h_0_w_0_fused_1) % T.int64(14) * T.int64(2) + w_1)
                        T.reads(inverse_local[v_co, v_n * T.int64(196) + v_h // T.int64(2) * T.int64(14) + v_w // T.int64(2), v_h % T.int64(2), v_w % T.int64(2)], p2[v_n, v_co, v_h, v_w], p3[v_n, v_co, T.int64(0), T.int64(0)])
                        T.writes(T_relu[v_n, v_co, v_h, v_w])
                        T_relu[v_n, v_co, v_h, v_w] = T.max(inverse_local[v_co, v_n * T.int64(196) + v_h // T.int64(2) * T.int64(14) + v_w // T.int64(2), v_h % T.int64(2), v_w % T.int64(2)] + p2[v_n, v_co, v_h, v_w] + p3[v_n, v_co, T.int64(0), T.int64(0)], T.float32(0))
b0 = sch.get_block(name="data_pack", func_name="main")
b1 = sch.get_block(name="bgemm", func_name="main")
b2 = sch.get_block(name="inverse", func_name="main")
b3 = sch.get_block(name="conv2d_winograd", func_name="main")
b4 = sch.get_block(name="T_add", func_name="main")
b5 = sch.get_block(name="T_add_1", func_name="main")
b6 = sch.get_block(name="T_relu", func_name="main")
b7 = sch.get_block(name="root", func_name="main")
b8, b9 = sch.get_producers(block=b2)
sch.compute_inline(block=b9)
b10, = sch.get_consumers(block=b2)
l11, l12, l13, l14 = sch.get_loops(block=b10)
l15, l16 = sch.split(loop=l13, factors=[None, 2], preserve_unit_iters=True)
l17, l18 = sch.split(loop=l14, factors=[None, 2], preserve_unit_iters=True)
sch.reorder(l15, l17, l16, l18)
sch.compute_at(block=b2, loop=l17, preserve_unit_loops=True, index=-1)
sch.set_scope(block=b2, buffer_index=0, storage_scope="local")
l19, l20, l21, l22, l23, l24, l25, l26, l27, l28 = sch.get_loops(block=b2)
sch.unroll(loop=l25)
sch.unroll(loop=l26)
sch.unroll(loop=l27)
sch.unroll(loop=l28)
b29, b30 = sch.get_producers(block=b0)
sch.compute_inline(block=b30)
b31, = sch.get_producers(block=b29)
l32, l33, l34, l35, l36, l37 = sch.get_loops(block=b0)
sch.reorder(l34, l35, l32, l33, l36, l37)
sch.unroll(loop=l32)
sch.unroll(loop=l33)
sch.unroll(loop=l36)
sch.unroll(loop=l37)
l38 = sch.fuse(l34, l35, preserve_unit_iters=True)
v39 = sch.sample_categorical(candidates=[32, 64, 128, 256, 512, 1024], probs=[0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666], decision=0)
l40, l41 = sch.split(loop=l38, factors=[None, v39], preserve_unit_iters=True)
sch.bind(loop=l40, thread_axis="blockIdx.x")
sch.bind(loop=l41, thread_axis="threadIdx.x")
b42 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b42, loop=l41, preserve_unit_loops=True, index=-1)
sch.compute_at(block=b29, loop=l41, preserve_unit_loops=True, index=-1)
sch.set_scope(block=b29, buffer_index=0, storage_scope="local")
sch.compute_inline(block=b31)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l43, l44, l45, l46, l47 = sch.get_loops(block=b1)
v48, v49, v50, v51, v52 = sch.sample_perfect_tile(loop=l43, n=5, max_innermost_factor=64, decision=[1, 4, 1, 1, 1])
l53, l54, l55, l56, l57 = sch.split(loop=l43, factors=[v48, v49, v50, v51, v52], preserve_unit_iters=True)
v58, v59, v60, v61, v62 = sch.sample_perfect_tile(loop=l44, n=5, max_innermost_factor=64, decision=[1, 2, 2, 1, 1])
l63, l64, l65, l66, l67 = sch.split(loop=l44, factors=[v58, v59, v60, v61, v62], preserve_unit_iters=True)
v68, v69, v70, v71, v72 = sch.sample_perfect_tile(loop=l45, n=5, max_innermost_factor=64, decision=[1, 1, 8, 2, 8])
l73, l74, l75, l76, l77 = sch.split(loop=l45, factors=[v68, v69, v70, v71, v72], preserve_unit_iters=True)
v78, v79, v80, v81, v82 = sch.sample_perfect_tile(loop=l46, n=5, max_innermost_factor=64, decision=[14, 1, 2, 1, 7])
l83, l84, l85, l86, l87 = sch.split(loop=l46, factors=[v78, v79, v80, v81, v82], preserve_unit_iters=True)
v88, v89, v90 = sch.sample_perfect_tile(loop=l47, n=3, max_innermost_factor=64, decision=[32, 2, 2])
l91, l92, l93 = sch.split(loop=l47, factors=[v88, v89, v90], preserve_unit_iters=True)
sch.reorder(l53, l63, l73, l83, l54, l64, l74, l84, l55, l65, l75, l85, l91, l92, l56, l66, l76, l86, l93, l57, l67, l77, l87)
l94 = sch.fuse(l53, l63, l73, l83, preserve_unit_iters=True)
sch.bind(loop=l94, thread_axis="blockIdx.x")
l95 = sch.fuse(l54, l64, l74, l84, preserve_unit_iters=True)
sch.bind(loop=l95, thread_axis="vthread.x")
l96 = sch.fuse(l55, l65, l75, l85, preserve_unit_iters=True)
sch.bind(loop=l96, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b97 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b97, loop=l96, preserve_unit_loops=True, index=-1)
b98 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b98, loop=l91, preserve_unit_loops=True, index=-1)
l99, l100, l101, l102, l103, l104, l105, l106 = sch.get_loops(block=b98)
l107 = sch.fuse(l103, l104, l105, l106, preserve_unit_iters=True)
v108 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b98, ann_key="meta_schedule.cooperative_fetch", ann_val=v108)
b109 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b109, loop=l91, preserve_unit_loops=True, index=-1)
l110, l111, l112, l113, l114, l115, l116, l117 = sch.get_loops(block=b109)
l118 = sch.fuse(l114, l115, l116, l117, preserve_unit_iters=True)
v119 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b109, ann_key="meta_schedule.cooperative_fetch", ann_val=v119)
sch.reverse_compute_inline(block=b6)
sch.reverse_compute_inline(block=b5)
sch.reverse_compute_inline(block=b4)
v120 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b7, ann_key="meta_schedule.unroll_explicit", ann_val=v120)
l121, l122, l123, l124, l125, l126 = sch.get_loops(block=b3)
l127 = sch.fuse(l121, l122, l123, l124, preserve_unit_iters=True)
v128 = sch.sample_categorical(candidates=[32, 64, 128, 256, 512, 1024], probs=[0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666], decision=3)
l129, l130 = sch.split(loop=l127, factors=[None, v128], preserve_unit_iters=True)
sch.bind(loop=l129, thread_axis="blockIdx.x")
sch.bind(loop=l130, thread_axis="threadIdx.x")
sch.enter_postproc()
sch.unannotate(block_or_loop=b98, ann_key="meta_schedule.cooperative_fetch")
l131, l132, l133, l134, l135 = sch.get_loops(block=b98)
l136, l137, l138 = sch.split(loop=l135, factors=[None, 32, 2], preserve_unit_iters=True)
sch.vectorize(loop=l138)
sch.bind(loop=l137, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b109, ann_key="meta_schedule.cooperative_fetch")
l139, l140, l141, l142, l143 = sch.get_loops(block=b109)
l144, l145 = sch.split(loop=l143, factors=[None, 32], preserve_unit_iters=True)
sch.bind(loop=l145, thread_axis="threadIdx.x")
b146 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b146, ann_key="meta_schedule.unroll_explicit")
b147, b148, b149, b150, b151, b152, b153, b154, b155 = sch.get_child_blocks(b146)
l156, l157, l158, l159, l160, l161 = sch.get_loops(block=b147)
l162, l163, l164, l165, l166, l167 = sch.get_loops(block=b148)
sch.annotate(block_or_loop=l162, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l162, ann_key="pragma_unroll_explicit", ann_val=1)
l168, l169, l170, l171, l172, l173 = sch.get_loops(block=b149)
l174, l175, l176, l177, l178, l179, l180 = sch.get_loops(block=b150)
l181, l182, l183, l184, l185, l186 = sch.get_loops(block=b151)
l187, l188, l189, l190, l191, l192, l193, l194, l195, l196, l197, l198, l199, l200 = sch.get_loops(block=b152)
sch.annotate(block_or_loop=l187, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l187, ann_key="pragma_unroll_explicit", ann_val=1)
l201, l202, l203, l204, l205, l206, l207 = sch.get_loops(block=b153)
l208, l209, l210, l211, l212, l213, l214, l215 = sch.get_loops(block=b154)
sch.annotate(block_or_loop=l208, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l208, ann_key="pragma_unroll_explicit", ann_val=1)
l216, l217, l218, l219 = sch.get_loops(block=b155)
b220 = sch.get_block(name="data_pack", func_name="main")
l221, l222, l223, l224, l225, l226 = sch.get_loops(block=b220)
b227 = sch.decompose_reduction(block=b220, loop=l225)
b228 = sch.get_block(name="bgemm", func_name="main")
l229, l230, l231, l232, l233, l234, l235, l236, l237, l238, l239, l240, l241, l242 = sch.get_loops(block=b228)
b243 = sch.decompose_reduction(block=b228, loop=l232)
b244 = sch.get_block(name="inverse", func_name="main")
l245, l246, l247, l248, l249, l250, l251, l252 = sch.get_loops(block=b244)
b253 = sch.decompose_reduction(block=b244, loop=l251)
2023-11-11 07:43:10 [INFO] [evolutionary_search.cc:713] Generating candidates......
2023-11-11 07:43:13 [INFO] [evolutionary_search.cc:715] Picked top 102 candidate(s) from database
2023-11-11 07:43:21 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310fb046d8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310f554558)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310dc5ddb8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310bbcd2d8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56310b1dbf38)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310dc5d048)]: 399 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310dc26e38)]: 0 failure(s)
2023-11-11 07:43:27 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310fb046d8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310f554558)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310dc5ddb8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310bbcd2d8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56310b1dbf38)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310dc5d048)]: 802 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310dc26e38)]: 0 failure(s)
2023-11-11 07:43:34 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310fb046d8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310f554558)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310dc5ddb8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310bbcd2d8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56310b1dbf38)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310dc5d048)]: 1194 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310dc26e38)]: 0 failure(s)
2023-11-11 07:43:41 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310fb046d8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310f554558)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310dc5ddb8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310bbcd2d8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56310b1dbf38)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310dc5d048)]: 1590 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310dc26e38)]: 0 failure(s)
2023-11-11 07:43:41 [INFO] [evolutionary_search.cc:723] Sampled 50 candidate(s)
2023-11-11 07:44:04 [INFO] [evolutionary_search.cc:621] Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310fb046d8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310f554558)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310dc5ddb8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310bbcd2d8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56310b1dbf38)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310dc5d048)]: 236 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310dc26e38)]: 0 failure(s)
2023-11-11 07:44:30 [INFO] [evolutionary_search.cc:621] Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310fb046d8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310f554558)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310dc5ddb8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310bbcd2d8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56310b1dbf38)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310dc5d048)]: 200 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310dc26e38)]: 0 failure(s)
2023-11-11 07:44:55 [INFO] [evolutionary_search.cc:621] Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310fb046d8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310f554558)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310dc5ddb8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310bbcd2d8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56310b1dbf38)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310dc5d048)]: 174 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310dc26e38)]: 0 failure(s)
2023-11-11 07:45:20 [INFO] [evolutionary_search.cc:621] Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310fb046d8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310f554558)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310dc5ddb8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310bbcd2d8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56310b1dbf38)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310dc5d048)]: 180 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310dc26e38)]: 0 failure(s)
2023-11-11 07:45:27 [INFO] [evolutionary_search.cc:649] Scores of the best 64 candidates:
[1 : 16]:	1.0940  1.0357  1.0339  1.0316  1.0308  1.0267  1.0232  1.0226  1.0215  1.0133  1.0127  1.0122  1.0119  1.0117  1.0104  1.0102
[17 : 32]:	1.0099  1.0099  1.0095  1.0088  1.0088  1.0080  1.0076  1.0076  1.0073  1.0069  1.0065  1.0062  1.0061  1.0056  1.0048  1.0046
[33 : 48]:	1.0045  1.0044  1.0043  1.0041  1.0039  1.0015  0.9973  0.9962  0.9952  0.9950  0.9950  0.9944  0.9941  0.9941  0.9939  0.9938
[49 : 64]:	0.9935  0.9933  0.9933  0.9930  0.9930  0.9925  0.9924  0.9923  0.9918  0.9918  0.9916  0.9916  0.9906  0.9906  0.9901  0.9899
2023-11-11 07:45:28 [INFO] [evolutionary_search.cc:727] Got 64 candidate(s) with evolutionary search
2023-11-11 07:45:28 [INFO] [evolutionary_search.cc:730] Sending 64 candidates(s) for measurement
2023-11-11 07:46:00 [INFO] [task_scheduler.cc:121] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #577: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/home2/xiachunwei/Software/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/home2/xiachunwei/Software/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/home2/xiachunwei/Software/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/home2/xiachunwei/Software/tvm/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/home2/xiachunwei/Software/tvm/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/home2/xiachunwei/Software/tvm/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/home2/xiachunwei/Software/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/home2/xiachunwei/Software/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  4: _ZN3tvm7runtime13PackedFun
  3: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  2: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  0: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  File "/home2/xiachunwei/Software/tvm/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(128), T.int64(28), T.int64(28)), "float32"), p1: T.Buffer((T.int64(4), T.int64(4), T.int64(128), T.int64(128)), "float32"), p2: T.Buffer((T.int64(1), T.int64(128), T.int64(28), T.int64(28)), "float32"), p3: T.Buffer((T.int64(1), T.int64(128), T.int64(1), T.int64(1)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(128), T.int64(28), T.int64(28)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        input_tile_local = T.alloc_buffer((T.int64(128), T.int64(196), T.int64(4), T.int64(4)), scope="local")
        data_pack = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(128), T.int64(196)))
        bgemm = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(128), T.int64(196)))
        inverse_local = T.alloc_buffer((T.int64(128), T.int64(196), T.int64(2), T.int64(2)), scope="local")
        data_pack_local = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(128), T.int64(196)), scope="local")
        bgemm_local = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(128), T.int64(196)), scope="local")
        data_pack_shared = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(128), T.int64(196)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(128), T.int64(128)), scope="shared")
        for ci_p_fused_0 in T.thread_binding(T.int64(98), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 1024, "pragma_unroll_explicit": 1}):
            for ci_p_fused_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(1), T.int64(4), T.int64(4)):
                    with T.block("input_tile"):
                        v_ci = T.axis.spatial(T.int64(128), (ci_p_fused_0 * T.int64(256) + ci_p_fused_1) // T.int64(196) + ax0)
                        v_p = T.axis.spatial(T.int64(196), (ci_p_fused_0 * T.int64(60) + ci_p_fused_1) % T.int64(196) + ax1)
                        v_eps, v_nu = T.axis.remap("SS", [ax2, ax3])
                        T.reads(p0[v_p // T.int64(196), v_ci, v_p % T.int64(196) // T.int64(14) * T.int64(2) + v_eps - T.int64(1), v_p % T.int64(14) * T.int64(2) + v_nu - T.int64(1)])
                        T.writes(input_tile_local[v_ci, v_p, v_eps, v_nu])
                        T.block_attr({"schedule_rule": "None"})
                        input_tile_local[v_ci, v_p, v_eps, v_nu] = T.if_then_else(T.int64(1) <= v_p % T.int64(196) // T.int64(14) * T.int64(2) + v_eps and v_p % T.int64(196) // T.int64(14) * T.int64(2) + v_eps < T.int64(29) and T.int64(1) <= v_p % T.int64(14) * T.int64(2) + v_nu and v_p % T.int64(14) * T.int64(2) + v_nu < T.int64(29), p0[v_p // T.int64(196), v_ci, v_p % T.int64(196) // T.int64(14) * T.int64(2) + v_eps - T.int64(1), v_p % T.int64(14) * T.int64(2) + v_nu - T.int64(1)], T.float32(0))
                for eps in T.unroll(T.int64(4)):
                    for nu in T.unroll(T.int64(4)):
                        with T.block("data_pack_init"):
                            v_eps, v_nu = T.axis.remap("SS", [eps, nu])
                            v_ci = T.axis.spatial(T.int64(128), (ci_p_fused_0 * T.int64(256) + ci_p_fused_1) // T.int64(196))
                            v_p = T.axis.spatial(T.int64(196), (ci_p_fused_0 * T.int64(256) + ci_p_fused_1) % T.int64(196))
                            T.reads()
                            T.writes(data_pack_local[v_eps, v_nu, v_ci, v_p])
                            T.block_attr({"schedule_rule": "conv2d_nchw_winograd_data_pack"})
                            data_pack_local[v_eps, v_nu, v_ci, v_p] = T.float32(0)
                        for r_a in T.unroll(T.int64(4)):
                            for r_b in T.unroll(T.int64(4)):
                                with T.block("data_pack_update"):
                                    v_eps, v_nu = T.axis.remap("SS", [eps, nu])
                                    v_ci = T.axis.spatial(T.int64(128), (ci_p_fused_0 * T.int64(256) + ci_p_fused_1) // T.int64(196))
                                    v_p = T.axis.spatial(T.int64(196), (ci_p_fused_0 * T.int64(256) + ci_p_fused_1) % T.int64(196))
                                    v_r_a, v_r_b = T.axis.remap("RR", [r_a, r_b])
                                    T.reads(data_pack_local[v_eps, v_nu, v_ci, v_p], input_tile_local[v_ci, v_p, v_r_a, v_r_b])
                                    T.writes(data_pack_local[v_eps, v_nu, v_ci, v_p])
                                    T.block_attr({"schedule_rule": "conv2d_nchw_winograd_data_pack"})
                                    data_pack_local[v_eps, v_nu, v_ci, v_p] = data_pack_local[v_eps, v_nu, v_ci, v_p] + input_tile_local[v_ci, v_p, v_r_a, v_r_b] * T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(3), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(1), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(0), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(3), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(1), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(0), T.float32(1), T.float32(0))))))))))))))))) * T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(3), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(1), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(0), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(3), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(1), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(0), T.float32(1), T.float32(0)))))))))))))))))
                for ax0, ax1, ax2, ax3 in T.grid(T.int64(4), T.int64(4), T.int64(1), T.int64(1)):
                    with T.block("data_pack_local"):
                        v0, v1 = T.axis.remap("SS", [ax0, ax1])
                        v2 = T.axis.spatial(T.int64(128), (ci_p_fused_0 * T.int64(256) + ci_p_fused_1) // T.int64(196) + ax2)
                        v3 = T.axis.spatial(T.int64(196), (ci_p_fused_0 * T.int64(60) + ci_p_fused_1) % T.int64(196) + ax3)
                        T.reads(data_pack_local[v0, v1, v2, v3])
                        T.writes(data_pack[v0, v1, v2, v3])
                        data_pack[v0, v1, v2, v3] = data_pack_local[v0, v1, v2, v3]
        for eps_0_nu_0_co_0_p_0_fused in T.thread_binding(T.int64(224), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 1024, "pragma_unroll_explicit": 1}):
            for eps_1_nu_1_co_1_p_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for eps_2_nu_2_co_2_p_2_fused in T.thread_binding(T.int64(224), thread="threadIdx.x"):
                    for eps_3_init, nu_3_init, co_3_init, p_3_init, eps_4_init, nu_4_init, co_4_init, p_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(4), T.int64(2)):
                        with T.block("bgemm_init"):
                            v_eps = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(56) + eps_3_init + eps_4_init)
                            v_nu = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused % T.int64(56) // T.int64(14) + nu_3_init + nu_4_init)
                            v_co = T.axis.spatial(T.int64(128), eps_0_nu_0_co_0_p_0_fused % T.int64(14) // T.int64(7) * T.int64(64) + eps_2_nu_2_co_2_p_2_fused // T.int64(14) * T.int64(4) + co_3_init * T.int64(4) + co_4_init)
                            v_p = T.axis.spatial(T.int64(196), eps_0_nu_0_co_0_p_0_fused % T.int64(7) * T.int64(28) + eps_2_nu_2_co_2_p_2_fused % T.int64(14) * T.int64(2) + p_3_init * T.int64(2) + p_4_init)
                            T.reads()
                            T.writes(bgemm_local[v_eps, v_nu, v_co, v_p])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            bgemm_local[v_eps, v_nu, v_co, v_p] = T.float32(0)
                    for ci_0 in range(T.int64(64)):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(1)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(224), thread="threadIdx.x"):
                                with T.block("data_pack_shared"):
                                    v0 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(56))
                                    v1 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused % T.int64(56) // T.int64(14))
                                    v2 = T.axis.spatial(T.int64(128), ci_0 * T.int64(2) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(224) + ax0_ax1_ax2_ax3_fused_1) // T.int64(28))
                                    v3 = T.axis.spatial(T.int64(196), eps_0_nu_0_co_0_p_0_fused % T.int64(7) * T.int64(28) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(224) + ax0_ax1_ax2_ax3_fused_1) % T.int64(28))
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * T.int64(224) + ax0_ax1_ax2_ax3_fused_1 < T.int64(56))
                                    T.reads(data_pack[v0, v1, v2, v3])
                                    T.writes(data_pack_shared[v0, v1, v2, v3])
                                    data_pack_shared[v0, v1, v2, v3] = data_pack[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(1)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(224), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("p1_shared"):
                                        v0 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(56))
                                        v1 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused % T.int64(56) // T.int64(14))
                                        v2 = T.axis.spatial(T.int64(128), ci_0 * T.int64(2) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(896) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) // T.int64(64))
                                        v3 = T.axis.spatial(T.int64(128), eps_0_nu_0_co_0_p_0_fused % T.int64(14) // T.int64(7) * T.int64(64) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(896) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) % T.int64(64))
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * T.int64(224) + ax0_ax1_ax2_ax3_fused_1) * T.int64(4) + ax0_ax1_ax2_ax3_fused_2 < T.int64(128))
                                        T.reads(p1[v0, v1, v2, v3])
                                        T.writes(p1_shared[v0, v1, v2, v3])
                                        p1_shared[v0, v1, v2, v3] = p1[v0, v1, v2, v3]
                        for ci_1, eps_3, nu_3, co_3, p_3, ci_2, eps_4, nu_4, co_4, p_4 in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(1), T.int64(1), T.int64(4), T.int64(2)):
                            with T.block("bgemm_update"):
                                v_eps = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(56) + eps_3 + eps_4)
                                v_nu = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused % T.int64(56) // T.int64(14) + nu_3 + nu_4)
                                v_co = T.axis.spatial(T.int64(128), eps_0_nu_0_co_0_p_0_fused % T.int64(14) // T.int64(7) * T.int64(64) + eps_2_nu_2_co_2_p_2_fused // T.int64(14) * T.int64(4) + co_3 * T.int64(4) + co_4)
                                v_p = T.axis.spatial(T.int64(196), eps_0_nu_0_co_0_p_0_fused % T.int64(7) * T.int64(28) + eps_2_nu_2_co_2_p_2_fused % T.int64(14) * T.int64(2) + p_3 * T.int64(2) + p_4)
                                v_ci = T.axis.reduce(T.int64(128), ci_0 * T.int64(2) + ci_1 * T.int64(2) + ci_2)
                                T.reads(bgemm_local[v_eps, v_nu, v_co, v_p], data_pack_shared[v_eps, v_nu, v_ci, v_p], p1_shared[v_eps, v_nu, v_ci, v_co])
                                T.writes(bgemm_local[v_eps, v_nu, v_co, v_p])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                bgemm_local[v_eps, v_nu, v_co, v_p] = bgemm_local[v_eps, v_nu, v_co, v_p] + data_pack_shared[v_eps, v_nu, v_ci, v_p] * p1_shared[v_eps, v_nu, v_ci, v_co]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(1), T.int64(4), T.int64(2)):
                        with T.block("bgemm_local"):
                            v0 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(56) + ax0)
                            v1 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused % T.int64(56) // T.int64(14) + ax1)
                            v2 = T.axis.spatial(T.int64(128), eps_0_nu_0_co_0_p_0_fused % T.int64(14) // T.int64(7) * T.int64(64) + eps_2_nu_2_co_2_p_2_fused // T.int64(14) * T.int64(4) + ax2)
                            v3 = T.axis.spatial(T.int64(196), eps_0_nu_0_co_0_p_0_fused % T.int64(7) * T.int64(28) + eps_2_nu_2_co_2_p_2_fused % T.int64(14) * T.int64(2) + ax3)
                            T.reads(bgemm_local[v0, v1, v2, v3])
                            T.writes(bgemm[v0, v1, v2, v3])
                            bgemm[v0, v1, v2, v3] = bgemm_local[v0, v1, v2, v3]
        for n_co_h_0_w_0_fused_0 in T.thread_binding(T.int64(784), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 1024, "pragma_unroll_explicit": 1}):
            for n_co_h_0_w_0_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                    for ax2 in T.unroll(T.int64(2)):
                        for ax3 in T.unroll(T.int64(2)):
                            with T.block("inverse_init"):
                                v_co = T.axis.spatial(T.int64(128), (n_co_h_0_w_0_fused_0 * T.int64(32) + n_co_h_0_w_0_fused_1) // T.int64(196) + ax0)
                                v_p = T.axis.spatial(T.int64(196), (n_co_h_0_w_0_fused_0 * T.int64(32) + n_co_h_0_w_0_fused_1) % T.int64(196) + ax1)
                                v_vh, v_vw = T.axis.remap("SS", [ax2, ax3])
                                T.reads()
                                T.writes(inverse_local[v_co, v_p, v_vh, v_vw])
                                T.block_attr({"schedule_rule": "conv2d_nchw_winograd_inverse"})
                                inverse_local[v_co, v_p, v_vh, v_vw] = T.float32(0)
                            for ax4 in T.unroll(T.int64(4)):
                                for ax5 in T.unroll(T.int64(4)):
                                    with T.block("inverse_update"):
                                        v_co = T.axis.spatial(T.int64(128), (n_co_h_0_w_0_fused_0 * T.int64(32) + n_co_h_0_w_0_fused_1) // T.int64(196) + ax0)
                                        v_p = T.axis.spatial(T.int64(196), (n_co_h_0_w_0_fused_0 * T.int64(32) + n_co_h_0_w_0_fused_1) % T.int64(196) + ax1)
                                        v_vh, v_vw, v_r_a, v_r_b = T.axis.remap("SSRR", [ax2, ax3, ax4, ax5])
                                        T.reads(inverse_local[v_co, v_p, v_vh, v_vw], bgemm[v_r_a, v_r_b, v_co, v_p])
                                        T.writes(inverse_local[v_co, v_p, v_vh, v_vw])
                                        T.block_attr({"schedule_rule": "conv2d_nchw_winograd_inverse"})
                                        inverse_local[v_co, v_p, v_vh, v_vw] = inverse_local[v_co, v_p, v_vh, v_vw] + bgemm[v_r_a, v_r_b, v_co, v_p] * T.Select(v_r_a % T.int64(4) == T.int64(3) and v_vh % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_vh % T.int64(2) == T.int64(0), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_vh % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_vh % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_vh % T.int64(2) == T.int64(1), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_vh % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_vh % T.int64(2) == T.int64(1), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_vh % T.int64(2) == T.int64(0), T.float32(1), T.float32(0))))))))) * T.Select(v_r_b % T.int64(4) == T.int64(3) and v_vw % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_vw % T.int64(2) == T.int64(0), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_vw % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_vw % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_vw % T.int64(2) == T.int64(1), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_vw % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_vw % T.int64(2) == T.int64(1), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_vw % T.int64(2) == T.int64(0), T.float32(1), T.float32(0)))))))))
                for h_1, w_1 in T.grid(T.int64(2), T.int64(2)):
                    with T.block("conv2d_winograd"):
                        v_n = T.axis.spatial(T.int64(1), T.int64(0))
                        v_co = T.axis.spatial(T.int64(128), (n_co_h_0_w_0_fused_0 * T.int64(32) + n_co_h_0_w_0_fused_1) // T.int64(196))
                        v_h = T.axis.spatial(T.int64(28), (n_co_h_0_w_0_fused_0 * T.int64(32) + n_co_h_0_w_0_fused_1) % T.int64(196) // T.int64(14) * T.int64(2) + h_1)
                        v_w = T.axis.spatial(T.int64(28), (n_co_h_0_w_0_fused_0 * T.int64(32) + n_co_h_0_w_0_fused_1) % T.int64(14) * T.int64(2) + w_1)
                        T.reads(inverse_local[v_co, v_n * T.int64(196) + v_h // T.int64(2) * T.int64(14) + v_w // T.int64(2), v_h % T.int64(2), v_w % T.int64(2)], p2[v_n, v_co, v_h, v_w], p3[v_n, v_co, T.int64(0), T.int64(0)])
                        T.writes(T_relu[v_n, v_co, v_h, v_w])
                        T_relu[v_n, v_co, v_h, v_w] = T.max(inverse_local[v_co, v_n * T.int64(196) + v_h // T.int64(2) * T.int64(14) + v_w // T.int64(2), v_h % T.int64(2), v_w % T.int64(2)] + p2[v_n, v_co, v_h, v_w] + p3[v_n, v_co, T.int64(0), T.int64(0)], T.float32(0))
b0 = sch.get_block(name="data_pack", func_name="main")
b1 = sch.get_block(name="bgemm", func_name="main")
b2 = sch.get_block(name="inverse", func_name="main")
b3 = sch.get_block(name="conv2d_winograd", func_name="main")
b4 = sch.get_block(name="T_add", func_name="main")
b5 = sch.get_block(name="T_add_1", func_name="main")
b6 = sch.get_block(name="T_relu", func_name="main")
b7 = sch.get_block(name="root", func_name="main")
b8, b9 = sch.get_producers(block=b2)
sch.compute_inline(block=b9)
b10, = sch.get_consumers(block=b2)
l11, l12, l13, l14 = sch.get_loops(block=b10)
l15, l16 = sch.split(loop=l13, factors=[None, 2], preserve_unit_iters=True)
l17, l18 = sch.split(loop=l14, factors=[None, 2], preserve_unit_iters=True)
sch.reorder(l15, l17, l16, l18)
sch.compute_at(block=b2, loop=l17, preserve_unit_loops=True, index=-1)
sch.set_scope(block=b2, buffer_index=0, storage_scope="local")
l19, l20, l21, l22, l23, l24, l25, l26, l27, l28 = sch.get_loops(block=b2)
sch.unroll(loop=l25)
sch.unroll(loop=l26)
sch.unroll(loop=l27)
sch.unroll(loop=l28)
b29, b30 = sch.get_producers(block=b0)
sch.compute_inline(block=b30)
b31, = sch.get_producers(block=b29)
l32, l33, l34, l35, l36, l37 = sch.get_loops(block=b0)
sch.reorder(l34, l35, l32, l33, l36, l37)
sch.unroll(loop=l32)
sch.unroll(loop=l33)
sch.unroll(loop=l36)
sch.unroll(loop=l37)
l38 = sch.fuse(l34, l35, preserve_unit_iters=True)
v39 = sch.sample_categorical(candidates=[32, 64, 128, 256, 512, 1024], probs=[0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666], decision=3)
l40, l41 = sch.split(loop=l38, factors=[None, v39], preserve_unit_iters=True)
sch.bind(loop=l40, thread_axis="blockIdx.x")
sch.bind(loop=l41, thread_axis="threadIdx.x")
b42 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b42, loop=l41, preserve_unit_loops=True, index=-1)
sch.compute_at(block=b29, loop=l41, preserve_unit_loops=True, index=-1)
sch.set_scope(block=b29, buffer_index=0, storage_scope="local")
sch.compute_inline(block=b31)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l43, l44, l45, l46, l47 = sch.get_loops(block=b1)
v48, v49, v50, v51, v52 = sch.sample_perfect_tile(loop=l43, n=5, max_innermost_factor=64, decision=[4, 1, 1, 1, 1])
l53, l54, l55, l56, l57 = sch.split(loop=l43, factors=[v48, v49, v50, v51, v52], preserve_unit_iters=True)
v58, v59, v60, v61, v62 = sch.sample_perfect_tile(loop=l44, n=5, max_innermost_factor=64, decision=[4, 1, 1, 1, 1])
l63, l64, l65, l66, l67 = sch.split(loop=l44, factors=[v58, v59, v60, v61, v62], preserve_unit_iters=True)
v68, v69, v70, v71, v72 = sch.sample_perfect_tile(loop=l45, n=5, max_innermost_factor=64, decision=[2, 1, 16, 1, 4])
l73, l74, l75, l76, l77 = sch.split(loop=l45, factors=[v68, v69, v70, v71, v72], preserve_unit_iters=True)
v78, v79, v80, v81, v82 = sch.sample_perfect_tile(loop=l46, n=5, max_innermost_factor=64, decision=[7, 1, 14, 1, 2])
l83, l84, l85, l86, l87 = sch.split(loop=l46, factors=[v78, v79, v80, v81, v82], preserve_unit_iters=True)
v88, v89, v90 = sch.sample_perfect_tile(loop=l47, n=3, max_innermost_factor=64, decision=[64, 1, 2])
l91, l92, l93 = sch.split(loop=l47, factors=[v88, v89, v90], preserve_unit_iters=True)
sch.reorder(l53, l63, l73, l83, l54, l64, l74, l84, l55, l65, l75, l85, l91, l92, l56, l66, l76, l86, l93, l57, l67, l77, l87)
l94 = sch.fuse(l53, l63, l73, l83, preserve_unit_iters=True)
sch.bind(loop=l94, thread_axis="blockIdx.x")
l95 = sch.fuse(l54, l64, l74, l84, preserve_unit_iters=True)
sch.bind(loop=l95, thread_axis="vthread.x")
l96 = sch.fuse(l55, l65, l75, l85, preserve_unit_iters=True)
sch.bind(loop=l96, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b97 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b97, loop=l96, preserve_unit_loops=True, index=-1)
b98 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b98, loop=l91, preserve_unit_loops=True, index=-1)
l99, l100, l101, l102, l103, l104, l105, l106 = sch.get_loops(block=b98)
l107 = sch.fuse(l103, l104, l105, l106, preserve_unit_iters=True)
v108 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b98, ann_key="meta_schedule.cooperative_fetch", ann_val=v108)
b109 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b109, loop=l91, preserve_unit_loops=True, index=-1)
l110, l111, l112, l113, l114, l115, l116, l117 = sch.get_loops(block=b109)
l118 = sch.fuse(l114, l115, l116, l117, preserve_unit_iters=True)
v119 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b109, ann_key="meta_schedule.cooperative_fetch", ann_val=v119)
sch.reverse_compute_inline(block=b6)
sch.reverse_compute_inline(block=b5)
sch.reverse_compute_inline(block=b4)
v120 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b7, ann_key="meta_schedule.unroll_explicit", ann_val=v120)
l121, l122, l123, l124, l125, l126 = sch.get_loops(block=b3)
l127 = sch.fuse(l121, l122, l123, l124, preserve_unit_iters=True)
v128 = sch.sample_categorical(candidates=[32, 64, 128, 256, 512, 1024], probs=[0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666], decision=0)
l129, l130 = sch.split(loop=l127, factors=[None, v128], preserve_unit_iters=True)
sch.bind(loop=l129, thread_axis="blockIdx.x")
sch.bind(loop=l130, thread_axis="threadIdx.x")
sch.enter_postproc()
sch.unannotate(block_or_loop=b98, ann_key="meta_schedule.cooperative_fetch")
l131, l132, l133, l134, l135 = sch.get_loops(block=b98)
l136, l137 = sch.split(loop=l135, factors=[None, 224], preserve_unit_iters=True)
sch.bind(loop=l137, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b109, ann_key="meta_schedule.cooperative_fetch")
l138, l139, l140, l141, l142 = sch.get_loops(block=b109)
l143, l144, l145 = sch.split(loop=l142, factors=[None, 224, 4], preserve_unit_iters=True)
sch.vectorize(loop=l145)
sch.bind(loop=l144, thread_axis="threadIdx.x")
b146 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b146, ann_key="meta_schedule.unroll_explicit")
b147, b148, b149, b150, b151, b152, b153, b154, b155 = sch.get_child_blocks(b146)
l156, l157, l158, l159, l160, l161 = sch.get_loops(block=b147)
l162, l163, l164, l165, l166, l167 = sch.get_loops(block=b148)
sch.annotate(block_or_loop=l162, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l162, ann_key="pragma_unroll_explicit", ann_val=1)
l168, l169, l170, l171, l172, l173 = sch.get_loops(block=b149)
l174, l175, l176, l177, l178, l179 = sch.get_loops(block=b150)
l180, l181, l182, l183, l184, l185, l186 = sch.get_loops(block=b151)
l187, l188, l189, l190, l191, l192, l193, l194, l195, l196, l197, l198, l199, l200 = sch.get_loops(block=b152)
sch.annotate(block_or_loop=l187, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l187, ann_key="pragma_unroll_explicit", ann_val=1)
l201, l202, l203, l204, l205, l206, l207 = sch.get_loops(block=b153)
l208, l209, l210, l211, l212, l213, l214, l215 = sch.get_loops(block=b154)
sch.annotate(block_or_loop=l208, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l208, ann_key="pragma_unroll_explicit", ann_val=1)
l216, l217, l218, l219 = sch.get_loops(block=b155)
b220 = sch.get_block(name="data_pack", func_name="main")
l221, l222, l223, l224, l225, l226 = sch.get_loops(block=b220)
b227 = sch.decompose_reduction(block=b220, loop=l225)
b228 = sch.get_block(name="bgemm", func_name="main")
l229, l230, l231, l232, l233, l234, l235, l236, l237, l238, l239, l240, l241, l242 = sch.get_loops(block=b228)
b243 = sch.decompose_reduction(block=b228, loop=l232)
b244 = sch.get_block(name="inverse", func_name="main")
l245, l246, l247, l248, l249, l250, l251, l252 = sch.get_loops(block=b244)
b253 = sch.decompose_reduction(block=b244, loop=l251)
2023-11-11 07:46:00 [INFO] [task_scheduler.cc:121] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #578: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/home2/xiachunwei/Software/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/home2/xiachunwei/Software/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/home2/xiachunwei/Software/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/home2/xiachunwei/Software/tvm/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/home2/xiachunwei/Software/tvm/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/home2/xiachunwei/Software/tvm/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/home2/xiachunwei/Software/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/home2/xiachunwei/Software/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  4: _ZN3tvm7runtime13PackedFun
  3: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  2: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  0: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  File "/home2/xiachunwei/Software/tvm/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(128), T.int64(28), T.int64(28)), "float32"), p1: T.Buffer((T.int64(4), T.int64(4), T.int64(128), T.int64(128)), "float32"), p2: T.Buffer((T.int64(1), T.int64(128), T.int64(28), T.int64(28)), "float32"), p3: T.Buffer((T.int64(1), T.int64(128), T.int64(1), T.int64(1)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(128), T.int64(28), T.int64(28)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        input_tile_local = T.alloc_buffer((T.int64(128), T.int64(196), T.int64(4), T.int64(4)), scope="local")
        data_pack = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(128), T.int64(196)))
        bgemm = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(128), T.int64(196)))
        inverse_local = T.alloc_buffer((T.int64(128), T.int64(196), T.int64(2), T.int64(2)), scope="local")
        data_pack_local = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(128), T.int64(196)), scope="local")
        bgemm_local = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(128), T.int64(196)), scope="local")
        data_pack_shared = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(128), T.int64(196)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(128), T.int64(128)), scope="shared")
        for ci_p_fused_0 in T.thread_binding(T.int64(98), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 1024, "pragma_unroll_explicit": 1}):
            for ci_p_fused_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(1), T.int64(4), T.int64(4)):
                    with T.block("input_tile"):
                        v_ci = T.axis.spatial(T.int64(128), (ci_p_fused_0 * T.int64(256) + ci_p_fused_1) // T.int64(196) + ax0)
                        v_p = T.axis.spatial(T.int64(196), (ci_p_fused_0 * T.int64(60) + ci_p_fused_1) % T.int64(196) + ax1)
                        v_eps, v_nu = T.axis.remap("SS", [ax2, ax3])
                        T.reads(p0[v_p // T.int64(196), v_ci, v_p % T.int64(196) // T.int64(14) * T.int64(2) + v_eps - T.int64(1), v_p % T.int64(14) * T.int64(2) + v_nu - T.int64(1)])
                        T.writes(input_tile_local[v_ci, v_p, v_eps, v_nu])
                        T.block_attr({"schedule_rule": "None"})
                        input_tile_local[v_ci, v_p, v_eps, v_nu] = T.if_then_else(T.int64(1) <= v_p % T.int64(196) // T.int64(14) * T.int64(2) + v_eps and v_p % T.int64(196) // T.int64(14) * T.int64(2) + v_eps < T.int64(29) and T.int64(1) <= v_p % T.int64(14) * T.int64(2) + v_nu and v_p % T.int64(14) * T.int64(2) + v_nu < T.int64(29), p0[v_p // T.int64(196), v_ci, v_p % T.int64(196) // T.int64(14) * T.int64(2) + v_eps - T.int64(1), v_p % T.int64(14) * T.int64(2) + v_nu - T.int64(1)], T.float32(0))
                for eps in T.unroll(T.int64(4)):
                    for nu in T.unroll(T.int64(4)):
                        with T.block("data_pack_init"):
                            v_eps, v_nu = T.axis.remap("SS", [eps, nu])
                            v_ci = T.axis.spatial(T.int64(128), (ci_p_fused_0 * T.int64(256) + ci_p_fused_1) // T.int64(196))
                            v_p = T.axis.spatial(T.int64(196), (ci_p_fused_0 * T.int64(256) + ci_p_fused_1) % T.int64(196))
                            T.reads()
                            T.writes(data_pack_local[v_eps, v_nu, v_ci, v_p])
                            T.block_attr({"schedule_rule": "conv2d_nchw_winograd_data_pack"})
                            data_pack_local[v_eps, v_nu, v_ci, v_p] = T.float32(0)
                        for r_a in T.unroll(T.int64(4)):
                            for r_b in T.unroll(T.int64(4)):
                                with T.block("data_pack_update"):
                                    v_eps, v_nu = T.axis.remap("SS", [eps, nu])
                                    v_ci = T.axis.spatial(T.int64(128), (ci_p_fused_0 * T.int64(256) + ci_p_fused_1) // T.int64(196))
                                    v_p = T.axis.spatial(T.int64(196), (ci_p_fused_0 * T.int64(256) + ci_p_fused_1) % T.int64(196))
                                    v_r_a, v_r_b = T.axis.remap("RR", [r_a, r_b])
                                    T.reads(data_pack_local[v_eps, v_nu, v_ci, v_p], input_tile_local[v_ci, v_p, v_r_a, v_r_b])
                                    T.writes(data_pack_local[v_eps, v_nu, v_ci, v_p])
                                    T.block_attr({"schedule_rule": "conv2d_nchw_winograd_data_pack"})
                                    data_pack_local[v_eps, v_nu, v_ci, v_p] = data_pack_local[v_eps, v_nu, v_ci, v_p] + input_tile_local[v_ci, v_p, v_r_a, v_r_b] * T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(3), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(1), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(0), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(3), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(1), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(0), T.float32(1), T.float32(0))))))))))))))))) * T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(3), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(1), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(0), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(3), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(1), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(0), T.float32(1), T.float32(0)))))))))))))))))
                for ax0, ax1, ax2, ax3 in T.grid(T.int64(4), T.int64(4), T.int64(1), T.int64(1)):
                    with T.block("data_pack_local"):
                        v0, v1 = T.axis.remap("SS", [ax0, ax1])
                        v2 = T.axis.spatial(T.int64(128), (ci_p_fused_0 * T.int64(256) + ci_p_fused_1) // T.int64(196) + ax2)
                        v3 = T.axis.spatial(T.int64(196), (ci_p_fused_0 * T.int64(60) + ci_p_fused_1) % T.int64(196) + ax3)
                        T.reads(data_pack_local[v0, v1, v2, v3])
                        T.writes(data_pack[v0, v1, v2, v3])
                        data_pack[v0, v1, v2, v3] = data_pack_local[v0, v1, v2, v3]
        for eps_0_nu_0_co_0_p_0_fused in T.thread_binding(T.int64(224), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 1024, "pragma_unroll_explicit": 1}):
            for eps_1_nu_1_co_1_p_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for eps_2_nu_2_co_2_p_2_fused in T.thread_binding(T.int64(224), thread="threadIdx.x"):
                    for eps_3_init, nu_3_init, co_3_init, p_3_init, eps_4_init, nu_4_init, co_4_init, p_4_init in T.grid(T.int64(1), T.int64(1), T.int64(4), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(2)):
                        with T.block("bgemm_init"):
                            v_eps = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(56) + eps_3_init + eps_4_init)
                            v_nu = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused % T.int64(56) // T.int64(14) + nu_3_init + nu_4_init)
                            v_co = T.axis.spatial(T.int64(128), eps_0_nu_0_co_0_p_0_fused % T.int64(14) // T.int64(7) * T.int64(64) + eps_2_nu_2_co_2_p_2_fused // T.int64(14) * T.int64(4) + co_3_init + co_4_init)
                            v_p = T.axis.spatial(T.int64(196), eps_0_nu_0_co_0_p_0_fused % T.int64(7) * T.int64(28) + eps_2_nu_2_co_2_p_2_fused % T.int64(14) * T.int64(2) + p_3_init * T.int64(2) + p_4_init)
                            T.reads()
                            T.writes(bgemm_local[v_eps, v_nu, v_co, v_p])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            bgemm_local[v_eps, v_nu, v_co, v_p] = T.float32(0)
                    for ci_0 in range(T.int64(32)):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(1)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(224), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("data_pack_shared"):
                                        v0 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(56))
                                        v1 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused % T.int64(56) // T.int64(14))
                                        v2 = T.axis.spatial(T.int64(128), ci_0 * T.int64(4) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(896) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) // T.int64(28))
                                        v3 = T.axis.spatial(T.int64(196), eps_0_nu_0_co_0_p_0_fused % T.int64(7) * T.int64(28) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(896) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) % T.int64(28))
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * T.int64(224) + ax0_ax1_ax2_ax3_fused_1) * T.int64(4) + ax0_ax1_ax2_ax3_fused_2 < T.int64(112))
                                        T.reads(data_pack[v0, v1, v2, v3])
                                        T.writes(data_pack_shared[v0, v1, v2, v3])
                                        data_pack_shared[v0, v1, v2, v3] = data_pack[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(1)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(224), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("p1_shared"):
                                        v0 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(56))
                                        v1 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused % T.int64(56) // T.int64(14))
                                        v2 = T.axis.spatial(T.int64(128), ci_0 * T.int64(4) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(448) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) // T.int64(64))
                                        v3 = T.axis.spatial(T.int64(128), eps_0_nu_0_co_0_p_0_fused % T.int64(14) // T.int64(7) * T.int64(64) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(448) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) % T.int64(64))
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * T.int64(224) + ax0_ax1_ax2_ax3_fused_1) * T.int64(2) + ax0_ax1_ax2_ax3_fused_2 < T.int64(256))
                                        T.reads(p1[v0, v1, v2, v3])
                                        T.writes(p1_shared[v0, v1, v2, v3])
                                        p1_shared[v0, v1, v2, v3] = p1[v0, v1, v2, v3]
                        for ci_1, eps_3, nu_3, co_3, p_3, ci_2, eps_4, nu_4, co_4, p_4 in T.grid(T.int64(4), T.int64(1), T.int64(1), T.int64(4), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(2)):
                            with T.block("bgemm_update"):
                                v_eps = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(56) + eps_3 + eps_4)
                                v_nu = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused % T.int64(56) // T.int64(14) + nu_3 + nu_4)
                                v_co = T.axis.spatial(T.int64(128), eps_0_nu_0_co_0_p_0_fused % T.int64(14) // T.int64(7) * T.int64(64) + eps_2_nu_2_co_2_p_2_fused // T.int64(14) * T.int64(4) + co_3 + co_4)
                                v_p = T.axis.spatial(T.int64(196), eps_0_nu_0_co_0_p_0_fused % T.int64(7) * T.int64(28) + eps_2_nu_2_co_2_p_2_fused % T.int64(14) * T.int64(2) + p_3 * T.int64(2) + p_4)
                                v_ci = T.axis.reduce(T.int64(128), ci_0 * T.int64(4) + ci_1 + ci_2)
                                T.reads(bgemm_local[v_eps, v_nu, v_co, v_p], data_pack_shared[v_eps, v_nu, v_ci, v_p], p1_shared[v_eps, v_nu, v_ci, v_co])
                                T.writes(bgemm_local[v_eps, v_nu, v_co, v_p])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                bgemm_local[v_eps, v_nu, v_co, v_p] = bgemm_local[v_eps, v_nu, v_co, v_p] + data_pack_shared[v_eps, v_nu, v_ci, v_p] * p1_shared[v_eps, v_nu, v_ci, v_co]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(1), T.int64(4), T.int64(2)):
                        with T.block("bgemm_local"):
                            v0 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(56) + ax0)
                            v1 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused % T.int64(56) // T.int64(14) + ax1)
                            v2 = T.axis.spatial(T.int64(128), eps_0_nu_0_co_0_p_0_fused % T.int64(14) // T.int64(7) * T.int64(64) + eps_2_nu_2_co_2_p_2_fused // T.int64(14) * T.int64(4) + ax2)
                            v3 = T.axis.spatial(T.int64(196), eps_0_nu_0_co_0_p_0_fused % T.int64(7) * T.int64(28) + eps_2_nu_2_co_2_p_2_fused % T.int64(14) * T.int64(2) + ax3)
                            T.reads(bgemm_local[v0, v1, v2, v3])
                            T.writes(bgemm[v0, v1, v2, v3])
                            bgemm[v0, v1, v2, v3] = bgemm_local[v0, v1, v2, v3]
        for n_co_h_0_w_0_fused_0 in T.thread_binding(T.int64(25), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 1024, "pragma_unroll_explicit": 1}):
            for n_co_h_0_w_0_fused_1 in T.thread_binding(T.int64(1024), thread="threadIdx.x"):
                for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                    for ax2 in T.unroll(T.int64(2)):
                        for ax3 in T.unroll(T.int64(2)):
                            with T.block("inverse_init"):
                                v_co = T.axis.spatial(T.int64(128), (n_co_h_0_w_0_fused_0 * T.int64(1024) + n_co_h_0_w_0_fused_1) // T.int64(196) + ax0)
                                v_p = T.axis.spatial(T.int64(196), (n_co_h_0_w_0_fused_0 * T.int64(1024) + n_co_h_0_w_0_fused_1) % T.int64(196) + ax1)
                                v_vh, v_vw = T.axis.remap("SS", [ax2, ax3])
                                T.where(n_co_h_0_w_0_fused_0 * T.int64(1024) + n_co_h_0_w_0_fused_1 < T.int64(25088))
                                T.reads()
                                T.writes(inverse_local[v_co, v_p, v_vh, v_vw])
                                T.block_attr({"schedule_rule": "conv2d_nchw_winograd_inverse"})
                                inverse_local[v_co, v_p, v_vh, v_vw] = T.float32(0)
                            for ax4 in T.unroll(T.int64(4)):
                                for ax5 in T.unroll(T.int64(4)):
                                    with T.block("inverse_update"):
                                        v_co = T.axis.spatial(T.int64(128), (n_co_h_0_w_0_fused_0 * T.int64(1024) + n_co_h_0_w_0_fused_1) // T.int64(196) + ax0)
                                        v_p = T.axis.spatial(T.int64(196), (n_co_h_0_w_0_fused_0 * T.int64(1024) + n_co_h_0_w_0_fused_1) % T.int64(196) + ax1)
                                        v_vh, v_vw, v_r_a, v_r_b = T.axis.remap("SSRR", [ax2, ax3, ax4, ax5])
                                        T.where(n_co_h_0_w_0_fused_0 * T.int64(1024) + n_co_h_0_w_0_fused_1 < T.int64(25088))
                                        T.reads(inverse_local[v_co, v_p, v_vh, v_vw], bgemm[v_r_a, v_r_b, v_co, v_p])
                                        T.writes(inverse_local[v_co, v_p, v_vh, v_vw])
                                        T.block_attr({"schedule_rule": "conv2d_nchw_winograd_inverse"})
                                        inverse_local[v_co, v_p, v_vh, v_vw] = inverse_local[v_co, v_p, v_vh, v_vw] + bgemm[v_r_a, v_r_b, v_co, v_p] * T.Select(v_r_a % T.int64(4) == T.int64(3) and v_vh % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_vh % T.int64(2) == T.int64(0), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_vh % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_vh % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_vh % T.int64(2) == T.int64(1), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_vh % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_vh % T.int64(2) == T.int64(1), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_vh % T.int64(2) == T.int64(0), T.float32(1), T.float32(0))))))))) * T.Select(v_r_b % T.int64(4) == T.int64(3) and v_vw % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_vw % T.int64(2) == T.int64(0), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_vw % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_vw % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_vw % T.int64(2) == T.int64(1), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_vw % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_vw % T.int64(2) == T.int64(1), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_vw % T.int64(2) == T.int64(0), T.float32(1), T.float32(0)))))))))
                for h_1, w_1 in T.grid(T.int64(2), T.int64(2)):
                    with T.block("conv2d_winograd"):
                        v_n = T.axis.spatial(T.int64(1), T.int64(0))
                        v_co = T.axis.spatial(T.int64(128), (n_co_h_0_w_0_fused_0 * T.int64(1024) + n_co_h_0_w_0_fused_1) // T.int64(196))
                        v_h = T.axis.spatial(T.int64(28), (n_co_h_0_w_0_fused_0 * T.int64(1024) + n_co_h_0_w_0_fused_1) % T.int64(196) // T.int64(14) * T.int64(2) + h_1)
                        v_w = T.axis.spatial(T.int64(28), (n_co_h_0_w_0_fused_0 * T.int64(1024) + n_co_h_0_w_0_fused_1) % T.int64(14) * T.int64(2) + w_1)
                        T.where(n_co_h_0_w_0_fused_0 * T.int64(1024) + n_co_h_0_w_0_fused_1 < T.int64(25088))
                        T.reads(inverse_local[v_co, v_n * T.int64(196) + v_h // T.int64(2) * T.int64(14) + v_w // T.int64(2), v_h % T.int64(2), v_w % T.int64(2)], p2[v_n, v_co, v_h, v_w], p3[v_n, v_co, T.int64(0), T.int64(0)])
                        T.writes(T_relu[v_n, v_co, v_h, v_w])
                        T_relu[v_n, v_co, v_h, v_w] = T.max(inverse_local[v_co, v_n * T.int64(196) + v_h // T.int64(2) * T.int64(14) + v_w // T.int64(2), v_h % T.int64(2), v_w % T.int64(2)] + p2[v_n, v_co, v_h, v_w] + p3[v_n, v_co, T.int64(0), T.int64(0)], T.float32(0))
b0 = sch.get_block(name="data_pack", func_name="main")
b1 = sch.get_block(name="bgemm", func_name="main")
b2 = sch.get_block(name="inverse", func_name="main")
b3 = sch.get_block(name="conv2d_winograd", func_name="main")
b4 = sch.get_block(name="T_add", func_name="main")
b5 = sch.get_block(name="T_add_1", func_name="main")
b6 = sch.get_block(name="T_relu", func_name="main")
b7 = sch.get_block(name="root", func_name="main")
b8, b9 = sch.get_producers(block=b2)
sch.compute_inline(block=b9)
b10, = sch.get_consumers(block=b2)
l11, l12, l13, l14 = sch.get_loops(block=b10)
l15, l16 = sch.split(loop=l13, factors=[None, 2], preserve_unit_iters=True)
l17, l18 = sch.split(loop=l14, factors=[None, 2], preserve_unit_iters=True)
sch.reorder(l15, l17, l16, l18)
sch.compute_at(block=b2, loop=l17, preserve_unit_loops=True, index=-1)
sch.set_scope(block=b2, buffer_index=0, storage_scope="local")
l19, l20, l21, l22, l23, l24, l25, l26, l27, l28 = sch.get_loops(block=b2)
sch.unroll(loop=l25)
sch.unroll(loop=l26)
sch.unroll(loop=l27)
sch.unroll(loop=l28)
b29, b30 = sch.get_producers(block=b0)
sch.compute_inline(block=b30)
b31, = sch.get_producers(block=b29)
l32, l33, l34, l35, l36, l37 = sch.get_loops(block=b0)
sch.reorder(l34, l35, l32, l33, l36, l37)
sch.unroll(loop=l32)
sch.unroll(loop=l33)
sch.unroll(loop=l36)
sch.unroll(loop=l37)
l38 = sch.fuse(l34, l35, preserve_unit_iters=True)
v39 = sch.sample_categorical(candidates=[32, 64, 128, 256, 512, 1024], probs=[0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666], decision=3)
l40, l41 = sch.split(loop=l38, factors=[None, v39], preserve_unit_iters=True)
sch.bind(loop=l40, thread_axis="blockIdx.x")
sch.bind(loop=l41, thread_axis="threadIdx.x")
b42 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b42, loop=l41, preserve_unit_loops=True, index=-1)
sch.compute_at(block=b29, loop=l41, preserve_unit_loops=True, index=-1)
sch.set_scope(block=b29, buffer_index=0, storage_scope="local")
sch.compute_inline(block=b31)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l43, l44, l45, l46, l47 = sch.get_loops(block=b1)
v48, v49, v50, v51, v52 = sch.sample_perfect_tile(loop=l43, n=5, max_innermost_factor=64, decision=[4, 1, 1, 1, 1])
l53, l54, l55, l56, l57 = sch.split(loop=l43, factors=[v48, v49, v50, v51, v52], preserve_unit_iters=True)
v58, v59, v60, v61, v62 = sch.sample_perfect_tile(loop=l44, n=5, max_innermost_factor=64, decision=[4, 1, 1, 1, 1])
l63, l64, l65, l66, l67 = sch.split(loop=l44, factors=[v58, v59, v60, v61, v62], preserve_unit_iters=True)
v68, v69, v70, v71, v72 = sch.sample_perfect_tile(loop=l45, n=5, max_innermost_factor=64, decision=[2, 1, 16, 4, 1])
l73, l74, l75, l76, l77 = sch.split(loop=l45, factors=[v68, v69, v70, v71, v72], preserve_unit_iters=True)
v78, v79, v80, v81, v82 = sch.sample_perfect_tile(loop=l46, n=5, max_innermost_factor=64, decision=[7, 1, 14, 1, 2])
l83, l84, l85, l86, l87 = sch.split(loop=l46, factors=[v78, v79, v80, v81, v82], preserve_unit_iters=True)
v88, v89, v90 = sch.sample_perfect_tile(loop=l47, n=3, max_innermost_factor=64, decision=[32, 4, 1])
l91, l92, l93 = sch.split(loop=l47, factors=[v88, v89, v90], preserve_unit_iters=True)
sch.reorder(l53, l63, l73, l83, l54, l64, l74, l84, l55, l65, l75, l85, l91, l92, l56, l66, l76, l86, l93, l57, l67, l77, l87)
l94 = sch.fuse(l53, l63, l73, l83, preserve_unit_iters=True)
sch.bind(loop=l94, thread_axis="blockIdx.x")
l95 = sch.fuse(l54, l64, l74, l84, preserve_unit_iters=True)
sch.bind(loop=l95, thread_axis="vthread.x")
l96 = sch.fuse(l55, l65, l75, l85, preserve_unit_iters=True)
sch.bind(loop=l96, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b97 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b97, loop=l96, preserve_unit_loops=True, index=-1)
b98 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b98, loop=l91, preserve_unit_loops=True, index=-1)
l99, l100, l101, l102, l103, l104, l105, l106 = sch.get_loops(block=b98)
l107 = sch.fuse(l103, l104, l105, l106, preserve_unit_iters=True)
v108 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b98, ann_key="meta_schedule.cooperative_fetch", ann_val=v108)
b109 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b109, loop=l91, preserve_unit_loops=True, index=-1)
l110, l111, l112, l113, l114, l115, l116, l117 = sch.get_loops(block=b109)
l118 = sch.fuse(l114, l115, l116, l117, preserve_unit_iters=True)
v119 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b109, ann_key="meta_schedule.cooperative_fetch", ann_val=v119)
sch.reverse_compute_inline(block=b6)
sch.reverse_compute_inline(block=b5)
sch.reverse_compute_inline(block=b4)
v120 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b7, ann_key="meta_schedule.unroll_explicit", ann_val=v120)
l121, l122, l123, l124, l125, l126 = sch.get_loops(block=b3)
l127 = sch.fuse(l121, l122, l123, l124, preserve_unit_iters=True)
v128 = sch.sample_categorical(candidates=[32, 64, 128, 256, 512, 1024], probs=[0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666], decision=5)
l129, l130 = sch.split(loop=l127, factors=[None, v128], preserve_unit_iters=True)
sch.bind(loop=l129, thread_axis="blockIdx.x")
sch.bind(loop=l130, thread_axis="threadIdx.x")
sch.enter_postproc()
sch.unannotate(block_or_loop=b98, ann_key="meta_schedule.cooperative_fetch")
l131, l132, l133, l134, l135 = sch.get_loops(block=b98)
l136, l137, l138 = sch.split(loop=l135, factors=[None, 224, 4], preserve_unit_iters=True)
sch.vectorize(loop=l138)
sch.bind(loop=l137, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b109, ann_key="meta_schedule.cooperative_fetch")
l139, l140, l141, l142, l143 = sch.get_loops(block=b109)
l144, l145, l146 = sch.split(loop=l143, factors=[None, 224, 2], preserve_unit_iters=True)
sch.vectorize(loop=l146)
sch.bind(loop=l145, thread_axis="threadIdx.x")
b147 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b147, ann_key="meta_schedule.unroll_explicit")
b148, b149, b150, b151, b152, b153, b154, b155, b156 = sch.get_child_blocks(b147)
l157, l158, l159, l160, l161, l162 = sch.get_loops(block=b148)
l163, l164, l165, l166, l167, l168 = sch.get_loops(block=b149)
sch.annotate(block_or_loop=l163, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l163, ann_key="pragma_unroll_explicit", ann_val=1)
l169, l170, l171, l172, l173, l174 = sch.get_loops(block=b150)
l175, l176, l177, l178, l179, l180, l181 = sch.get_loops(block=b151)
l182, l183, l184, l185, l186, l187, l188 = sch.get_loops(block=b152)
l189, l190, l191, l192, l193, l194, l195, l196, l197, l198, l199, l200, l201, l202 = sch.get_loops(block=b153)
sch.annotate(block_or_loop=l189, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l189, ann_key="pragma_unroll_explicit", ann_val=1)
l203, l204, l205, l206, l207, l208, l209 = sch.get_loops(block=b154)
l210, l211, l212, l213, l214, l215, l216, l217 = sch.get_loops(block=b155)
sch.annotate(block_or_loop=l210, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l210, ann_key="pragma_unroll_explicit", ann_val=1)
l218, l219, l220, l221 = sch.get_loops(block=b156)
b222 = sch.get_block(name="data_pack", func_name="main")
l223, l224, l225, l226, l227, l228 = sch.get_loops(block=b222)
b229 = sch.decompose_reduction(block=b222, loop=l227)
b230 = sch.get_block(name="bgemm", func_name="main")
l231, l232, l233, l234, l235, l236, l237, l238, l239, l240, l241, l242, l243, l244 = sch.get_loops(block=b230)
b245 = sch.decompose_reduction(block=b230, loop=l234)
b246 = sch.get_block(name="inverse", func_name="main")
l247, l248, l249, l250, l251, l252, l253, l254 = sch.get_loops(block=b246)
b255 = sch.decompose_reduction(block=b246, loop=l253)
2023-11-11 07:46:00 [INFO] [task_scheduler.cc:121] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #579: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/home2/xiachunwei/Software/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/home2/xiachunwei/Software/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/home2/xiachunwei/Software/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/home2/xiachunwei/Software/tvm/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/home2/xiachunwei/Software/tvm/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/home2/xiachunwei/Software/tvm/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/home2/xiachunwei/Software/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/home2/xiachunwei/Software/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  4: _ZN3tvm7runtime13PackedFun
  3: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  2: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  0: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  File "/home2/xiachunwei/Software/tvm/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(128), T.int64(28), T.int64(28)), "float32"), p1: T.Buffer((T.int64(4), T.int64(4), T.int64(128), T.int64(128)), "float32"), p2: T.Buffer((T.int64(1), T.int64(128), T.int64(28), T.int64(28)), "float32"), p3: T.Buffer((T.int64(1), T.int64(128), T.int64(1), T.int64(1)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(128), T.int64(28), T.int64(28)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        input_tile_local = T.alloc_buffer((T.int64(128), T.int64(196), T.int64(4), T.int64(4)), scope="local")
        data_pack = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(128), T.int64(196)))
        bgemm = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(128), T.int64(196)))
        inverse_local = T.alloc_buffer((T.int64(128), T.int64(196), T.int64(2), T.int64(2)), scope="local")
        data_pack_local = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(128), T.int64(196)), scope="local")
        bgemm_local = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(128), T.int64(196)), scope="local")
        data_pack_shared = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(128), T.int64(196)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(128), T.int64(128)), scope="shared")
        for ci_p_fused_0 in T.thread_binding(T.int64(98), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 1024, "pragma_unroll_explicit": 1}):
            for ci_p_fused_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(1), T.int64(4), T.int64(4)):
                    with T.block("input_tile"):
                        v_ci = T.axis.spatial(T.int64(128), (ci_p_fused_0 * T.int64(256) + ci_p_fused_1) // T.int64(196) + ax0)
                        v_p = T.axis.spatial(T.int64(196), (ci_p_fused_0 * T.int64(60) + ci_p_fused_1) % T.int64(196) + ax1)
                        v_eps, v_nu = T.axis.remap("SS", [ax2, ax3])
                        T.reads(p0[v_p // T.int64(196), v_ci, v_p % T.int64(196) // T.int64(14) * T.int64(2) + v_eps - T.int64(1), v_p % T.int64(14) * T.int64(2) + v_nu - T.int64(1)])
                        T.writes(input_tile_local[v_ci, v_p, v_eps, v_nu])
                        T.block_attr({"schedule_rule": "None"})
                        input_tile_local[v_ci, v_p, v_eps, v_nu] = T.if_then_else(T.int64(1) <= v_p % T.int64(196) // T.int64(14) * T.int64(2) + v_eps and v_p % T.int64(196) // T.int64(14) * T.int64(2) + v_eps < T.int64(29) and T.int64(1) <= v_p % T.int64(14) * T.int64(2) + v_nu and v_p % T.int64(14) * T.int64(2) + v_nu < T.int64(29), p0[v_p // T.int64(196), v_ci, v_p % T.int64(196) // T.int64(14) * T.int64(2) + v_eps - T.int64(1), v_p % T.int64(14) * T.int64(2) + v_nu - T.int64(1)], T.float32(0))
                for eps in T.unroll(T.int64(4)):
                    for nu in T.unroll(T.int64(4)):
                        with T.block("data_pack_init"):
                            v_eps, v_nu = T.axis.remap("SS", [eps, nu])
                            v_ci = T.axis.spatial(T.int64(128), (ci_p_fused_0 * T.int64(256) + ci_p_fused_1) // T.int64(196))
                            v_p = T.axis.spatial(T.int64(196), (ci_p_fused_0 * T.int64(256) + ci_p_fused_1) % T.int64(196))
                            T.reads()
                            T.writes(data_pack_local[v_eps, v_nu, v_ci, v_p])
                            T.block_attr({"schedule_rule": "conv2d_nchw_winograd_data_pack"})
                            data_pack_local[v_eps, v_nu, v_ci, v_p] = T.float32(0)
                        for r_a in T.unroll(T.int64(4)):
                            for r_b in T.unroll(T.int64(4)):
                                with T.block("data_pack_update"):
                                    v_eps, v_nu = T.axis.remap("SS", [eps, nu])
                                    v_ci = T.axis.spatial(T.int64(128), (ci_p_fused_0 * T.int64(256) + ci_p_fused_1) // T.int64(196))
                                    v_p = T.axis.spatial(T.int64(196), (ci_p_fused_0 * T.int64(256) + ci_p_fused_1) % T.int64(196))
                                    v_r_a, v_r_b = T.axis.remap("RR", [r_a, r_b])
                                    T.reads(data_pack_local[v_eps, v_nu, v_ci, v_p], input_tile_local[v_ci, v_p, v_r_a, v_r_b])
                                    T.writes(data_pack_local[v_eps, v_nu, v_ci, v_p])
                                    T.block_attr({"schedule_rule": "conv2d_nchw_winograd_data_pack"})
                                    data_pack_local[v_eps, v_nu, v_ci, v_p] = data_pack_local[v_eps, v_nu, v_ci, v_p] + input_tile_local[v_ci, v_p, v_r_a, v_r_b] * T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(3), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(1), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(0), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(3), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(1), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(0), T.float32(1), T.float32(0))))))))))))))))) * T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(3), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(1), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(0), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(3), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(1), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(0), T.float32(1), T.float32(0)))))))))))))))))
                for ax0, ax1, ax2, ax3 in T.grid(T.int64(4), T.int64(4), T.int64(1), T.int64(1)):
                    with T.block("data_pack_local"):
                        v0, v1 = T.axis.remap("SS", [ax0, ax1])
                        v2 = T.axis.spatial(T.int64(128), (ci_p_fused_0 * T.int64(256) + ci_p_fused_1) // T.int64(196) + ax2)
                        v3 = T.axis.spatial(T.int64(196), (ci_p_fused_0 * T.int64(60) + ci_p_fused_1) % T.int64(196) + ax3)
                        T.reads(data_pack_local[v0, v1, v2, v3])
                        T.writes(data_pack[v0, v1, v2, v3])
                        data_pack[v0, v1, v2, v3] = data_pack_local[v0, v1, v2, v3]
        for eps_0_nu_0_co_0_p_0_fused in T.thread_binding(T.int64(224), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 1024, "pragma_unroll_explicit": 1}):
            for eps_1_nu_1_co_1_p_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for eps_2_nu_2_co_2_p_2_fused in T.thread_binding(T.int64(448), thread="threadIdx.x"):
                    for eps_3_init, nu_3_init, co_3_init, p_3_init, eps_4_init, nu_4_init, co_4_init, p_4_init in T.grid(T.int64(1), T.int64(1), T.int64(4), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("bgemm_init"):
                            v_eps = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(56) + eps_3_init + eps_4_init)
                            v_nu = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused % T.int64(56) // T.int64(14) + nu_3_init + nu_4_init)
                            v_co = T.axis.spatial(T.int64(128), eps_0_nu_0_co_0_p_0_fused % T.int64(14) // T.int64(7) * T.int64(64) + eps_2_nu_2_co_2_p_2_fused // T.int64(28) * T.int64(4) + co_3_init + co_4_init)
                            v_p = T.axis.spatial(T.int64(196), eps_0_nu_0_co_0_p_0_fused % T.int64(7) * T.int64(28) + eps_2_nu_2_co_2_p_2_fused % T.int64(28) + p_3_init + p_4_init)
                            T.reads()
                            T.writes(bgemm_local[v_eps, v_nu, v_co, v_p])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            bgemm_local[v_eps, v_nu, v_co, v_p] = T.float32(0)
                    for ci_0 in range(T.int64(1)):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(4)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(448), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("data_pack_shared"):
                                        v0 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(56))
                                        v1 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused % T.int64(56) // T.int64(14))
                                        v2 = T.axis.spatial(T.int64(128), (ax0_ax1_ax2_ax3_fused_0 * T.int64(896) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) // T.int64(28))
                                        v3 = T.axis.spatial(T.int64(196), eps_0_nu_0_co_0_p_0_fused % T.int64(7) * T.int64(28) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(896) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) % T.int64(28))
                                        T.reads(data_pack[v0, v1, v2, v3])
                                        T.writes(data_pack_shared[v0, v1, v2, v3])
                                        data_pack_shared[v0, v1, v2, v3] = data_pack[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(5)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(448), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("p1_shared"):
                                        v0 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(56))
                                        v1 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused % T.int64(56) // T.int64(14))
                                        v2 = T.axis.spatial(T.int64(128), (ax0_ax1_ax2_ax3_fused_0 * T.int64(1792) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) // T.int64(64))
                                        v3 = T.axis.spatial(T.int64(128), eps_0_nu_0_co_0_p_0_fused % T.int64(14) // T.int64(7) * T.int64(64) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(1792) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) % T.int64(64))
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * T.int64(448) + ax0_ax1_ax2_ax3_fused_1) * T.int64(4) + ax0_ax1_ax2_ax3_fused_2 < T.int64(8192))
                                        T.reads(p1[v0, v1, v2, v3])
                                        T.writes(p1_shared[v0, v1, v2, v3])
                                        p1_shared[v0, v1, v2, v3] = p1[v0, v1, v2, v3]
                        for ci_1, eps_3, nu_3, co_3, p_3, ci_2, eps_4, nu_4, co_4, p_4 in T.grid(T.int64(32), T.int64(1), T.int64(1), T.int64(4), T.int64(1), T.int64(4), T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                            with T.block("bgemm_update"):
                                v_eps = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(56) + eps_3 + eps_4)
                                v_nu = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused % T.int64(56) // T.int64(14) + nu_3 + nu_4)
                                v_co = T.axis.spatial(T.int64(128), eps_0_nu_0_co_0_p_0_fused % T.int64(14) // T.int64(7) * T.int64(64) + eps_2_nu_2_co_2_p_2_fused // T.int64(28) * T.int64(4) + co_3 + co_4)
                                v_p = T.axis.spatial(T.int64(196), eps_0_nu_0_co_0_p_0_fused % T.int64(7) * T.int64(28) + eps_2_nu_2_co_2_p_2_fused % T.int64(28) + p_3 + p_4)
                                v_ci = T.axis.reduce(T.int64(128), ci_0 * T.int64(128) + ci_1 * T.int64(4) + ci_2)
                                T.reads(bgemm_local[v_eps, v_nu, v_co, v_p], data_pack_shared[v_eps, v_nu, v_ci, v_p], p1_shared[v_eps, v_nu, v_ci, v_co])
                                T.writes(bgemm_local[v_eps, v_nu, v_co, v_p])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                bgemm_local[v_eps, v_nu, v_co, v_p] = bgemm_local[v_eps, v_nu, v_co, v_p] + data_pack_shared[v_eps, v_nu, v_ci, v_p] * p1_shared[v_eps, v_nu, v_ci, v_co]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(1), T.int64(4), T.int64(1)):
                        with T.block("bgemm_local"):
                            v0 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(56) + ax0)
                            v1 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused % T.int64(56) // T.int64(14) + ax1)
                            v2 = T.axis.spatial(T.int64(128), eps_0_nu_0_co_0_p_0_fused % T.int64(14) // T.int64(7) * T.int64(64) + eps_2_nu_2_co_2_p_2_fused // T.int64(28) * T.int64(4) + ax2)
                            v3 = T.axis.spatial(T.int64(196), eps_0_nu_0_co_0_p_0_fused % T.int64(7) * T.int64(28) + eps_2_nu_2_co_2_p_2_fused % T.int64(28) + ax3)
                            T.reads(bgemm_local[v0, v1, v2, v3])
                            T.writes(bgemm[v0, v1, v2, v3])
                            bgemm[v0, v1, v2, v3] = bgemm_local[v0, v1, v2, v3]
        for n_co_h_0_w_0_fused_0 in T.thread_binding(T.int64(196), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 1024, "pragma_unroll_explicit": 1}):
            for n_co_h_0_w_0_fused_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                    for ax2 in T.unroll(T.int64(2)):
                        for ax3 in T.unroll(T.int64(2)):
                            with T.block("inverse_init"):
                                v_co = T.axis.spatial(T.int64(128), (n_co_h_0_w_0_fused_0 * T.int64(128) + n_co_h_0_w_0_fused_1) // T.int64(196) + ax0)
                                v_p = T.axis.spatial(T.int64(196), (n_co_h_0_w_0_fused_0 * T.int64(128) + n_co_h_0_w_0_fused_1) % T.int64(196) + ax1)
                                v_vh, v_vw = T.axis.remap("SS", [ax2, ax3])
                                T.reads()
                                T.writes(inverse_local[v_co, v_p, v_vh, v_vw])
                                T.block_attr({"schedule_rule": "conv2d_nchw_winograd_inverse"})
                                inverse_local[v_co, v_p, v_vh, v_vw] = T.float32(0)
                            for ax4 in T.unroll(T.int64(4)):
                                for ax5 in T.unroll(T.int64(4)):
                                    with T.block("inverse_update"):
                                        v_co = T.axis.spatial(T.int64(128), (n_co_h_0_w_0_fused_0 * T.int64(128) + n_co_h_0_w_0_fused_1) // T.int64(196) + ax0)
                                        v_p = T.axis.spatial(T.int64(196), (n_co_h_0_w_0_fused_0 * T.int64(128) + n_co_h_0_w_0_fused_1) % T.int64(196) + ax1)
                                        v_vh, v_vw, v_r_a, v_r_b = T.axis.remap("SSRR", [ax2, ax3, ax4, ax5])
                                        T.reads(inverse_local[v_co, v_p, v_vh, v_vw], bgemm[v_r_a, v_r_b, v_co, v_p])
                                        T.writes(inverse_local[v_co, v_p, v_vh, v_vw])
                                        T.block_attr({"schedule_rule": "conv2d_nchw_winograd_inverse"})
                                        inverse_local[v_co, v_p, v_vh, v_vw] = inverse_local[v_co, v_p, v_vh, v_vw] + bgemm[v_r_a, v_r_b, v_co, v_p] * T.Select(v_r_a % T.int64(4) == T.int64(3) and v_vh % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_vh % T.int64(2) == T.int64(0), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_vh % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_vh % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_vh % T.int64(2) == T.int64(1), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_vh % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_vh % T.int64(2) == T.int64(1), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_vh % T.int64(2) == T.int64(0), T.float32(1), T.float32(0))))))))) * T.Select(v_r_b % T.int64(4) == T.int64(3) and v_vw % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_vw % T.int64(2) == T.int64(0), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_vw % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_vw % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_vw % T.int64(2) == T.int64(1), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_vw % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_vw % T.int64(2) == T.int64(1), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_vw % T.int64(2) == T.int64(0), T.float32(1), T.float32(0)))))))))
                for h_1, w_1 in T.grid(T.int64(2), T.int64(2)):
                    with T.block("conv2d_winograd"):
                        v_n = T.axis.spatial(T.int64(1), T.int64(0))
                        v_co = T.axis.spatial(T.int64(128), (n_co_h_0_w_0_fused_0 * T.int64(128) + n_co_h_0_w_0_fused_1) // T.int64(196))
                        v_h = T.axis.spatial(T.int64(28), (n_co_h_0_w_0_fused_0 * T.int64(128) + n_co_h_0_w_0_fused_1) % T.int64(196) // T.int64(14) * T.int64(2) + h_1)
                        v_w = T.axis.spatial(T.int64(28), (n_co_h_0_w_0_fused_0 * T.int64(128) + n_co_h_0_w_0_fused_1) % T.int64(14) * T.int64(2) + w_1)
                        T.reads(inverse_local[v_co, v_n * T.int64(196) + v_h // T.int64(2) * T.int64(14) + v_w // T.int64(2), v_h % T.int64(2), v_w % T.int64(2)], p2[v_n, v_co, v_h, v_w], p3[v_n, v_co, T.int64(0), T.int64(0)])
                        T.writes(T_relu[v_n, v_co, v_h, v_w])
                        T_relu[v_n, v_co, v_h, v_w] = T.max(inverse_local[v_co, v_n * T.int64(196) + v_h // T.int64(2) * T.int64(14) + v_w // T.int64(2), v_h % T.int64(2), v_w % T.int64(2)] + p2[v_n, v_co, v_h, v_w] + p3[v_n, v_co, T.int64(0), T.int64(0)], T.float32(0))
b0 = sch.get_block(name="data_pack", func_name="main")
b1 = sch.get_block(name="bgemm", func_name="main")
b2 = sch.get_block(name="inverse", func_name="main")
b3 = sch.get_block(name="conv2d_winograd", func_name="main")
b4 = sch.get_block(name="T_add", func_name="main")
b5 = sch.get_block(name="T_add_1", func_name="main")
b6 = sch.get_block(name="T_relu", func_name="main")
b7 = sch.get_block(name="root", func_name="main")
b8, b9 = sch.get_producers(block=b2)
sch.compute_inline(block=b9)
b10, = sch.get_consumers(block=b2)
l11, l12, l13, l14 = sch.get_loops(block=b10)
l15, l16 = sch.split(loop=l13, factors=[None, 2], preserve_unit_iters=True)
l17, l18 = sch.split(loop=l14, factors=[None, 2], preserve_unit_iters=True)
sch.reorder(l15, l17, l16, l18)
sch.compute_at(block=b2, loop=l17, preserve_unit_loops=True, index=-1)
sch.set_scope(block=b2, buffer_index=0, storage_scope="local")
l19, l20, l21, l22, l23, l24, l25, l26, l27, l28 = sch.get_loops(block=b2)
sch.unroll(loop=l25)
sch.unroll(loop=l26)
sch.unroll(loop=l27)
sch.unroll(loop=l28)
b29, b30 = sch.get_producers(block=b0)
sch.compute_inline(block=b30)
b31, = sch.get_producers(block=b29)
l32, l33, l34, l35, l36, l37 = sch.get_loops(block=b0)
sch.reorder(l34, l35, l32, l33, l36, l37)
sch.unroll(loop=l32)
sch.unroll(loop=l33)
sch.unroll(loop=l36)
sch.unroll(loop=l37)
l38 = sch.fuse(l34, l35, preserve_unit_iters=True)
v39 = sch.sample_categorical(candidates=[32, 64, 128, 256, 512, 1024], probs=[0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666], decision=3)
l40, l41 = sch.split(loop=l38, factors=[None, v39], preserve_unit_iters=True)
sch.bind(loop=l40, thread_axis="blockIdx.x")
sch.bind(loop=l41, thread_axis="threadIdx.x")
b42 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b42, loop=l41, preserve_unit_loops=True, index=-1)
sch.compute_at(block=b29, loop=l41, preserve_unit_loops=True, index=-1)
sch.set_scope(block=b29, buffer_index=0, storage_scope="local")
sch.compute_inline(block=b31)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l43, l44, l45, l46, l47 = sch.get_loops(block=b1)
v48, v49, v50, v51, v52 = sch.sample_perfect_tile(loop=l43, n=5, max_innermost_factor=64, decision=[4, 1, 1, 1, 1])
l53, l54, l55, l56, l57 = sch.split(loop=l43, factors=[v48, v49, v50, v51, v52], preserve_unit_iters=True)
v58, v59, v60, v61, v62 = sch.sample_perfect_tile(loop=l44, n=5, max_innermost_factor=64, decision=[4, 1, 1, 1, 1])
l63, l64, l65, l66, l67 = sch.split(loop=l44, factors=[v58, v59, v60, v61, v62], preserve_unit_iters=True)
v68, v69, v70, v71, v72 = sch.sample_perfect_tile(loop=l45, n=5, max_innermost_factor=64, decision=[2, 1, 16, 4, 1])
l73, l74, l75, l76, l77 = sch.split(loop=l45, factors=[v68, v69, v70, v71, v72], preserve_unit_iters=True)
v78, v79, v80, v81, v82 = sch.sample_perfect_tile(loop=l46, n=5, max_innermost_factor=64, decision=[7, 1, 28, 1, 1])
l83, l84, l85, l86, l87 = sch.split(loop=l46, factors=[v78, v79, v80, v81, v82], preserve_unit_iters=True)
v88, v89, v90 = sch.sample_perfect_tile(loop=l47, n=3, max_innermost_factor=64, decision=[1, 32, 4])
l91, l92, l93 = sch.split(loop=l47, factors=[v88, v89, v90], preserve_unit_iters=True)
sch.reorder(l53, l63, l73, l83, l54, l64, l74, l84, l55, l65, l75, l85, l91, l92, l56, l66, l76, l86, l93, l57, l67, l77, l87)
l94 = sch.fuse(l53, l63, l73, l83, preserve_unit_iters=True)
sch.bind(loop=l94, thread_axis="blockIdx.x")
l95 = sch.fuse(l54, l64, l74, l84, preserve_unit_iters=True)
sch.bind(loop=l95, thread_axis="vthread.x")
l96 = sch.fuse(l55, l65, l75, l85, preserve_unit_iters=True)
sch.bind(loop=l96, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b97 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b97, loop=l96, preserve_unit_loops=True, index=-1)
b98 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b98, loop=l91, preserve_unit_loops=True, index=-1)
l99, l100, l101, l102, l103, l104, l105, l106 = sch.get_loops(block=b98)
l107 = sch.fuse(l103, l104, l105, l106, preserve_unit_iters=True)
v108 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b98, ann_key="meta_schedule.cooperative_fetch", ann_val=v108)
b109 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b109, loop=l91, preserve_unit_loops=True, index=-1)
l110, l111, l112, l113, l114, l115, l116, l117 = sch.get_loops(block=b109)
l118 = sch.fuse(l114, l115, l116, l117, preserve_unit_iters=True)
v119 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b109, ann_key="meta_schedule.cooperative_fetch", ann_val=v119)
sch.reverse_compute_inline(block=b6)
sch.reverse_compute_inline(block=b5)
sch.reverse_compute_inline(block=b4)
v120 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b7, ann_key="meta_schedule.unroll_explicit", ann_val=v120)
l121, l122, l123, l124, l125, l126 = sch.get_loops(block=b3)
l127 = sch.fuse(l121, l122, l123, l124, preserve_unit_iters=True)
v128 = sch.sample_categorical(candidates=[32, 64, 128, 256, 512, 1024], probs=[0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666], decision=2)
l129, l130 = sch.split(loop=l127, factors=[None, v128], preserve_unit_iters=True)
sch.bind(loop=l129, thread_axis="blockIdx.x")
sch.bind(loop=l130, thread_axis="threadIdx.x")
sch.enter_postproc()
sch.unannotate(block_or_loop=b98, ann_key="meta_schedule.cooperative_fetch")
l131, l132, l133, l134, l135 = sch.get_loops(block=b98)
l136, l137, l138 = sch.split(loop=l135, factors=[None, 448, 2], preserve_unit_iters=True)
sch.vectorize(loop=l138)
sch.bind(loop=l137, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b109, ann_key="meta_schedule.cooperative_fetch")
l139, l140, l141, l142, l143 = sch.get_loops(block=b109)
l144, l145, l146 = sch.split(loop=l143, factors=[None, 448, 4], preserve_unit_iters=True)
sch.vectorize(loop=l146)
sch.bind(loop=l145, thread_axis="threadIdx.x")
b147 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b147, ann_key="meta_schedule.unroll_explicit")
b148, b149, b150, b151, b152, b153, b154, b155, b156 = sch.get_child_blocks(b147)
l157, l158, l159, l160, l161, l162 = sch.get_loops(block=b148)
l163, l164, l165, l166, l167, l168 = sch.get_loops(block=b149)
sch.annotate(block_or_loop=l163, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l163, ann_key="pragma_unroll_explicit", ann_val=1)
l169, l170, l171, l172, l173, l174 = sch.get_loops(block=b150)
l175, l176, l177, l178, l179, l180, l181 = sch.get_loops(block=b151)
l182, l183, l184, l185, l186, l187, l188 = sch.get_loops(block=b152)
l189, l190, l191, l192, l193, l194, l195, l196, l197, l198, l199, l200, l201, l202 = sch.get_loops(block=b153)
sch.annotate(block_or_loop=l189, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l189, ann_key="pragma_unroll_explicit", ann_val=1)
l203, l204, l205, l206, l207, l208, l209 = sch.get_loops(block=b154)
l210, l211, l212, l213, l214, l215, l216, l217 = sch.get_loops(block=b155)
sch.annotate(block_or_loop=l210, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l210, ann_key="pragma_unroll_explicit", ann_val=1)
l218, l219, l220, l221 = sch.get_loops(block=b156)
b222 = sch.get_block(name="data_pack", func_name="main")
l223, l224, l225, l226, l227, l228 = sch.get_loops(block=b222)
b229 = sch.decompose_reduction(block=b222, loop=l227)
b230 = sch.get_block(name="bgemm", func_name="main")
l231, l232, l233, l234, l235, l236, l237, l238, l239, l240, l241, l242, l243, l244 = sch.get_loops(block=b230)
b245 = sch.decompose_reduction(block=b230, loop=l234)
b246 = sch.get_block(name="inverse", func_name="main")
l247, l248, l249, l250, l251, l252, l253, l254 = sch.get_loops(block=b246)
b255 = sch.decompose_reduction(block=b246, loop=l253)
2023-11-11 07:46:00 [INFO] [task_scheduler.cc:121] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #580: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/home2/xiachunwei/Software/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/home2/xiachunwei/Software/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/home2/xiachunwei/Software/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/home2/xiachunwei/Software/tvm/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/home2/xiachunwei/Software/tvm/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/home2/xiachunwei/Software/tvm/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/home2/xiachunwei/Software/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/home2/xiachunwei/Software/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  4: _ZN3tvm7runtime13PackedFun
  3: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  2: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  0: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  File "/home2/xiachunwei/Software/tvm/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(128), T.int64(28), T.int64(28)), "float32"), p1: T.Buffer((T.int64(4), T.int64(4), T.int64(128), T.int64(128)), "float32"), p2: T.Buffer((T.int64(1), T.int64(128), T.int64(28), T.int64(28)), "float32"), p3: T.Buffer((T.int64(1), T.int64(128), T.int64(1), T.int64(1)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(128), T.int64(28), T.int64(28)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        input_tile_local = T.alloc_buffer((T.int64(128), T.int64(196), T.int64(4), T.int64(4)), scope="local")
        data_pack = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(128), T.int64(196)))
        bgemm = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(128), T.int64(196)))
        inverse_local = T.alloc_buffer((T.int64(128), T.int64(196), T.int64(2), T.int64(2)), scope="local")
        data_pack_local = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(128), T.int64(196)), scope="local")
        bgemm_local = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(128), T.int64(196)), scope="local")
        data_pack_shared = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(128), T.int64(196)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(128), T.int64(128)), scope="shared")
        for ci_p_fused_0 in T.thread_binding(T.int64(98), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 1024, "pragma_unroll_explicit": 1}):
            for ci_p_fused_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(1), T.int64(4), T.int64(4)):
                    with T.block("input_tile"):
                        v_ci = T.axis.spatial(T.int64(128), (ci_p_fused_0 * T.int64(256) + ci_p_fused_1) // T.int64(196) + ax0)
                        v_p = T.axis.spatial(T.int64(196), (ci_p_fused_0 * T.int64(60) + ci_p_fused_1) % T.int64(196) + ax1)
                        v_eps, v_nu = T.axis.remap("SS", [ax2, ax3])
                        T.reads(p0[v_p // T.int64(196), v_ci, v_p % T.int64(196) // T.int64(14) * T.int64(2) + v_eps - T.int64(1), v_p % T.int64(14) * T.int64(2) + v_nu - T.int64(1)])
                        T.writes(input_tile_local[v_ci, v_p, v_eps, v_nu])
                        T.block_attr({"schedule_rule": "None"})
                        input_tile_local[v_ci, v_p, v_eps, v_nu] = T.if_then_else(T.int64(1) <= v_p % T.int64(196) // T.int64(14) * T.int64(2) + v_eps and v_p % T.int64(196) // T.int64(14) * T.int64(2) + v_eps < T.int64(29) and T.int64(1) <= v_p % T.int64(14) * T.int64(2) + v_nu and v_p % T.int64(14) * T.int64(2) + v_nu < T.int64(29), p0[v_p // T.int64(196), v_ci, v_p % T.int64(196) // T.int64(14) * T.int64(2) + v_eps - T.int64(1), v_p % T.int64(14) * T.int64(2) + v_nu - T.int64(1)], T.float32(0))
                for eps in T.unroll(T.int64(4)):
                    for nu in T.unroll(T.int64(4)):
                        with T.block("data_pack_init"):
                            v_eps, v_nu = T.axis.remap("SS", [eps, nu])
                            v_ci = T.axis.spatial(T.int64(128), (ci_p_fused_0 * T.int64(256) + ci_p_fused_1) // T.int64(196))
                            v_p = T.axis.spatial(T.int64(196), (ci_p_fused_0 * T.int64(256) + ci_p_fused_1) % T.int64(196))
                            T.reads()
                            T.writes(data_pack_local[v_eps, v_nu, v_ci, v_p])
                            T.block_attr({"schedule_rule": "conv2d_nchw_winograd_data_pack"})
                            data_pack_local[v_eps, v_nu, v_ci, v_p] = T.float32(0)
                        for r_a in T.unroll(T.int64(4)):
                            for r_b in T.unroll(T.int64(4)):
                                with T.block("data_pack_update"):
                                    v_eps, v_nu = T.axis.remap("SS", [eps, nu])
                                    v_ci = T.axis.spatial(T.int64(128), (ci_p_fused_0 * T.int64(256) + ci_p_fused_1) // T.int64(196))
                                    v_p = T.axis.spatial(T.int64(196), (ci_p_fused_0 * T.int64(256) + ci_p_fused_1) % T.int64(196))
                                    v_r_a, v_r_b = T.axis.remap("RR", [r_a, r_b])
                                    T.reads(data_pack_local[v_eps, v_nu, v_ci, v_p], input_tile_local[v_ci, v_p, v_r_a, v_r_b])
                                    T.writes(data_pack_local[v_eps, v_nu, v_ci, v_p])
                                    T.block_attr({"schedule_rule": "conv2d_nchw_winograd_data_pack"})
                                    data_pack_local[v_eps, v_nu, v_ci, v_p] = data_pack_local[v_eps, v_nu, v_ci, v_p] + input_tile_local[v_ci, v_p, v_r_a, v_r_b] * T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(3), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(1), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(0), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(3), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(1), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(0), T.float32(1), T.float32(0))))))))))))))))) * T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(3), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(1), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(0), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(3), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(1), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(0), T.float32(1), T.float32(0)))))))))))))))))
                for ax0, ax1, ax2, ax3 in T.grid(T.int64(4), T.int64(4), T.int64(1), T.int64(1)):
                    with T.block("data_pack_local"):
                        v0, v1 = T.axis.remap("SS", [ax0, ax1])
                        v2 = T.axis.spatial(T.int64(128), (ci_p_fused_0 * T.int64(256) + ci_p_fused_1) // T.int64(196) + ax2)
                        v3 = T.axis.spatial(T.int64(196), (ci_p_fused_0 * T.int64(60) + ci_p_fused_1) % T.int64(196) + ax3)
                        T.reads(data_pack_local[v0, v1, v2, v3])
                        T.writes(data_pack[v0, v1, v2, v3])
                        data_pack[v0, v1, v2, v3] = data_pack_local[v0, v1, v2, v3]
        for eps_0_nu_0_co_0_p_0_fused in T.thread_binding(T.int64(224), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 1024, "pragma_unroll_explicit": 1}):
            for eps_1_nu_1_co_1_p_1_fused in T.thread_binding(T.int64(1), thread="vthread.x"):
                for eps_2_nu_2_co_2_p_2_fused in T.thread_binding(T.int64(224), thread="threadIdx.x"):
                    for eps_3_init, nu_3_init, co_3_init, p_3_init, eps_4_init, nu_4_init, co_4_init, p_4_init in T.grid(T.int64(1), T.int64(1), T.int64(2), T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(2)):
                        with T.block("bgemm_init"):
                            v_eps = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(56) + eps_3_init + eps_4_init)
                            v_nu = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused % T.int64(56) // T.int64(14) + nu_3_init + nu_4_init)
                            v_co = T.axis.spatial(T.int64(128), eps_0_nu_0_co_0_p_0_fused % T.int64(14) // T.int64(7) * T.int64(64) + eps_2_nu_2_co_2_p_2_fused // T.int64(14) * T.int64(4) + co_3_init * T.int64(2) + co_4_init)
                            v_p = T.axis.spatial(T.int64(196), eps_0_nu_0_co_0_p_0_fused % T.int64(7) * T.int64(28) + eps_2_nu_2_co_2_p_2_fused % T.int64(14) * T.int64(2) + p_3_init * T.int64(2) + p_4_init)
                            T.reads()
                            T.writes(bgemm_local[v_eps, v_nu, v_co, v_p])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            bgemm_local[v_eps, v_nu, v_co, v_p] = T.float32(0)
                    for ci_0 in range(T.int64(32)):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(1)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(224), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("data_pack_shared"):
                                        v0 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(56))
                                        v1 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused % T.int64(56) // T.int64(14))
                                        v2 = T.axis.spatial(T.int64(128), ci_0 * T.int64(4) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(896) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) // T.int64(28))
                                        v3 = T.axis.spatial(T.int64(196), eps_0_nu_0_co_0_p_0_fused % T.int64(7) * T.int64(28) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(896) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) % T.int64(28))
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * T.int64(224) + ax0_ax1_ax2_ax3_fused_1) * T.int64(4) + ax0_ax1_ax2_ax3_fused_2 < T.int64(112))
                                        T.reads(data_pack[v0, v1, v2, v3])
                                        T.writes(data_pack_shared[v0, v1, v2, v3])
                                        data_pack_shared[v0, v1, v2, v3] = data_pack[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(2)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(224), thread="threadIdx.x"):
                                with T.block("p1_shared"):
                                    v0 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(56))
                                    v1 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused % T.int64(56) // T.int64(14))
                                    v2 = T.axis.spatial(T.int64(128), ci_0 * T.int64(4) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(224) + ax0_ax1_ax2_ax3_fused_1) // T.int64(64))
                                    v3 = T.axis.spatial(T.int64(128), eps_0_nu_0_co_0_p_0_fused % T.int64(14) // T.int64(7) * T.int64(64) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(224) + ax0_ax1_ax2_ax3_fused_1) % T.int64(64))
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * T.int64(224) + ax0_ax1_ax2_ax3_fused_1 < T.int64(256))
                                    T.reads(p1[v0, v1, v2, v3])
                                    T.writes(p1_shared[v0, v1, v2, v3])
                                    p1_shared[v0, v1, v2, v3] = p1[v0, v1, v2, v3]
                        for ci_1, eps_3, nu_3, co_3, p_3, ci_2, eps_4, nu_4, co_4, p_4 in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(1), T.int64(4), T.int64(1), T.int64(1), T.int64(2), T.int64(2)):
                            with T.block("bgemm_update"):
                                v_eps = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(56) + eps_3 + eps_4)
                                v_nu = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused % T.int64(56) // T.int64(14) + nu_3 + nu_4)
                                v_co = T.axis.spatial(T.int64(128), eps_0_nu_0_co_0_p_0_fused % T.int64(14) // T.int64(7) * T.int64(64) + eps_2_nu_2_co_2_p_2_fused // T.int64(14) * T.int64(4) + co_3 * T.int64(2) + co_4)
                                v_p = T.axis.spatial(T.int64(196), eps_0_nu_0_co_0_p_0_fused % T.int64(7) * T.int64(28) + eps_2_nu_2_co_2_p_2_fused % T.int64(14) * T.int64(2) + p_3 * T.int64(2) + p_4)
                                v_ci = T.axis.reduce(T.int64(128), ci_0 * T.int64(4) + ci_1 * T.int64(4) + ci_2)
                                T.reads(bgemm_local[v_eps, v_nu, v_co, v_p], data_pack_shared[v_eps, v_nu, v_ci, v_p], p1_shared[v_eps, v_nu, v_ci, v_co])
                                T.writes(bgemm_local[v_eps, v_nu, v_co, v_p])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                bgemm_local[v_eps, v_nu, v_co, v_p] = bgemm_local[v_eps, v_nu, v_co, v_p] + data_pack_shared[v_eps, v_nu, v_ci, v_p] * p1_shared[v_eps, v_nu, v_ci, v_co]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(1), T.int64(4), T.int64(2)):
                        with T.block("bgemm_local"):
                            v0 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(56) + ax0)
                            v1 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused % T.int64(56) // T.int64(14) + ax1)
                            v2 = T.axis.spatial(T.int64(128), eps_0_nu_0_co_0_p_0_fused % T.int64(14) // T.int64(7) * T.int64(64) + eps_2_nu_2_co_2_p_2_fused // T.int64(14) * T.int64(4) + ax2)
                            v3 = T.axis.spatial(T.int64(196), eps_0_nu_0_co_0_p_0_fused % T.int64(7) * T.int64(28) + eps_2_nu_2_co_2_p_2_fused % T.int64(14) * T.int64(2) + ax3)
                            T.reads(bgemm_local[v0, v1, v2, v3])
                            T.writes(bgemm[v0, v1, v2, v3])
                            bgemm[v0, v1, v2, v3] = bgemm_local[v0, v1, v2, v3]
        for n_co_h_0_w_0_fused_0 in T.thread_binding(T.int64(25), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 1024, "pragma_unroll_explicit": 1}):
            for n_co_h_0_w_0_fused_1 in T.thread_binding(T.int64(1024), thread="threadIdx.x"):
                for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                    for ax2 in T.unroll(T.int64(2)):
                        for ax3 in T.unroll(T.int64(2)):
                            with T.block("inverse_init"):
                                v_co = T.axis.spatial(T.int64(128), (n_co_h_0_w_0_fused_0 * T.int64(1024) + n_co_h_0_w_0_fused_1) // T.int64(196) + ax0)
                                v_p = T.axis.spatial(T.int64(196), (n_co_h_0_w_0_fused_0 * T.int64(1024) + n_co_h_0_w_0_fused_1) % T.int64(196) + ax1)
                                v_vh, v_vw = T.axis.remap("SS", [ax2, ax3])
                                T.where(n_co_h_0_w_0_fused_0 * T.int64(1024) + n_co_h_0_w_0_fused_1 < T.int64(25088))
                                T.reads()
                                T.writes(inverse_local[v_co, v_p, v_vh, v_vw])
                                T.block_attr({"schedule_rule": "conv2d_nchw_winograd_inverse"})
                                inverse_local[v_co, v_p, v_vh, v_vw] = T.float32(0)
                            for ax4 in T.unroll(T.int64(4)):
                                for ax5 in T.unroll(T.int64(4)):
                                    with T.block("inverse_update"):
                                        v_co = T.axis.spatial(T.int64(128), (n_co_h_0_w_0_fused_0 * T.int64(1024) + n_co_h_0_w_0_fused_1) // T.int64(196) + ax0)
                                        v_p = T.axis.spatial(T.int64(196), (n_co_h_0_w_0_fused_0 * T.int64(1024) + n_co_h_0_w_0_fused_1) % T.int64(196) + ax1)
                                        v_vh, v_vw, v_r_a, v_r_b = T.axis.remap("SSRR", [ax2, ax3, ax4, ax5])
                                        T.where(n_co_h_0_w_0_fused_0 * T.int64(1024) + n_co_h_0_w_0_fused_1 < T.int64(25088))
                                        T.reads(inverse_local[v_co, v_p, v_vh, v_vw], bgemm[v_r_a, v_r_b, v_co, v_p])
                                        T.writes(inverse_local[v_co, v_p, v_vh, v_vw])
                                        T.block_attr({"schedule_rule": "conv2d_nchw_winograd_inverse"})
                                        inverse_local[v_co, v_p, v_vh, v_vw] = inverse_local[v_co, v_p, v_vh, v_vw] + bgemm[v_r_a, v_r_b, v_co, v_p] * T.Select(v_r_a % T.int64(4) == T.int64(3) and v_vh % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_vh % T.int64(2) == T.int64(0), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_vh % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_vh % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_vh % T.int64(2) == T.int64(1), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_vh % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_vh % T.int64(2) == T.int64(1), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_vh % T.int64(2) == T.int64(0), T.float32(1), T.float32(0))))))))) * T.Select(v_r_b % T.int64(4) == T.int64(3) and v_vw % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_vw % T.int64(2) == T.int64(0), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_vw % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_vw % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_vw % T.int64(2) == T.int64(1), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_vw % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_vw % T.int64(2) == T.int64(1), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_vw % T.int64(2) == T.int64(0), T.float32(1), T.float32(0)))))))))
                for h_1, w_1 in T.grid(T.int64(2), T.int64(2)):
                    with T.block("conv2d_winograd"):
                        v_n = T.axis.spatial(T.int64(1), T.int64(0))
                        v_co = T.axis.spatial(T.int64(128), (n_co_h_0_w_0_fused_0 * T.int64(1024) + n_co_h_0_w_0_fused_1) // T.int64(196))
                        v_h = T.axis.spatial(T.int64(28), (n_co_h_0_w_0_fused_0 * T.int64(1024) + n_co_h_0_w_0_fused_1) % T.int64(196) // T.int64(14) * T.int64(2) + h_1)
                        v_w = T.axis.spatial(T.int64(28), (n_co_h_0_w_0_fused_0 * T.int64(1024) + n_co_h_0_w_0_fused_1) % T.int64(14) * T.int64(2) + w_1)
                        T.where(n_co_h_0_w_0_fused_0 * T.int64(1024) + n_co_h_0_w_0_fused_1 < T.int64(25088))
                        T.reads(inverse_local[v_co, v_n * T.int64(196) + v_h // T.int64(2) * T.int64(14) + v_w // T.int64(2), v_h % T.int64(2), v_w % T.int64(2)], p2[v_n, v_co, v_h, v_w], p3[v_n, v_co, T.int64(0), T.int64(0)])
                        T.writes(T_relu[v_n, v_co, v_h, v_w])
                        T_relu[v_n, v_co, v_h, v_w] = T.max(inverse_local[v_co, v_n * T.int64(196) + v_h // T.int64(2) * T.int64(14) + v_w // T.int64(2), v_h % T.int64(2), v_w % T.int64(2)] + p2[v_n, v_co, v_h, v_w] + p3[v_n, v_co, T.int64(0), T.int64(0)], T.float32(0))
b0 = sch.get_block(name="data_pack", func_name="main")
b1 = sch.get_block(name="bgemm", func_name="main")
b2 = sch.get_block(name="inverse", func_name="main")
b3 = sch.get_block(name="conv2d_winograd", func_name="main")
b4 = sch.get_block(name="T_add", func_name="main")
b5 = sch.get_block(name="T_add_1", func_name="main")
b6 = sch.get_block(name="T_relu", func_name="main")
b7 = sch.get_block(name="root", func_name="main")
b8, b9 = sch.get_producers(block=b2)
sch.compute_inline(block=b9)
b10, = sch.get_consumers(block=b2)
l11, l12, l13, l14 = sch.get_loops(block=b10)
l15, l16 = sch.split(loop=l13, factors=[None, 2], preserve_unit_iters=True)
l17, l18 = sch.split(loop=l14, factors=[None, 2], preserve_unit_iters=True)
sch.reorder(l15, l17, l16, l18)
sch.compute_at(block=b2, loop=l17, preserve_unit_loops=True, index=-1)
sch.set_scope(block=b2, buffer_index=0, storage_scope="local")
l19, l20, l21, l22, l23, l24, l25, l26, l27, l28 = sch.get_loops(block=b2)
sch.unroll(loop=l25)
sch.unroll(loop=l26)
sch.unroll(loop=l27)
sch.unroll(loop=l28)
b29, b30 = sch.get_producers(block=b0)
sch.compute_inline(block=b30)
b31, = sch.get_producers(block=b29)
l32, l33, l34, l35, l36, l37 = sch.get_loops(block=b0)
sch.reorder(l34, l35, l32, l33, l36, l37)
sch.unroll(loop=l32)
sch.unroll(loop=l33)
sch.unroll(loop=l36)
sch.unroll(loop=l37)
l38 = sch.fuse(l34, l35, preserve_unit_iters=True)
v39 = sch.sample_categorical(candidates=[32, 64, 128, 256, 512, 1024], probs=[0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666], decision=3)
l40, l41 = sch.split(loop=l38, factors=[None, v39], preserve_unit_iters=True)
sch.bind(loop=l40, thread_axis="blockIdx.x")
sch.bind(loop=l41, thread_axis="threadIdx.x")
b42 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b42, loop=l41, preserve_unit_loops=True, index=-1)
sch.compute_at(block=b29, loop=l41, preserve_unit_loops=True, index=-1)
sch.set_scope(block=b29, buffer_index=0, storage_scope="local")
sch.compute_inline(block=b31)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l43, l44, l45, l46, l47 = sch.get_loops(block=b1)
v48, v49, v50, v51, v52 = sch.sample_perfect_tile(loop=l43, n=5, max_innermost_factor=64, decision=[4, 1, 1, 1, 1])
l53, l54, l55, l56, l57 = sch.split(loop=l43, factors=[v48, v49, v50, v51, v52], preserve_unit_iters=True)
v58, v59, v60, v61, v62 = sch.sample_perfect_tile(loop=l44, n=5, max_innermost_factor=64, decision=[4, 1, 1, 1, 1])
l63, l64, l65, l66, l67 = sch.split(loop=l44, factors=[v58, v59, v60, v61, v62], preserve_unit_iters=True)
v68, v69, v70, v71, v72 = sch.sample_perfect_tile(loop=l45, n=5, max_innermost_factor=64, decision=[2, 1, 16, 2, 2])
l73, l74, l75, l76, l77 = sch.split(loop=l45, factors=[v68, v69, v70, v71, v72], preserve_unit_iters=True)
v78, v79, v80, v81, v82 = sch.sample_perfect_tile(loop=l46, n=5, max_innermost_factor=64, decision=[7, 1, 14, 1, 2])
l83, l84, l85, l86, l87 = sch.split(loop=l46, factors=[v78, v79, v80, v81, v82], preserve_unit_iters=True)
v88, v89, v90 = sch.sample_perfect_tile(loop=l47, n=3, max_innermost_factor=64, decision=[32, 1, 4])
l91, l92, l93 = sch.split(loop=l47, factors=[v88, v89, v90], preserve_unit_iters=True)
sch.reorder(l53, l63, l73, l83, l54, l64, l74, l84, l55, l65, l75, l85, l91, l92, l56, l66, l76, l86, l93, l57, l67, l77, l87)
l94 = sch.fuse(l53, l63, l73, l83, preserve_unit_iters=True)
sch.bind(loop=l94, thread_axis="blockIdx.x")
l95 = sch.fuse(l54, l64, l74, l84, preserve_unit_iters=True)
sch.bind(loop=l95, thread_axis="vthread.x")
l96 = sch.fuse(l55, l65, l75, l85, preserve_unit_iters=True)
sch.bind(loop=l96, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b97 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b97, loop=l96, preserve_unit_loops=True, index=-1)
b98 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b98, loop=l91, preserve_unit_loops=True, index=-1)
l99, l100, l101, l102, l103, l104, l105, l106 = sch.get_loops(block=b98)
l107 = sch.fuse(l103, l104, l105, l106, preserve_unit_iters=True)
v108 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b98, ann_key="meta_schedule.cooperative_fetch", ann_val=v108)
b109 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b109, loop=l91, preserve_unit_loops=True, index=-1)
l110, l111, l112, l113, l114, l115, l116, l117 = sch.get_loops(block=b109)
l118 = sch.fuse(l114, l115, l116, l117, preserve_unit_iters=True)
v119 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b109, ann_key="meta_schedule.cooperative_fetch", ann_val=v119)
sch.reverse_compute_inline(block=b6)
sch.reverse_compute_inline(block=b5)
sch.reverse_compute_inline(block=b4)
v120 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b7, ann_key="meta_schedule.unroll_explicit", ann_val=v120)
l121, l122, l123, l124, l125, l126 = sch.get_loops(block=b3)
l127 = sch.fuse(l121, l122, l123, l124, preserve_unit_iters=True)
v128 = sch.sample_categorical(candidates=[32, 64, 128, 256, 512, 1024], probs=[0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666], decision=5)
l129, l130 = sch.split(loop=l127, factors=[None, v128], preserve_unit_iters=True)
sch.bind(loop=l129, thread_axis="blockIdx.x")
sch.bind(loop=l130, thread_axis="threadIdx.x")
sch.enter_postproc()
sch.unannotate(block_or_loop=b98, ann_key="meta_schedule.cooperative_fetch")
l131, l132, l133, l134, l135 = sch.get_loops(block=b98)
l136, l137, l138 = sch.split(loop=l135, factors=[None, 224, 4], preserve_unit_iters=True)
sch.vectorize(loop=l138)
sch.bind(loop=l137, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b109, ann_key="meta_schedule.cooperative_fetch")
l139, l140, l141, l142, l143 = sch.get_loops(block=b109)
l144, l145 = sch.split(loop=l143, factors=[None, 224], preserve_unit_iters=True)
sch.bind(loop=l145, thread_axis="threadIdx.x")
b146 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b146, ann_key="meta_schedule.unroll_explicit")
b147, b148, b149, b150, b151, b152, b153, b154, b155 = sch.get_child_blocks(b146)
l156, l157, l158, l159, l160, l161 = sch.get_loops(block=b147)
l162, l163, l164, l165, l166, l167 = sch.get_loops(block=b148)
sch.annotate(block_or_loop=l162, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l162, ann_key="pragma_unroll_explicit", ann_val=1)
l168, l169, l170, l171, l172, l173 = sch.get_loops(block=b149)
l174, l175, l176, l177, l178, l179, l180 = sch.get_loops(block=b150)
l181, l182, l183, l184, l185, l186 = sch.get_loops(block=b151)
l187, l188, l189, l190, l191, l192, l193, l194, l195, l196, l197, l198, l199, l200 = sch.get_loops(block=b152)
sch.annotate(block_or_loop=l187, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l187, ann_key="pragma_unroll_explicit", ann_val=1)
l201, l202, l203, l204, l205, l206, l207 = sch.get_loops(block=b153)
l208, l209, l210, l211, l212, l213, l214, l215 = sch.get_loops(block=b154)
sch.annotate(block_or_loop=l208, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l208, ann_key="pragma_unroll_explicit", ann_val=1)
l216, l217, l218, l219 = sch.get_loops(block=b155)
b220 = sch.get_block(name="data_pack", func_name="main")
l221, l222, l223, l224, l225, l226 = sch.get_loops(block=b220)
b227 = sch.decompose_reduction(block=b220, loop=l225)
b228 = sch.get_block(name="bgemm", func_name="main")
l229, l230, l231, l232, l233, l234, l235, l236, l237, l238, l239, l240, l241, l242 = sch.get_loops(block=b228)
b243 = sch.decompose_reduction(block=b228, loop=l232)
b244 = sch.get_block(name="inverse", func_name="main")
l245, l246, l247, l248, l249, l250, l251, l252 = sch.get_loops(block=b244)
b253 = sch.decompose_reduction(block=b244, loop=l251)
2023-11-11 07:46:00 [INFO] [task_scheduler.cc:121] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #581: Error in running:
LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/home2/xiachunwei/Software/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/home2/xiachunwei/Software/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 152, in _worker_func
    repeated_args: List[T_ARGUMENT_LIST] = f_alloc_argument(
  File "/home2/xiachunwei/Software/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 360, in default_alloc_argument
    return alloc_argument_common(f_random_fill, device, args_info, alloc_repeat)
  File "/home2/xiachunwei/Software/tvm/python/tvm/meta_schedule/runner/utils.py", line 74, in alloc_argument_common
    arg: Any = dispatcher.get(arg_type, None)(*arg_info)
  File "/home2/xiachunwei/Software/tvm/python/tvm/meta_schedule/runner/utils.py", line 56, in alloc_tensor
    arg = ndarray.empty(shape=shape, dtype=dtype, device=device)
  File "/home2/xiachunwei/Software/tvm/python/tvm/runtime/ndarray.py", line 391, in empty
    arr = _ffi_api.TVMArrayAllocWithScope(shape, dtype, device, mem_scope)
  File "/home2/xiachunwei/Software/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/home2/xiachunwei/Software/tvm/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
tvm.error.InternalError: Traceback (most recent call last):
  4: _ZN3tvm7runtime13PackedFun
  3: tvm::runtime::TypedPackedFunc<tvm::runtime::NDArray (tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>::AssignTypedLambda<tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)>(tvm::runtime::NDArray (*)(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const
  2: tvm::runtime::NDArray::Empty(tvm::runtime::ShapeTuple, DLDataType, DLDevice, tvm::runtime::Optional<tvm::runtime::String>)
  1: tvm::runtime::DeviceAPI::AllocDataSpace(DLDevice, int, long const*, DLDataType, tvm::runtime::Optional<tvm::runtime::String>)
  0: tvm::runtime::CUDADeviceAPI::AllocDataSpace(DLDevice, unsigned long, unsigned long, DLDataType)
  File "/home2/xiachunwei/Software/tvm/src/runtime/cuda/cuda_device_api.cc", line 126
InternalError: Check failed: (e == cudaSuccess || e == cudaErrorCudartUnloading) is false: CUDA: misaligned address

# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(128), T.int64(28), T.int64(28)), "float32"), p1: T.Buffer((T.int64(4), T.int64(4), T.int64(128), T.int64(128)), "float32"), p2: T.Buffer((T.int64(1), T.int64(128), T.int64(28), T.int64(28)), "float32"), p3: T.Buffer((T.int64(1), T.int64(128), T.int64(1), T.int64(1)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(128), T.int64(28), T.int64(28)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        input_tile_local = T.alloc_buffer((T.int64(128), T.int64(196), T.int64(4), T.int64(4)), scope="local")
        data_pack = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(128), T.int64(196)))
        bgemm = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(128), T.int64(196)))
        inverse_local = T.alloc_buffer((T.int64(128), T.int64(196), T.int64(2), T.int64(2)), scope="local")
        data_pack_local = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(128), T.int64(196)), scope="local")
        bgemm_local = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(128), T.int64(196)), scope="local")
        data_pack_shared = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(128), T.int64(196)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(128), T.int64(128)), scope="shared")
        for ci_p_fused_0 in T.thread_binding(T.int64(98), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 1024, "pragma_unroll_explicit": 1}):
            for ci_p_fused_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(1), T.int64(4), T.int64(4)):
                    with T.block("input_tile"):
                        v_ci = T.axis.spatial(T.int64(128), (ci_p_fused_0 * T.int64(256) + ci_p_fused_1) // T.int64(196) + ax0)
                        v_p = T.axis.spatial(T.int64(196), (ci_p_fused_0 * T.int64(60) + ci_p_fused_1) % T.int64(196) + ax1)
                        v_eps, v_nu = T.axis.remap("SS", [ax2, ax3])
                        T.reads(p0[v_p // T.int64(196), v_ci, v_p % T.int64(196) // T.int64(14) * T.int64(2) + v_eps - T.int64(1), v_p % T.int64(14) * T.int64(2) + v_nu - T.int64(1)])
                        T.writes(input_tile_local[v_ci, v_p, v_eps, v_nu])
                        T.block_attr({"schedule_rule": "None"})
                        input_tile_local[v_ci, v_p, v_eps, v_nu] = T.if_then_else(T.int64(1) <= v_p % T.int64(196) // T.int64(14) * T.int64(2) + v_eps and v_p % T.int64(196) // T.int64(14) * T.int64(2) + v_eps < T.int64(29) and T.int64(1) <= v_p % T.int64(14) * T.int64(2) + v_nu and v_p % T.int64(14) * T.int64(2) + v_nu < T.int64(29), p0[v_p // T.int64(196), v_ci, v_p % T.int64(196) // T.int64(14) * T.int64(2) + v_eps - T.int64(1), v_p % T.int64(14) * T.int64(2) + v_nu - T.int64(1)], T.float32(0))
                for eps in T.unroll(T.int64(4)):
                    for nu in T.unroll(T.int64(4)):
                        with T.block("data_pack_init"):
                            v_eps, v_nu = T.axis.remap("SS", [eps, nu])
                            v_ci = T.axis.spatial(T.int64(128), (ci_p_fused_0 * T.int64(256) + ci_p_fused_1) // T.int64(196))
                            v_p = T.axis.spatial(T.int64(196), (ci_p_fused_0 * T.int64(256) + ci_p_fused_1) % T.int64(196))
                            T.reads()
                            T.writes(data_pack_local[v_eps, v_nu, v_ci, v_p])
                            T.block_attr({"schedule_rule": "conv2d_nchw_winograd_data_pack"})
                            data_pack_local[v_eps, v_nu, v_ci, v_p] = T.float32(0)
                        for r_a in T.unroll(T.int64(4)):
                            for r_b in T.unroll(T.int64(4)):
                                with T.block("data_pack_update"):
                                    v_eps, v_nu = T.axis.remap("SS", [eps, nu])
                                    v_ci = T.axis.spatial(T.int64(128), (ci_p_fused_0 * T.int64(256) + ci_p_fused_1) // T.int64(196))
                                    v_p = T.axis.spatial(T.int64(196), (ci_p_fused_0 * T.int64(256) + ci_p_fused_1) % T.int64(196))
                                    v_r_a, v_r_b = T.axis.remap("RR", [r_a, r_b])
                                    T.reads(data_pack_local[v_eps, v_nu, v_ci, v_p], input_tile_local[v_ci, v_p, v_r_a, v_r_b])
                                    T.writes(data_pack_local[v_eps, v_nu, v_ci, v_p])
                                    T.block_attr({"schedule_rule": "conv2d_nchw_winograd_data_pack"})
                                    data_pack_local[v_eps, v_nu, v_ci, v_p] = data_pack_local[v_eps, v_nu, v_ci, v_p] + input_tile_local[v_ci, v_p, v_r_a, v_r_b] * T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(3), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(1), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(0), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(3), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(1), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(0), T.float32(1), T.float32(0))))))))))))))))) * T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(3), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(1), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(0), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(3), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(1), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(0), T.float32(1), T.float32(0)))))))))))))))))
                for ax0, ax1, ax2, ax3 in T.grid(T.int64(4), T.int64(4), T.int64(1), T.int64(1)):
                    with T.block("data_pack_local"):
                        v0, v1 = T.axis.remap("SS", [ax0, ax1])
                        v2 = T.axis.spatial(T.int64(128), (ci_p_fused_0 * T.int64(256) + ci_p_fused_1) // T.int64(196) + ax2)
                        v3 = T.axis.spatial(T.int64(196), (ci_p_fused_0 * T.int64(60) + ci_p_fused_1) % T.int64(196) + ax3)
                        T.reads(data_pack_local[v0, v1, v2, v3])
                        T.writes(data_pack[v0, v1, v2, v3])
                        data_pack[v0, v1, v2, v3] = data_pack_local[v0, v1, v2, v3]
        for eps_0_nu_0_co_0_p_0_fused in T.thread_binding(T.int64(224), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 1024, "pragma_unroll_explicit": 1}):
            for eps_1_nu_1_co_1_p_1_fused in T.thread_binding(T.int64(2), thread="vthread.x"):
                for eps_2_nu_2_co_2_p_2_fused in T.thread_binding(T.int64(224), thread="threadIdx.x"):
                    for eps_3_init, nu_3_init, co_3_init, p_3_init, eps_4_init, nu_4_init, co_4_init, p_4_init in T.grid(T.int64(1), T.int64(1), T.int64(4), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("bgemm_init"):
                            v_eps = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(56) + eps_3_init + eps_4_init)
                            v_nu = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused % T.int64(56) // T.int64(14) + nu_3_init + nu_4_init)
                            v_co = T.axis.spatial(T.int64(128), eps_0_nu_0_co_0_p_0_fused % T.int64(14) // T.int64(7) * T.int64(64) + eps_2_nu_2_co_2_p_2_fused // T.int64(14) * T.int64(4) + co_3_init + co_4_init)
                            v_p = T.axis.spatial(T.int64(196), eps_0_nu_0_co_0_p_0_fused % T.int64(7) * T.int64(28) + eps_1_nu_1_co_1_p_1_fused * T.int64(14) + eps_2_nu_2_co_2_p_2_fused % T.int64(14) + p_3_init + p_4_init)
                            T.reads()
                            T.writes(bgemm_local[v_eps, v_nu, v_co, v_p])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            bgemm_local[v_eps, v_nu, v_co, v_p] = T.float32(0)
                    for ci_0 in range(T.int64(1)):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(8)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(224), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("data_pack_shared"):
                                        v0 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(56))
                                        v1 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused % T.int64(56) // T.int64(14))
                                        v2 = T.axis.spatial(T.int64(128), (ax0_ax1_ax2_ax3_fused_0 * T.int64(448) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) // T.int64(28))
                                        v3 = T.axis.spatial(T.int64(196), eps_0_nu_0_co_0_p_0_fused % T.int64(7) * T.int64(28) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(448) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) % T.int64(28))
                                        T.reads(data_pack[v0, v1, v2, v3])
                                        T.writes(data_pack_shared[v0, v1, v2, v3])
                                        data_pack_shared[v0, v1, v2, v3] = data_pack[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(19)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(224), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("p1_shared"):
                                        v0 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(56))
                                        v1 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused % T.int64(56) // T.int64(14))
                                        v2 = T.axis.spatial(T.int64(128), (ax0_ax1_ax2_ax3_fused_0 * T.int64(448) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) // T.int64(64))
                                        v3 = T.axis.spatial(T.int64(128), eps_0_nu_0_co_0_p_0_fused % T.int64(14) // T.int64(7) * T.int64(64) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(448) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) % T.int64(64))
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * T.int64(224) + ax0_ax1_ax2_ax3_fused_1) * T.int64(2) + ax0_ax1_ax2_ax3_fused_2 < T.int64(8192))
                                        T.reads(p1[v0, v1, v2, v3])
                                        T.writes(p1_shared[v0, v1, v2, v3])
                                        p1_shared[v0, v1, v2, v3] = p1[v0, v1, v2, v3]
                        for ci_1, eps_3, nu_3, co_3, p_3, ci_2, eps_4, nu_4, co_4, p_4 in T.grid(T.int64(4), T.int64(1), T.int64(1), T.int64(4), T.int64(1), T.int64(32), T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                            with T.block("bgemm_update"):
                                v_eps = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(56) + eps_3 + eps_4)
                                v_nu = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused % T.int64(56) // T.int64(14) + nu_3 + nu_4)
                                v_co = T.axis.spatial(T.int64(128), eps_0_nu_0_co_0_p_0_fused % T.int64(14) // T.int64(7) * T.int64(64) + eps_2_nu_2_co_2_p_2_fused // T.int64(14) * T.int64(4) + co_3 + co_4)
                                v_p = T.axis.spatial(T.int64(196), eps_0_nu_0_co_0_p_0_fused % T.int64(7) * T.int64(28) + eps_1_nu_1_co_1_p_1_fused * T.int64(14) + eps_2_nu_2_co_2_p_2_fused % T.int64(14) + p_3 + p_4)
                                v_ci = T.axis.reduce(T.int64(128), ci_0 * T.int64(128) + ci_1 * T.int64(32) + ci_2)
                                T.reads(bgemm_local[v_eps, v_nu, v_co, v_p], data_pack_shared[v_eps, v_nu, v_ci, v_p], p1_shared[v_eps, v_nu, v_ci, v_co])
                                T.writes(bgemm_local[v_eps, v_nu, v_co, v_p])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                bgemm_local[v_eps, v_nu, v_co, v_p] = bgemm_local[v_eps, v_nu, v_co, v_p] + data_pack_shared[v_eps, v_nu, v_ci, v_p] * p1_shared[v_eps, v_nu, v_ci, v_co]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(1), T.int64(4), T.int64(1)):
                        with T.block("bgemm_local"):
                            v0 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(56) + ax0)
                            v1 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused % T.int64(56) // T.int64(14) + ax1)
                            v2 = T.axis.spatial(T.int64(128), eps_0_nu_0_co_0_p_0_fused % T.int64(14) // T.int64(7) * T.int64(64) + eps_2_nu_2_co_2_p_2_fused // T.int64(14) * T.int64(4) + ax2)
                            v3 = T.axis.spatial(T.int64(196), eps_0_nu_0_co_0_p_0_fused % T.int64(7) * T.int64(28) + eps_1_nu_1_co_1_p_1_fused * T.int64(14) + eps_2_nu_2_co_2_p_2_fused % T.int64(14) + ax3)
                            T.reads(bgemm_local[v0, v1, v2, v3])
                            T.writes(bgemm[v0, v1, v2, v3])
                            bgemm[v0, v1, v2, v3] = bgemm_local[v0, v1, v2, v3]
        for n_co_h_0_w_0_fused_0 in T.thread_binding(T.int64(784), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 1024, "pragma_unroll_explicit": 1}):
            for n_co_h_0_w_0_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                    for ax2 in T.unroll(T.int64(2)):
                        for ax3 in T.unroll(T.int64(2)):
                            with T.block("inverse_init"):
                                v_co = T.axis.spatial(T.int64(128), (n_co_h_0_w_0_fused_0 * T.int64(32) + n_co_h_0_w_0_fused_1) // T.int64(196) + ax0)
                                v_p = T.axis.spatial(T.int64(196), (n_co_h_0_w_0_fused_0 * T.int64(32) + n_co_h_0_w_0_fused_1) % T.int64(196) + ax1)
                                v_vh, v_vw = T.axis.remap("SS", [ax2, ax3])
                                T.reads()
                                T.writes(inverse_local[v_co, v_p, v_vh, v_vw])
                                T.block_attr({"schedule_rule": "conv2d_nchw_winograd_inverse"})
                                inverse_local[v_co, v_p, v_vh, v_vw] = T.float32(0)
                            for ax4 in T.unroll(T.int64(4)):
                                for ax5 in T.unroll(T.int64(4)):
                                    with T.block("inverse_update"):
                                        v_co = T.axis.spatial(T.int64(128), (n_co_h_0_w_0_fused_0 * T.int64(32) + n_co_h_0_w_0_fused_1) // T.int64(196) + ax0)
                                        v_p = T.axis.spatial(T.int64(196), (n_co_h_0_w_0_fused_0 * T.int64(32) + n_co_h_0_w_0_fused_1) % T.int64(196) + ax1)
                                        v_vh, v_vw, v_r_a, v_r_b = T.axis.remap("SSRR", [ax2, ax3, ax4, ax5])
                                        T.reads(inverse_local[v_co, v_p, v_vh, v_vw], bgemm[v_r_a, v_r_b, v_co, v_p])
                                        T.writes(inverse_local[v_co, v_p, v_vh, v_vw])
                                        T.block_attr({"schedule_rule": "conv2d_nchw_winograd_inverse"})
                                        inverse_local[v_co, v_p, v_vh, v_vw] = inverse_local[v_co, v_p, v_vh, v_vw] + bgemm[v_r_a, v_r_b, v_co, v_p] * T.Select(v_r_a % T.int64(4) == T.int64(3) and v_vh % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_vh % T.int64(2) == T.int64(0), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_vh % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_vh % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_vh % T.int64(2) == T.int64(1), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_vh % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_vh % T.int64(2) == T.int64(1), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_vh % T.int64(2) == T.int64(0), T.float32(1), T.float32(0))))))))) * T.Select(v_r_b % T.int64(4) == T.int64(3) and v_vw % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_vw % T.int64(2) == T.int64(0), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_vw % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_vw % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_vw % T.int64(2) == T.int64(1), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_vw % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_vw % T.int64(2) == T.int64(1), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_vw % T.int64(2) == T.int64(0), T.float32(1), T.float32(0)))))))))
                for h_1, w_1 in T.grid(T.int64(2), T.int64(2)):
                    with T.block("conv2d_winograd"):
                        v_n = T.axis.spatial(T.int64(1), T.int64(0))
                        v_co = T.axis.spatial(T.int64(128), (n_co_h_0_w_0_fused_0 * T.int64(32) + n_co_h_0_w_0_fused_1) // T.int64(196))
                        v_h = T.axis.spatial(T.int64(28), (n_co_h_0_w_0_fused_0 * T.int64(32) + n_co_h_0_w_0_fused_1) % T.int64(196) // T.int64(14) * T.int64(2) + h_1)
                        v_w = T.axis.spatial(T.int64(28), (n_co_h_0_w_0_fused_0 * T.int64(32) + n_co_h_0_w_0_fused_1) % T.int64(14) * T.int64(2) + w_1)
                        T.reads(inverse_local[v_co, v_n * T.int64(196) + v_h // T.int64(2) * T.int64(14) + v_w // T.int64(2), v_h % T.int64(2), v_w % T.int64(2)], p2[v_n, v_co, v_h, v_w], p3[v_n, v_co, T.int64(0), T.int64(0)])
                        T.writes(T_relu[v_n, v_co, v_h, v_w])
                        T_relu[v_n, v_co, v_h, v_w] = T.max(inverse_local[v_co, v_n * T.int64(196) + v_h // T.int64(2) * T.int64(14) + v_w // T.int64(2), v_h % T.int64(2), v_w % T.int64(2)] + p2[v_n, v_co, v_h, v_w] + p3[v_n, v_co, T.int64(0), T.int64(0)], T.float32(0))
b0 = sch.get_block(name="data_pack", func_name="main")
b1 = sch.get_block(name="bgemm", func_name="main")
b2 = sch.get_block(name="inverse", func_name="main")
b3 = sch.get_block(name="conv2d_winograd", func_name="main")
b4 = sch.get_block(name="T_add", func_name="main")
b5 = sch.get_block(name="T_add_1", func_name="main")
b6 = sch.get_block(name="T_relu", func_name="main")
b7 = sch.get_block(name="root", func_name="main")
b8, b9 = sch.get_producers(block=b2)
sch.compute_inline(block=b9)
b10, = sch.get_consumers(block=b2)
l11, l12, l13, l14 = sch.get_loops(block=b10)
l15, l16 = sch.split(loop=l13, factors=[None, 2], preserve_unit_iters=True)
l17, l18 = sch.split(loop=l14, factors=[None, 2], preserve_unit_iters=True)
sch.reorder(l15, l17, l16, l18)
sch.compute_at(block=b2, loop=l17, preserve_unit_loops=True, index=-1)
sch.set_scope(block=b2, buffer_index=0, storage_scope="local")
l19, l20, l21, l22, l23, l24, l25, l26, l27, l28 = sch.get_loops(block=b2)
sch.unroll(loop=l25)
sch.unroll(loop=l26)
sch.unroll(loop=l27)
sch.unroll(loop=l28)
b29, b30 = sch.get_producers(block=b0)
sch.compute_inline(block=b30)
b31, = sch.get_producers(block=b29)
l32, l33, l34, l35, l36, l37 = sch.get_loops(block=b0)
sch.reorder(l34, l35, l32, l33, l36, l37)
sch.unroll(loop=l32)
sch.unroll(loop=l33)
sch.unroll(loop=l36)
sch.unroll(loop=l37)
l38 = sch.fuse(l34, l35, preserve_unit_iters=True)
v39 = sch.sample_categorical(candidates=[32, 64, 128, 256, 512, 1024], probs=[0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666], decision=3)
l40, l41 = sch.split(loop=l38, factors=[None, v39], preserve_unit_iters=True)
sch.bind(loop=l40, thread_axis="blockIdx.x")
sch.bind(loop=l41, thread_axis="threadIdx.x")
b42 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b42, loop=l41, preserve_unit_loops=True, index=-1)
sch.compute_at(block=b29, loop=l41, preserve_unit_loops=True, index=-1)
sch.set_scope(block=b29, buffer_index=0, storage_scope="local")
sch.compute_inline(block=b31)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l43, l44, l45, l46, l47 = sch.get_loops(block=b1)
v48, v49, v50, v51, v52 = sch.sample_perfect_tile(loop=l43, n=5, max_innermost_factor=64, decision=[4, 1, 1, 1, 1])
l53, l54, l55, l56, l57 = sch.split(loop=l43, factors=[v48, v49, v50, v51, v52], preserve_unit_iters=True)
v58, v59, v60, v61, v62 = sch.sample_perfect_tile(loop=l44, n=5, max_innermost_factor=64, decision=[4, 1, 1, 1, 1])
l63, l64, l65, l66, l67 = sch.split(loop=l44, factors=[v58, v59, v60, v61, v62], preserve_unit_iters=True)
v68, v69, v70, v71, v72 = sch.sample_perfect_tile(loop=l45, n=5, max_innermost_factor=64, decision=[2, 1, 16, 4, 1])
l73, l74, l75, l76, l77 = sch.split(loop=l45, factors=[v68, v69, v70, v71, v72], preserve_unit_iters=True)
v78, v79, v80, v81, v82 = sch.sample_perfect_tile(loop=l46, n=5, max_innermost_factor=64, decision=[7, 2, 14, 1, 1])
l83, l84, l85, l86, l87 = sch.split(loop=l46, factors=[v78, v79, v80, v81, v82], preserve_unit_iters=True)
v88, v89, v90 = sch.sample_perfect_tile(loop=l47, n=3, max_innermost_factor=64, decision=[1, 4, 32])
l91, l92, l93 = sch.split(loop=l47, factors=[v88, v89, v90], preserve_unit_iters=True)
sch.reorder(l53, l63, l73, l83, l54, l64, l74, l84, l55, l65, l75, l85, l91, l92, l56, l66, l76, l86, l93, l57, l67, l77, l87)
l94 = sch.fuse(l53, l63, l73, l83, preserve_unit_iters=True)
sch.bind(loop=l94, thread_axis="blockIdx.x")
l95 = sch.fuse(l54, l64, l74, l84, preserve_unit_iters=True)
sch.bind(loop=l95, thread_axis="vthread.x")
l96 = sch.fuse(l55, l65, l75, l85, preserve_unit_iters=True)
sch.bind(loop=l96, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b97 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b97, loop=l96, preserve_unit_loops=True, index=-1)
b98 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b98, loop=l91, preserve_unit_loops=True, index=-1)
l99, l100, l101, l102, l103, l104, l105, l106 = sch.get_loops(block=b98)
l107 = sch.fuse(l103, l104, l105, l106, preserve_unit_iters=True)
v108 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b98, ann_key="meta_schedule.cooperative_fetch", ann_val=v108)
b109 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b109, loop=l91, preserve_unit_loops=True, index=-1)
l110, l111, l112, l113, l114, l115, l116, l117 = sch.get_loops(block=b109)
l118 = sch.fuse(l114, l115, l116, l117, preserve_unit_iters=True)
v119 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b109, ann_key="meta_schedule.cooperative_fetch", ann_val=v119)
sch.reverse_compute_inline(block=b6)
sch.reverse_compute_inline(block=b5)
sch.reverse_compute_inline(block=b4)
v120 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b7, ann_key="meta_schedule.unroll_explicit", ann_val=v120)
l121, l122, l123, l124, l125, l126 = sch.get_loops(block=b3)
l127 = sch.fuse(l121, l122, l123, l124, preserve_unit_iters=True)
v128 = sch.sample_categorical(candidates=[32, 64, 128, 256, 512, 1024], probs=[0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666], decision=0)
l129, l130 = sch.split(loop=l127, factors=[None, v128], preserve_unit_iters=True)
sch.bind(loop=l129, thread_axis="blockIdx.x")
sch.bind(loop=l130, thread_axis="threadIdx.x")
sch.enter_postproc()
sch.unannotate(block_or_loop=b98, ann_key="meta_schedule.cooperative_fetch")
l131, l132, l133, l134, l135 = sch.get_loops(block=b98)
l136, l137, l138 = sch.split(loop=l135, factors=[None, 224, 2], preserve_unit_iters=True)
sch.vectorize(loop=l138)
sch.bind(loop=l137, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b109, ann_key="meta_schedule.cooperative_fetch")
l139, l140, l141, l142, l143 = sch.get_loops(block=b109)
l144, l145, l146 = sch.split(loop=l143, factors=[None, 224, 2], preserve_unit_iters=True)
sch.vectorize(loop=l146)
sch.bind(loop=l145, thread_axis="threadIdx.x")
b147 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b147, ann_key="meta_schedule.unroll_explicit")
b148, b149, b150, b151, b152, b153, b154, b155, b156 = sch.get_child_blocks(b147)
l157, l158, l159, l160, l161, l162 = sch.get_loops(block=b148)
l163, l164, l165, l166, l167, l168 = sch.get_loops(block=b149)
sch.annotate(block_or_loop=l163, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l163, ann_key="pragma_unroll_explicit", ann_val=1)
l169, l170, l171, l172, l173, l174 = sch.get_loops(block=b150)
l175, l176, l177, l178, l179, l180, l181 = sch.get_loops(block=b151)
l182, l183, l184, l185, l186, l187, l188 = sch.get_loops(block=b152)
l189, l190, l191, l192, l193, l194, l195, l196, l197, l198, l199, l200, l201, l202 = sch.get_loops(block=b153)
sch.annotate(block_or_loop=l189, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l189, ann_key="pragma_unroll_explicit", ann_val=1)
l203, l204, l205, l206, l207, l208, l209 = sch.get_loops(block=b154)
l210, l211, l212, l213, l214, l215, l216, l217 = sch.get_loops(block=b155)
sch.annotate(block_or_loop=l210, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l210, ann_key="pragma_unroll_explicit", ann_val=1)
l218, l219, l220, l221 = sch.get_loops(block=b156)
b222 = sch.get_block(name="data_pack", func_name="main")
l223, l224, l225, l226, l227, l228 = sch.get_loops(block=b222)
b229 = sch.decompose_reduction(block=b222, loop=l227)
b230 = sch.get_block(name="bgemm", func_name="main")
l231, l232, l233, l234, l235, l236, l237, l238, l239, l240, l241, l242, l243, l244 = sch.get_loops(block=b230)
b245 = sch.decompose_reduction(block=b230, loop=l234)
b246 = sch.get_block(name="inverse", func_name="main")
l247, l248, l249, l250, l251, l252, l253, l254 = sch.get_loops(block=b246)
b255 = sch.decompose_reduction(block=b246, loop=l253)
2023-11-11 07:46:00 [INFO] [task_scheduler.cc:121] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #582: Error in running:
LocalRunner: An exception occurred
Subprocess terminated
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(128), T.int64(28), T.int64(28)), "float32"), p1: T.Buffer((T.int64(4), T.int64(4), T.int64(128), T.int64(128)), "float32"), p2: T.Buffer((T.int64(1), T.int64(128), T.int64(28), T.int64(28)), "float32"), p3: T.Buffer((T.int64(1), T.int64(128), T.int64(1), T.int64(1)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(128), T.int64(28), T.int64(28)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        input_tile_local = T.alloc_buffer((T.int64(128), T.int64(196), T.int64(4), T.int64(4)), scope="local")
        data_pack = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(128), T.int64(196)))
        bgemm = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(128), T.int64(196)))
        inverse_local = T.alloc_buffer((T.int64(128), T.int64(196), T.int64(2), T.int64(2)), scope="local")
        data_pack_local = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(128), T.int64(196)), scope="local")
        bgemm_local = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(128), T.int64(196)), scope="local")
        data_pack_shared = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(128), T.int64(196)), scope="shared")
        p1_shared = T.alloc_buffer((T.int64(4), T.int64(4), T.int64(128), T.int64(128)), scope="shared")
        for ci_p_fused_0 in T.thread_binding(T.int64(98), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 1024, "pragma_unroll_explicit": 1}):
            for ci_p_fused_1 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(1), T.int64(4), T.int64(4)):
                    with T.block("input_tile"):
                        v_ci = T.axis.spatial(T.int64(128), (ci_p_fused_0 * T.int64(256) + ci_p_fused_1) // T.int64(196) + ax0)
                        v_p = T.axis.spatial(T.int64(196), (ci_p_fused_0 * T.int64(60) + ci_p_fused_1) % T.int64(196) + ax1)
                        v_eps, v_nu = T.axis.remap("SS", [ax2, ax3])
                        T.reads(p0[v_p // T.int64(196), v_ci, v_p % T.int64(196) // T.int64(14) * T.int64(2) + v_eps - T.int64(1), v_p % T.int64(14) * T.int64(2) + v_nu - T.int64(1)])
                        T.writes(input_tile_local[v_ci, v_p, v_eps, v_nu])
                        T.block_attr({"schedule_rule": "None"})
                        input_tile_local[v_ci, v_p, v_eps, v_nu] = T.if_then_else(T.int64(1) <= v_p % T.int64(196) // T.int64(14) * T.int64(2) + v_eps and v_p % T.int64(196) // T.int64(14) * T.int64(2) + v_eps < T.int64(29) and T.int64(1) <= v_p % T.int64(14) * T.int64(2) + v_nu and v_p % T.int64(14) * T.int64(2) + v_nu < T.int64(29), p0[v_p // T.int64(196), v_ci, v_p % T.int64(196) // T.int64(14) * T.int64(2) + v_eps - T.int64(1), v_p % T.int64(14) * T.int64(2) + v_nu - T.int64(1)], T.float32(0))
                for eps in T.unroll(T.int64(4)):
                    for nu in T.unroll(T.int64(4)):
                        with T.block("data_pack_init"):
                            v_eps, v_nu = T.axis.remap("SS", [eps, nu])
                            v_ci = T.axis.spatial(T.int64(128), (ci_p_fused_0 * T.int64(256) + ci_p_fused_1) // T.int64(196))
                            v_p = T.axis.spatial(T.int64(196), (ci_p_fused_0 * T.int64(256) + ci_p_fused_1) % T.int64(196))
                            T.reads()
                            T.writes(data_pack_local[v_eps, v_nu, v_ci, v_p])
                            T.block_attr({"schedule_rule": "conv2d_nchw_winograd_data_pack"})
                            data_pack_local[v_eps, v_nu, v_ci, v_p] = T.float32(0)
                        for r_a in T.unroll(T.int64(4)):
                            for r_b in T.unroll(T.int64(4)):
                                with T.block("data_pack_update"):
                                    v_eps, v_nu = T.axis.remap("SS", [eps, nu])
                                    v_ci = T.axis.spatial(T.int64(128), (ci_p_fused_0 * T.int64(256) + ci_p_fused_1) // T.int64(196))
                                    v_p = T.axis.spatial(T.int64(196), (ci_p_fused_0 * T.int64(256) + ci_p_fused_1) % T.int64(196))
                                    v_r_a, v_r_b = T.axis.remap("RR", [r_a, r_b])
                                    T.reads(data_pack_local[v_eps, v_nu, v_ci, v_p], input_tile_local[v_ci, v_p, v_r_a, v_r_b])
                                    T.writes(data_pack_local[v_eps, v_nu, v_ci, v_p])
                                    T.block_attr({"schedule_rule": "conv2d_nchw_winograd_data_pack"})
                                    data_pack_local[v_eps, v_nu, v_ci, v_p] = data_pack_local[v_eps, v_nu, v_ci, v_p] + input_tile_local[v_ci, v_p, v_r_a, v_r_b] * T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(3), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_eps % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(1), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_eps % T.int64(4) == T.int64(0), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(3), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(1), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_eps % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_eps % T.int64(4) == T.int64(0), T.float32(1), T.float32(0))))))))))))))))) * T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(3), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_nu % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(1), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_nu % T.int64(4) == T.int64(0), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(3), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(2), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(1), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_nu % T.int64(4) == T.int64(0), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(3), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(2), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(1), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_nu % T.int64(4) == T.int64(0), T.float32(1), T.float32(0)))))))))))))))))
                for ax0, ax1, ax2, ax3 in T.grid(T.int64(4), T.int64(4), T.int64(1), T.int64(1)):
                    with T.block("data_pack_local"):
                        v0, v1 = T.axis.remap("SS", [ax0, ax1])
                        v2 = T.axis.spatial(T.int64(128), (ci_p_fused_0 * T.int64(256) + ci_p_fused_1) // T.int64(196) + ax2)
                        v3 = T.axis.spatial(T.int64(196), (ci_p_fused_0 * T.int64(60) + ci_p_fused_1) % T.int64(196) + ax3)
                        T.reads(data_pack_local[v0, v1, v2, v3])
                        T.writes(data_pack[v0, v1, v2, v3])
                        data_pack[v0, v1, v2, v3] = data_pack_local[v0, v1, v2, v3]
        for eps_0_nu_0_co_0_p_0_fused in T.thread_binding(T.int64(224), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 1024, "pragma_unroll_explicit": 1}):
            for eps_1_nu_1_co_1_p_1_fused in T.thread_binding(T.int64(2), thread="vthread.x"):
                for eps_2_nu_2_co_2_p_2_fused in T.thread_binding(T.int64(224), thread="threadIdx.x"):
                    for eps_3_init, nu_3_init, co_3_init, p_3_init, eps_4_init, nu_4_init, co_4_init, p_4_init in T.grid(T.int64(1), T.int64(1), T.int64(4), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                        with T.block("bgemm_init"):
                            v_eps = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(56) + eps_3_init + eps_4_init)
                            v_nu = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused % T.int64(56) // T.int64(14) + nu_3_init + nu_4_init)
                            v_co = T.axis.spatial(T.int64(128), eps_0_nu_0_co_0_p_0_fused % T.int64(14) // T.int64(7) * T.int64(64) + eps_2_nu_2_co_2_p_2_fused // T.int64(14) * T.int64(4) + co_3_init + co_4_init)
                            v_p = T.axis.spatial(T.int64(196), eps_0_nu_0_co_0_p_0_fused % T.int64(7) * T.int64(28) + eps_1_nu_1_co_1_p_1_fused * T.int64(14) + eps_2_nu_2_co_2_p_2_fused % T.int64(14) + p_3_init + p_4_init)
                            T.reads()
                            T.writes(bgemm_local[v_eps, v_nu, v_co, v_p])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            bgemm_local[v_eps, v_nu, v_co, v_p] = T.float32(0)
                    for ci_0 in range(T.int64(1)):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(8)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(224), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("data_pack_shared"):
                                        v0 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(56))
                                        v1 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused % T.int64(56) // T.int64(14))
                                        v2 = T.axis.spatial(T.int64(128), (ax0_ax1_ax2_ax3_fused_0 * T.int64(448) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) // T.int64(28))
                                        v3 = T.axis.spatial(T.int64(196), eps_0_nu_0_co_0_p_0_fused % T.int64(7) * T.int64(28) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(448) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) % T.int64(28))
                                        T.reads(data_pack[v0, v1, v2, v3])
                                        T.writes(data_pack_shared[v0, v1, v2, v3])
                                        data_pack_shared[v0, v1, v2, v3] = data_pack[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(37)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(224), thread="threadIdx.x"):
                                with T.block("p1_shared"):
                                    v0 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(56))
                                    v1 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused % T.int64(56) // T.int64(14))
                                    v2 = T.axis.spatial(T.int64(128), (ax0_ax1_ax2_ax3_fused_0 * T.int64(224) + ax0_ax1_ax2_ax3_fused_1) // T.int64(64))
                                    v3 = T.axis.spatial(T.int64(128), eps_0_nu_0_co_0_p_0_fused % T.int64(14) // T.int64(7) * T.int64(64) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(224) + ax0_ax1_ax2_ax3_fused_1) % T.int64(64))
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * T.int64(224) + ax0_ax1_ax2_ax3_fused_1 < T.int64(8192))
                                    T.reads(p1[v0, v1, v2, v3])
                                    T.writes(p1_shared[v0, v1, v2, v3])
                                    p1_shared[v0, v1, v2, v3] = p1[v0, v1, v2, v3]
                        for ci_1, eps_3, nu_3, co_3, p_3, ci_2, eps_4, nu_4, co_4, p_4 in T.grid(T.int64(4), T.int64(1), T.int64(1), T.int64(4), T.int64(1), T.int64(32), T.int64(1), T.int64(1), T.int64(1), T.int64(1)):
                            with T.block("bgemm_update"):
                                v_eps = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(56) + eps_3 + eps_4)
                                v_nu = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused % T.int64(56) // T.int64(14) + nu_3 + nu_4)
                                v_co = T.axis.spatial(T.int64(128), eps_0_nu_0_co_0_p_0_fused % T.int64(14) // T.int64(7) * T.int64(64) + eps_2_nu_2_co_2_p_2_fused // T.int64(14) * T.int64(4) + co_3 + co_4)
                                v_p = T.axis.spatial(T.int64(196), eps_0_nu_0_co_0_p_0_fused % T.int64(7) * T.int64(28) + eps_1_nu_1_co_1_p_1_fused * T.int64(14) + eps_2_nu_2_co_2_p_2_fused % T.int64(14) + p_3 + p_4)
                                v_ci = T.axis.reduce(T.int64(128), ci_0 * T.int64(128) + ci_1 * T.int64(32) + ci_2)
                                T.reads(bgemm_local[v_eps, v_nu, v_co, v_p], data_pack_shared[v_eps, v_nu, v_ci, v_p], p1_shared[v_eps, v_nu, v_ci, v_co])
                                T.writes(bgemm_local[v_eps, v_nu, v_co, v_p])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 1024, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                bgemm_local[v_eps, v_nu, v_co, v_p] = bgemm_local[v_eps, v_nu, v_co, v_p] + data_pack_shared[v_eps, v_nu, v_ci, v_p] * p1_shared[v_eps, v_nu, v_ci, v_co]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(1), T.int64(4), T.int64(1)):
                        with T.block("bgemm_local"):
                            v0 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused // T.int64(56) + ax0)
                            v1 = T.axis.spatial(T.int64(4), eps_0_nu_0_co_0_p_0_fused % T.int64(56) // T.int64(14) + ax1)
                            v2 = T.axis.spatial(T.int64(128), eps_0_nu_0_co_0_p_0_fused % T.int64(14) // T.int64(7) * T.int64(64) + eps_2_nu_2_co_2_p_2_fused // T.int64(14) * T.int64(4) + ax2)
                            v3 = T.axis.spatial(T.int64(196), eps_0_nu_0_co_0_p_0_fused % T.int64(7) * T.int64(28) + eps_1_nu_1_co_1_p_1_fused * T.int64(14) + eps_2_nu_2_co_2_p_2_fused % T.int64(14) + ax3)
                            T.reads(bgemm_local[v0, v1, v2, v3])
                            T.writes(bgemm[v0, v1, v2, v3])
                            bgemm[v0, v1, v2, v3] = bgemm_local[v0, v1, v2, v3]
        for n_co_h_0_w_0_fused_0 in T.thread_binding(T.int64(784), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 1024, "pragma_unroll_explicit": 1}):
            for n_co_h_0_w_0_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                    for ax2 in T.unroll(T.int64(2)):
                        for ax3 in T.unroll(T.int64(2)):
                            with T.block("inverse_init"):
                                v_co = T.axis.spatial(T.int64(128), (n_co_h_0_w_0_fused_0 * T.int64(32) + n_co_h_0_w_0_fused_1) // T.int64(196) + ax0)
                                v_p = T.axis.spatial(T.int64(196), (n_co_h_0_w_0_fused_0 * T.int64(32) + n_co_h_0_w_0_fused_1) % T.int64(196) + ax1)
                                v_vh, v_vw = T.axis.remap("SS", [ax2, ax3])
                                T.reads()
                                T.writes(inverse_local[v_co, v_p, v_vh, v_vw])
                                T.block_attr({"schedule_rule": "conv2d_nchw_winograd_inverse"})
                                inverse_local[v_co, v_p, v_vh, v_vw] = T.float32(0)
                            for ax4 in T.unroll(T.int64(4)):
                                for ax5 in T.unroll(T.int64(4)):
                                    with T.block("inverse_update"):
                                        v_co = T.axis.spatial(T.int64(128), (n_co_h_0_w_0_fused_0 * T.int64(32) + n_co_h_0_w_0_fused_1) // T.int64(196) + ax0)
                                        v_p = T.axis.spatial(T.int64(196), (n_co_h_0_w_0_fused_0 * T.int64(32) + n_co_h_0_w_0_fused_1) % T.int64(196) + ax1)
                                        v_vh, v_vw, v_r_a, v_r_b = T.axis.remap("SSRR", [ax2, ax3, ax4, ax5])
                                        T.reads(inverse_local[v_co, v_p, v_vh, v_vw], bgemm[v_r_a, v_r_b, v_co, v_p])
                                        T.writes(inverse_local[v_co, v_p, v_vh, v_vw])
                                        T.block_attr({"schedule_rule": "conv2d_nchw_winograd_inverse"})
                                        inverse_local[v_co, v_p, v_vh, v_vw] = inverse_local[v_co, v_p, v_vh, v_vw] + bgemm[v_r_a, v_r_b, v_co, v_p] * T.Select(v_r_a % T.int64(4) == T.int64(3) and v_vh % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(3) and v_vh % T.int64(2) == T.int64(0), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_vh % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(2) and v_vh % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_vh % T.int64(2) == T.int64(1), T.float32(-1), T.Select(v_r_a % T.int64(4) == T.int64(1) and v_vh % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_vh % T.int64(2) == T.int64(1), T.float32(0), T.Select(v_r_a % T.int64(4) == T.int64(0) and v_vh % T.int64(2) == T.int64(0), T.float32(1), T.float32(0))))))))) * T.Select(v_r_b % T.int64(4) == T.int64(3) and v_vw % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(3) and v_vw % T.int64(2) == T.int64(0), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_vw % T.int64(2) == T.int64(1), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(2) and v_vw % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_vw % T.int64(2) == T.int64(1), T.float32(-1), T.Select(v_r_b % T.int64(4) == T.int64(1) and v_vw % T.int64(2) == T.int64(0), T.float32(1), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_vw % T.int64(2) == T.int64(1), T.float32(0), T.Select(v_r_b % T.int64(4) == T.int64(0) and v_vw % T.int64(2) == T.int64(0), T.float32(1), T.float32(0)))))))))
                for h_1, w_1 in T.grid(T.int64(2), T.int64(2)):
                    with T.block("conv2d_winograd"):
                        v_n = T.axis.spatial(T.int64(1), T.int64(0))
                        v_co = T.axis.spatial(T.int64(128), (n_co_h_0_w_0_fused_0 * T.int64(32) + n_co_h_0_w_0_fused_1) // T.int64(196))
                        v_h = T.axis.spatial(T.int64(28), (n_co_h_0_w_0_fused_0 * T.int64(32) + n_co_h_0_w_0_fused_1) % T.int64(196) // T.int64(14) * T.int64(2) + h_1)
                        v_w = T.axis.spatial(T.int64(28), (n_co_h_0_w_0_fused_0 * T.int64(32) + n_co_h_0_w_0_fused_1) % T.int64(14) * T.int64(2) + w_1)
                        T.reads(inverse_local[v_co, v_n * T.int64(196) + v_h // T.int64(2) * T.int64(14) + v_w // T.int64(2), v_h % T.int64(2), v_w % T.int64(2)], p2[v_n, v_co, v_h, v_w], p3[v_n, v_co, T.int64(0), T.int64(0)])
                        T.writes(T_relu[v_n, v_co, v_h, v_w])
                        T_relu[v_n, v_co, v_h, v_w] = T.max(inverse_local[v_co, v_n * T.int64(196) + v_h // T.int64(2) * T.int64(14) + v_w // T.int64(2), v_h % T.int64(2), v_w % T.int64(2)] + p2[v_n, v_co, v_h, v_w] + p3[v_n, v_co, T.int64(0), T.int64(0)], T.float32(0))
b0 = sch.get_block(name="data_pack", func_name="main")
b1 = sch.get_block(name="bgemm", func_name="main")
b2 = sch.get_block(name="inverse", func_name="main")
b3 = sch.get_block(name="conv2d_winograd", func_name="main")
b4 = sch.get_block(name="T_add", func_name="main")
b5 = sch.get_block(name="T_add_1", func_name="main")
b6 = sch.get_block(name="T_relu", func_name="main")
b7 = sch.get_block(name="root", func_name="main")
b8, b9 = sch.get_producers(block=b2)
sch.compute_inline(block=b9)
b10, = sch.get_consumers(block=b2)
l11, l12, l13, l14 = sch.get_loops(block=b10)
l15, l16 = sch.split(loop=l13, factors=[None, 2], preserve_unit_iters=True)
l17, l18 = sch.split(loop=l14, factors=[None, 2], preserve_unit_iters=True)
sch.reorder(l15, l17, l16, l18)
sch.compute_at(block=b2, loop=l17, preserve_unit_loops=True, index=-1)
sch.set_scope(block=b2, buffer_index=0, storage_scope="local")
l19, l20, l21, l22, l23, l24, l25, l26, l27, l28 = sch.get_loops(block=b2)
sch.unroll(loop=l25)
sch.unroll(loop=l26)
sch.unroll(loop=l27)
sch.unroll(loop=l28)
b29, b30 = sch.get_producers(block=b0)
sch.compute_inline(block=b30)
b31, = sch.get_producers(block=b29)
l32, l33, l34, l35, l36, l37 = sch.get_loops(block=b0)
sch.reorder(l34, l35, l32, l33, l36, l37)
sch.unroll(loop=l32)
sch.unroll(loop=l33)
sch.unroll(loop=l36)
sch.unroll(loop=l37)
l38 = sch.fuse(l34, l35, preserve_unit_iters=True)
v39 = sch.sample_categorical(candidates=[32, 64, 128, 256, 512, 1024], probs=[0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666], decision=3)
l40, l41 = sch.split(loop=l38, factors=[None, v39], preserve_unit_iters=True)
sch.bind(loop=l40, thread_axis="blockIdx.x")
sch.bind(loop=l41, thread_axis="threadIdx.x")
b42 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b42, loop=l41, preserve_unit_loops=True, index=-1)
sch.compute_at(block=b29, loop=l41, preserve_unit_loops=True, index=-1)
sch.set_scope(block=b29, buffer_index=0, storage_scope="local")
sch.compute_inline(block=b31)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l43, l44, l45, l46, l47 = sch.get_loops(block=b1)
v48, v49, v50, v51, v52 = sch.sample_perfect_tile(loop=l43, n=5, max_innermost_factor=64, decision=[4, 1, 1, 1, 1])
l53, l54, l55, l56, l57 = sch.split(loop=l43, factors=[v48, v49, v50, v51, v52], preserve_unit_iters=True)
v58, v59, v60, v61, v62 = sch.sample_perfect_tile(loop=l44, n=5, max_innermost_factor=64, decision=[4, 1, 1, 1, 1])
l63, l64, l65, l66, l67 = sch.split(loop=l44, factors=[v58, v59, v60, v61, v62], preserve_unit_iters=True)
v68, v69, v70, v71, v72 = sch.sample_perfect_tile(loop=l45, n=5, max_innermost_factor=64, decision=[2, 1, 16, 4, 1])
l73, l74, l75, l76, l77 = sch.split(loop=l45, factors=[v68, v69, v70, v71, v72], preserve_unit_iters=True)
v78, v79, v80, v81, v82 = sch.sample_perfect_tile(loop=l46, n=5, max_innermost_factor=64, decision=[7, 2, 14, 1, 1])
l83, l84, l85, l86, l87 = sch.split(loop=l46, factors=[v78, v79, v80, v81, v82], preserve_unit_iters=True)
v88, v89, v90 = sch.sample_perfect_tile(loop=l47, n=3, max_innermost_factor=64, decision=[1, 4, 32])
l91, l92, l93 = sch.split(loop=l47, factors=[v88, v89, v90], preserve_unit_iters=True)
sch.reorder(l53, l63, l73, l83, l54, l64, l74, l84, l55, l65, l75, l85, l91, l92, l56, l66, l76, l86, l93, l57, l67, l77, l87)
l94 = sch.fuse(l53, l63, l73, l83, preserve_unit_iters=True)
sch.bind(loop=l94, thread_axis="blockIdx.x")
l95 = sch.fuse(l54, l64, l74, l84, preserve_unit_iters=True)
sch.bind(loop=l95, thread_axis="vthread.x")
l96 = sch.fuse(l55, l65, l75, l85, preserve_unit_iters=True)
sch.bind(loop=l96, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b97 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b97, loop=l96, preserve_unit_loops=True, index=-1)
b98 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b98, loop=l91, preserve_unit_loops=True, index=-1)
l99, l100, l101, l102, l103, l104, l105, l106 = sch.get_loops(block=b98)
l107 = sch.fuse(l103, l104, l105, l106, preserve_unit_iters=True)
v108 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b98, ann_key="meta_schedule.cooperative_fetch", ann_val=v108)
b109 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b109, loop=l91, preserve_unit_loops=True, index=-1)
l110, l111, l112, l113, l114, l115, l116, l117 = sch.get_loops(block=b109)
l118 = sch.fuse(l114, l115, l116, l117, preserve_unit_iters=True)
v119 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b109, ann_key="meta_schedule.cooperative_fetch", ann_val=v119)
sch.reverse_compute_inline(block=b6)
sch.reverse_compute_inline(block=b5)
sch.reverse_compute_inline(block=b4)
v120 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b7, ann_key="meta_schedule.unroll_explicit", ann_val=v120)
l121, l122, l123, l124, l125, l126 = sch.get_loops(block=b3)
l127 = sch.fuse(l121, l122, l123, l124, preserve_unit_iters=True)
v128 = sch.sample_categorical(candidates=[32, 64, 128, 256, 512, 1024], probs=[0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666], decision=0)
l129, l130 = sch.split(loop=l127, factors=[None, v128], preserve_unit_iters=True)
sch.bind(loop=l129, thread_axis="blockIdx.x")
sch.bind(loop=l130, thread_axis="threadIdx.x")
sch.enter_postproc()
sch.unannotate(block_or_loop=b98, ann_key="meta_schedule.cooperative_fetch")
l131, l132, l133, l134, l135 = sch.get_loops(block=b98)
l136, l137, l138 = sch.split(loop=l135, factors=[None, 224, 2], preserve_unit_iters=True)
sch.vectorize(loop=l138)
sch.bind(loop=l137, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b109, ann_key="meta_schedule.cooperative_fetch")
l139, l140, l141, l142, l143 = sch.get_loops(block=b109)
l144, l145 = sch.split(loop=l143, factors=[None, 224], preserve_unit_iters=True)
sch.bind(loop=l145, thread_axis="threadIdx.x")
b146 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b146, ann_key="meta_schedule.unroll_explicit")
b147, b148, b149, b150, b151, b152, b153, b154, b155 = sch.get_child_blocks(b146)
l156, l157, l158, l159, l160, l161 = sch.get_loops(block=b147)
l162, l163, l164, l165, l166, l167 = sch.get_loops(block=b148)
sch.annotate(block_or_loop=l162, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l162, ann_key="pragma_unroll_explicit", ann_val=1)
l168, l169, l170, l171, l172, l173 = sch.get_loops(block=b149)
l174, l175, l176, l177, l178, l179, l180 = sch.get_loops(block=b150)
l181, l182, l183, l184, l185, l186 = sch.get_loops(block=b151)
l187, l188, l189, l190, l191, l192, l193, l194, l195, l196, l197, l198, l199, l200 = sch.get_loops(block=b152)
sch.annotate(block_or_loop=l187, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l187, ann_key="pragma_unroll_explicit", ann_val=1)
l201, l202, l203, l204, l205, l206, l207 = sch.get_loops(block=b153)
l208, l209, l210, l211, l212, l213, l214, l215 = sch.get_loops(block=b154)
sch.annotate(block_or_loop=l208, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l208, ann_key="pragma_unroll_explicit", ann_val=1)
l216, l217, l218, l219 = sch.get_loops(block=b155)
b220 = sch.get_block(name="data_pack", func_name="main")
l221, l222, l223, l224, l225, l226 = sch.get_loops(block=b220)
b227 = sch.decompose_reduction(block=b220, loop=l225)
b228 = sch.get_block(name="bgemm", func_name="main")
l229, l230, l231, l232, l233, l234, l235, l236, l237, l238, l239, l240, l241, l242 = sch.get_loops(block=b228)
b243 = sch.decompose_reduction(block=b228, loop=l232)
b244 = sch.get_block(name="inverse", func_name="main")
l245, l246, l247, l248, l249, l250, l251, l252 = sch.get_loops(block=b244)
b253 = sch.decompose_reduction(block=b244, loop=l251)
2023-11-11 07:46:00 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #583: GFLOPs: 4911.0488. Time: 25.8898 us. Best GFLOPs: 5560.2558
2023-11-11 07:46:00 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #584: GFLOPs: 2841.7421. Time: 44.7423 us. Best GFLOPs: 5560.2558
2023-11-11 07:46:00 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #585: GFLOPs: 4938.0751. Time: 25.7481 us. Best GFLOPs: 5560.2558
2023-11-11 07:46:00 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #586: GFLOPs: 5456.9413. Time: 23.2999 us. Best GFLOPs: 5560.2558
2023-11-11 07:46:00 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #587: GFLOPs: 4288.0701. Time: 29.6511 us. Best GFLOPs: 5560.2558
2023-11-11 07:46:00 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #588: GFLOPs: 5505.2057. Time: 23.0956 us. Best GFLOPs: 5560.2558
2023-11-11 07:46:00 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #589: GFLOPs: 5315.5677. Time: 23.9195 us. Best GFLOPs: 5560.2558
2023-11-11 07:46:00 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #590: GFLOPs: 5486.9551. Time: 23.1724 us. Best GFLOPs: 5560.2558
2023-11-11 07:46:00 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #591: GFLOPs: 5504.6495. Time: 23.0979 us. Best GFLOPs: 5560.2558
2023-11-11 07:46:00 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #592: GFLOPs: 5609.6789. Time: 22.6655 us. Best GFLOPs: 5609.6789
2023-11-11 07:46:00 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #593: GFLOPs: 5457.6187. Time: 23.2970 us. Best GFLOPs: 5609.6789
2023-11-11 07:46:00 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #594: GFLOPs: 5456.5257. Time: 23.3016 us. Best GFLOPs: 5609.6789
2023-11-11 07:46:00 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #595: GFLOPs: 5505.4244. Time: 23.0947 us. Best GFLOPs: 5609.6789
2023-11-11 07:46:00 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #596: GFLOPs: 5505.9001. Time: 23.0927 us. Best GFLOPs: 5609.6789
2023-11-11 07:46:00 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #597: GFLOPs: 5504.9129. Time: 23.0968 us. Best GFLOPs: 5609.6789
2023-11-11 07:46:00 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #598: GFLOPs: 5498.8374. Time: 23.1223 us. Best GFLOPs: 5609.6789
2023-11-11 07:46:00 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #599: GFLOPs: 5496.5947. Time: 23.1318 us. Best GFLOPs: 5609.6789
2023-11-11 07:46:00 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #600: GFLOPs: 5500.8587. Time: 23.1138 us. Best GFLOPs: 5609.6789
2023-11-11 07:46:00 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #601: GFLOPs: 5458.5197. Time: 23.2931 us. Best GFLOPs: 5609.6789
2023-11-11 07:46:00 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #602: GFLOPs: 5498.5237. Time: 23.1237 us. Best GFLOPs: 5609.6789
2023-11-11 07:46:00 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #603: GFLOPs: 5461.4098. Time: 23.2808 us. Best GFLOPs: 5609.6789
2023-11-11 07:46:00 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #604: GFLOPs: 5507.0686. Time: 23.0878 us. Best GFLOPs: 5609.6789
2023-11-11 07:46:00 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #605: GFLOPs: 5476.1383. Time: 23.2182 us. Best GFLOPs: 5609.6789
2023-11-11 07:46:00 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #606: GFLOPs: 4886.0521. Time: 26.0222 us. Best GFLOPs: 5609.6789
2023-11-11 07:46:00 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #607: GFLOPs: 5461.5242. Time: 23.2803 us. Best GFLOPs: 5609.6789
2023-11-11 07:46:00 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #608: GFLOPs: 5460.8215. Time: 23.2833 us. Best GFLOPs: 5609.6789
2023-11-11 07:46:00 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #609: GFLOPs: 5504.3999. Time: 23.0990 us. Best GFLOPs: 5609.6789
2023-11-11 07:46:00 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #610: GFLOPs: 4644.0775. Time: 27.3781 us. Best GFLOPs: 5609.6789
2023-11-11 07:46:00 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #611: GFLOPs: 4645.1123. Time: 27.3720 us. Best GFLOPs: 5609.6789
2023-11-11 07:46:00 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #612: GFLOPs: 4638.5404. Time: 27.4108 us. Best GFLOPs: 5609.6789
2023-11-11 07:46:00 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #613: GFLOPs: 5462.2123. Time: 23.2774 us. Best GFLOPs: 5609.6789
2023-11-11 07:46:00 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #614: GFLOPs: 4900.3865. Time: 25.9461 us. Best GFLOPs: 5609.6789
2023-11-11 07:46:00 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #615: GFLOPs: 5593.6820. Time: 22.7303 us. Best GFLOPs: 5609.6789
2023-11-11 07:46:00 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #616: GFLOPs: 5230.0539. Time: 24.3106 us. Best GFLOPs: 5609.6789
2023-11-11 07:46:00 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #617: GFLOPs: 5548.9425. Time: 22.9136 us. Best GFLOPs: 5609.6789
2023-11-11 07:46:00 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #618: GFLOPs: 5508.4322. Time: 23.0821 us. Best GFLOPs: 5609.6789
2023-11-11 07:46:00 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #619: GFLOPs: 5507.8754. Time: 23.0844 us. Best GFLOPs: 5609.6789
2023-11-11 07:46:00 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #620: GFLOPs: 5802.7483. Time: 21.9113 us. Best GFLOPs: 5802.7483
2023-11-11 07:46:00 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #621: GFLOPs: 4920.9678. Time: 25.8376 us. Best GFLOPs: 5802.7483
2023-11-11 07:46:00 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #622: GFLOPs: 5497.7946. Time: 23.1267 us. Best GFLOPs: 5802.7483
2023-11-11 07:46:00 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #623: GFLOPs: 5543.8960. Time: 22.9344 us. Best GFLOPs: 5802.7483
2023-11-11 07:46:00 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #624: GFLOPs: 4918.1228. Time: 25.8525 us. Best GFLOPs: 5802.7483
2023-11-11 07:46:00 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #625: GFLOPs: 5550.5759. Time: 22.9068 us. Best GFLOPs: 5802.7483
2023-11-11 07:46:00 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #626: GFLOPs: 5508.8774. Time: 23.0802 us. Best GFLOPs: 5802.7483
2023-11-11 07:46:00 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #627: GFLOPs: 5508.0093. Time: 23.0838 us. Best GFLOPs: 5802.7483
2023-11-11 07:46:00 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #628: GFLOPs: 5615.0489. Time: 22.6438 us. Best GFLOPs: 5802.7483
2023-11-11 07:46:00 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #629: GFLOPs: 4926.3817. Time: 25.8092 us. Best GFLOPs: 5802.7483
2023-11-11 07:46:00 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #630: GFLOPs: 5550.8746. Time: 22.9056 us. Best GFLOPs: 5802.7483
2023-11-11 07:46:00 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #631: GFLOPs: 5497.3582. Time: 23.1286 us. Best GFLOPs: 5802.7483
2023-11-11 07:46:00 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #632: GFLOPs: 5503.0850. Time: 23.1045 us. Best GFLOPs: 5802.7483
2023-11-11 07:46:00 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #633: GFLOPs: 5495.9541. Time: 23.1345 us. Best GFLOPs: 5802.7483
2023-11-11 07:46:00 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #634: GFLOPs: 5550.1680. Time: 22.9085 us. Best GFLOPs: 5802.7483
2023-11-11 07:46:00 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #635: GFLOPs: 5803.1373. Time: 21.9099 us. Best GFLOPs: 5803.1373
2023-11-11 07:46:00 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #636: GFLOPs: 5539.9488. Time: 22.9508 us. Best GFLOPs: 5803.1373
2023-11-11 07:46:00 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #637: GFLOPs: 5497.5433. Time: 23.1278 us. Best GFLOPs: 5803.1373
2023-11-11 07:46:00 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #638: GFLOPs: 107.1592. Time: 1186.5148 us. Best GFLOPs: 5803.1373
2023-11-11 07:46:00 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #639: GFLOPs: 1584.8964. Time: 80.2235 us. Best GFLOPs: 5803.1373
2023-11-11 07:46:00 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #640: GFLOPs: 2040.1918. Time: 62.3206 us. Best GFLOPs: 5803.1373
2023-11-11 08:35:43 [INFO] [evolutionary_search.cc:713] Generating candidates......
2023-11-11 08:35:46 [INFO] [evolutionary_search.cc:715] Picked top 102 candidate(s) from database
2023-11-11 08:35:53 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310fb046d8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310f554558)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310dc5ddb8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310bbcd2d8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56310b1dbf38)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310dc5d048)]: 395 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310dc26e38)]: 0 failure(s)
2023-11-11 08:36:00 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310fb046d8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310f554558)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310dc5ddb8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310bbcd2d8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56310b1dbf38)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310dc5d048)]: 787 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310dc26e38)]: 0 failure(s)
2023-11-11 08:36:06 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310fb046d8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310f554558)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310dc5ddb8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310bbcd2d8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56310b1dbf38)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310dc5d048)]: 1189 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310dc26e38)]: 0 failure(s)
2023-11-11 08:36:13 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310fb046d8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310f554558)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310dc5ddb8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310bbcd2d8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56310b1dbf38)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310dc5d048)]: 1582 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310dc26e38)]: 0 failure(s)
2023-11-11 08:36:13 [INFO] [evolutionary_search.cc:723] Sampled 58 candidate(s)
2023-11-11 08:36:36 [INFO] [evolutionary_search.cc:621] Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310fb046d8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310f554558)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310dc5ddb8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310bbcd2d8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56310b1dbf38)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310dc5d048)]: 246 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310dc26e38)]: 0 failure(s)
2023-11-11 08:37:00 [INFO] [evolutionary_search.cc:621] Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310fb046d8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310f554558)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310dc5ddb8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310bbcd2d8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56310b1dbf38)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310dc5d048)]: 193 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310dc26e38)]: 0 failure(s)
2023-11-11 08:37:26 [INFO] [evolutionary_search.cc:621] Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310fb046d8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310f554558)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310dc5ddb8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310bbcd2d8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56310b1dbf38)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310dc5d048)]: 204 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310dc26e38)]: 0 failure(s)
2023-11-11 08:37:50 [INFO] [evolutionary_search.cc:621] Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310fb046d8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310f554558)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310dc5ddb8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310bbcd2d8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56310b1dbf38)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310dc5d048)]: 199 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310dc26e38)]: 0 failure(s)
2023-11-11 08:37:56 [INFO] [evolutionary_search.cc:649] Scores of the best 64 candidates:
[1 : 16]:	0.9901  0.9886  0.9873  0.9856  0.9836  0.9835  0.9796  0.9788  0.9773  0.9768  0.9742  0.9693  0.9607  0.9580  0.9574  0.9573
[17 : 32]:	0.9568  0.9566  0.9554  0.9548  0.9533  0.9532  0.9525  0.9521  0.9520  0.9519  0.9518  0.9516  0.9515  0.9514  0.9511  0.9511
[33 : 48]:	0.9510  0.9504  0.9504  0.9503  0.9502  0.9499  0.9499  0.9499  0.9499  0.9495  0.9492  0.9486  0.9485  0.9482  0.9482  0.9481
[49 : 64]:	0.9479  0.9478  0.9472  0.9472  0.9469  0.9467  0.9465  0.9465  0.9465  0.9463  0.9462  0.9461  0.9461  0.9461  0.9459  0.9458
2023-11-11 08:37:57 [INFO] [evolutionary_search.cc:727] Got 64 candidate(s) with evolutionary search
2023-11-11 08:37:57 [INFO] [evolutionary_search.cc:730] Sending 64 candidates(s) for measurement
2023-11-11 08:38:32 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #641: GFLOPs: 5898.7706. Time: 21.5547 us. Best GFLOPs: 5898.7706
2023-11-11 08:38:32 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #642: GFLOPs: 5832.2293. Time: 21.8006 us. Best GFLOPs: 5898.7706
2023-11-11 08:38:32 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #643: GFLOPs: 5761.2378. Time: 22.0692 us. Best GFLOPs: 5898.7706
2023-11-11 08:38:32 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #644: GFLOPs: 5831.2474. Time: 21.8043 us. Best GFLOPs: 5898.7706
2023-11-11 08:38:32 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #645: GFLOPs: 5840.9207. Time: 21.7681 us. Best GFLOPs: 5898.7706
2023-11-11 08:38:32 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #646: GFLOPs: 5721.8732. Time: 22.2210 us. Best GFLOPs: 5898.7706
2023-11-11 08:38:32 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #647: GFLOPs: 5788.0034. Time: 21.9672 us. Best GFLOPs: 5898.7706
2023-11-11 08:38:32 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #648: GFLOPs: 5762.8011. Time: 22.0632 us. Best GFLOPs: 5898.7706
2023-11-11 08:38:32 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #649: GFLOPs: 5831.5021. Time: 21.8033 us. Best GFLOPs: 5898.7706
2023-11-11 08:38:32 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #650: GFLOPs: 5689.0447. Time: 22.3493 us. Best GFLOPs: 5898.7706
2023-11-11 08:38:32 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #651: GFLOPs: 5840.1390. Time: 21.7711 us. Best GFLOPs: 5898.7706
2023-11-11 08:38:32 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #652: GFLOPs: 5777.5099. Time: 22.0071 us. Best GFLOPs: 5898.7706
2023-11-11 08:38:32 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #653: GFLOPs: 5547.5316. Time: 22.9194 us. Best GFLOPs: 5898.7706
2023-11-11 08:38:32 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #654: GFLOPs: 5721.7818. Time: 22.2214 us. Best GFLOPs: 5898.7706
2023-11-11 08:38:32 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #655: GFLOPs: 5221.4250. Time: 24.3508 us. Best GFLOPs: 5898.7706
2023-11-11 08:38:32 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #656: GFLOPs: 5228.2452. Time: 24.3191 us. Best GFLOPs: 5898.7706
2023-11-11 08:38:32 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #657: GFLOPs: 5548.3852. Time: 22.9159 us. Best GFLOPs: 5898.7706
2023-11-11 08:38:32 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #658: GFLOPs: 5547.9399. Time: 22.9177 us. Best GFLOPs: 5898.7706
2023-11-11 08:38:32 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #659: GFLOPs: 5585.8427. Time: 22.7622 us. Best GFLOPs: 5898.7706
2023-11-11 08:38:32 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #660: GFLOPs: 5248.0994. Time: 24.2271 us. Best GFLOPs: 5898.7706
2023-11-11 08:38:32 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #661: GFLOPs: 5547.9769. Time: 22.9175 us. Best GFLOPs: 5898.7706
2023-11-11 08:38:32 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #662: GFLOPs: 5847.8414. Time: 21.7424 us. Best GFLOPs: 5898.7706
2023-11-11 08:38:32 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #663: GFLOPs: 5548.3852. Time: 22.9159 us. Best GFLOPs: 5898.7706
2023-11-11 08:38:32 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #664: GFLOPs: 5199.2531. Time: 24.4547 us. Best GFLOPs: 5898.7706
2023-11-11 08:38:32 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #665: GFLOPs: 5549.4251. Time: 22.9116 us. Best GFLOPs: 5898.7706
2023-11-11 08:38:32 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #666: GFLOPs: 5516.6347. Time: 23.0477 us. Best GFLOPs: 5898.7706
2023-11-11 08:38:32 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #667: GFLOPs: 5489.2238. Time: 23.1628 us. Best GFLOPs: 5898.7706
2023-11-11 08:38:32 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #668: GFLOPs: 5564.8835. Time: 22.8479 us. Best GFLOPs: 5898.7706
2023-11-11 08:38:32 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #669: GFLOPs: 5567.1632. Time: 22.8386 us. Best GFLOPs: 5898.7706
2023-11-11 08:38:32 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #670: GFLOPs: 5484.1210. Time: 23.1844 us. Best GFLOPs: 5898.7706
2023-11-11 08:38:32 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #671: GFLOPs: 5166.2663. Time: 24.6108 us. Best GFLOPs: 5898.7706
2023-11-11 08:38:32 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #672: GFLOPs: 5502.2459. Time: 23.1080 us. Best GFLOPs: 5898.7706
2023-11-11 08:38:32 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #673: GFLOPs: 5524.4735. Time: 23.0150 us. Best GFLOPs: 5898.7706
2023-11-11 08:38:32 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #674: GFLOPs: 5579.7153. Time: 22.7872 us. Best GFLOPs: 5898.7706
2023-11-11 08:38:32 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #675: GFLOPs: 5503.6121. Time: 23.1023 us. Best GFLOPs: 5898.7706
2023-11-11 08:38:32 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #676: GFLOPs: 5692.4955. Time: 22.3357 us. Best GFLOPs: 5898.7706
2023-11-11 08:38:32 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #677: GFLOPs: 5436.7158. Time: 23.3865 us. Best GFLOPs: 5898.7706
2023-11-11 08:38:32 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #678: GFLOPs: 5506.1322. Time: 23.0917 us. Best GFLOPs: 5898.7706
2023-11-11 08:38:32 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #679: GFLOPs: 5517.8374. Time: 23.0427 us. Best GFLOPs: 5898.7706
2023-11-11 08:38:32 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #680: GFLOPs: 5491.6356. Time: 23.1527 us. Best GFLOPs: 5898.7706
2023-11-11 08:38:32 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #681: GFLOPs: 5523.3651. Time: 23.0197 us. Best GFLOPs: 5898.7706
2023-11-11 08:38:32 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #682: GFLOPs: 5201.6263. Time: 24.4435 us. Best GFLOPs: 5898.7706
2023-11-11 08:38:32 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #683: GFLOPs: 5241.7906. Time: 24.2562 us. Best GFLOPs: 5898.7706
2023-11-11 08:38:32 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #684: GFLOPs: 5545.8987. Time: 22.9261 us. Best GFLOPs: 5898.7706
2023-11-11 08:38:32 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #685: GFLOPs: 5201.1724. Time: 24.4456 us. Best GFLOPs: 5898.7706
2023-11-11 08:38:32 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #686: GFLOPs: 5647.1687. Time: 22.5150 us. Best GFLOPs: 5898.7706
2023-11-11 08:38:32 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #687: GFLOPs: 5463.4169. Time: 23.2722 us. Best GFLOPs: 5898.7706
2023-11-11 08:38:32 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #688: GFLOPs: 5489.9669. Time: 23.1597 us. Best GFLOPs: 5898.7706
2023-11-11 08:38:32 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #689: GFLOPs: 5491.1300. Time: 23.1548 us. Best GFLOPs: 5898.7706
2023-11-11 08:38:32 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #690: GFLOPs: 5488.5018. Time: 23.1659 us. Best GFLOPs: 5898.7706
2023-11-11 08:38:32 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #691: GFLOPs: 5436.6797. Time: 23.3867 us. Best GFLOPs: 5898.7706
2023-11-11 08:38:32 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #692: GFLOPs: 5490.9487. Time: 23.1556 us. Best GFLOPs: 5898.7706
2023-11-11 08:38:32 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #693: GFLOPs: 5499.0964. Time: 23.1213 us. Best GFLOPs: 5898.7706
2023-11-11 08:38:32 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #694: GFLOPs: 5556.4177. Time: 22.8827 us. Best GFLOPs: 5898.7706
2023-11-11 08:38:32 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #695: GFLOPs: 5519.2785. Time: 23.0367 us. Best GFLOPs: 5898.7706
2023-11-11 08:38:32 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #696: GFLOPs: 5433.1752. Time: 23.4018 us. Best GFLOPs: 5898.7706
2023-11-11 08:38:32 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #697: GFLOPs: 5517.5395. Time: 23.0440 us. Best GFLOPs: 5898.7706
2023-11-11 08:38:32 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #698: GFLOPs: 5408.1627. Time: 23.5100 us. Best GFLOPs: 5898.7706
2023-11-11 08:38:32 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #699: GFLOPs: 5437.2916. Time: 23.3841 us. Best GFLOPs: 5898.7706
2023-11-11 08:38:32 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #700: GFLOPs: 5449.1968. Time: 23.3330 us. Best GFLOPs: 5898.7706
2023-11-11 08:38:32 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #701: GFLOPs: 5451.1417. Time: 23.3247 us. Best GFLOPs: 5898.7706
2023-11-11 08:38:32 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #702: GFLOPs: 1420.9573. Time: 89.4791 us. Best GFLOPs: 5898.7706
2023-11-11 08:38:32 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #703: GFLOPs: 42.8606. Time: 2966.4980 us. Best GFLOPs: 5898.7706
2023-11-11 08:38:32 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #704: GFLOPs: 355.0288. Time: 358.1286 us. Best GFLOPs: 5898.7706
2023-11-11 09:21:23 [INFO] [evolutionary_search.cc:713] Generating candidates......
2023-11-11 09:21:27 [INFO] [evolutionary_search.cc:715] Picked top 102 candidate(s) from database
2023-11-11 09:21:34 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310fb046d8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310f554558)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310dc5ddb8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310bbcd2d8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56310b1dbf38)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310dc5d048)]: 396 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310dc26e38)]: 0 failure(s)
2023-11-11 09:21:40 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310fb046d8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310f554558)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310dc5ddb8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310bbcd2d8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56310b1dbf38)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310dc5d048)]: 793 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310dc26e38)]: 0 failure(s)
2023-11-11 09:21:47 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310fb046d8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310f554558)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310dc5ddb8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310bbcd2d8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56310b1dbf38)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310dc5d048)]: 1183 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310dc26e38)]: 0 failure(s)
2023-11-11 09:21:54 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310fb046d8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310f554558)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310dc5ddb8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310bbcd2d8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56310b1dbf38)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310dc5d048)]: 1581 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310dc26e38)]: 0 failure(s)
2023-11-11 09:21:54 [INFO] [evolutionary_search.cc:723] Sampled 59 candidate(s)
2023-11-11 09:22:17 [INFO] [evolutionary_search.cc:621] Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310fb046d8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310f554558)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310dc5ddb8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310bbcd2d8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56310b1dbf38)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310dc5d048)]: 221 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310dc26e38)]: 0 failure(s)
2023-11-11 09:22:42 [INFO] [evolutionary_search.cc:621] Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310fb046d8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310f554558)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310dc5ddb8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310bbcd2d8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56310b1dbf38)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310dc5d048)]: 203 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310dc26e38)]: 0 failure(s)
2023-11-11 09:23:07 [INFO] [evolutionary_search.cc:621] Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310fb046d8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310f554558)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310dc5ddb8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310bbcd2d8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56310b1dbf38)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310dc5d048)]: 186 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310dc26e38)]: 0 failure(s)
2023-11-11 09:23:34 [INFO] [evolutionary_search.cc:621] Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310fb046d8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310f554558)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310dc5ddb8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310bbcd2d8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56310b1dbf38)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310dc5d048)]: 229 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310dc26e38)]: 0 failure(s)
2023-11-11 09:23:40 [INFO] [evolutionary_search.cc:649] Scores of the best 64 candidates:
[1 : 16]:	0.9917  0.9887  0.9870  0.9864  0.9864  0.9847  0.9833  0.9817  0.9816  0.9806  0.9805  0.9803  0.9797  0.9789  0.9789  0.9788
[17 : 32]:	0.9782  0.9779  0.9769  0.9767  0.9752  0.9751  0.9741  0.9734  0.9729  0.9722  0.9706  0.9705  0.9704  0.9684  0.9683  0.9678
[33 : 48]:	0.9665  0.9654  0.9641  0.9628  0.9627  0.9625  0.9624  0.9597  0.9593  0.9591  0.9585  0.9580  0.9570  0.9570  0.9564  0.9560
[49 : 64]:	0.9557  0.9553  0.9541  0.9540  0.9539  0.9535  0.9533  0.9533  0.9532  0.9532  0.9529  0.9526  0.9526  0.9525  0.9524  0.9524
2023-11-11 09:23:41 [INFO] [evolutionary_search.cc:727] Got 64 candidate(s) with evolutionary search
2023-11-11 09:23:41 [INFO] [evolutionary_search.cc:730] Sending 64 candidates(s) for measurement
2023-11-11 09:24:33 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #705: GFLOPs: 5889.7738. Time: 21.5876 us. Best GFLOPs: 5898.7706
2023-11-11 09:24:33 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #706: GFLOPs: 5885.0084. Time: 21.6051 us. Best GFLOPs: 5898.7706
2023-11-11 09:24:33 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #707: GFLOPs: 5972.1821. Time: 21.2897 us. Best GFLOPs: 5972.1821
2023-11-11 09:24:33 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #708: GFLOPs: 5852.6969. Time: 21.7243 us. Best GFLOPs: 5972.1821
2023-11-11 09:24:33 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #709: GFLOPs: 5887.3305. Time: 21.5965 us. Best GFLOPs: 5972.1821
2023-11-11 09:24:33 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #710: GFLOPs: 5972.2619. Time: 21.2894 us. Best GFLOPs: 5972.2619
2023-11-11 09:24:33 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #711: GFLOPs: 5953.3776. Time: 21.3569 us. Best GFLOPs: 5972.2619
2023-11-11 09:24:33 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #712: GFLOPs: 5884.0337. Time: 21.6086 us. Best GFLOPs: 5972.2619
2023-11-11 09:24:33 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #713: GFLOPs: 5972.5633. Time: 21.2883 us. Best GFLOPs: 5972.5633
2023-11-11 09:24:33 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #714: GFLOPs: 5885.7291. Time: 21.6024 us. Best GFLOPs: 5972.5633
2023-11-11 09:24:33 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #715: GFLOPs: 5796.1816. Time: 21.9362 us. Best GFLOPs: 5972.5633
2023-11-11 09:24:33 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #716: GFLOPs: 5897.1079. Time: 21.5607 us. Best GFLOPs: 5972.5633
2023-11-11 09:24:33 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #717: GFLOPs: 5859.6995. Time: 21.6984 us. Best GFLOPs: 5972.5633
2023-11-11 09:24:33 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #718: GFLOPs: 5877.6165. Time: 21.6322 us. Best GFLOPs: 5972.5633
2023-11-11 09:24:33 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #719: GFLOPs: 5877.3764. Time: 21.6331 us. Best GFLOPs: 5972.5633
2023-11-11 09:24:33 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #720: GFLOPs: 5757.5826. Time: 22.0832 us. Best GFLOPs: 5972.5633
2023-11-11 09:24:33 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #721: GFLOPs: 5897.3055. Time: 21.5600 us. Best GFLOPs: 5972.5633
2023-11-11 09:24:33 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #722: GFLOPs: 5831.5618. Time: 21.8031 us. Best GFLOPs: 5972.5633
2023-11-11 09:24:33 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #723: GFLOPs: 5884.9685. Time: 21.6052 us. Best GFLOPs: 5972.5633
2023-11-11 09:24:33 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #724: GFLOPs: 5858.9562. Time: 21.7011 us. Best GFLOPs: 5972.5633
2023-11-11 09:24:33 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #725: GFLOPs: 5890.2152. Time: 21.5860 us. Best GFLOPs: 5972.5633
2023-11-11 09:24:33 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #726: GFLOPs: 5851.3514. Time: 21.7293 us. Best GFLOPs: 5972.5633
2023-11-11 09:24:33 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #727: GFLOPs: 5789.3972. Time: 21.9619 us. Best GFLOPs: 5972.5633
2023-11-11 09:24:33 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #728: GFLOPs: 5764.8198. Time: 22.0555 us. Best GFLOPs: 5972.5633
2023-11-11 09:24:33 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #729: GFLOPs: 5642.5834. Time: 22.5333 us. Best GFLOPs: 5972.5633
2023-11-11 09:24:33 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #730: GFLOPs: 5789.4358. Time: 21.9617 us. Best GFLOPs: 5972.5633
2023-11-11 09:24:33 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #731: GFLOPs: 5877.0969. Time: 21.6341 us. Best GFLOPs: 5972.5633
2023-11-11 09:24:33 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #732: GFLOPs: 5762.1720. Time: 22.0656 us. Best GFLOPs: 5972.5633
2023-11-11 09:24:33 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #733: GFLOPs: 5859.4325. Time: 21.6994 us. Best GFLOPs: 5972.5633
2023-11-11 09:24:33 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #734: GFLOPs: 5859.1547. Time: 21.7004 us. Best GFLOPs: 5972.5633
2023-11-11 09:24:33 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #735: GFLOPs: 5705.2565. Time: 22.2858 us. Best GFLOPs: 5972.5633
2023-11-11 09:24:33 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #736: GFLOPs: 5922.7920. Time: 21.4672 us. Best GFLOPs: 5972.5633
2023-11-11 09:24:33 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #737: GFLOPs: 5810.8514. Time: 21.8808 us. Best GFLOPs: 5972.5633
2023-11-11 09:24:33 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #738: GFLOPs: 5852.3802. Time: 21.7255 us. Best GFLOPs: 5972.5633
2023-11-11 09:24:33 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #739: GFLOPs: 5782.7187. Time: 21.9872 us. Best GFLOPs: 5972.5633
2023-11-11 09:24:33 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #740: GFLOPs: 5815.3804. Time: 21.8637 us. Best GFLOPs: 5972.5633
2023-11-11 09:24:33 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #741: GFLOPs: 5849.8093. Time: 21.7351 us. Best GFLOPs: 5972.5633
2023-11-11 09:24:33 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #742: GFLOPs: 5885.9260. Time: 21.6017 us. Best GFLOPs: 5972.5633
2023-11-11 09:24:33 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #743: GFLOPs: 5768.1726. Time: 22.0427 us. Best GFLOPs: 5972.5633
2023-11-11 09:24:33 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #744: GFLOPs: 5873.5871. Time: 21.6471 us. Best GFLOPs: 5972.5633
2023-11-11 09:24:33 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #745: GFLOPs: 5814.2114. Time: 21.8681 us. Best GFLOPs: 5972.5633
2023-11-11 09:24:33 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #746: GFLOPs: 5577.6507. Time: 22.7956 us. Best GFLOPs: 5972.5633
2023-11-11 09:24:33 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #747: GFLOPs: 5305.9667. Time: 23.9628 us. Best GFLOPs: 5972.5633
2023-11-11 09:24:33 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #748: GFLOPs: 5896.1534. Time: 21.5642 us. Best GFLOPs: 5972.5633
2023-11-11 09:24:33 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #749: GFLOPs: 5685.0440. Time: 22.3650 us. Best GFLOPs: 5972.5633
2023-11-11 09:24:33 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #750: GFLOPs: 5752.6023. Time: 22.1023 us. Best GFLOPs: 5972.5633
2023-11-11 09:24:33 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #751: GFLOPs: 5722.2625. Time: 22.2195 us. Best GFLOPs: 5972.5633
2023-11-11 09:24:33 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #752: GFLOPs: 5302.9554. Time: 23.9764 us. Best GFLOPs: 5972.5633
2023-11-11 09:24:33 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #753: GFLOPs: 5878.6535. Time: 21.6284 us. Best GFLOPs: 5972.5633
2023-11-11 09:24:33 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #754: GFLOPs: 5546.5546. Time: 22.9234 us. Best GFLOPs: 5972.5633
2023-11-11 09:24:33 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #755: GFLOPs: 5603.1037. Time: 22.6921 us. Best GFLOPs: 5972.5633
2023-11-11 09:24:33 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #756: GFLOPs: 5619.8829. Time: 22.6243 us. Best GFLOPs: 5972.5633
2023-11-11 09:24:33 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #757: GFLOPs: 5724.7170. Time: 22.2100 us. Best GFLOPs: 5972.5633
2023-11-11 09:24:33 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #758: GFLOPs: 5603.6635. Time: 22.6898 us. Best GFLOPs: 5972.5633
2023-11-11 09:24:33 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #759: GFLOPs: 5615.2643. Time: 22.6429 us. Best GFLOPs: 5972.5633
2023-11-11 09:24:33 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #760: GFLOPs: 5613.7275. Time: 22.6491 us. Best GFLOPs: 5972.5633
2023-11-11 09:24:33 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #761: GFLOPs: 5615.2014. Time: 22.6432 us. Best GFLOPs: 5972.5633
2023-11-11 09:24:33 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #762: GFLOPs: 5856.1817. Time: 21.7114 us. Best GFLOPs: 5972.5633
2023-11-11 09:24:33 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #763: GFLOPs: 5782.6404. Time: 21.9875 us. Best GFLOPs: 5972.5633
2023-11-11 09:24:33 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #764: GFLOPs: 5683.0081. Time: 22.3730 us. Best GFLOPs: 5972.5633
2023-11-11 09:24:33 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #765: GFLOPs: 5564.9089. Time: 22.8478 us. Best GFLOPs: 5972.5633
2023-11-11 09:24:33 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #766: GFLOPs: 590.7207. Time: 215.2388 us. Best GFLOPs: 5972.5633
2023-11-11 09:24:33 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #767: GFLOPs: 303.0090. Time: 419.6113 us. Best GFLOPs: 5972.5633
2023-11-11 09:24:33 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #768: GFLOPs: 134.3692. Time: 946.2436 us. Best GFLOPs: 5972.5633
2023-11-11 09:54:38 [INFO] [evolutionary_search.cc:713] Generating candidates......
2023-11-11 09:54:41 [INFO] [evolutionary_search.cc:715] Picked top 102 candidate(s) from database
2023-11-11 09:54:48 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310fb046d8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310f554558)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310dc5ddb8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310bbcd2d8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56310b1dbf38)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310dc5d048)]: 394 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310dc26e38)]: 0 failure(s)
2023-11-11 09:54:54 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310fb046d8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310f554558)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310dc5ddb8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310bbcd2d8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56310b1dbf38)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310dc5d048)]: 793 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310dc26e38)]: 0 failure(s)
2023-11-11 09:55:01 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310fb046d8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310f554558)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310dc5ddb8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310bbcd2d8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56310b1dbf38)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310dc5d048)]: 1183 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310dc26e38)]: 0 failure(s)
2023-11-11 09:55:07 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310fb046d8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310f554558)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310dc5ddb8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310bbcd2d8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56310b1dbf38)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310dc5d048)]: 1574 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310dc26e38)]: 0 failure(s)
2023-11-11 09:55:07 [INFO] [evolutionary_search.cc:723] Sampled 66 candidate(s)
2023-11-11 09:55:29 [INFO] [evolutionary_search.cc:621] Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310fb046d8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310f554558)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310dc5ddb8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310bbcd2d8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56310b1dbf38)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310dc5d048)]: 238 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310dc26e38)]: 0 failure(s)
2023-11-11 09:55:53 [INFO] [evolutionary_search.cc:621] Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310fb046d8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310f554558)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310dc5ddb8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310bbcd2d8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56310b1dbf38)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310dc5d048)]: 210 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310dc26e38)]: 0 failure(s)
2023-11-11 09:56:17 [INFO] [evolutionary_search.cc:621] Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310fb046d8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310f554558)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310dc5ddb8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310bbcd2d8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56310b1dbf38)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310dc5d048)]: 192 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310dc26e38)]: 0 failure(s)
2023-11-11 09:56:41 [INFO] [evolutionary_search.cc:621] Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310fb046d8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310f554558)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310dc5ddb8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310bbcd2d8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56310b1dbf38)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310dc5d048)]: 184 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310dc26e38)]: 0 failure(s)
2023-11-11 09:56:47 [INFO] [evolutionary_search.cc:649] Scores of the best 64 candidates:
[1 : 16]:	1.0557  1.0012  0.9890  0.9856  0.9828  0.9826  0.9823  0.9821  0.9813  0.9813  0.9801  0.9799  0.9796  0.9795  0.9792  0.9789
[17 : 32]:	0.9789  0.9788  0.9788  0.9779  0.9775  0.9775  0.9773  0.9770  0.9770  0.9770  0.9769  0.9768  0.9768  0.9767  0.9763  0.9763
[33 : 48]:	0.9762  0.9761  0.9760  0.9758  0.9754  0.9754  0.9753  0.9750  0.9748  0.9747  0.9747  0.9747  0.9747  0.9741  0.9741  0.9739
[49 : 64]:	0.9735  0.9734  0.9725  0.9721  0.9719  0.9719  0.9717  0.9716  0.9715  0.9715  0.9711  0.9711  0.9711  0.9710  0.9706  0.9705
2023-11-11 09:56:47 [INFO] [evolutionary_search.cc:727] Got 64 candidate(s) with evolutionary search
2023-11-11 09:56:47 [INFO] [evolutionary_search.cc:730] Sending 64 candidates(s) for measurement
2023-11-11 09:57:22 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #769: GFLOPs: 3442.9314. Time: 36.9296 us. Best GFLOPs: 5972.5633
2023-11-11 09:57:22 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #770: GFLOPs: 3414.9036. Time: 37.2327 us. Best GFLOPs: 5972.5633
2023-11-11 09:57:22 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #771: GFLOPs: 5869.7238. Time: 21.6613 us. Best GFLOPs: 5972.5633
2023-11-11 09:57:22 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #772: GFLOPs: 5871.3163. Time: 21.6554 us. Best GFLOPs: 5972.5633
2023-11-11 09:57:22 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #773: GFLOPs: 5875.4211. Time: 21.6403 us. Best GFLOPs: 5972.5633
2023-11-11 09:57:22 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #774: GFLOPs: 5820.1118. Time: 21.8460 us. Best GFLOPs: 5972.5633
2023-11-11 09:57:22 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #775: GFLOPs: 5808.1612. Time: 21.8909 us. Best GFLOPs: 5972.5633
2023-11-11 09:57:22 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #776: GFLOPs: 5873.9061. Time: 21.6459 us. Best GFLOPs: 5972.5633
2023-11-11 09:57:22 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #777: GFLOPs: 5807.4987. Time: 21.8934 us. Best GFLOPs: 5972.5633
2023-11-11 09:57:22 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #778: GFLOPs: 5800.0657. Time: 21.9215 us. Best GFLOPs: 5972.5633
2023-11-11 09:57:22 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #779: GFLOPs: 5750.7125. Time: 22.1096 us. Best GFLOPs: 5972.5633
2023-11-11 09:57:22 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #780: GFLOPs: 5807.7710. Time: 21.8924 us. Best GFLOPs: 5972.5633
2023-11-11 09:57:22 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #781: GFLOPs: 5805.1108. Time: 21.9024 us. Best GFLOPs: 5972.5633
2023-11-11 09:57:22 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #782: GFLOPs: 5738.2559. Time: 22.1576 us. Best GFLOPs: 5972.5633
2023-11-11 09:57:22 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #783: GFLOPs: 5836.9887. Time: 21.7828 us. Best GFLOPs: 5972.5633
2023-11-11 09:57:22 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #784: GFLOPs: 5814.7162. Time: 21.8662 us. Best GFLOPs: 5972.5633
2023-11-11 09:57:22 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #785: GFLOPs: 5807.4825. Time: 21.8935 us. Best GFLOPs: 5972.5633
2023-11-11 09:57:22 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #786: GFLOPs: 5750.5493. Time: 22.1102 us. Best GFLOPs: 5972.5633
2023-11-11 09:57:22 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #787: GFLOPs: 5890.2493. Time: 21.5858 us. Best GFLOPs: 5972.5633
2023-11-11 09:57:22 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #788: GFLOPs: 5866.9392. Time: 21.6716 us. Best GFLOPs: 5972.5633
2023-11-11 09:57:22 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #789: GFLOPs: 5741.0367. Time: 22.1469 us. Best GFLOPs: 5972.5633
2023-11-11 09:57:22 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #790: GFLOPs: 5852.6178. Time: 21.7246 us. Best GFLOPs: 5972.5633
2023-11-11 09:57:22 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #791: GFLOPs: 5889.1222. Time: 21.5900 us. Best GFLOPs: 5972.5633
2023-11-11 09:57:22 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #792: GFLOPs: 5805.6679. Time: 21.9003 us. Best GFLOPs: 5972.5633
2023-11-11 09:57:22 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #793: GFLOPs: 5817.6082. Time: 21.8554 us. Best GFLOPs: 5972.5633
2023-11-11 09:57:22 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #794: GFLOPs: 5760.2268. Time: 22.0731 us. Best GFLOPs: 5972.5633
2023-11-11 09:57:22 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #795: GFLOPs: 5759.3500. Time: 22.0764 us. Best GFLOPs: 5972.5633
2023-11-11 09:57:22 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #796: GFLOPs: 5807.5614. Time: 21.8932 us. Best GFLOPs: 5972.5633
2023-11-11 09:57:22 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #797: GFLOPs: 5740.6504. Time: 22.1484 us. Best GFLOPs: 5972.5633
2023-11-11 09:57:22 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #798: GFLOPs: 5737.3680. Time: 22.1610 us. Best GFLOPs: 5972.5633
2023-11-11 09:57:22 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #799: GFLOPs: 5757.3493. Time: 22.0841 us. Best GFLOPs: 5972.5633
2023-11-11 09:57:22 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #800: GFLOPs: 5737.4456. Time: 22.1607 us. Best GFLOPs: 5972.5633
2023-11-11 09:57:22 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #801: GFLOPs: 5752.3455. Time: 22.1033 us. Best GFLOPs: 5972.5633
2023-11-11 09:57:22 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #802: GFLOPs: 5800.6897. Time: 21.9191 us. Best GFLOPs: 5972.5633
2023-11-11 09:57:22 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #803: GFLOPs: 5809.2864. Time: 21.8867 us. Best GFLOPs: 5972.5633
2023-11-11 09:57:22 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #804: GFLOPs: 5800.3870. Time: 21.9203 us. Best GFLOPs: 5972.5633
2023-11-11 09:57:22 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #805: GFLOPs: 5735.0924. Time: 22.1698 us. Best GFLOPs: 5972.5633
2023-11-11 09:57:22 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #806: GFLOPs: 5870.8210. Time: 21.6573 us. Best GFLOPs: 5972.5633
2023-11-11 09:57:22 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #807: GFLOPs: 5734.9378. Time: 22.1704 us. Best GFLOPs: 5972.5633
2023-11-11 09:57:22 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #808: GFLOPs: 5668.1819. Time: 22.4315 us. Best GFLOPs: 5972.5633
2023-11-11 09:57:22 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #809: GFLOPs: 5802.8206. Time: 21.9111 us. Best GFLOPs: 5972.5633
2023-11-11 09:57:22 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #810: GFLOPs: 5801.8815. Time: 21.9146 us. Best GFLOPs: 5972.5633
2023-11-11 09:57:22 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #811: GFLOPs: 5802.5153. Time: 21.9122 us. Best GFLOPs: 5972.5633
2023-11-11 09:57:22 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #812: GFLOPs: 5748.7744. Time: 22.1171 us. Best GFLOPs: 5972.5633
2023-11-11 09:57:22 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #813: GFLOPs: 5750.8716. Time: 22.1090 us. Best GFLOPs: 5972.5633
2023-11-11 09:57:22 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #814: GFLOPs: 5871.4753. Time: 21.6549 us. Best GFLOPs: 5972.5633
2023-11-11 09:57:22 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #815: GFLOPs: 5793.3108. Time: 21.9470 us. Best GFLOPs: 5972.5633
2023-11-11 09:57:22 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #816: GFLOPs: 5829.7942. Time: 21.8097 us. Best GFLOPs: 5972.5633
2023-11-11 09:57:22 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #817: GFLOPs: 5860.9401. Time: 21.6938 us. Best GFLOPs: 5972.5633
2023-11-11 09:57:22 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #818: GFLOPs: 5757.6214. Time: 22.0831 us. Best GFLOPs: 5972.5633
2023-11-11 09:57:22 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #819: GFLOPs: 5741.9277. Time: 22.1434 us. Best GFLOPs: 5972.5633
2023-11-11 09:57:22 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #820: GFLOPs: 5782.2904. Time: 21.9889 us. Best GFLOPs: 5972.5633
2023-11-11 09:57:22 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #821: GFLOPs: 5807.3425. Time: 21.8940 us. Best GFLOPs: 5972.5633
2023-11-11 09:57:22 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #822: GFLOPs: 5667.9530. Time: 22.4324 us. Best GFLOPs: 5972.5633
2023-11-11 09:57:22 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #823: GFLOPs: 5803.3730. Time: 21.9090 us. Best GFLOPs: 5972.5633
2023-11-11 09:57:22 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #824: GFLOPs: 5790.0983. Time: 21.9592 us. Best GFLOPs: 5972.5633
2023-11-11 09:57:22 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #825: GFLOPs: 5783.6991. Time: 21.9835 us. Best GFLOPs: 5972.5633
2023-11-11 09:57:22 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #826: GFLOPs: 5806.6124. Time: 21.8968 us. Best GFLOPs: 5972.5633
2023-11-11 09:57:22 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #827: GFLOPs: 5738.2174. Time: 22.1577 us. Best GFLOPs: 5972.5633
2023-11-11 09:57:22 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #828: GFLOPs: 5800.9266. Time: 21.9182 us. Best GFLOPs: 5972.5633
2023-11-11 09:57:22 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #829: GFLOPs: 5752.2310. Time: 22.1038 us. Best GFLOPs: 5972.5633
2023-11-11 09:57:22 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #830: GFLOPs: 49.4711. Time: 2570.1086 us. Best GFLOPs: 5972.5633
2023-11-11 09:57:22 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #831: GFLOPs: 531.1568. Time: 239.3756 us. Best GFLOPs: 5972.5633
2023-11-11 09:57:22 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #832: GFLOPs: 96.8750. Time: 1312.4742 us. Best GFLOPs: 5972.5633
2023-11-11 10:43:14 [INFO] [evolutionary_search.cc:713] Generating candidates......
2023-11-11 10:43:17 [INFO] [evolutionary_search.cc:715] Picked top 102 candidate(s) from database
2023-11-11 10:43:24 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310fb046d8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310f554558)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310dc5ddb8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310bbcd2d8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56310b1dbf38)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310dc5d048)]: 401 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310dc26e38)]: 0 failure(s)
2023-11-11 10:43:31 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310fb046d8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310f554558)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310dc5ddb8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310bbcd2d8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56310b1dbf38)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310dc5d048)]: 791 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310dc26e38)]: 0 failure(s)
2023-11-11 10:43:38 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310fb046d8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310f554558)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310dc5ddb8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310bbcd2d8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56310b1dbf38)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310dc5d048)]: 1182 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310dc26e38)]: 0 failure(s)
2023-11-11 10:43:45 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310fb046d8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310f554558)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310dc5ddb8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310bbcd2d8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56310b1dbf38)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310dc5d048)]: 1576 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310dc26e38)]: 0 failure(s)
2023-11-11 10:43:45 [INFO] [evolutionary_search.cc:723] Sampled 64 candidate(s)
2023-11-11 10:44:08 [INFO] [evolutionary_search.cc:621] Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310fb046d8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310f554558)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310dc5ddb8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310bbcd2d8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56310b1dbf38)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310dc5d048)]: 232 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310dc26e38)]: 0 failure(s)
2023-11-11 10:44:33 [INFO] [evolutionary_search.cc:621] Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310fb046d8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310f554558)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310dc5ddb8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310bbcd2d8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56310b1dbf38)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310dc5d048)]: 199 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310dc26e38)]: 0 failure(s)
2023-11-11 10:44:59 [INFO] [evolutionary_search.cc:621] Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310fb046d8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310f554558)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310dc5ddb8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310bbcd2d8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56310b1dbf38)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310dc5d048)]: 223 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310dc26e38)]: 0 failure(s)
2023-11-11 10:45:24 [INFO] [evolutionary_search.cc:621] Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310fb046d8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310f554558)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310dc5ddb8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310bbcd2d8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56310b1dbf38)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310dc5d048)]: 178 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310dc26e38)]: 0 failure(s)
2023-11-11 10:45:30 [INFO] [evolutionary_search.cc:649] Scores of the best 64 candidates:
[1 : 16]:	0.9895  0.9888  0.9887  0.9884  0.9883  0.9880  0.9878  0.9866  0.9864  0.9864  0.9856  0.9848  0.9847  0.9840  0.9839  0.9838
[17 : 32]:	0.9838  0.9833  0.9832  0.9832  0.9832  0.9820  0.9820  0.9818  0.9816  0.9814  0.9813  0.9811  0.9811  0.9805  0.9795  0.9794
[33 : 48]:	0.9793  0.9792  0.9791  0.9790  0.9789  0.9785  0.9783  0.9782  0.9782  0.9781  0.9779  0.9778  0.9777  0.9773  0.9773  0.9772
[49 : 64]:	0.9772  0.9771  0.9771  0.9771  0.9771  0.9767  0.9765  0.9763  0.9763  0.9763  0.9762  0.9760  0.9759  0.9757  0.9756  0.9754
2023-11-11 10:45:30 [INFO] [evolutionary_search.cc:727] Got 64 candidate(s) with evolutionary search
2023-11-11 10:45:31 [INFO] [evolutionary_search.cc:730] Sending 64 candidates(s) for measurement
2023-11-11 10:46:07 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #833: GFLOPs: 5760.4055. Time: 22.0724 us. Best GFLOPs: 5972.5633
2023-11-11 10:46:07 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #834: GFLOPs: 5799.3880. Time: 21.9240 us. Best GFLOPs: 5972.5633
2023-11-11 10:46:07 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #835: GFLOPs: 5813.9741. Time: 21.8690 us. Best GFLOPs: 5972.5633
2023-11-11 10:46:07 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #836: GFLOPs: 5749.9761. Time: 22.1124 us. Best GFLOPs: 5972.5633
2023-11-11 10:46:07 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #837: GFLOPs: 5728.0831. Time: 22.1970 us. Best GFLOPs: 5972.5633
2023-11-11 10:46:07 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #838: GFLOPs: 5743.9359. Time: 22.1357 us. Best GFLOPs: 5972.5633
2023-11-11 10:46:07 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #839: GFLOPs: 5813.1690. Time: 21.8721 us. Best GFLOPs: 5972.5633
2023-11-11 10:46:07 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #840: GFLOPs: 5841.2425. Time: 21.7669 us. Best GFLOPs: 5972.5633
2023-11-11 10:46:07 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #841: GFLOPs: 5734.8518. Time: 22.1708 us. Best GFLOPs: 5972.5633
2023-11-11 10:46:07 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #842: GFLOPs: 5832.3476. Time: 21.8001 us. Best GFLOPs: 5972.5633
2023-11-11 10:46:07 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #843: GFLOPs: 5815.8101. Time: 21.8621 us. Best GFLOPs: 5972.5633
2023-11-11 10:46:07 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #844: GFLOPs: 5813.3102. Time: 21.8715 us. Best GFLOPs: 5972.5633
2023-11-11 10:46:07 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #845: GFLOPs: 5747.8760. Time: 22.1205 us. Best GFLOPs: 5972.5633
2023-11-11 10:46:07 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #846: GFLOPs: 5748.7543. Time: 22.1171 us. Best GFLOPs: 5972.5633
2023-11-11 10:46:07 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #847: GFLOPs: 5815.7573. Time: 21.8623 us. Best GFLOPs: 5972.5633
2023-11-11 10:46:07 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #848: GFLOPs: 5746.5670. Time: 22.1256 us. Best GFLOPs: 5972.5633
2023-11-11 10:46:07 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #849: GFLOPs: 5786.8427. Time: 21.9716 us. Best GFLOPs: 5972.5633
2023-11-11 10:46:07 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #850: GFLOPs: 5741.7376. Time: 22.1442 us. Best GFLOPs: 5972.5633
2023-11-11 10:46:07 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #851: GFLOPs: 5832.8537. Time: 21.7982 us. Best GFLOPs: 5972.5633
2023-11-11 10:46:07 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #852: GFLOPs: 5815.8881. Time: 21.8618 us. Best GFLOPs: 5972.5633
2023-11-11 10:46:07 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #853: GFLOPs: 5748.1550. Time: 22.1194 us. Best GFLOPs: 5972.5633
2023-11-11 10:46:07 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #854: GFLOPs: 5746.8843. Time: 22.1243 us. Best GFLOPs: 5972.5633
2023-11-11 10:46:07 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #855: GFLOPs: 5816.9044. Time: 21.8580 us. Best GFLOPs: 5972.5633
2023-11-11 10:46:07 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #856: GFLOPs: 5746.8843. Time: 22.1243 us. Best GFLOPs: 5972.5633
2023-11-11 10:46:07 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #857: GFLOPs: 5706.3406. Time: 22.2815 us. Best GFLOPs: 5972.5633
2023-11-11 10:46:07 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #858: GFLOPs: 5804.9871. Time: 21.9029 us. Best GFLOPs: 5972.5633
2023-11-11 10:46:07 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #859: GFLOPs: 5759.8479. Time: 22.0745 us. Best GFLOPs: 5972.5633
2023-11-11 10:46:07 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #860: GFLOPs: 5681.8556. Time: 22.3775 us. Best GFLOPs: 5972.5633
2023-11-11 10:46:07 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #861: GFLOPs: 5749.3273. Time: 22.1149 us. Best GFLOPs: 5972.5633
2023-11-11 10:46:07 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #862: GFLOPs: 5793.7373. Time: 21.9454 us. Best GFLOPs: 5972.5633
2023-11-11 10:46:07 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #863: GFLOPs: 5679.6088. Time: 22.3864 us. Best GFLOPs: 5972.5633
2023-11-11 10:46:07 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #864: GFLOPs: 5814.8721. Time: 21.8657 us. Best GFLOPs: 5972.5633
2023-11-11 10:46:07 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #865: GFLOPs: 5750.4022. Time: 22.1108 us. Best GFLOPs: 5972.5633
2023-11-11 10:46:07 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #866: GFLOPs: 5748.8523. Time: 22.1168 us. Best GFLOPs: 5972.5633
2023-11-11 10:46:07 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #867: GFLOPs: 5720.0935. Time: 22.2280 us. Best GFLOPs: 5972.5633
2023-11-11 10:46:07 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #868: GFLOPs: 5748.7543. Time: 22.1171 us. Best GFLOPs: 5972.5633
2023-11-11 10:46:07 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #869: GFLOPs: 5740.6504. Time: 22.1484 us. Best GFLOPs: 5972.5633
2023-11-11 10:46:07 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #870: GFLOPs: 5812.2562. Time: 21.8755 us. Best GFLOPs: 5972.5633
2023-11-11 10:46:07 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #871: GFLOPs: 5806.4467. Time: 21.8974 us. Best GFLOPs: 5972.5633
2023-11-11 10:46:07 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #872: GFLOPs: 5787.3566. Time: 21.9696 us. Best GFLOPs: 5972.5633
2023-11-11 10:46:07 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #873: GFLOPs: 5787.3455. Time: 21.9697 us. Best GFLOPs: 5972.5633
2023-11-11 10:46:07 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #874: GFLOPs: 5756.9748. Time: 22.0856 us. Best GFLOPs: 5972.5633
2023-11-11 10:46:07 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #875: GFLOPs: 5741.2295. Time: 22.1461 us. Best GFLOPs: 5972.5633
2023-11-11 10:46:07 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #876: GFLOPs: 5692.8374. Time: 22.3344 us. Best GFLOPs: 5972.5633
2023-11-11 10:46:07 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #877: GFLOPs: 5678.3987. Time: 22.3912 us. Best GFLOPs: 5972.5633
2023-11-11 10:46:07 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #878: GFLOPs: 5767.9856. Time: 22.0434 us. Best GFLOPs: 5972.5633
2023-11-11 10:46:07 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #879: GFLOPs: 5816.2731. Time: 21.8604 us. Best GFLOPs: 5972.5633
2023-11-11 10:46:07 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #880: GFLOPs: 5748.2710. Time: 22.1190 us. Best GFLOPs: 5972.5633
2023-11-11 10:46:07 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #881: GFLOPs: 5748.7357. Time: 22.1172 us. Best GFLOPs: 5972.5633
2023-11-11 10:46:07 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #882: GFLOPs: 5626.9774. Time: 22.5958 us. Best GFLOPs: 5972.5633
2023-11-11 10:46:07 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #883: GFLOPs: 5787.5778. Time: 21.9688 us. Best GFLOPs: 5972.5633
2023-11-11 10:46:07 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #884: GFLOPs: 5815.5754. Time: 21.8630 us. Best GFLOPs: 5972.5633
2023-11-11 10:46:07 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #885: GFLOPs: 5741.3568. Time: 22.1456 us. Best GFLOPs: 5972.5633
2023-11-11 10:46:07 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #886: GFLOPs: 5627.4783. Time: 22.5938 us. Best GFLOPs: 5972.5633
2023-11-11 10:46:07 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #887: GFLOPs: 5815.8881. Time: 21.8618 us. Best GFLOPs: 5972.5633
2023-11-11 10:46:07 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #888: GFLOPs: 5816.2008. Time: 21.8607 us. Best GFLOPs: 5972.5633
2023-11-11 10:46:07 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #889: GFLOPs: 5749.4559. Time: 22.1144 us. Best GFLOPs: 5972.5633
2023-11-11 10:46:07 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #890: GFLOPs: 5809.4084. Time: 21.8862 us. Best GFLOPs: 5972.5633
2023-11-11 10:46:07 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #891: GFLOPs: 5785.8755. Time: 21.9752 us. Best GFLOPs: 5972.5633
2023-11-11 10:46:07 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #892: GFLOPs: 5764.7420. Time: 22.0558 us. Best GFLOPs: 5972.5633
2023-11-11 10:46:07 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #893: GFLOPs: 5735.0924. Time: 22.1698 us. Best GFLOPs: 5972.5633
2023-11-11 10:46:07 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #894: GFLOPs: 138.9305. Time: 915.1767 us. Best GFLOPs: 5972.5633
2023-11-11 10:46:07 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #895: GFLOPs: 2596.8076. Time: 48.9624 us. Best GFLOPs: 5972.5633
2023-11-11 10:46:07 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #896: GFLOPs: 81.8638. Time: 1553.1401 us. Best GFLOPs: 5972.5633
2023-11-11 10:58:34 [INFO] [evolutionary_search.cc:713] Generating candidates......
2023-11-11 10:58:37 [INFO] [evolutionary_search.cc:715] Picked top 102 candidate(s) from database
2023-11-11 10:58:44 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310fb046d8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310f554558)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310dc5ddb8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310bbcd2d8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56310b1dbf38)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310dc5d048)]: 396 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310dc26e38)]: 0 failure(s)
2023-11-11 10:58:51 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310fb046d8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310f554558)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310dc5ddb8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310bbcd2d8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56310b1dbf38)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310dc5d048)]: 790 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310dc26e38)]: 0 failure(s)
2023-11-11 10:58:57 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310fb046d8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310f554558)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310dc5ddb8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310bbcd2d8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56310b1dbf38)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310dc5d048)]: 1190 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310dc26e38)]: 0 failure(s)
2023-11-11 10:59:04 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310fb046d8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310f554558)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310dc5ddb8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310bbcd2d8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56310b1dbf38)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310dc5d048)]: 1590 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310dc26e38)]: 0 failure(s)
2023-11-11 10:59:04 [INFO] [evolutionary_search.cc:723] Sampled 50 candidate(s)
2023-11-11 10:59:27 [INFO] [evolutionary_search.cc:621] Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310fb046d8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310f554558)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310dc5ddb8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310bbcd2d8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56310b1dbf38)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310dc5d048)]: 228 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310dc26e38)]: 0 failure(s)
2023-11-11 10:59:53 [INFO] [evolutionary_search.cc:621] Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310fb046d8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310f554558)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310dc5ddb8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310bbcd2d8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56310b1dbf38)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310dc5d048)]: 223 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310dc26e38)]: 0 failure(s)
2023-11-11 11:00:19 [INFO] [evolutionary_search.cc:621] Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310fb046d8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310f554558)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310dc5ddb8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310bbcd2d8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56310b1dbf38)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310dc5d048)]: 209 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310dc26e38)]: 0 failure(s)
2023-11-11 11:00:43 [INFO] [evolutionary_search.cc:621] Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310fb046d8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310f554558)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310dc5ddb8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310bbcd2d8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56310b1dbf38)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310dc5d048)]: 183 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310dc26e38)]: 0 failure(s)
2023-11-11 11:00:50 [INFO] [evolutionary_search.cc:649] Scores of the best 64 candidates:
[1 : 16]:	0.9932  0.9893  0.9877  0.9870  0.9868  0.9867  0.9867  0.9859  0.9858  0.9853  0.9851  0.9844  0.9842  0.9841  0.9840  0.9840
[17 : 32]:	0.9835  0.9832  0.9831  0.9831  0.9830  0.9830  0.9829  0.9822  0.9820  0.9818  0.9818  0.9816  0.9813  0.9812  0.9811  0.9808
[33 : 48]:	0.9808  0.9807  0.9806  0.9806  0.9803  0.9802  0.9801  0.9800  0.9798  0.9796  0.9796  0.9796  0.9791  0.9790  0.9790  0.9789
[49 : 64]:	0.9789  0.9788  0.9787  0.9784  0.9784  0.9783  0.9782  0.9781  0.9777  0.9772  0.9771  0.9770  0.9769  0.9769  0.9768  0.9767
2023-11-11 11:00:50 [INFO] [evolutionary_search.cc:727] Got 64 candidate(s) with evolutionary search
2023-11-11 11:00:50 [INFO] [evolutionary_search.cc:730] Sending 64 candidates(s) for measurement
2023-11-11 11:01:27 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #897: GFLOPs: 5825.2037. Time: 21.8269 us. Best GFLOPs: 5972.5633
2023-11-11 11:01:27 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #898: GFLOPs: 5869.1740. Time: 21.6634 us. Best GFLOPs: 5972.5633
2023-11-11 11:01:27 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #899: GFLOPs: 5886.4384. Time: 21.5998 us. Best GFLOPs: 5972.5633
2023-11-11 11:01:27 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #900: GFLOPs: 5946.9295. Time: 21.3801 us. Best GFLOPs: 5972.5633
2023-11-11 11:01:27 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #901: GFLOPs: 5887.3854. Time: 21.5963 us. Best GFLOPs: 5972.5633
2023-11-11 11:01:27 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #902: GFLOPs: 5844.5161. Time: 21.7547 us. Best GFLOPs: 5972.5633
2023-11-11 11:01:27 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #903: GFLOPs: 5932.7118. Time: 21.4313 us. Best GFLOPs: 5972.5633
2023-11-11 11:01:27 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #904: GFLOPs: 5873.6866. Time: 21.6467 us. Best GFLOPs: 5972.5633
2023-11-11 11:01:27 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #905: GFLOPs: 5858.1236. Time: 21.7042 us. Best GFLOPs: 5972.5633
2023-11-11 11:01:27 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #906: GFLOPs: 4986.3160. Time: 25.4990 us. Best GFLOPs: 5972.5633
2023-11-11 11:01:27 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #907: GFLOPs: 5823.1265. Time: 21.8347 us. Best GFLOPs: 5972.5633
2023-11-11 11:01:27 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #908: GFLOPs: 5888.8525. Time: 21.5910 us. Best GFLOPs: 5972.5633
2023-11-11 11:01:27 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #909: GFLOPs: 5887.2499. Time: 21.5968 us. Best GFLOPs: 5972.5633
2023-11-11 11:01:27 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #910: GFLOPs: 5830.6188. Time: 21.8066 us. Best GFLOPs: 5972.5633
2023-11-11 11:01:27 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #911: GFLOPs: 5846.4403. Time: 21.7476 us. Best GFLOPs: 5972.5633
2023-11-11 11:01:27 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #912: GFLOPs: 5828.7734. Time: 21.8135 us. Best GFLOPs: 5972.5633
2023-11-11 11:01:27 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #913: GFLOPs: 5804.9482. Time: 21.9030 us. Best GFLOPs: 5972.5633
2023-11-11 11:01:27 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #914: GFLOPs: 5887.2279. Time: 21.5969 us. Best GFLOPs: 5972.5633
2023-11-11 11:01:27 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #915: GFLOPs: 5847.8729. Time: 21.7423 us. Best GFLOPs: 5972.5633
2023-11-11 11:01:27 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #916: GFLOPs: 5846.1290. Time: 21.7487 us. Best GFLOPs: 5972.5633
2023-11-11 11:01:27 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #917: GFLOPs: 5895.0104. Time: 21.5684 us. Best GFLOPs: 5972.5633
2023-11-11 11:01:27 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #918: GFLOPs: 5877.2563. Time: 21.6336 us. Best GFLOPs: 5972.5633
2023-11-11 11:01:27 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #919: GFLOPs: 5842.7413. Time: 21.7614 us. Best GFLOPs: 5972.5633
2023-11-11 11:01:27 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #920: GFLOPs: 5887.8109. Time: 21.5948 us. Best GFLOPs: 5972.5633
2023-11-11 11:01:27 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #921: GFLOPs: 5936.0636. Time: 21.4192 us. Best GFLOPs: 5972.5633
2023-11-11 11:01:27 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #922: GFLOPs: 5853.2111. Time: 21.7224 us. Best GFLOPs: 5972.5633
2023-11-11 11:01:27 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #923: GFLOPs: 5937.3633. Time: 21.4146 us. Best GFLOPs: 5972.5633
2023-11-11 11:01:27 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #924: GFLOPs: 5882.5562. Time: 21.6141 us. Best GFLOPs: 5972.5633
2023-11-11 11:01:27 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #925: GFLOPs: 5813.0533. Time: 21.8725 us. Best GFLOPs: 5972.5633
2023-11-11 11:01:27 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #926: GFLOPs: 5894.2194. Time: 21.5713 us. Best GFLOPs: 5972.5633
2023-11-11 11:01:27 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #927: GFLOPs: 5888.5692. Time: 21.5920 us. Best GFLOPs: 5972.5633
2023-11-11 11:01:27 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #928: GFLOPs: 5884.1285. Time: 21.6083 us. Best GFLOPs: 5972.5633
2023-11-11 11:01:27 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #929: GFLOPs: 5875.8597. Time: 21.6387 us. Best GFLOPs: 5972.5633
2023-11-11 11:01:27 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #930: GFLOPs: 5841.7947. Time: 21.7649 us. Best GFLOPs: 5972.5633
2023-11-11 11:01:27 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #931: GFLOPs: 5876.4584. Time: 21.6365 us. Best GFLOPs: 5972.5633
2023-11-11 11:01:27 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #932: GFLOPs: 5878.4816. Time: 21.6291 us. Best GFLOPs: 5972.5633
2023-11-11 11:01:27 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #933: GFLOPs: 5792.3559. Time: 21.9507 us. Best GFLOPs: 5972.5633
2023-11-11 11:01:27 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #934: GFLOPs: 5807.4035. Time: 21.8938 us. Best GFLOPs: 5972.5633
2023-11-11 11:01:27 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #935: GFLOPs: 5874.6295. Time: 21.6432 us. Best GFLOPs: 5972.5633
2023-11-11 11:01:27 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #936: GFLOPs: 5897.7415. Time: 21.5584 us. Best GFLOPs: 5972.5633
2023-11-11 11:01:27 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #937: GFLOPs: 5749.9761. Time: 22.1124 us. Best GFLOPs: 5972.5633
2023-11-11 11:01:27 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #938: GFLOPs: 5870.1540. Time: 21.6597 us. Best GFLOPs: 5972.5633
2023-11-11 11:01:27 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #939: GFLOPs: 5830.5011. Time: 21.8070 us. Best GFLOPs: 5972.5633
2023-11-11 11:01:27 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #940: GFLOPs: 5874.9579. Time: 21.6420 us. Best GFLOPs: 5972.5633
2023-11-11 11:01:27 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #941: GFLOPs: 5798.6784. Time: 21.9267 us. Best GFLOPs: 5972.5633
2023-11-11 11:01:27 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #942: GFLOPs: 5816.7487. Time: 21.8586 us. Best GFLOPs: 5972.5633
2023-11-11 11:01:27 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #943: GFLOPs: 5842.3464. Time: 21.7628 us. Best GFLOPs: 5972.5633
2023-11-11 11:01:27 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #944: GFLOPs: 5805.9790. Time: 21.8991 us. Best GFLOPs: 5972.5633
2023-11-11 11:01:27 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #945: GFLOPs: 5749.3558. Time: 22.1148 us. Best GFLOPs: 5972.5633
2023-11-11 11:01:27 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #946: GFLOPs: 5896.3940. Time: 21.5633 us. Best GFLOPs: 5972.5633
2023-11-11 11:01:27 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #947: GFLOPs: 5813.3102. Time: 21.8715 us. Best GFLOPs: 5972.5633
2023-11-11 11:01:27 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #948: GFLOPs: 5820.0336. Time: 21.8463 us. Best GFLOPs: 5972.5633
2023-11-11 11:01:27 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #949: GFLOPs: 5864.7459. Time: 21.6797 us. Best GFLOPs: 5972.5633
2023-11-11 11:01:27 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #950: GFLOPs: 5887.5036. Time: 21.5959 us. Best GFLOPs: 5972.5633
2023-11-11 11:01:27 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #951: GFLOPs: 5859.3527. Time: 21.6997 us. Best GFLOPs: 5972.5633
2023-11-11 11:01:27 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #952: GFLOPs: 5864.5935. Time: 21.6803 us. Best GFLOPs: 5972.5633
2023-11-11 11:01:27 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #953: GFLOPs: 5893.4287. Time: 21.5742 us. Best GFLOPs: 5972.5633
2023-11-11 11:01:27 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #954: GFLOPs: 5868.2328. Time: 21.6668 us. Best GFLOPs: 5972.5633
2023-11-11 11:01:27 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #955: GFLOPs: 5578.2867. Time: 22.7930 us. Best GFLOPs: 5972.5633
2023-11-11 11:01:27 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #956: GFLOPs: 5806.9682. Time: 21.8954 us. Best GFLOPs: 5972.5633
2023-11-11 11:01:27 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #957: GFLOPs: 5813.3995. Time: 21.8712 us. Best GFLOPs: 5972.5633
2023-11-11 11:01:27 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #958: GFLOPs: 1786.2264. Time: 71.1813 us. Best GFLOPs: 5972.5633
2023-11-11 11:01:27 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #959: GFLOPs: 606.4775. Time: 209.6467 us. Best GFLOPs: 5972.5633
2023-11-11 11:01:27 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #960: GFLOPs: 1194.8989. Time: 106.4073 us. Best GFLOPs: 5972.5633
2023-11-11 11:16:29 [INFO] [evolutionary_search.cc:713] Generating candidates......
2023-11-11 11:16:32 [INFO] [evolutionary_search.cc:715] Picked top 102 candidate(s) from database
2023-11-11 11:16:39 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310fb046d8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310f554558)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310dc5ddb8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310bbcd2d8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56310b1dbf38)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310dc5d048)]: 401 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310dc26e38)]: 0 failure(s)
2023-11-11 11:16:46 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310fb046d8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310f554558)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310dc5ddb8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310bbcd2d8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56310b1dbf38)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310dc5d048)]: 790 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310dc26e38)]: 0 failure(s)
2023-11-11 11:16:53 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310fb046d8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310f554558)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310dc5ddb8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310bbcd2d8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56310b1dbf38)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310dc5d048)]: 1188 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310dc26e38)]: 0 failure(s)
2023-11-11 11:16:59 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310fb046d8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310f554558)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310dc5ddb8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310bbcd2d8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56310b1dbf38)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310dc5d048)]: 1589 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310dc26e38)]: 0 failure(s)
2023-11-11 11:16:59 [INFO] [evolutionary_search.cc:723] Sampled 51 candidate(s)
2023-11-11 11:17:21 [INFO] [evolutionary_search.cc:621] Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310fb046d8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310f554558)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310dc5ddb8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310bbcd2d8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56310b1dbf38)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310dc5d048)]: 213 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310dc26e38)]: 0 failure(s)
2023-11-11 11:17:45 [INFO] [evolutionary_search.cc:621] Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310fb046d8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310f554558)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310dc5ddb8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310bbcd2d8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56310b1dbf38)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310dc5d048)]: 173 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310dc26e38)]: 0 failure(s)
2023-11-11 11:18:10 [INFO] [evolutionary_search.cc:621] Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310fb046d8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310f554558)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310dc5ddb8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310bbcd2d8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56310b1dbf38)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310dc5d048)]: 206 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310dc26e38)]: 0 failure(s)
2023-11-11 11:18:34 [INFO] [evolutionary_search.cc:621] Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x56310fb046d8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x56310f554558)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x56310dc5ddb8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x56310bbcd2d8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x56310b1dbf38)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x56310dc5d048)]: 173 failure(s)
Postproc #6 [meta_schedule.RewriteTensorize(0x56310dc26e38)]: 0 failure(s)
2023-11-11 11:18:40 [INFO] [evolutionary_search.cc:649] Scores of the best 40 candidates:
[1 : 16]:	0.9986  0.9943  0.9936  0.9932  0.9928  0.9920  0.9916  0.9915  0.9900  0.9889  0.9884  0.9881  0.9880  0.9876  0.9874  0.9873
[17 : 32]:	0.9872  0.9868  0.9867  0.9855  0.9854  0.9853  0.9850  0.9846  0.9844  0.9842  0.9841  0.9839  0.9838  0.9837  0.9837  0.9835
[33 : 40]:	0.9835  0.9833  0.9832  0.9831  0.9827  0.9827  0.9826  0.9825
2023-11-11 11:18:41 [INFO] [evolutionary_search.cc:727] Got 40 candidate(s) with evolutionary search
2023-11-11 11:18:41 [INFO] [evolutionary_search.cc:730] Sending 40 candidates(s) for measurement
2023-11-11 11:19:04 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #961: GFLOPs: 5969.7431. Time: 21.2984 us. Best GFLOPs: 5972.5633
2023-11-11 11:19:04 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #962: GFLOPs: 5891.2971. Time: 21.5820 us. Best GFLOPs: 5972.5633
2023-11-11 11:19:04 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #963: GFLOPs: 5926.4196. Time: 21.4541 us. Best GFLOPs: 5972.5633
2023-11-11 11:19:04 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #964: GFLOPs: 6031.2832. Time: 21.0811 us. Best GFLOPs: 6031.2832
2023-11-11 11:19:04 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #965: GFLOPs: 6032.4428. Time: 21.0770 us. Best GFLOPs: 6032.4428
2023-11-11 11:19:04 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #966: GFLOPs: 5963.1166. Time: 21.3221 us. Best GFLOPs: 6032.4428
2023-11-11 11:19:04 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #967: GFLOPs: 6019.3063. Time: 21.1230 us. Best GFLOPs: 6032.4428
2023-11-11 11:19:04 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #968: GFLOPs: 6018.9001. Time: 21.1245 us. Best GFLOPs: 6032.4428
2023-11-11 11:19:04 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #969: GFLOPs: 5939.9728. Time: 21.4051 us. Best GFLOPs: 6032.4428
2023-11-11 11:19:04 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #970: GFLOPs: 5835.6505. Time: 21.7878 us. Best GFLOPs: 6032.4428
2023-11-11 11:19:04 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #971: GFLOPs: 6011.2721. Time: 21.1513 us. Best GFLOPs: 6032.4428
2023-11-11 11:19:04 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #972: GFLOPs: 5905.3246. Time: 21.5307 us. Best GFLOPs: 6032.4428
2023-11-11 11:19:04 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #973: GFLOPs: 5965.1011. Time: 21.3150 us. Best GFLOPs: 6032.4428
2023-11-11 11:19:04 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #974: GFLOPs: 5969.1036. Time: 21.3007 us. Best GFLOPs: 6032.4428
2023-11-11 11:19:04 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #975: GFLOPs: 5891.8077. Time: 21.5801 us. Best GFLOPs: 6032.4428
2023-11-11 11:19:04 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #976: GFLOPs: 5944.9571. Time: 21.3872 us. Best GFLOPs: 6032.4428
2023-11-11 11:19:04 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #977: GFLOPs: 5946.5347. Time: 21.3815 us. Best GFLOPs: 6032.4428
2023-11-11 11:19:04 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #978: GFLOPs: 6022.6399. Time: 21.1113 us. Best GFLOPs: 6032.4428
2023-11-11 11:19:04 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #979: GFLOPs: 6023.2497. Time: 21.1092 us. Best GFLOPs: 6032.4428
2023-11-11 11:19:04 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #980: GFLOPs: 5961.7412. Time: 21.3270 us. Best GFLOPs: 6032.4428
2023-11-11 11:19:04 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #981: GFLOPs: 5902.4967. Time: 21.5411 us. Best GFLOPs: 6032.4428
2023-11-11 11:19:04 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #982: GFLOPs: 5857.3945. Time: 21.7069 us. Best GFLOPs: 6032.4428
2023-11-11 11:19:04 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #983: GFLOPs: 5995.2640. Time: 21.2077 us. Best GFLOPs: 6032.4428
2023-11-11 11:19:04 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #984: GFLOPs: 5968.2249. Time: 21.3038 us. Best GFLOPs: 6032.4428
2023-11-11 11:19:04 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #985: GFLOPs: 5904.8816. Time: 21.5324 us. Best GFLOPs: 6032.4428
2023-11-11 11:19:04 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #986: GFLOPs: 5964.4530. Time: 21.3173 us. Best GFLOPs: 6032.4428
2023-11-11 11:19:04 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #987: GFLOPs: 6021.2165. Time: 21.1163 us. Best GFLOPs: 6032.4428
2023-11-11 11:19:04 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #988: GFLOPs: 5963.9140. Time: 21.3192 us. Best GFLOPs: 6032.4428
2023-11-11 11:19:04 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #989: GFLOPs: 5831.9634. Time: 21.8016 us. Best GFLOPs: 6032.4428
2023-11-11 11:19:04 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #990: GFLOPs: 5903.9644. Time: 21.5357 us. Best GFLOPs: 6032.4428
2023-11-11 11:19:04 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #991: GFLOPs: 5906.2665. Time: 21.5273 us. Best GFLOPs: 6032.4428
2023-11-11 11:19:04 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #992: GFLOPs: 5876.0993. Time: 21.6378 us. Best GFLOPs: 6032.4428
2023-11-11 11:19:04 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #993: GFLOPs: 5973.3421. Time: 21.2856 us. Best GFLOPs: 6032.4428
2023-11-11 11:19:04 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #994: GFLOPs: 6022.5174. Time: 21.1118 us. Best GFLOPs: 6032.4428
2023-11-11 11:19:04 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #995: GFLOPs: 5965.0197. Time: 21.3153 us. Best GFLOPs: 6032.4428
2023-11-11 11:19:04 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #996: GFLOPs: 5889.1616. Time: 21.5898 us. Best GFLOPs: 6032.4428
2023-11-11 11:19:04 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #997: GFLOPs: 5921.9139. Time: 21.4704 us. Best GFLOPs: 6032.4428
2023-11-11 11:19:04 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #998: GFLOPs: 5991.5455. Time: 21.2209 us. Best GFLOPs: 6032.4428
2023-11-11 11:19:04 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #999: GFLOPs: 2278.3123. Time: 55.8071 us. Best GFLOPs: 6032.4428
2023-11-11 11:19:04 [INFO] [task_scheduler.cc:131] [Task #15: fused_nn_contrib_conv2d_winograd_without_weight_transform_add_add_nn_relu_1] Trial #1000: GFLOPs: 14.9293. Time: 8516.5227 us. Best GFLOPs: 6032.4428
